I0509 15:25:02.022431      21 e2e.go:126] Starting e2e run "f3f539e6-417f-4a68-8004-5f74a75285d0" on Ginkgo node 1
May  9 15:25:02.041: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1683645901 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
May  9 15:25:02.154: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 15:25:02.156: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0509 15:25:02.158332      21 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
May  9 15:25:02.194: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May  9 15:25:02.234: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May  9 15:25:02.234: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
May  9 15:25:02.234: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May  9 15:25:02.241: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
May  9 15:25:02.241: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May  9 15:25:02.241: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'wormhole' (0 seconds elapsed)
May  9 15:25:02.241: INFO: e2e test version: v1.26.4
May  9 15:25:02.243: INFO: kube-apiserver version: v1.26.4
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
May  9 15:25:02.243: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 15:25:02.264: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.110 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    May  9 15:25:02.154: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 15:25:02.156: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0509 15:25:02.158332      21 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    May  9 15:25:02.194: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    May  9 15:25:02.234: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    May  9 15:25:02.234: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
    May  9 15:25:02.234: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    May  9 15:25:02.241: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
    May  9 15:25:02.241: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    May  9 15:25:02.241: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'wormhole' (0 seconds elapsed)
    May  9 15:25:02.241: INFO: e2e test version: v1.26.4
    May  9 15:25:02.243: INFO: kube-apiserver version: v1.26.4
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    May  9 15:25:02.243: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 15:25:02.264: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:25:02.286
May  9 15:25:02.286: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename resourcequota 05/09/23 15:25:02.287
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:02.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:02.308
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 05/09/23 15:25:02.312
STEP: Creating a ResourceQuota 05/09/23 15:25:07.319
STEP: Ensuring resource quota status is calculated 05/09/23 15:25:07.327
STEP: Creating a Pod that fits quota 05/09/23 15:25:09.337
STEP: Ensuring ResourceQuota status captures the pod usage 05/09/23 15:25:09.356
STEP: Not allowing a pod to be created that exceeds remaining quota 05/09/23 15:25:11.363
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 05/09/23 15:25:11.366
STEP: Ensuring a pod cannot update its resource requirements 05/09/23 15:25:11.369
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 05/09/23 15:25:11.379
STEP: Deleting the pod 05/09/23 15:25:13.384
STEP: Ensuring resource quota status released the pod usage 05/09/23 15:25:13.404
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  9 15:25:15.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1357" for this suite. 05/09/23 15:25:15.42
------------------------------
â€¢ [SLOW TEST] [13.143 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:25:02.286
    May  9 15:25:02.286: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename resourcequota 05/09/23 15:25:02.287
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:02.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:02.308
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 05/09/23 15:25:02.312
    STEP: Creating a ResourceQuota 05/09/23 15:25:07.319
    STEP: Ensuring resource quota status is calculated 05/09/23 15:25:07.327
    STEP: Creating a Pod that fits quota 05/09/23 15:25:09.337
    STEP: Ensuring ResourceQuota status captures the pod usage 05/09/23 15:25:09.356
    STEP: Not allowing a pod to be created that exceeds remaining quota 05/09/23 15:25:11.363
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 05/09/23 15:25:11.366
    STEP: Ensuring a pod cannot update its resource requirements 05/09/23 15:25:11.369
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 05/09/23 15:25:11.379
    STEP: Deleting the pod 05/09/23 15:25:13.384
    STEP: Ensuring resource quota status released the pod usage 05/09/23 15:25:13.404
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  9 15:25:15.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1357" for this suite. 05/09/23 15:25:15.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:25:15.43
May  9 15:25:15.430: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubectl 05/09/23 15:25:15.431
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:15.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:15.457
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 05/09/23 15:25:15.462
May  9 15:25:15.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-9699 cluster-info'
May  9 15:25:15.565: INFO: stderr: ""
May  9 15:25:15.565: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.3.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  9 15:25:15.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9699" for this suite. 05/09/23 15:25:15.571
------------------------------
â€¢ [0.149 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:25:15.43
    May  9 15:25:15.430: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubectl 05/09/23 15:25:15.431
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:15.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:15.457
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 05/09/23 15:25:15.462
    May  9 15:25:15.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-9699 cluster-info'
    May  9 15:25:15.565: INFO: stderr: ""
    May  9 15:25:15.565: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.3.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  9 15:25:15.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9699" for this suite. 05/09/23 15:25:15.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:25:15.579
May  9 15:25:15.580: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubectl 05/09/23 15:25:15.581
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:15.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:15.599
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 05/09/23 15:25:15.603
May  9 15:25:15.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6519 api-versions'
May  9 15:25:15.702: INFO: stderr: ""
May  9 15:25:15.703: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nkube.cloud.ovh.com/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  9 15:25:15.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6519" for this suite. 05/09/23 15:25:15.709
------------------------------
â€¢ [0.139 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:25:15.579
    May  9 15:25:15.580: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubectl 05/09/23 15:25:15.581
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:15.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:15.599
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 05/09/23 15:25:15.603
    May  9 15:25:15.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6519 api-versions'
    May  9 15:25:15.702: INFO: stderr: ""
    May  9 15:25:15.703: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nkube.cloud.ovh.com/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  9 15:25:15.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6519" for this suite. 05/09/23 15:25:15.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:25:15.72
May  9 15:25:15.720: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename pods 05/09/23 15:25:15.721
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:15.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:15.788
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 05/09/23 15:25:15.792
STEP: submitting the pod to kubernetes 05/09/23 15:25:15.792
STEP: verifying QOS class is set on the pod 05/09/23 15:25:15.802
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
May  9 15:25:15.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6659" for this suite. 05/09/23 15:25:15.814
------------------------------
â€¢ [0.102 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:25:15.72
    May  9 15:25:15.720: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename pods 05/09/23 15:25:15.721
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:15.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:15.788
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 05/09/23 15:25:15.792
    STEP: submitting the pod to kubernetes 05/09/23 15:25:15.792
    STEP: verifying QOS class is set on the pod 05/09/23 15:25:15.802
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    May  9 15:25:15.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6659" for this suite. 05/09/23 15:25:15.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:25:15.823
May  9 15:25:15.823: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename webhook 05/09/23 15:25:15.824
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:15.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:15.842
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/09/23 15:25:15.867
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 15:25:16.084
STEP: Deploying the webhook pod 05/09/23 15:25:16.095
STEP: Wait for the deployment to be ready 05/09/23 15:25:16.112
May  9 15:25:16.138: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/09/23 15:25:18.158
STEP: Verifying the service has paired with the endpoint 05/09/23 15:25:18.174
May  9 15:25:19.174: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 05/09/23 15:25:19.506
STEP: Creating a configMap that should be mutated 05/09/23 15:25:19.528
STEP: Deleting the collection of validation webhooks 05/09/23 15:25:19.59
STEP: Creating a configMap that should not be mutated 05/09/23 15:25:19.663
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 15:25:19.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6466" for this suite. 05/09/23 15:25:19.745
STEP: Destroying namespace "webhook-6466-markers" for this suite. 05/09/23 15:25:19.756
------------------------------
â€¢ [3.944 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:25:15.823
    May  9 15:25:15.823: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename webhook 05/09/23 15:25:15.824
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:15.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:15.842
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/09/23 15:25:15.867
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 15:25:16.084
    STEP: Deploying the webhook pod 05/09/23 15:25:16.095
    STEP: Wait for the deployment to be ready 05/09/23 15:25:16.112
    May  9 15:25:16.138: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/09/23 15:25:18.158
    STEP: Verifying the service has paired with the endpoint 05/09/23 15:25:18.174
    May  9 15:25:19.174: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 05/09/23 15:25:19.506
    STEP: Creating a configMap that should be mutated 05/09/23 15:25:19.528
    STEP: Deleting the collection of validation webhooks 05/09/23 15:25:19.59
    STEP: Creating a configMap that should not be mutated 05/09/23 15:25:19.663
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 15:25:19.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6466" for this suite. 05/09/23 15:25:19.745
    STEP: Destroying namespace "webhook-6466-markers" for this suite. 05/09/23 15:25:19.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:25:19.768
May  9 15:25:19.768: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 15:25:19.769
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:19.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:19.789
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 05/09/23 15:25:19.793
May  9 15:25:19.804: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b571b12f-3b8a-47ba-8a69-bb2db5e0c7c7" in namespace "projected-4031" to be "Succeeded or Failed"
May  9 15:25:19.809: INFO: Pod "downwardapi-volume-b571b12f-3b8a-47ba-8a69-bb2db5e0c7c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.713619ms
May  9 15:25:21.836: INFO: Pod "downwardapi-volume-b571b12f-3b8a-47ba-8a69-bb2db5e0c7c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032290961s
May  9 15:25:23.816: INFO: Pod "downwardapi-volume-b571b12f-3b8a-47ba-8a69-bb2db5e0c7c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012659653s
STEP: Saw pod success 05/09/23 15:25:23.816
May  9 15:25:23.817: INFO: Pod "downwardapi-volume-b571b12f-3b8a-47ba-8a69-bb2db5e0c7c7" satisfied condition "Succeeded or Failed"
May  9 15:25:23.821: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-b571b12f-3b8a-47ba-8a69-bb2db5e0c7c7 container client-container: <nil>
STEP: delete the pod 05/09/23 15:25:23.879
May  9 15:25:23.897: INFO: Waiting for pod downwardapi-volume-b571b12f-3b8a-47ba-8a69-bb2db5e0c7c7 to disappear
May  9 15:25:23.904: INFO: Pod downwardapi-volume-b571b12f-3b8a-47ba-8a69-bb2db5e0c7c7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  9 15:25:23.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4031" for this suite. 05/09/23 15:25:23.911
------------------------------
â€¢ [4.152 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:25:19.768
    May  9 15:25:19.768: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 15:25:19.769
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:19.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:19.789
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 05/09/23 15:25:19.793
    May  9 15:25:19.804: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b571b12f-3b8a-47ba-8a69-bb2db5e0c7c7" in namespace "projected-4031" to be "Succeeded or Failed"
    May  9 15:25:19.809: INFO: Pod "downwardapi-volume-b571b12f-3b8a-47ba-8a69-bb2db5e0c7c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.713619ms
    May  9 15:25:21.836: INFO: Pod "downwardapi-volume-b571b12f-3b8a-47ba-8a69-bb2db5e0c7c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032290961s
    May  9 15:25:23.816: INFO: Pod "downwardapi-volume-b571b12f-3b8a-47ba-8a69-bb2db5e0c7c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012659653s
    STEP: Saw pod success 05/09/23 15:25:23.816
    May  9 15:25:23.817: INFO: Pod "downwardapi-volume-b571b12f-3b8a-47ba-8a69-bb2db5e0c7c7" satisfied condition "Succeeded or Failed"
    May  9 15:25:23.821: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-b571b12f-3b8a-47ba-8a69-bb2db5e0c7c7 container client-container: <nil>
    STEP: delete the pod 05/09/23 15:25:23.879
    May  9 15:25:23.897: INFO: Waiting for pod downwardapi-volume-b571b12f-3b8a-47ba-8a69-bb2db5e0c7c7 to disappear
    May  9 15:25:23.904: INFO: Pod downwardapi-volume-b571b12f-3b8a-47ba-8a69-bb2db5e0c7c7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  9 15:25:23.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4031" for this suite. 05/09/23 15:25:23.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:25:23.923
May  9 15:25:23.923: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename containers 05/09/23 15:25:23.924
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:23.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:23.944
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
May  9 15:25:23.960: INFO: Waiting up to 5m0s for pod "client-containers-9ac13d79-063c-4110-b60f-c5c4adcee884" in namespace "containers-7481" to be "running"
May  9 15:25:23.964: INFO: Pod "client-containers-9ac13d79-063c-4110-b60f-c5c4adcee884": Phase="Pending", Reason="", readiness=false. Elapsed: 3.930228ms
May  9 15:25:25.971: INFO: Pod "client-containers-9ac13d79-063c-4110-b60f-c5c4adcee884": Phase="Running", Reason="", readiness=true. Elapsed: 2.011407843s
May  9 15:25:25.971: INFO: Pod "client-containers-9ac13d79-063c-4110-b60f-c5c4adcee884" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
May  9 15:25:25.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-7481" for this suite. 05/09/23 15:25:25.99
------------------------------
â€¢ [2.077 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:25:23.923
    May  9 15:25:23.923: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename containers 05/09/23 15:25:23.924
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:23.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:23.944
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    May  9 15:25:23.960: INFO: Waiting up to 5m0s for pod "client-containers-9ac13d79-063c-4110-b60f-c5c4adcee884" in namespace "containers-7481" to be "running"
    May  9 15:25:23.964: INFO: Pod "client-containers-9ac13d79-063c-4110-b60f-c5c4adcee884": Phase="Pending", Reason="", readiness=false. Elapsed: 3.930228ms
    May  9 15:25:25.971: INFO: Pod "client-containers-9ac13d79-063c-4110-b60f-c5c4adcee884": Phase="Running", Reason="", readiness=true. Elapsed: 2.011407843s
    May  9 15:25:25.971: INFO: Pod "client-containers-9ac13d79-063c-4110-b60f-c5c4adcee884" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    May  9 15:25:25.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-7481" for this suite. 05/09/23 15:25:25.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:25:26.002
May  9 15:25:26.002: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename configmap 05/09/23 15:25:26.003
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:26.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:26.037
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-0d2e76b3-b323-4784-9ad2-c6d7207bfa40 05/09/23 15:25:26.047
STEP: Creating a pod to test consume configMaps 05/09/23 15:25:26.053
May  9 15:25:26.063: INFO: Waiting up to 5m0s for pod "pod-configmaps-2d5dfa72-2335-4145-b188-b51169567877" in namespace "configmap-5134" to be "Succeeded or Failed"
May  9 15:25:26.068: INFO: Pod "pod-configmaps-2d5dfa72-2335-4145-b188-b51169567877": Phase="Pending", Reason="", readiness=false. Elapsed: 5.433085ms
May  9 15:25:28.075: INFO: Pod "pod-configmaps-2d5dfa72-2335-4145-b188-b51169567877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012072187s
May  9 15:25:30.076: INFO: Pod "pod-configmaps-2d5dfa72-2335-4145-b188-b51169567877": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012883942s
May  9 15:25:32.076: INFO: Pod "pod-configmaps-2d5dfa72-2335-4145-b188-b51169567877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013036361s
STEP: Saw pod success 05/09/23 15:25:32.076
May  9 15:25:32.076: INFO: Pod "pod-configmaps-2d5dfa72-2335-4145-b188-b51169567877" satisfied condition "Succeeded or Failed"
May  9 15:25:32.081: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 pod pod-configmaps-2d5dfa72-2335-4145-b188-b51169567877 container configmap-volume-test: <nil>
STEP: delete the pod 05/09/23 15:25:32.129
May  9 15:25:32.152: INFO: Waiting for pod pod-configmaps-2d5dfa72-2335-4145-b188-b51169567877 to disappear
May  9 15:25:32.160: INFO: Pod pod-configmaps-2d5dfa72-2335-4145-b188-b51169567877 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  9 15:25:32.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5134" for this suite. 05/09/23 15:25:32.166
------------------------------
â€¢ [SLOW TEST] [6.172 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:25:26.002
    May  9 15:25:26.002: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename configmap 05/09/23 15:25:26.003
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:26.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:26.037
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-0d2e76b3-b323-4784-9ad2-c6d7207bfa40 05/09/23 15:25:26.047
    STEP: Creating a pod to test consume configMaps 05/09/23 15:25:26.053
    May  9 15:25:26.063: INFO: Waiting up to 5m0s for pod "pod-configmaps-2d5dfa72-2335-4145-b188-b51169567877" in namespace "configmap-5134" to be "Succeeded or Failed"
    May  9 15:25:26.068: INFO: Pod "pod-configmaps-2d5dfa72-2335-4145-b188-b51169567877": Phase="Pending", Reason="", readiness=false. Elapsed: 5.433085ms
    May  9 15:25:28.075: INFO: Pod "pod-configmaps-2d5dfa72-2335-4145-b188-b51169567877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012072187s
    May  9 15:25:30.076: INFO: Pod "pod-configmaps-2d5dfa72-2335-4145-b188-b51169567877": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012883942s
    May  9 15:25:32.076: INFO: Pod "pod-configmaps-2d5dfa72-2335-4145-b188-b51169567877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013036361s
    STEP: Saw pod success 05/09/23 15:25:32.076
    May  9 15:25:32.076: INFO: Pod "pod-configmaps-2d5dfa72-2335-4145-b188-b51169567877" satisfied condition "Succeeded or Failed"
    May  9 15:25:32.081: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 pod pod-configmaps-2d5dfa72-2335-4145-b188-b51169567877 container configmap-volume-test: <nil>
    STEP: delete the pod 05/09/23 15:25:32.129
    May  9 15:25:32.152: INFO: Waiting for pod pod-configmaps-2d5dfa72-2335-4145-b188-b51169567877 to disappear
    May  9 15:25:32.160: INFO: Pod pod-configmaps-2d5dfa72-2335-4145-b188-b51169567877 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  9 15:25:32.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5134" for this suite. 05/09/23 15:25:32.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:25:32.177
May  9 15:25:32.177: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename dns 05/09/23 15:25:32.178
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:32.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:32.199
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 05/09/23 15:25:32.203
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4267.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4267.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 05/09/23 15:25:32.21
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4267.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4267.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 05/09/23 15:25:32.21
STEP: creating a pod to probe DNS 05/09/23 15:25:32.21
STEP: submitting the pod to kubernetes 05/09/23 15:25:32.21
May  9 15:25:32.222: INFO: Waiting up to 15m0s for pod "dns-test-bf4cf479-58a4-4ebc-abad-c79e7fe56fc7" in namespace "dns-4267" to be "running"
May  9 15:25:32.228: INFO: Pod "dns-test-bf4cf479-58a4-4ebc-abad-c79e7fe56fc7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.249847ms
May  9 15:25:34.235: INFO: Pod "dns-test-bf4cf479-58a4-4ebc-abad-c79e7fe56fc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012759794s
May  9 15:25:36.235: INFO: Pod "dns-test-bf4cf479-58a4-4ebc-abad-c79e7fe56fc7": Phase="Running", Reason="", readiness=true. Elapsed: 4.012355173s
May  9 15:25:36.235: INFO: Pod "dns-test-bf4cf479-58a4-4ebc-abad-c79e7fe56fc7" satisfied condition "running"
STEP: retrieving the pod 05/09/23 15:25:36.235
STEP: looking for the results for each expected name from probers 05/09/23 15:25:36.24
May  9 15:25:36.279: INFO: DNS probes using dns-4267/dns-test-bf4cf479-58a4-4ebc-abad-c79e7fe56fc7 succeeded

STEP: deleting the pod 05/09/23 15:25:36.279
STEP: deleting the test headless service 05/09/23 15:25:36.294
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May  9 15:25:36.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4267" for this suite. 05/09/23 15:25:36.331
------------------------------
â€¢ [4.162 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:25:32.177
    May  9 15:25:32.177: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename dns 05/09/23 15:25:32.178
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:32.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:32.199
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 05/09/23 15:25:32.203
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4267.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4267.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     05/09/23 15:25:32.21
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4267.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4267.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     05/09/23 15:25:32.21
    STEP: creating a pod to probe DNS 05/09/23 15:25:32.21
    STEP: submitting the pod to kubernetes 05/09/23 15:25:32.21
    May  9 15:25:32.222: INFO: Waiting up to 15m0s for pod "dns-test-bf4cf479-58a4-4ebc-abad-c79e7fe56fc7" in namespace "dns-4267" to be "running"
    May  9 15:25:32.228: INFO: Pod "dns-test-bf4cf479-58a4-4ebc-abad-c79e7fe56fc7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.249847ms
    May  9 15:25:34.235: INFO: Pod "dns-test-bf4cf479-58a4-4ebc-abad-c79e7fe56fc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012759794s
    May  9 15:25:36.235: INFO: Pod "dns-test-bf4cf479-58a4-4ebc-abad-c79e7fe56fc7": Phase="Running", Reason="", readiness=true. Elapsed: 4.012355173s
    May  9 15:25:36.235: INFO: Pod "dns-test-bf4cf479-58a4-4ebc-abad-c79e7fe56fc7" satisfied condition "running"
    STEP: retrieving the pod 05/09/23 15:25:36.235
    STEP: looking for the results for each expected name from probers 05/09/23 15:25:36.24
    May  9 15:25:36.279: INFO: DNS probes using dns-4267/dns-test-bf4cf479-58a4-4ebc-abad-c79e7fe56fc7 succeeded

    STEP: deleting the pod 05/09/23 15:25:36.279
    STEP: deleting the test headless service 05/09/23 15:25:36.294
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May  9 15:25:36.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4267" for this suite. 05/09/23 15:25:36.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:25:36.34
May  9 15:25:36.340: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 15:25:36.34
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:36.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:36.366
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 05/09/23 15:25:36.37
May  9 15:25:36.379: INFO: Waiting up to 5m0s for pod "downwardapi-volume-50bfcea6-4b25-4cb1-99bc-4041e0a9212e" in namespace "projected-5870" to be "Succeeded or Failed"
May  9 15:25:36.386: INFO: Pod "downwardapi-volume-50bfcea6-4b25-4cb1-99bc-4041e0a9212e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.225219ms
May  9 15:25:38.391: INFO: Pod "downwardapi-volume-50bfcea6-4b25-4cb1-99bc-4041e0a9212e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012384951s
May  9 15:25:40.393: INFO: Pod "downwardapi-volume-50bfcea6-4b25-4cb1-99bc-4041e0a9212e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014187864s
May  9 15:25:42.394: INFO: Pod "downwardapi-volume-50bfcea6-4b25-4cb1-99bc-4041e0a9212e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014792668s
STEP: Saw pod success 05/09/23 15:25:42.394
May  9 15:25:42.394: INFO: Pod "downwardapi-volume-50bfcea6-4b25-4cb1-99bc-4041e0a9212e" satisfied condition "Succeeded or Failed"
May  9 15:25:42.401: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-50bfcea6-4b25-4cb1-99bc-4041e0a9212e container client-container: <nil>
STEP: delete the pod 05/09/23 15:25:42.413
May  9 15:25:42.430: INFO: Waiting for pod downwardapi-volume-50bfcea6-4b25-4cb1-99bc-4041e0a9212e to disappear
May  9 15:25:42.436: INFO: Pod downwardapi-volume-50bfcea6-4b25-4cb1-99bc-4041e0a9212e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  9 15:25:42.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5870" for this suite. 05/09/23 15:25:42.444
------------------------------
â€¢ [SLOW TEST] [6.112 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:25:36.34
    May  9 15:25:36.340: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 15:25:36.34
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:36.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:36.366
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 05/09/23 15:25:36.37
    May  9 15:25:36.379: INFO: Waiting up to 5m0s for pod "downwardapi-volume-50bfcea6-4b25-4cb1-99bc-4041e0a9212e" in namespace "projected-5870" to be "Succeeded or Failed"
    May  9 15:25:36.386: INFO: Pod "downwardapi-volume-50bfcea6-4b25-4cb1-99bc-4041e0a9212e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.225219ms
    May  9 15:25:38.391: INFO: Pod "downwardapi-volume-50bfcea6-4b25-4cb1-99bc-4041e0a9212e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012384951s
    May  9 15:25:40.393: INFO: Pod "downwardapi-volume-50bfcea6-4b25-4cb1-99bc-4041e0a9212e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014187864s
    May  9 15:25:42.394: INFO: Pod "downwardapi-volume-50bfcea6-4b25-4cb1-99bc-4041e0a9212e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014792668s
    STEP: Saw pod success 05/09/23 15:25:42.394
    May  9 15:25:42.394: INFO: Pod "downwardapi-volume-50bfcea6-4b25-4cb1-99bc-4041e0a9212e" satisfied condition "Succeeded or Failed"
    May  9 15:25:42.401: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-50bfcea6-4b25-4cb1-99bc-4041e0a9212e container client-container: <nil>
    STEP: delete the pod 05/09/23 15:25:42.413
    May  9 15:25:42.430: INFO: Waiting for pod downwardapi-volume-50bfcea6-4b25-4cb1-99bc-4041e0a9212e to disappear
    May  9 15:25:42.436: INFO: Pod downwardapi-volume-50bfcea6-4b25-4cb1-99bc-4041e0a9212e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  9 15:25:42.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5870" for this suite. 05/09/23 15:25:42.444
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:25:42.452
May  9 15:25:42.453: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename containers 05/09/23 15:25:42.453
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:42.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:42.472
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 05/09/23 15:25:42.476
May  9 15:25:42.486: INFO: Waiting up to 5m0s for pod "client-containers-0ce6f413-0744-4e70-a721-5f475eaad9b1" in namespace "containers-5018" to be "Succeeded or Failed"
May  9 15:25:42.492: INFO: Pod "client-containers-0ce6f413-0744-4e70-a721-5f475eaad9b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.71689ms
May  9 15:25:44.500: INFO: Pod "client-containers-0ce6f413-0744-4e70-a721-5f475eaad9b1": Phase="Running", Reason="", readiness=false. Elapsed: 2.014802154s
May  9 15:25:46.499: INFO: Pod "client-containers-0ce6f413-0744-4e70-a721-5f475eaad9b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012884265s
STEP: Saw pod success 05/09/23 15:25:46.499
May  9 15:25:46.499: INFO: Pod "client-containers-0ce6f413-0744-4e70-a721-5f475eaad9b1" satisfied condition "Succeeded or Failed"
May  9 15:25:46.504: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod client-containers-0ce6f413-0744-4e70-a721-5f475eaad9b1 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 15:25:46.518
May  9 15:25:46.533: INFO: Waiting for pod client-containers-0ce6f413-0744-4e70-a721-5f475eaad9b1 to disappear
May  9 15:25:46.538: INFO: Pod client-containers-0ce6f413-0744-4e70-a721-5f475eaad9b1 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
May  9 15:25:46.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-5018" for this suite. 05/09/23 15:25:46.544
------------------------------
â€¢ [4.101 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:25:42.452
    May  9 15:25:42.453: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename containers 05/09/23 15:25:42.453
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:42.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:42.472
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 05/09/23 15:25:42.476
    May  9 15:25:42.486: INFO: Waiting up to 5m0s for pod "client-containers-0ce6f413-0744-4e70-a721-5f475eaad9b1" in namespace "containers-5018" to be "Succeeded or Failed"
    May  9 15:25:42.492: INFO: Pod "client-containers-0ce6f413-0744-4e70-a721-5f475eaad9b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.71689ms
    May  9 15:25:44.500: INFO: Pod "client-containers-0ce6f413-0744-4e70-a721-5f475eaad9b1": Phase="Running", Reason="", readiness=false. Elapsed: 2.014802154s
    May  9 15:25:46.499: INFO: Pod "client-containers-0ce6f413-0744-4e70-a721-5f475eaad9b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012884265s
    STEP: Saw pod success 05/09/23 15:25:46.499
    May  9 15:25:46.499: INFO: Pod "client-containers-0ce6f413-0744-4e70-a721-5f475eaad9b1" satisfied condition "Succeeded or Failed"
    May  9 15:25:46.504: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod client-containers-0ce6f413-0744-4e70-a721-5f475eaad9b1 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 15:25:46.518
    May  9 15:25:46.533: INFO: Waiting for pod client-containers-0ce6f413-0744-4e70-a721-5f475eaad9b1 to disappear
    May  9 15:25:46.538: INFO: Pod client-containers-0ce6f413-0744-4e70-a721-5f475eaad9b1 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    May  9 15:25:46.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-5018" for this suite. 05/09/23 15:25:46.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:25:46.554
May  9 15:25:46.555: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename container-probe 05/09/23 15:25:46.555
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:46.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:46.579
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-eef7eff2-e6e1-4581-941b-cbe83a8db664 in namespace container-probe-8086 05/09/23 15:25:46.584
May  9 15:25:46.595: INFO: Waiting up to 5m0s for pod "busybox-eef7eff2-e6e1-4581-941b-cbe83a8db664" in namespace "container-probe-8086" to be "not pending"
May  9 15:25:46.599: INFO: Pod "busybox-eef7eff2-e6e1-4581-941b-cbe83a8db664": Phase="Pending", Reason="", readiness=false. Elapsed: 3.890137ms
May  9 15:25:48.605: INFO: Pod "busybox-eef7eff2-e6e1-4581-941b-cbe83a8db664": Phase="Running", Reason="", readiness=true. Elapsed: 2.009357428s
May  9 15:25:48.605: INFO: Pod "busybox-eef7eff2-e6e1-4581-941b-cbe83a8db664" satisfied condition "not pending"
May  9 15:25:48.605: INFO: Started pod busybox-eef7eff2-e6e1-4581-941b-cbe83a8db664 in namespace container-probe-8086
STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 15:25:48.605
May  9 15:25:48.610: INFO: Initial restart count of pod busybox-eef7eff2-e6e1-4581-941b-cbe83a8db664 is 0
STEP: deleting the pod 05/09/23 15:29:49.637
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May  9 15:29:49.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8086" for this suite. 05/09/23 15:29:49.663
------------------------------
â€¢ [SLOW TEST] [243.116 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:25:46.554
    May  9 15:25:46.555: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename container-probe 05/09/23 15:25:46.555
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:25:46.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:25:46.579
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-eef7eff2-e6e1-4581-941b-cbe83a8db664 in namespace container-probe-8086 05/09/23 15:25:46.584
    May  9 15:25:46.595: INFO: Waiting up to 5m0s for pod "busybox-eef7eff2-e6e1-4581-941b-cbe83a8db664" in namespace "container-probe-8086" to be "not pending"
    May  9 15:25:46.599: INFO: Pod "busybox-eef7eff2-e6e1-4581-941b-cbe83a8db664": Phase="Pending", Reason="", readiness=false. Elapsed: 3.890137ms
    May  9 15:25:48.605: INFO: Pod "busybox-eef7eff2-e6e1-4581-941b-cbe83a8db664": Phase="Running", Reason="", readiness=true. Elapsed: 2.009357428s
    May  9 15:25:48.605: INFO: Pod "busybox-eef7eff2-e6e1-4581-941b-cbe83a8db664" satisfied condition "not pending"
    May  9 15:25:48.605: INFO: Started pod busybox-eef7eff2-e6e1-4581-941b-cbe83a8db664 in namespace container-probe-8086
    STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 15:25:48.605
    May  9 15:25:48.610: INFO: Initial restart count of pod busybox-eef7eff2-e6e1-4581-941b-cbe83a8db664 is 0
    STEP: deleting the pod 05/09/23 15:29:49.637
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May  9 15:29:49.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8086" for this suite. 05/09/23 15:29:49.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:29:49.672
May  9 15:29:49.672: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubectl 05/09/23 15:29:49.673
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:29:49.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:29:49.694
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 05/09/23 15:29:49.698
May  9 15:29:49.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3146 create -f -'
May  9 15:29:49.955: INFO: stderr: ""
May  9 15:29:49.955: INFO: stdout: "pod/pause created\n"
May  9 15:29:49.955: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May  9 15:29:49.955: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3146" to be "running and ready"
May  9 15:29:49.961: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.698034ms
May  9 15:29:49.961: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'nodepool-8cc7f47e-9b0c-4801-88-node-7ad816' to be 'Running' but was 'Pending'
May  9 15:29:51.967: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.011633958s
May  9 15:29:51.967: INFO: Pod "pause" satisfied condition "running and ready"
May  9 15:29:51.967: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 05/09/23 15:29:51.967
May  9 15:29:51.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3146 label pods pause testing-label=testing-label-value'
May  9 15:29:52.065: INFO: stderr: ""
May  9 15:29:52.065: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 05/09/23 15:29:52.065
May  9 15:29:52.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3146 get pod pause -L testing-label'
May  9 15:29:52.150: INFO: stderr: ""
May  9 15:29:52.150: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod 05/09/23 15:29:52.15
May  9 15:29:52.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3146 label pods pause testing-label-'
May  9 15:29:52.247: INFO: stderr: ""
May  9 15:29:52.247: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 05/09/23 15:29:52.247
May  9 15:29:52.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3146 get pod pause -L testing-label'
May  9 15:29:52.331: INFO: stderr: ""
May  9 15:29:52.331: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 05/09/23 15:29:52.331
May  9 15:29:52.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3146 delete --grace-period=0 --force -f -'
May  9 15:29:52.428: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  9 15:29:52.428: INFO: stdout: "pod \"pause\" force deleted\n"
May  9 15:29:52.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3146 get rc,svc -l name=pause --no-headers'
May  9 15:29:52.520: INFO: stderr: "No resources found in kubectl-3146 namespace.\n"
May  9 15:29:52.520: INFO: stdout: ""
May  9 15:29:52.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3146 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  9 15:29:52.601: INFO: stderr: ""
May  9 15:29:52.601: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  9 15:29:52.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3146" for this suite. 05/09/23 15:29:52.608
------------------------------
â€¢ [2.943 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:29:49.672
    May  9 15:29:49.672: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubectl 05/09/23 15:29:49.673
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:29:49.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:29:49.694
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 05/09/23 15:29:49.698
    May  9 15:29:49.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3146 create -f -'
    May  9 15:29:49.955: INFO: stderr: ""
    May  9 15:29:49.955: INFO: stdout: "pod/pause created\n"
    May  9 15:29:49.955: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    May  9 15:29:49.955: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3146" to be "running and ready"
    May  9 15:29:49.961: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.698034ms
    May  9 15:29:49.961: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'nodepool-8cc7f47e-9b0c-4801-88-node-7ad816' to be 'Running' but was 'Pending'
    May  9 15:29:51.967: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.011633958s
    May  9 15:29:51.967: INFO: Pod "pause" satisfied condition "running and ready"
    May  9 15:29:51.967: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 05/09/23 15:29:51.967
    May  9 15:29:51.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3146 label pods pause testing-label=testing-label-value'
    May  9 15:29:52.065: INFO: stderr: ""
    May  9 15:29:52.065: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 05/09/23 15:29:52.065
    May  9 15:29:52.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3146 get pod pause -L testing-label'
    May  9 15:29:52.150: INFO: stderr: ""
    May  9 15:29:52.150: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 05/09/23 15:29:52.15
    May  9 15:29:52.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3146 label pods pause testing-label-'
    May  9 15:29:52.247: INFO: stderr: ""
    May  9 15:29:52.247: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 05/09/23 15:29:52.247
    May  9 15:29:52.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3146 get pod pause -L testing-label'
    May  9 15:29:52.331: INFO: stderr: ""
    May  9 15:29:52.331: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 05/09/23 15:29:52.331
    May  9 15:29:52.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3146 delete --grace-period=0 --force -f -'
    May  9 15:29:52.428: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  9 15:29:52.428: INFO: stdout: "pod \"pause\" force deleted\n"
    May  9 15:29:52.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3146 get rc,svc -l name=pause --no-headers'
    May  9 15:29:52.520: INFO: stderr: "No resources found in kubectl-3146 namespace.\n"
    May  9 15:29:52.520: INFO: stdout: ""
    May  9 15:29:52.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3146 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May  9 15:29:52.601: INFO: stderr: ""
    May  9 15:29:52.601: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  9 15:29:52.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3146" for this suite. 05/09/23 15:29:52.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:29:52.616
May  9 15:29:52.616: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename webhook 05/09/23 15:29:52.617
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:29:52.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:29:52.636
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/09/23 15:29:52.657
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 15:29:53.438
STEP: Deploying the webhook pod 05/09/23 15:29:53.455
STEP: Wait for the deployment to be ready 05/09/23 15:29:53.474
May  9 15:29:53.484: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/09/23 15:29:55.511
STEP: Verifying the service has paired with the endpoint 05/09/23 15:29:55.53
May  9 15:29:56.530: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 05/09/23 15:29:56.538
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 05/09/23 15:29:56.54
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 05/09/23 15:29:56.54
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 05/09/23 15:29:56.54
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 05/09/23 15:29:56.543
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 05/09/23 15:29:56.543
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 05/09/23 15:29:56.545
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 15:29:56.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3429" for this suite. 05/09/23 15:29:56.596
STEP: Destroying namespace "webhook-3429-markers" for this suite. 05/09/23 15:29:56.606
------------------------------
â€¢ [3.998 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:29:52.616
    May  9 15:29:52.616: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename webhook 05/09/23 15:29:52.617
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:29:52.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:29:52.636
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/09/23 15:29:52.657
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 15:29:53.438
    STEP: Deploying the webhook pod 05/09/23 15:29:53.455
    STEP: Wait for the deployment to be ready 05/09/23 15:29:53.474
    May  9 15:29:53.484: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/09/23 15:29:55.511
    STEP: Verifying the service has paired with the endpoint 05/09/23 15:29:55.53
    May  9 15:29:56.530: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 05/09/23 15:29:56.538
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 05/09/23 15:29:56.54
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 05/09/23 15:29:56.54
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 05/09/23 15:29:56.54
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 05/09/23 15:29:56.543
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 05/09/23 15:29:56.543
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 05/09/23 15:29:56.545
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 15:29:56.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3429" for this suite. 05/09/23 15:29:56.596
    STEP: Destroying namespace "webhook-3429-markers" for this suite. 05/09/23 15:29:56.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:29:56.616
May  9 15:29:56.616: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename cronjob 05/09/23 15:29:56.617
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:29:56.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:29:56.637
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 05/09/23 15:29:56.64
STEP: Ensuring no jobs are scheduled 05/09/23 15:29:56.647
STEP: Ensuring no job exists by listing jobs explicitly 05/09/23 15:34:56.659
STEP: Removing cronjob 05/09/23 15:34:56.664
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May  9 15:34:56.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5465" for this suite. 05/09/23 15:34:56.68
------------------------------
â€¢ [SLOW TEST] [300.072 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:29:56.616
    May  9 15:29:56.616: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename cronjob 05/09/23 15:29:56.617
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:29:56.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:29:56.637
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 05/09/23 15:29:56.64
    STEP: Ensuring no jobs are scheduled 05/09/23 15:29:56.647
    STEP: Ensuring no job exists by listing jobs explicitly 05/09/23 15:34:56.659
    STEP: Removing cronjob 05/09/23 15:34:56.664
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May  9 15:34:56.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5465" for this suite. 05/09/23 15:34:56.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:34:56.69
May  9 15:34:56.690: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename replicaset 05/09/23 15:34:56.691
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:34:56.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:34:56.714
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 05/09/23 15:34:56.728
STEP: Verify that the required pods have come up. 05/09/23 15:34:56.735
May  9 15:34:56.742: INFO: Pod name sample-pod: Found 0 pods out of 1
May  9 15:35:01.750: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/09/23 15:35:01.75
STEP: Getting /status 05/09/23 15:35:01.75
May  9 15:35:01.758: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 05/09/23 15:35:01.758
May  9 15:35:01.768: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 05/09/23 15:35:01.768
May  9 15:35:01.772: INFO: Observed &ReplicaSet event: ADDED
May  9 15:35:01.772: INFO: Observed &ReplicaSet event: MODIFIED
May  9 15:35:01.772: INFO: Observed &ReplicaSet event: MODIFIED
May  9 15:35:01.772: INFO: Observed &ReplicaSet event: MODIFIED
May  9 15:35:01.772: INFO: Found replicaset test-rs in namespace replicaset-4068 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  9 15:35:01.772: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 05/09/23 15:35:01.772
May  9 15:35:01.773: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May  9 15:35:01.780: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 05/09/23 15:35:01.78
May  9 15:35:01.783: INFO: Observed &ReplicaSet event: ADDED
May  9 15:35:01.783: INFO: Observed &ReplicaSet event: MODIFIED
May  9 15:35:01.783: INFO: Observed &ReplicaSet event: MODIFIED
May  9 15:35:01.784: INFO: Observed &ReplicaSet event: MODIFIED
May  9 15:35:01.784: INFO: Observed replicaset test-rs in namespace replicaset-4068 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  9 15:35:01.784: INFO: Observed &ReplicaSet event: MODIFIED
May  9 15:35:01.784: INFO: Found replicaset test-rs in namespace replicaset-4068 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
May  9 15:35:01.784: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May  9 15:35:01.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4068" for this suite. 05/09/23 15:35:01.791
------------------------------
â€¢ [SLOW TEST] [5.109 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:34:56.69
    May  9 15:34:56.690: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename replicaset 05/09/23 15:34:56.691
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:34:56.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:34:56.714
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 05/09/23 15:34:56.728
    STEP: Verify that the required pods have come up. 05/09/23 15:34:56.735
    May  9 15:34:56.742: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  9 15:35:01.750: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/09/23 15:35:01.75
    STEP: Getting /status 05/09/23 15:35:01.75
    May  9 15:35:01.758: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 05/09/23 15:35:01.758
    May  9 15:35:01.768: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 05/09/23 15:35:01.768
    May  9 15:35:01.772: INFO: Observed &ReplicaSet event: ADDED
    May  9 15:35:01.772: INFO: Observed &ReplicaSet event: MODIFIED
    May  9 15:35:01.772: INFO: Observed &ReplicaSet event: MODIFIED
    May  9 15:35:01.772: INFO: Observed &ReplicaSet event: MODIFIED
    May  9 15:35:01.772: INFO: Found replicaset test-rs in namespace replicaset-4068 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  9 15:35:01.772: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 05/09/23 15:35:01.772
    May  9 15:35:01.773: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May  9 15:35:01.780: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 05/09/23 15:35:01.78
    May  9 15:35:01.783: INFO: Observed &ReplicaSet event: ADDED
    May  9 15:35:01.783: INFO: Observed &ReplicaSet event: MODIFIED
    May  9 15:35:01.783: INFO: Observed &ReplicaSet event: MODIFIED
    May  9 15:35:01.784: INFO: Observed &ReplicaSet event: MODIFIED
    May  9 15:35:01.784: INFO: Observed replicaset test-rs in namespace replicaset-4068 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  9 15:35:01.784: INFO: Observed &ReplicaSet event: MODIFIED
    May  9 15:35:01.784: INFO: Found replicaset test-rs in namespace replicaset-4068 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    May  9 15:35:01.784: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May  9 15:35:01.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4068" for this suite. 05/09/23 15:35:01.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:35:01.8
May  9 15:35:01.800: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename deployment 05/09/23 15:35:01.801
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:35:01.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:35:01.82
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
May  9 15:35:01.828: INFO: Creating deployment "webserver-deployment"
May  9 15:35:01.834: INFO: Waiting for observed generation 1
May  9 15:35:03.847: INFO: Waiting for all required pods to come up
May  9 15:35:03.855: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 05/09/23 15:35:03.855
May  9 15:35:03.855: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-cn4x2" in namespace "deployment-2919" to be "running"
May  9 15:35:03.855: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2tz4s" in namespace "deployment-2919" to be "running"
May  9 15:35:03.855: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4skkx" in namespace "deployment-2919" to be "running"
May  9 15:35:03.855: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hblhn" in namespace "deployment-2919" to be "running"
May  9 15:35:03.855: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hh2rn" in namespace "deployment-2919" to be "running"
May  9 15:35:03.855: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jzf8x" in namespace "deployment-2919" to be "running"
May  9 15:35:03.855: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-spbh7" in namespace "deployment-2919" to be "running"
May  9 15:35:03.859: INFO: Pod "webserver-deployment-7f5969cbc7-hblhn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.466559ms
May  9 15:35:03.861: INFO: Pod "webserver-deployment-7f5969cbc7-2tz4s": Phase="Pending", Reason="", readiness=false. Elapsed: 5.820646ms
May  9 15:35:03.861: INFO: Pod "webserver-deployment-7f5969cbc7-cn4x2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.911146ms
May  9 15:35:03.861: INFO: Pod "webserver-deployment-7f5969cbc7-jzf8x": Phase="Pending", Reason="", readiness=false. Elapsed: 5.583774ms
May  9 15:35:03.861: INFO: Pod "webserver-deployment-7f5969cbc7-spbh7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.713087ms
May  9 15:35:03.861: INFO: Pod "webserver-deployment-7f5969cbc7-4skkx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019678ms
May  9 15:35:03.861: INFO: Pod "webserver-deployment-7f5969cbc7-hh2rn": Phase="Pending", Reason="", readiness=false. Elapsed: 5.88579ms
May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-4skkx": Phase="Running", Reason="", readiness=true. Elapsed: 2.01789502s
May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-4skkx" satisfied condition "running"
May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-hblhn": Phase="Running", Reason="", readiness=true. Elapsed: 2.017910506s
May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-hblhn" satisfied condition "running"
May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-2tz4s": Phase="Running", Reason="", readiness=true. Elapsed: 2.018108253s
May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-2tz4s" satisfied condition "running"
May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-jzf8x": Phase="Running", Reason="", readiness=true. Elapsed: 2.017795863s
May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-jzf8x" satisfied condition "running"
May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-spbh7": Phase="Running", Reason="", readiness=true. Elapsed: 2.017770773s
May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-spbh7" satisfied condition "running"
May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-hh2rn": Phase="Running", Reason="", readiness=true. Elapsed: 2.017962139s
May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-hh2rn" satisfied condition "running"
May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-cn4x2": Phase="Running", Reason="", readiness=true. Elapsed: 2.018292087s
May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-cn4x2" satisfied condition "running"
May  9 15:35:05.873: INFO: Waiting for deployment "webserver-deployment" to complete
May  9 15:35:05.889: INFO: Updating deployment "webserver-deployment" with a non-existent image
May  9 15:35:05.901: INFO: Updating deployment webserver-deployment
May  9 15:35:05.901: INFO: Waiting for observed generation 2
May  9 15:35:07.911: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May  9 15:35:07.915: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May  9 15:35:07.919: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May  9 15:35:07.932: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May  9 15:35:07.932: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May  9 15:35:07.936: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May  9 15:35:07.946: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May  9 15:35:07.946: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May  9 15:35:07.958: INFO: Updating deployment webserver-deployment
May  9 15:35:07.958: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May  9 15:35:07.973: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May  9 15:35:07.983: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  9 15:35:10.020: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2919  88fc0c75-6ebc-40a4-b769-b1b3af6f39e4 317711283 3 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003488e28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-09 15:35:07 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-05-09 15:35:08 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May  9 15:35:10.025: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-2919  8870b302-8b96-4004-87d0-332c1690f95f 317711241 3 2023-05-09 15:35:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 88fc0c75-6ebc-40a4-b769-b1b3af6f39e4 0xc003489317 0xc003489318}] [] [{kube-controller-manager Update apps/v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88fc0c75-6ebc-40a4-b769-b1b3af6f39e4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034893b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  9 15:35:10.026: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May  9 15:35:10.026: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-2919  27a34e3f-7901-4e79-a66c-5d0fa845b45b 317711183 3 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 88fc0c75-6ebc-40a4-b769-b1b3af6f39e4 0xc003489227 0xc003489228}] [] [{kube-controller-manager Update apps/v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88fc0c75-6ebc-40a4-b769-b1b3af6f39e4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034892b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May  9 15:35:10.039: INFO: Pod "webserver-deployment-7f5969cbc7-2tz4s" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2tz4s webserver-deployment-7f5969cbc7- deployment-2919  78a526a2-0509-4a66-88ea-5c248d861490 317710511 0 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c27bc15a09c71d6483a0f2c2c0e30a7dc607cc717f6523031ce20f5e40a9b151 cni.projectcalico.org/podIP:10.2.0.114/32 cni.projectcalico.org/podIPs:10.2.0.114/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419a837 0xc00419a838}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.0.114\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jrk7c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jrk7c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:10.2.0.114,StartTime:2023-05-09 15:35:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 15:35:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://59dacd9e94f11028d43d019f8337e317ce6d09c48a7e9cde1a70a363eb8db3df,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.0.114,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.039: INFO: Pod "webserver-deployment-7f5969cbc7-5mv52" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5mv52 webserver-deployment-7f5969cbc7- deployment-2919  5b0c03b2-031d-440e-bd71-8026be68bf98 317710390 0 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6e785aab7528168f0d4ff397d4b5c05bd9c8abbfd3ed15f0dc6b42f45b0d9690 cni.projectcalico.org/podIP:10.2.1.80/32 cni.projectcalico.org/podIPs:10.2.1.80/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419aa47 0xc00419aa48}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wqbpp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wqbpp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:10.2.1.80,StartTime:2023-05-09 15:35:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 15:35:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://85b2d04e4278205ab2100669e18c77163f575fb1aa67d678c1064c077607bfd6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.039: INFO: Pod "webserver-deployment-7f5969cbc7-6l5xt" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6l5xt webserver-deployment-7f5969cbc7- deployment-2919  7aa2fd5f-a3a4-47bc-bb79-2da7040e2716 317711359 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:774a16580cacecc798a9054dbb347fa73629e5a5270b33db66f1e41e24979d1f cni.projectcalico.org/podIP:10.2.2.109/32 cni.projectcalico.org/podIPs:10.2.2.109/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419ac67 0xc00419ac68}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zxctx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zxctx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:,StartTime:2023-05-09 15:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.039: INFO: Pod "webserver-deployment-7f5969cbc7-8nx7t" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8nx7t webserver-deployment-7f5969cbc7- deployment-2919  430dda99-fad0-4e35-ba85-6c8bcf63f271 317711388 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1581ac8ecc07b9ce7297f706c7ab8b6dfc46c33a41226195d4708fa811ac612c cni.projectcalico.org/podIP:10.2.1.86/32 cni.projectcalico.org/podIPs:10.2.1.86/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419ae67 0xc00419ae68}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w86dh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w86dh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:,StartTime:2023-05-09 15:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.040: INFO: Pod "webserver-deployment-7f5969cbc7-9h5m5" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9h5m5 webserver-deployment-7f5969cbc7- deployment-2919  c941ab1f-6b23-4734-82e0-7a4a57de6ca9 317711386 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:345853e53dc00eb96a104c07a9fc64f966645fb3a868446990cbca51510e26b9 cni.projectcalico.org/podIP:10.2.1.87/32 cni.projectcalico.org/podIPs:10.2.1.87/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419b057 0xc00419b058}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dsbzp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dsbzp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:,StartTime:2023-05-09 15:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.040: INFO: Pod "webserver-deployment-7f5969cbc7-cdbvx" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cdbvx webserver-deployment-7f5969cbc7- deployment-2919  0bc37f14-a095-4c79-a9ea-5443963b7e23 317711385 0 2023-05-09 15:35:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b12461a0f669a019ac3bb624cba6899bcd5a5cdd448d97ff719248c7c0222dd4 cni.projectcalico.org/podIP:10.2.0.120/32 cni.projectcalico.org/podIPs:10.2.0.120/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419b247 0xc00419b248}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nvk8z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nvk8z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:,StartTime:2023-05-09 15:35:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.040: INFO: Pod "webserver-deployment-7f5969cbc7-dnbxn" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dnbxn webserver-deployment-7f5969cbc7- deployment-2919  ab65d270-f13f-42a6-90b8-e5bf4eafaac2 317711339 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:71a87e59fc03182fa56c40c25f9482dd9e7c77d4207e8a711bb261e586fbf66b cni.projectcalico.org/podIP:10.2.2.108/32 cni.projectcalico.org/podIPs:10.2.2.108/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419b437 0xc00419b438}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k7sw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7sw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:,StartTime:2023-05-09 15:35:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.040: INFO: Pod "webserver-deployment-7f5969cbc7-fd5h5" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fd5h5 webserver-deployment-7f5969cbc7- deployment-2919  75c201a1-b1a5-41eb-a6ad-6a06b0905c8f 317710343 0 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:714a1b07991cf849d3af80a4f7c2f08ce2791a1b6a0f83efa503f86f9518cd66 cni.projectcalico.org/podIP:10.2.0.112/32 cni.projectcalico.org/podIPs:10.2.0.112/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419b627 0xc00419b628}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.0.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-65cts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-65cts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:10.2.0.112,StartTime:2023-05-09 15:35:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 15:35:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1163d1a203cd42e5c18405c842d86bc701a5d0af27f691569fdc9ad7cfd985e0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.0.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.041: INFO: Pod "webserver-deployment-7f5969cbc7-fznlr" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fznlr webserver-deployment-7f5969cbc7- deployment-2919  fbe289e8-0340-4504-afc5-382c8531beaf 317711429 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:040c691af49f24d641785e19049ac0dd3b3f0bfaeed99170ffc9425644434452 cni.projectcalico.org/podIP:10.2.1.89/32 cni.projectcalico.org/podIPs:10.2.1.89/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419b837 0xc00419b838}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rvtdx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rvtdx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.041: INFO: Pod "webserver-deployment-7f5969cbc7-hblhn" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hblhn webserver-deployment-7f5969cbc7- deployment-2919  a2a72b8e-8c48-4b0b-bde0-2333a9c7ea70 317710517 0 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:851a9aec966c1f1aaa76f937d13f63a18d5f28203a17dc440cb25d8485687f11 cni.projectcalico.org/podIP:10.2.0.113/32 cni.projectcalico.org/podIPs:10.2.0.113/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419b9c7 0xc00419b9c8}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.0.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rp4w8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rp4w8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:10.2.0.113,StartTime:2023-05-09 15:35:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 15:35:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bf681a149238b9a35c88b8560f051ab92d862ae385192e4813ae64aa353ca640,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.0.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.041: INFO: Pod "webserver-deployment-7f5969cbc7-hh2rn" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hh2rn webserver-deployment-7f5969cbc7- deployment-2919  0b9fccb9-1b66-4973-a943-e2985086641c 317710640 0 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:aefcdc1add3036285ed5345f2e0d65372133317f55465309bbb49478c870ccc8 cni.projectcalico.org/podIP:10.2.2.103/32 cni.projectcalico.org/podIPs:10.2.2.103/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419bbd7 0xc00419bbd8}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.2.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gj6c4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gj6c4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:10.2.2.103,StartTime:2023-05-09 15:35:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 15:35:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e5913d1de48495deb2a6048795d0d88b56a2efc74be813d7dcf8cc480ba06431,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.2.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.041: INFO: Pod "webserver-deployment-7f5969cbc7-jzf8x" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jzf8x webserver-deployment-7f5969cbc7- deployment-2919  7159cf31-e5bb-4e1a-bd8b-b52a99e7ca7e 317710481 0 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:74db02ad8f0c22638ae34d89050350885107320408553d6a2ca6f4be3906694e cni.projectcalico.org/podIP:10.2.2.105/32 cni.projectcalico.org/podIPs:10.2.2.105/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419bf27 0xc00419bf28}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.2.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v58v9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v58v9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:10.2.2.105,StartTime:2023-05-09 15:35:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 15:35:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f7f92cf76fac7a1abd2260d704e242a64c43ae23ce30048d35784fa21d8ea553,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.2.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.041: INFO: Pod "webserver-deployment-7f5969cbc7-m5g67" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m5g67 webserver-deployment-7f5969cbc7- deployment-2919  698e8e40-ee97-4bed-9c1c-93ef9765795e 317711389 0 2023-05-09 15:35:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:65d2d7e0bf257d85fb22794924177ba7094d07a2143fd38a7431b4b67e4c047e cni.projectcalico.org/podIP:10.2.2.111/32 cni.projectcalico.org/podIPs:10.2.2.111/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc004308147 0xc004308148}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5wpnc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5wpnc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:,StartTime:2023-05-09 15:35:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.042: INFO: Pod "webserver-deployment-7f5969cbc7-nmnt8" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nmnt8 webserver-deployment-7f5969cbc7- deployment-2919  658bc1ae-8719-4025-9b7c-8ad8f81f3cc9 317711436 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7d5e2668e5a2326fb6acf2a14a4b8020744c9cf8c11dca85d4e13ffc6b821e85 cni.projectcalico.org/podIP:10.2.1.90/32 cni.projectcalico.org/podIPs:10.2.1.90/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc004308337 0xc004308338}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wrkvr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wrkvr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.042: INFO: Pod "webserver-deployment-7f5969cbc7-pmll6" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pmll6 webserver-deployment-7f5969cbc7- deployment-2919  42795577-dfe0-4e3b-8aba-3a303b866f94 317710395 0 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ee81236ce01b9647604f0c61a5b6d7f83654186e44a69a20c440932f248fd76b cni.projectcalico.org/podIP:10.2.1.79/32 cni.projectcalico.org/podIPs:10.2.1.79/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc0043084c7 0xc0043084c8}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9g7xj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9g7xj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:10.2.1.79,StartTime:2023-05-09 15:35:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 15:35:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d98698432d6d0bed34572461e3b9977617dee9cd05efb01153d7a2b3300d6643,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.042: INFO: Pod "webserver-deployment-7f5969cbc7-r7frn" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-r7frn webserver-deployment-7f5969cbc7- deployment-2919  76bf0021-0336-4106-914f-c42ea0c9b9f4 317711165 0 2023-05-09 15:35:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc0043086b7 0xc0043086b8}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4tw68,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4tw68,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.042: INFO: Pod "webserver-deployment-7f5969cbc7-sbbds" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sbbds webserver-deployment-7f5969cbc7- deployment-2919  1905f9cd-126f-4278-8854-ee9a523c2116 317711371 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4c7e384849d8136a42f031a045ca9abf7bfa1f4a177b620ca888cc635deccdc3 cni.projectcalico.org/podIP:10.2.0.118/32 cni.projectcalico.org/podIPs:10.2.0.118/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc004308827 0xc004308828}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-62rhl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-62rhl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:,StartTime:2023-05-09 15:35:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.043: INFO: Pod "webserver-deployment-7f5969cbc7-spbh7" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-spbh7 webserver-deployment-7f5969cbc7- deployment-2919  21487be6-6525-4286-8996-9741abd2e0cd 317710476 0 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f59290b23ae99a12862fcddb627eff827cdac5cc61bd15e6e487a3486c67e9f0 cni.projectcalico.org/podIP:10.2.2.104/32 cni.projectcalico.org/podIPs:10.2.2.104/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc004308a27 0xc004308a28}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.2.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vsm4b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vsm4b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:10.2.2.104,StartTime:2023-05-09 15:35:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 15:35:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5585f1fcdf9ac133d17138dd9dadc2f31818e0fe0faf774a9528ec0e99749771,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.2.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.043: INFO: Pod "webserver-deployment-7f5969cbc7-vzgd5" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vzgd5 webserver-deployment-7f5969cbc7- deployment-2919  bc865636-10bf-4362-80e5-566923f62367 317711364 0 2023-05-09 15:35:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1d0decebeff42068f856417df872c3b5af0236a19bed769c186c0cbcca362492 cni.projectcalico.org/podIP:10.2.2.112/32 cni.projectcalico.org/podIPs:10.2.2.112/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc004308c47 0xc004308c48}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pmslw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pmslw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:,StartTime:2023-05-09 15:35:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.043: INFO: Pod "webserver-deployment-7f5969cbc7-wp7r4" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wp7r4 webserver-deployment-7f5969cbc7- deployment-2919  cc645f5d-0bf6-4e95-b0d6-08d1d2dc7d02 317711338 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bf3245439aa239e109b20a7f204b6dd7bcf2d85d503a8f548a09ccfc7c449a3c cni.projectcalico.org/podIP:10.2.0.116/32 cni.projectcalico.org/podIPs:10.2.0.116/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc004308e47 0xc004308e48}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6jqtv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6jqtv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:,StartTime:2023-05-09 15:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.043: INFO: Pod "webserver-deployment-d9f79cb5-2qkgg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2qkgg webserver-deployment-d9f79cb5- deployment-2919  0c202abc-fda6-418f-8209-cd5eb493d590 317711439 0 2023-05-09 15:35:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:1a4795855ae9f5ac68cc38fb1c3d1018ac3d9e0f2f5fc7fef25287164f2e8969 cni.projectcalico.org/podIP:10.2.2.107/32 cni.projectcalico.org/podIPs:10.2.2.107/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc004309037 0xc004309038}] [] [{calico Update v1 2023-05-09 15:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 15:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.2.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zrgxr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zrgxr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:10.2.2.107,StartTime:2023-05-09 15:35:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.2.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.043: INFO: Pod "webserver-deployment-d9f79cb5-7kgk5" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7kgk5 webserver-deployment-d9f79cb5- deployment-2919  15c0cffc-8856-41cd-8fe8-bc35b93d56cc 317711362 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:904ca51b9781f4e88bbaaddd45cf2c1f850185e0759b3d7c33471b305cad2f92 cni.projectcalico.org/podIP:10.2.1.85/32 cni.projectcalico.org/podIPs:10.2.1.85/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc004309277 0xc004309278}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8n62v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8n62v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:,StartTime:2023-05-09 15:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.044: INFO: Pod "webserver-deployment-d9f79cb5-7zvll" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7zvll webserver-deployment-d9f79cb5- deployment-2919  5f18139c-f461-44fe-98ef-7a320a1a12f9 317711067 0 2023-05-09 15:35:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:4d55a0690d5aa6c80df4319e5626082d1a9b160ff5e94ce0319bb6d03ad1a529 cni.projectcalico.org/podIP:10.2.1.83/32 cni.projectcalico.org/podIPs:10.2.1.83/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc004309487 0xc004309488}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fkflb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fkflb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:10.2.1.83,StartTime:2023-05-09 15:35:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.044: INFO: Pod "webserver-deployment-d9f79cb5-8gvwg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8gvwg webserver-deployment-d9f79cb5- deployment-2919  61cdcdba-3058-4b95-bd4f-16972b4482a8 317711435 0 2023-05-09 15:35:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d6b8ce6ac357ac97deeb6f79ca0e8a4274344976464d41b155e0f6c887c15a0b cni.projectcalico.org/podIP:10.2.0.115/32 cni.projectcalico.org/podIPs:10.2.0.115/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc0043096c7 0xc0043096c8}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.0.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7n96k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7n96k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:10.2.0.115,StartTime:2023-05-09 15:35:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.0.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.044: INFO: Pod "webserver-deployment-d9f79cb5-8qp5n" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8qp5n webserver-deployment-d9f79cb5- deployment-2919  8dd918bd-9681-467c-b529-803de26d46b9 317711363 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:73f15be5571457880d8e042e74ac64dc93e85d92e17a609efb9dc43ded6e7921 cni.projectcalico.org/podIP:10.2.2.110/32 cni.projectcalico.org/podIPs:10.2.2.110/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc004309907 0xc004309908}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c5qdl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c5qdl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:,StartTime:2023-05-09 15:35:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.044: INFO: Pod "webserver-deployment-d9f79cb5-fvd8g" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fvd8g webserver-deployment-d9f79cb5- deployment-2919  8404bc50-9e27-404d-bcdb-3adb1abbdf6b 317711006 0 2023-05-09 15:35:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f4e26b81a47ab0931c45f90ff75b2e7ae81acd49466552970f795af848faa7ad cni.projectcalico.org/podIP:10.2.2.106/32 cni.projectcalico.org/podIPs:10.2.2.106/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc004309b17 0xc004309b18}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.2.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t7x9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t7x9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:10.2.2.106,StartTime:2023-05-09 15:35:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.2.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.045: INFO: Pod "webserver-deployment-d9f79cb5-jrsj7" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jrsj7 webserver-deployment-d9f79cb5- deployment-2919  7ba0b996-f132-4d2e-bb1b-0d26f36f0306 317711382 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:c168a2eb637aa586e1476eb46680844d95b09580fa81a9a42b676894a7bde93d cni.projectcalico.org/podIP:10.2.2.113/32 cni.projectcalico.org/podIPs:10.2.2.113/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc004309d67 0xc004309d68}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fxdcd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fxdcd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:,StartTime:2023-05-09 15:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.045: INFO: Pod "webserver-deployment-d9f79cb5-phbll" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-phbll webserver-deployment-d9f79cb5- deployment-2919  71ac35c3-7ba9-43ef-8438-cf6ee9a52d46 317711360 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:9822b87870395b088a463a3a2cdc167a316a6591e2b6c095264063fc76e6d550 cni.projectcalico.org/podIP:10.2.0.117/32 cni.projectcalico.org/podIPs:10.2.0.117/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc004309f77 0xc004309f78}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kf9dg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kf9dg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:,StartTime:2023-05-09 15:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.045: INFO: Pod "webserver-deployment-d9f79cb5-slnnt" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-slnnt webserver-deployment-d9f79cb5- deployment-2919  1f63c5ba-f95b-4b91-9105-b2eebf98f811 317711393 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f8abf1833a6d15e4f5ba8baae830c3d05ba7decd14531e35e68212a2fe7ec2f4 cni.projectcalico.org/podIP:10.2.0.121/32 cni.projectcalico.org/podIPs:10.2.0.121/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc00291e187 0xc00291e188}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6xmd7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6xmd7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:,StartTime:2023-05-09 15:35:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.045: INFO: Pod "webserver-deployment-d9f79cb5-t6mjq" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-t6mjq webserver-deployment-d9f79cb5- deployment-2919  7ec58b90-607d-49c1-aed5-213d2dcfa521 317711392 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:cc1008c01c4218cb8a9c0c4e1d0d6857ce27cbf6992d3b2a6dba323192c340b8 cni.projectcalico.org/podIP:10.2.1.88/32 cni.projectcalico.org/podIPs:10.2.1.88/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc00291e397 0xc00291e398}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2vf7g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2vf7g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.045: INFO: Pod "webserver-deployment-d9f79cb5-wwfnj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wwfnj webserver-deployment-d9f79cb5- deployment-2919  ce8eb80d-9e86-417c-a347-d612048883bd 317711361 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d008119345d8061bc4b5b4756ddfc14a4f303714ae54c2124e4bd99ead1e819e cni.projectcalico.org/podIP:10.2.0.119/32 cni.projectcalico.org/podIPs:10.2.0.119/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc00291e537 0xc00291e538}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6krd7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6krd7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:,StartTime:2023-05-09 15:35:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.046: INFO: Pod "webserver-deployment-d9f79cb5-z9fnz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-z9fnz webserver-deployment-d9f79cb5- deployment-2919  48cfd155-2daf-45b7-8612-f99652d9783a 317710881 0 2023-05-09 15:35:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:329ccb2e77408cb79a0bf6f32bb84d4e3271d3c4ace6749f5d1b3b16821d61d4 cni.projectcalico.org/podIP:10.2.1.84/32 cni.projectcalico.org/podIPs:10.2.1.84/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc00291e757 0xc00291e758}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jjnh5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jjnh5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:,StartTime:2023-05-09 15:35:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:35:10.046: INFO: Pod "webserver-deployment-d9f79cb5-z9mpm" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-z9mpm webserver-deployment-d9f79cb5- deployment-2919  d174697c-2827-4c81-bf3b-10426d7ad9d4 317711177 0 2023-05-09 15:35:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc00291e947 0xc00291e948}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dvbt8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dvbt8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May  9 15:35:10.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2919" for this suite. 05/09/23 15:35:10.052
------------------------------
â€¢ [SLOW TEST] [8.262 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:35:01.8
    May  9 15:35:01.800: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename deployment 05/09/23 15:35:01.801
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:35:01.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:35:01.82
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    May  9 15:35:01.828: INFO: Creating deployment "webserver-deployment"
    May  9 15:35:01.834: INFO: Waiting for observed generation 1
    May  9 15:35:03.847: INFO: Waiting for all required pods to come up
    May  9 15:35:03.855: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 05/09/23 15:35:03.855
    May  9 15:35:03.855: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-cn4x2" in namespace "deployment-2919" to be "running"
    May  9 15:35:03.855: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2tz4s" in namespace "deployment-2919" to be "running"
    May  9 15:35:03.855: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4skkx" in namespace "deployment-2919" to be "running"
    May  9 15:35:03.855: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hblhn" in namespace "deployment-2919" to be "running"
    May  9 15:35:03.855: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hh2rn" in namespace "deployment-2919" to be "running"
    May  9 15:35:03.855: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jzf8x" in namespace "deployment-2919" to be "running"
    May  9 15:35:03.855: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-spbh7" in namespace "deployment-2919" to be "running"
    May  9 15:35:03.859: INFO: Pod "webserver-deployment-7f5969cbc7-hblhn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.466559ms
    May  9 15:35:03.861: INFO: Pod "webserver-deployment-7f5969cbc7-2tz4s": Phase="Pending", Reason="", readiness=false. Elapsed: 5.820646ms
    May  9 15:35:03.861: INFO: Pod "webserver-deployment-7f5969cbc7-cn4x2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.911146ms
    May  9 15:35:03.861: INFO: Pod "webserver-deployment-7f5969cbc7-jzf8x": Phase="Pending", Reason="", readiness=false. Elapsed: 5.583774ms
    May  9 15:35:03.861: INFO: Pod "webserver-deployment-7f5969cbc7-spbh7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.713087ms
    May  9 15:35:03.861: INFO: Pod "webserver-deployment-7f5969cbc7-4skkx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019678ms
    May  9 15:35:03.861: INFO: Pod "webserver-deployment-7f5969cbc7-hh2rn": Phase="Pending", Reason="", readiness=false. Elapsed: 5.88579ms
    May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-4skkx": Phase="Running", Reason="", readiness=true. Elapsed: 2.01789502s
    May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-4skkx" satisfied condition "running"
    May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-hblhn": Phase="Running", Reason="", readiness=true. Elapsed: 2.017910506s
    May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-hblhn" satisfied condition "running"
    May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-2tz4s": Phase="Running", Reason="", readiness=true. Elapsed: 2.018108253s
    May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-2tz4s" satisfied condition "running"
    May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-jzf8x": Phase="Running", Reason="", readiness=true. Elapsed: 2.017795863s
    May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-jzf8x" satisfied condition "running"
    May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-spbh7": Phase="Running", Reason="", readiness=true. Elapsed: 2.017770773s
    May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-spbh7" satisfied condition "running"
    May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-hh2rn": Phase="Running", Reason="", readiness=true. Elapsed: 2.017962139s
    May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-hh2rn" satisfied condition "running"
    May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-cn4x2": Phase="Running", Reason="", readiness=true. Elapsed: 2.018292087s
    May  9 15:35:05.873: INFO: Pod "webserver-deployment-7f5969cbc7-cn4x2" satisfied condition "running"
    May  9 15:35:05.873: INFO: Waiting for deployment "webserver-deployment" to complete
    May  9 15:35:05.889: INFO: Updating deployment "webserver-deployment" with a non-existent image
    May  9 15:35:05.901: INFO: Updating deployment webserver-deployment
    May  9 15:35:05.901: INFO: Waiting for observed generation 2
    May  9 15:35:07.911: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    May  9 15:35:07.915: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    May  9 15:35:07.919: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    May  9 15:35:07.932: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    May  9 15:35:07.932: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    May  9 15:35:07.936: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    May  9 15:35:07.946: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    May  9 15:35:07.946: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    May  9 15:35:07.958: INFO: Updating deployment webserver-deployment
    May  9 15:35:07.958: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    May  9 15:35:07.973: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    May  9 15:35:07.983: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  9 15:35:10.020: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-2919  88fc0c75-6ebc-40a4-b769-b1b3af6f39e4 317711283 3 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003488e28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-09 15:35:07 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-05-09 15:35:08 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    May  9 15:35:10.025: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-2919  8870b302-8b96-4004-87d0-332c1690f95f 317711241 3 2023-05-09 15:35:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 88fc0c75-6ebc-40a4-b769-b1b3af6f39e4 0xc003489317 0xc003489318}] [] [{kube-controller-manager Update apps/v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88fc0c75-6ebc-40a4-b769-b1b3af6f39e4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034893b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  9 15:35:10.026: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    May  9 15:35:10.026: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-2919  27a34e3f-7901-4e79-a66c-5d0fa845b45b 317711183 3 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 88fc0c75-6ebc-40a4-b769-b1b3af6f39e4 0xc003489227 0xc003489228}] [] [{kube-controller-manager Update apps/v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88fc0c75-6ebc-40a4-b769-b1b3af6f39e4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034892b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    May  9 15:35:10.039: INFO: Pod "webserver-deployment-7f5969cbc7-2tz4s" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2tz4s webserver-deployment-7f5969cbc7- deployment-2919  78a526a2-0509-4a66-88ea-5c248d861490 317710511 0 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c27bc15a09c71d6483a0f2c2c0e30a7dc607cc717f6523031ce20f5e40a9b151 cni.projectcalico.org/podIP:10.2.0.114/32 cni.projectcalico.org/podIPs:10.2.0.114/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419a837 0xc00419a838}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.0.114\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jrk7c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jrk7c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:10.2.0.114,StartTime:2023-05-09 15:35:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 15:35:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://59dacd9e94f11028d43d019f8337e317ce6d09c48a7e9cde1a70a363eb8db3df,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.0.114,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.039: INFO: Pod "webserver-deployment-7f5969cbc7-5mv52" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5mv52 webserver-deployment-7f5969cbc7- deployment-2919  5b0c03b2-031d-440e-bd71-8026be68bf98 317710390 0 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6e785aab7528168f0d4ff397d4b5c05bd9c8abbfd3ed15f0dc6b42f45b0d9690 cni.projectcalico.org/podIP:10.2.1.80/32 cni.projectcalico.org/podIPs:10.2.1.80/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419aa47 0xc00419aa48}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wqbpp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wqbpp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:10.2.1.80,StartTime:2023-05-09 15:35:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 15:35:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://85b2d04e4278205ab2100669e18c77163f575fb1aa67d678c1064c077607bfd6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.039: INFO: Pod "webserver-deployment-7f5969cbc7-6l5xt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6l5xt webserver-deployment-7f5969cbc7- deployment-2919  7aa2fd5f-a3a4-47bc-bb79-2da7040e2716 317711359 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:774a16580cacecc798a9054dbb347fa73629e5a5270b33db66f1e41e24979d1f cni.projectcalico.org/podIP:10.2.2.109/32 cni.projectcalico.org/podIPs:10.2.2.109/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419ac67 0xc00419ac68}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zxctx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zxctx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:,StartTime:2023-05-09 15:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.039: INFO: Pod "webserver-deployment-7f5969cbc7-8nx7t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8nx7t webserver-deployment-7f5969cbc7- deployment-2919  430dda99-fad0-4e35-ba85-6c8bcf63f271 317711388 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1581ac8ecc07b9ce7297f706c7ab8b6dfc46c33a41226195d4708fa811ac612c cni.projectcalico.org/podIP:10.2.1.86/32 cni.projectcalico.org/podIPs:10.2.1.86/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419ae67 0xc00419ae68}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w86dh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w86dh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:,StartTime:2023-05-09 15:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.040: INFO: Pod "webserver-deployment-7f5969cbc7-9h5m5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9h5m5 webserver-deployment-7f5969cbc7- deployment-2919  c941ab1f-6b23-4734-82e0-7a4a57de6ca9 317711386 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:345853e53dc00eb96a104c07a9fc64f966645fb3a868446990cbca51510e26b9 cni.projectcalico.org/podIP:10.2.1.87/32 cni.projectcalico.org/podIPs:10.2.1.87/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419b057 0xc00419b058}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dsbzp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dsbzp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:,StartTime:2023-05-09 15:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.040: INFO: Pod "webserver-deployment-7f5969cbc7-cdbvx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cdbvx webserver-deployment-7f5969cbc7- deployment-2919  0bc37f14-a095-4c79-a9ea-5443963b7e23 317711385 0 2023-05-09 15:35:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b12461a0f669a019ac3bb624cba6899bcd5a5cdd448d97ff719248c7c0222dd4 cni.projectcalico.org/podIP:10.2.0.120/32 cni.projectcalico.org/podIPs:10.2.0.120/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419b247 0xc00419b248}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nvk8z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nvk8z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:,StartTime:2023-05-09 15:35:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.040: INFO: Pod "webserver-deployment-7f5969cbc7-dnbxn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dnbxn webserver-deployment-7f5969cbc7- deployment-2919  ab65d270-f13f-42a6-90b8-e5bf4eafaac2 317711339 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:71a87e59fc03182fa56c40c25f9482dd9e7c77d4207e8a711bb261e586fbf66b cni.projectcalico.org/podIP:10.2.2.108/32 cni.projectcalico.org/podIPs:10.2.2.108/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419b437 0xc00419b438}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k7sw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7sw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:,StartTime:2023-05-09 15:35:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.040: INFO: Pod "webserver-deployment-7f5969cbc7-fd5h5" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fd5h5 webserver-deployment-7f5969cbc7- deployment-2919  75c201a1-b1a5-41eb-a6ad-6a06b0905c8f 317710343 0 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:714a1b07991cf849d3af80a4f7c2f08ce2791a1b6a0f83efa503f86f9518cd66 cni.projectcalico.org/podIP:10.2.0.112/32 cni.projectcalico.org/podIPs:10.2.0.112/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419b627 0xc00419b628}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.0.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-65cts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-65cts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:10.2.0.112,StartTime:2023-05-09 15:35:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 15:35:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1163d1a203cd42e5c18405c842d86bc701a5d0af27f691569fdc9ad7cfd985e0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.0.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.041: INFO: Pod "webserver-deployment-7f5969cbc7-fznlr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fznlr webserver-deployment-7f5969cbc7- deployment-2919  fbe289e8-0340-4504-afc5-382c8531beaf 317711429 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:040c691af49f24d641785e19049ac0dd3b3f0bfaeed99170ffc9425644434452 cni.projectcalico.org/podIP:10.2.1.89/32 cni.projectcalico.org/podIPs:10.2.1.89/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419b837 0xc00419b838}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rvtdx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rvtdx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.041: INFO: Pod "webserver-deployment-7f5969cbc7-hblhn" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hblhn webserver-deployment-7f5969cbc7- deployment-2919  a2a72b8e-8c48-4b0b-bde0-2333a9c7ea70 317710517 0 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:851a9aec966c1f1aaa76f937d13f63a18d5f28203a17dc440cb25d8485687f11 cni.projectcalico.org/podIP:10.2.0.113/32 cni.projectcalico.org/podIPs:10.2.0.113/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419b9c7 0xc00419b9c8}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.0.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rp4w8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rp4w8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:10.2.0.113,StartTime:2023-05-09 15:35:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 15:35:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bf681a149238b9a35c88b8560f051ab92d862ae385192e4813ae64aa353ca640,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.0.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.041: INFO: Pod "webserver-deployment-7f5969cbc7-hh2rn" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hh2rn webserver-deployment-7f5969cbc7- deployment-2919  0b9fccb9-1b66-4973-a943-e2985086641c 317710640 0 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:aefcdc1add3036285ed5345f2e0d65372133317f55465309bbb49478c870ccc8 cni.projectcalico.org/podIP:10.2.2.103/32 cni.projectcalico.org/podIPs:10.2.2.103/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419bbd7 0xc00419bbd8}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.2.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gj6c4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gj6c4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:10.2.2.103,StartTime:2023-05-09 15:35:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 15:35:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e5913d1de48495deb2a6048795d0d88b56a2efc74be813d7dcf8cc480ba06431,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.2.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.041: INFO: Pod "webserver-deployment-7f5969cbc7-jzf8x" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jzf8x webserver-deployment-7f5969cbc7- deployment-2919  7159cf31-e5bb-4e1a-bd8b-b52a99e7ca7e 317710481 0 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:74db02ad8f0c22638ae34d89050350885107320408553d6a2ca6f4be3906694e cni.projectcalico.org/podIP:10.2.2.105/32 cni.projectcalico.org/podIPs:10.2.2.105/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc00419bf27 0xc00419bf28}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.2.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v58v9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v58v9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:10.2.2.105,StartTime:2023-05-09 15:35:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 15:35:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f7f92cf76fac7a1abd2260d704e242a64c43ae23ce30048d35784fa21d8ea553,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.2.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.041: INFO: Pod "webserver-deployment-7f5969cbc7-m5g67" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m5g67 webserver-deployment-7f5969cbc7- deployment-2919  698e8e40-ee97-4bed-9c1c-93ef9765795e 317711389 0 2023-05-09 15:35:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:65d2d7e0bf257d85fb22794924177ba7094d07a2143fd38a7431b4b67e4c047e cni.projectcalico.org/podIP:10.2.2.111/32 cni.projectcalico.org/podIPs:10.2.2.111/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc004308147 0xc004308148}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5wpnc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5wpnc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:,StartTime:2023-05-09 15:35:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.042: INFO: Pod "webserver-deployment-7f5969cbc7-nmnt8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nmnt8 webserver-deployment-7f5969cbc7- deployment-2919  658bc1ae-8719-4025-9b7c-8ad8f81f3cc9 317711436 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7d5e2668e5a2326fb6acf2a14a4b8020744c9cf8c11dca85d4e13ffc6b821e85 cni.projectcalico.org/podIP:10.2.1.90/32 cni.projectcalico.org/podIPs:10.2.1.90/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc004308337 0xc004308338}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wrkvr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wrkvr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.042: INFO: Pod "webserver-deployment-7f5969cbc7-pmll6" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pmll6 webserver-deployment-7f5969cbc7- deployment-2919  42795577-dfe0-4e3b-8aba-3a303b866f94 317710395 0 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ee81236ce01b9647604f0c61a5b6d7f83654186e44a69a20c440932f248fd76b cni.projectcalico.org/podIP:10.2.1.79/32 cni.projectcalico.org/podIPs:10.2.1.79/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc0043084c7 0xc0043084c8}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9g7xj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9g7xj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:10.2.1.79,StartTime:2023-05-09 15:35:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 15:35:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d98698432d6d0bed34572461e3b9977617dee9cd05efb01153d7a2b3300d6643,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.042: INFO: Pod "webserver-deployment-7f5969cbc7-r7frn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-r7frn webserver-deployment-7f5969cbc7- deployment-2919  76bf0021-0336-4106-914f-c42ea0c9b9f4 317711165 0 2023-05-09 15:35:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc0043086b7 0xc0043086b8}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4tw68,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4tw68,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.042: INFO: Pod "webserver-deployment-7f5969cbc7-sbbds" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sbbds webserver-deployment-7f5969cbc7- deployment-2919  1905f9cd-126f-4278-8854-ee9a523c2116 317711371 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4c7e384849d8136a42f031a045ca9abf7bfa1f4a177b620ca888cc635deccdc3 cni.projectcalico.org/podIP:10.2.0.118/32 cni.projectcalico.org/podIPs:10.2.0.118/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc004308827 0xc004308828}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-62rhl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-62rhl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:,StartTime:2023-05-09 15:35:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.043: INFO: Pod "webserver-deployment-7f5969cbc7-spbh7" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-spbh7 webserver-deployment-7f5969cbc7- deployment-2919  21487be6-6525-4286-8996-9741abd2e0cd 317710476 0 2023-05-09 15:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f59290b23ae99a12862fcddb627eff827cdac5cc61bd15e6e487a3486c67e9f0 cni.projectcalico.org/podIP:10.2.2.104/32 cni.projectcalico.org/podIPs:10.2.2.104/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc004308a27 0xc004308a28}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.2.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vsm4b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vsm4b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:10.2.2.104,StartTime:2023-05-09 15:35:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 15:35:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5585f1fcdf9ac133d17138dd9dadc2f31818e0fe0faf774a9528ec0e99749771,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.2.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.043: INFO: Pod "webserver-deployment-7f5969cbc7-vzgd5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vzgd5 webserver-deployment-7f5969cbc7- deployment-2919  bc865636-10bf-4362-80e5-566923f62367 317711364 0 2023-05-09 15:35:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1d0decebeff42068f856417df872c3b5af0236a19bed769c186c0cbcca362492 cni.projectcalico.org/podIP:10.2.2.112/32 cni.projectcalico.org/podIPs:10.2.2.112/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc004308c47 0xc004308c48}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pmslw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pmslw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:,StartTime:2023-05-09 15:35:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.043: INFO: Pod "webserver-deployment-7f5969cbc7-wp7r4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wp7r4 webserver-deployment-7f5969cbc7- deployment-2919  cc645f5d-0bf6-4e95-b0d6-08d1d2dc7d02 317711338 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bf3245439aa239e109b20a7f204b6dd7bcf2d85d503a8f548a09ccfc7c449a3c cni.projectcalico.org/podIP:10.2.0.116/32 cni.projectcalico.org/podIPs:10.2.0.116/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 27a34e3f-7901-4e79-a66c-5d0fa845b45b 0xc004308e47 0xc004308e48}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27a34e3f-7901-4e79-a66c-5d0fa845b45b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6jqtv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6jqtv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:,StartTime:2023-05-09 15:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.043: INFO: Pod "webserver-deployment-d9f79cb5-2qkgg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2qkgg webserver-deployment-d9f79cb5- deployment-2919  0c202abc-fda6-418f-8209-cd5eb493d590 317711439 0 2023-05-09 15:35:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:1a4795855ae9f5ac68cc38fb1c3d1018ac3d9e0f2f5fc7fef25287164f2e8969 cni.projectcalico.org/podIP:10.2.2.107/32 cni.projectcalico.org/podIPs:10.2.2.107/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc004309037 0xc004309038}] [] [{calico Update v1 2023-05-09 15:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 15:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.2.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zrgxr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zrgxr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:10.2.2.107,StartTime:2023-05-09 15:35:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.2.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.043: INFO: Pod "webserver-deployment-d9f79cb5-7kgk5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7kgk5 webserver-deployment-d9f79cb5- deployment-2919  15c0cffc-8856-41cd-8fe8-bc35b93d56cc 317711362 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:904ca51b9781f4e88bbaaddd45cf2c1f850185e0759b3d7c33471b305cad2f92 cni.projectcalico.org/podIP:10.2.1.85/32 cni.projectcalico.org/podIPs:10.2.1.85/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc004309277 0xc004309278}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8n62v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8n62v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:,StartTime:2023-05-09 15:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.044: INFO: Pod "webserver-deployment-d9f79cb5-7zvll" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7zvll webserver-deployment-d9f79cb5- deployment-2919  5f18139c-f461-44fe-98ef-7a320a1a12f9 317711067 0 2023-05-09 15:35:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:4d55a0690d5aa6c80df4319e5626082d1a9b160ff5e94ce0319bb6d03ad1a529 cni.projectcalico.org/podIP:10.2.1.83/32 cni.projectcalico.org/podIPs:10.2.1.83/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc004309487 0xc004309488}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fkflb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fkflb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:10.2.1.83,StartTime:2023-05-09 15:35:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.044: INFO: Pod "webserver-deployment-d9f79cb5-8gvwg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8gvwg webserver-deployment-d9f79cb5- deployment-2919  61cdcdba-3058-4b95-bd4f-16972b4482a8 317711435 0 2023-05-09 15:35:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d6b8ce6ac357ac97deeb6f79ca0e8a4274344976464d41b155e0f6c887c15a0b cni.projectcalico.org/podIP:10.2.0.115/32 cni.projectcalico.org/podIPs:10.2.0.115/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc0043096c7 0xc0043096c8}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.0.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7n96k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7n96k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:10.2.0.115,StartTime:2023-05-09 15:35:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.0.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.044: INFO: Pod "webserver-deployment-d9f79cb5-8qp5n" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8qp5n webserver-deployment-d9f79cb5- deployment-2919  8dd918bd-9681-467c-b529-803de26d46b9 317711363 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:73f15be5571457880d8e042e74ac64dc93e85d92e17a609efb9dc43ded6e7921 cni.projectcalico.org/podIP:10.2.2.110/32 cni.projectcalico.org/podIPs:10.2.2.110/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc004309907 0xc004309908}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c5qdl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c5qdl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:,StartTime:2023-05-09 15:35:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.044: INFO: Pod "webserver-deployment-d9f79cb5-fvd8g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fvd8g webserver-deployment-d9f79cb5- deployment-2919  8404bc50-9e27-404d-bcdb-3adb1abbdf6b 317711006 0 2023-05-09 15:35:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f4e26b81a47ab0931c45f90ff75b2e7ae81acd49466552970f795af848faa7ad cni.projectcalico.org/podIP:10.2.2.106/32 cni.projectcalico.org/podIPs:10.2.2.106/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc004309b17 0xc004309b18}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.2.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t7x9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t7x9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:10.2.2.106,StartTime:2023-05-09 15:35:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.2.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.045: INFO: Pod "webserver-deployment-d9f79cb5-jrsj7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jrsj7 webserver-deployment-d9f79cb5- deployment-2919  7ba0b996-f132-4d2e-bb1b-0d26f36f0306 317711382 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:c168a2eb637aa586e1476eb46680844d95b09580fa81a9a42b676894a7bde93d cni.projectcalico.org/podIP:10.2.2.113/32 cni.projectcalico.org/podIPs:10.2.2.113/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc004309d67 0xc004309d68}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fxdcd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fxdcd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:,StartTime:2023-05-09 15:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.045: INFO: Pod "webserver-deployment-d9f79cb5-phbll" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-phbll webserver-deployment-d9f79cb5- deployment-2919  71ac35c3-7ba9-43ef-8438-cf6ee9a52d46 317711360 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:9822b87870395b088a463a3a2cdc167a316a6591e2b6c095264063fc76e6d550 cni.projectcalico.org/podIP:10.2.0.117/32 cni.projectcalico.org/podIPs:10.2.0.117/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc004309f77 0xc004309f78}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kf9dg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kf9dg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:,StartTime:2023-05-09 15:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.045: INFO: Pod "webserver-deployment-d9f79cb5-slnnt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-slnnt webserver-deployment-d9f79cb5- deployment-2919  1f63c5ba-f95b-4b91-9105-b2eebf98f811 317711393 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f8abf1833a6d15e4f5ba8baae830c3d05ba7decd14531e35e68212a2fe7ec2f4 cni.projectcalico.org/podIP:10.2.0.121/32 cni.projectcalico.org/podIPs:10.2.0.121/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc00291e187 0xc00291e188}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6xmd7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6xmd7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:,StartTime:2023-05-09 15:35:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.045: INFO: Pod "webserver-deployment-d9f79cb5-t6mjq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-t6mjq webserver-deployment-d9f79cb5- deployment-2919  7ec58b90-607d-49c1-aed5-213d2dcfa521 317711392 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:cc1008c01c4218cb8a9c0c4e1d0d6857ce27cbf6992d3b2a6dba323192c340b8 cni.projectcalico.org/podIP:10.2.1.88/32 cni.projectcalico.org/podIPs:10.2.1.88/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc00291e397 0xc00291e398}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2vf7g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2vf7g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.045: INFO: Pod "webserver-deployment-d9f79cb5-wwfnj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wwfnj webserver-deployment-d9f79cb5- deployment-2919  ce8eb80d-9e86-417c-a347-d612048883bd 317711361 0 2023-05-09 15:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d008119345d8061bc4b5b4756ddfc14a4f303714ae54c2124e4bd99ead1e819e cni.projectcalico.org/podIP:10.2.0.119/32 cni.projectcalico.org/podIPs:10.2.0.119/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc00291e537 0xc00291e538}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6krd7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6krd7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:,StartTime:2023-05-09 15:35:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.046: INFO: Pod "webserver-deployment-d9f79cb5-z9fnz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-z9fnz webserver-deployment-d9f79cb5- deployment-2919  48cfd155-2daf-45b7-8612-f99652d9783a 317710881 0 2023-05-09 15:35:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:329ccb2e77408cb79a0bf6f32bb84d4e3271d3c4ace6749f5d1b3b16821d61d4 cni.projectcalico.org/podIP:10.2.1.84/32 cni.projectcalico.org/podIPs:10.2.1.84/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc00291e757 0xc00291e758}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:35:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jjnh5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jjnh5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:,StartTime:2023-05-09 15:35:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:35:10.046: INFO: Pod "webserver-deployment-d9f79cb5-z9mpm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-z9mpm webserver-deployment-d9f79cb5- deployment-2919  d174697c-2827-4c81-bf3b-10426d7ad9d4 317711177 0 2023-05-09 15:35:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 8870b302-8b96-4004-87d0-332c1690f95f 0xc00291e947 0xc00291e948}] [] [{kube-controller-manager Update v1 2023-05-09 15:35:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8870b302-8b96-4004-87d0-332c1690f95f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dvbt8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dvbt8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:35:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May  9 15:35:10.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2919" for this suite. 05/09/23 15:35:10.052
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:35:10.066
May  9 15:35:10.066: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename ingress 05/09/23 15:35:10.067
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:35:10.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:35:10.097
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 05/09/23 15:35:10.101
STEP: getting /apis/networking.k8s.io 05/09/23 15:35:10.105
STEP: getting /apis/networking.k8s.iov1 05/09/23 15:35:10.107
STEP: creating 05/09/23 15:35:10.108
STEP: getting 05/09/23 15:35:10.136
STEP: listing 05/09/23 15:35:10.142
STEP: watching 05/09/23 15:35:10.147
May  9 15:35:10.147: INFO: starting watch
STEP: cluster-wide listing 05/09/23 15:35:10.149
STEP: cluster-wide watching 05/09/23 15:35:10.154
May  9 15:35:10.155: INFO: starting watch
STEP: patching 05/09/23 15:35:10.156
STEP: updating 05/09/23 15:35:10.164
May  9 15:35:10.179: INFO: waiting for watch events with expected annotations
May  9 15:35:10.179: INFO: saw patched and updated annotations
STEP: patching /status 05/09/23 15:35:10.179
STEP: updating /status 05/09/23 15:35:10.186
STEP: get /status 05/09/23 15:35:10.213
STEP: deleting 05/09/23 15:35:10.218
STEP: deleting a collection 05/09/23 15:35:10.251
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
May  9 15:35:10.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-7683" for this suite. 05/09/23 15:35:10.287
------------------------------
â€¢ [0.234 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:35:10.066
    May  9 15:35:10.066: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename ingress 05/09/23 15:35:10.067
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:35:10.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:35:10.097
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 05/09/23 15:35:10.101
    STEP: getting /apis/networking.k8s.io 05/09/23 15:35:10.105
    STEP: getting /apis/networking.k8s.iov1 05/09/23 15:35:10.107
    STEP: creating 05/09/23 15:35:10.108
    STEP: getting 05/09/23 15:35:10.136
    STEP: listing 05/09/23 15:35:10.142
    STEP: watching 05/09/23 15:35:10.147
    May  9 15:35:10.147: INFO: starting watch
    STEP: cluster-wide listing 05/09/23 15:35:10.149
    STEP: cluster-wide watching 05/09/23 15:35:10.154
    May  9 15:35:10.155: INFO: starting watch
    STEP: patching 05/09/23 15:35:10.156
    STEP: updating 05/09/23 15:35:10.164
    May  9 15:35:10.179: INFO: waiting for watch events with expected annotations
    May  9 15:35:10.179: INFO: saw patched and updated annotations
    STEP: patching /status 05/09/23 15:35:10.179
    STEP: updating /status 05/09/23 15:35:10.186
    STEP: get /status 05/09/23 15:35:10.213
    STEP: deleting 05/09/23 15:35:10.218
    STEP: deleting a collection 05/09/23 15:35:10.251
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    May  9 15:35:10.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-7683" for this suite. 05/09/23 15:35:10.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:35:10.301
May  9 15:35:10.301: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename events 05/09/23 15:35:10.302
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:35:10.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:35:10.329
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 05/09/23 15:35:10.341
STEP: get a list of Events with a label in the current namespace 05/09/23 15:35:10.36
STEP: delete a list of events 05/09/23 15:35:10.366
May  9 15:35:10.366: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 05/09/23 15:35:10.398
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
May  9 15:35:10.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-2424" for this suite. 05/09/23 15:35:10.41
------------------------------
â€¢ [0.120 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:35:10.301
    May  9 15:35:10.301: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename events 05/09/23 15:35:10.302
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:35:10.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:35:10.329
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 05/09/23 15:35:10.341
    STEP: get a list of Events with a label in the current namespace 05/09/23 15:35:10.36
    STEP: delete a list of events 05/09/23 15:35:10.366
    May  9 15:35:10.366: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 05/09/23 15:35:10.398
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    May  9 15:35:10.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-2424" for this suite. 05/09/23 15:35:10.41
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:35:10.422
May  9 15:35:10.422: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename downward-api 05/09/23 15:35:10.422
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:35:10.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:35:10.446
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 05/09/23 15:35:10.45
May  9 15:35:10.466: INFO: Waiting up to 5m0s for pod "downwardapi-volume-892a2e5e-3db2-4e91-9c0c-729be245b662" in namespace "downward-api-943" to be "Succeeded or Failed"
May  9 15:35:10.471: INFO: Pod "downwardapi-volume-892a2e5e-3db2-4e91-9c0c-729be245b662": Phase="Pending", Reason="", readiness=false. Elapsed: 5.005844ms
May  9 15:35:12.477: INFO: Pod "downwardapi-volume-892a2e5e-3db2-4e91-9c0c-729be245b662": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011494325s
May  9 15:35:14.480: INFO: Pod "downwardapi-volume-892a2e5e-3db2-4e91-9c0c-729be245b662": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014564385s
May  9 15:35:16.478: INFO: Pod "downwardapi-volume-892a2e5e-3db2-4e91-9c0c-729be245b662": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01185617s
STEP: Saw pod success 05/09/23 15:35:16.478
May  9 15:35:16.478: INFO: Pod "downwardapi-volume-892a2e5e-3db2-4e91-9c0c-729be245b662" satisfied condition "Succeeded or Failed"
May  9 15:35:16.489: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod downwardapi-volume-892a2e5e-3db2-4e91-9c0c-729be245b662 container client-container: <nil>
STEP: delete the pod 05/09/23 15:35:16.597
May  9 15:35:16.678: INFO: Waiting for pod downwardapi-volume-892a2e5e-3db2-4e91-9c0c-729be245b662 to disappear
May  9 15:35:16.682: INFO: Pod downwardapi-volume-892a2e5e-3db2-4e91-9c0c-729be245b662 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  9 15:35:16.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-943" for this suite. 05/09/23 15:35:16.699
------------------------------
â€¢ [SLOW TEST] [6.297 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:35:10.422
    May  9 15:35:10.422: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename downward-api 05/09/23 15:35:10.422
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:35:10.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:35:10.446
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 05/09/23 15:35:10.45
    May  9 15:35:10.466: INFO: Waiting up to 5m0s for pod "downwardapi-volume-892a2e5e-3db2-4e91-9c0c-729be245b662" in namespace "downward-api-943" to be "Succeeded or Failed"
    May  9 15:35:10.471: INFO: Pod "downwardapi-volume-892a2e5e-3db2-4e91-9c0c-729be245b662": Phase="Pending", Reason="", readiness=false. Elapsed: 5.005844ms
    May  9 15:35:12.477: INFO: Pod "downwardapi-volume-892a2e5e-3db2-4e91-9c0c-729be245b662": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011494325s
    May  9 15:35:14.480: INFO: Pod "downwardapi-volume-892a2e5e-3db2-4e91-9c0c-729be245b662": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014564385s
    May  9 15:35:16.478: INFO: Pod "downwardapi-volume-892a2e5e-3db2-4e91-9c0c-729be245b662": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01185617s
    STEP: Saw pod success 05/09/23 15:35:16.478
    May  9 15:35:16.478: INFO: Pod "downwardapi-volume-892a2e5e-3db2-4e91-9c0c-729be245b662" satisfied condition "Succeeded or Failed"
    May  9 15:35:16.489: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod downwardapi-volume-892a2e5e-3db2-4e91-9c0c-729be245b662 container client-container: <nil>
    STEP: delete the pod 05/09/23 15:35:16.597
    May  9 15:35:16.678: INFO: Waiting for pod downwardapi-volume-892a2e5e-3db2-4e91-9c0c-729be245b662 to disappear
    May  9 15:35:16.682: INFO: Pod downwardapi-volume-892a2e5e-3db2-4e91-9c0c-729be245b662 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  9 15:35:16.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-943" for this suite. 05/09/23 15:35:16.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:35:16.721
May  9 15:35:16.721: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename containers 05/09/23 15:35:16.722
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:35:16.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:35:16.801
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 05/09/23 15:35:16.881
May  9 15:35:16.893: INFO: Waiting up to 5m0s for pod "client-containers-a9304871-aa25-415a-a646-010a9cf3f792" in namespace "containers-5611" to be "Succeeded or Failed"
May  9 15:35:16.900: INFO: Pod "client-containers-a9304871-aa25-415a-a646-010a9cf3f792": Phase="Pending", Reason="", readiness=false. Elapsed: 7.101373ms
May  9 15:35:18.905: INFO: Pod "client-containers-a9304871-aa25-415a-a646-010a9cf3f792": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011514316s
May  9 15:35:20.906: INFO: Pod "client-containers-a9304871-aa25-415a-a646-010a9cf3f792": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012568774s
May  9 15:35:22.908: INFO: Pod "client-containers-a9304871-aa25-415a-a646-010a9cf3f792": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014575818s
STEP: Saw pod success 05/09/23 15:35:22.908
May  9 15:35:22.908: INFO: Pod "client-containers-a9304871-aa25-415a-a646-010a9cf3f792" satisfied condition "Succeeded or Failed"
May  9 15:35:22.914: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod client-containers-a9304871-aa25-415a-a646-010a9cf3f792 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 15:35:22.972
May  9 15:35:22.990: INFO: Waiting for pod client-containers-a9304871-aa25-415a-a646-010a9cf3f792 to disappear
May  9 15:35:22.994: INFO: Pod client-containers-a9304871-aa25-415a-a646-010a9cf3f792 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
May  9 15:35:22.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-5611" for this suite. 05/09/23 15:35:23
------------------------------
â€¢ [SLOW TEST] [6.289 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:35:16.721
    May  9 15:35:16.721: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename containers 05/09/23 15:35:16.722
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:35:16.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:35:16.801
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 05/09/23 15:35:16.881
    May  9 15:35:16.893: INFO: Waiting up to 5m0s for pod "client-containers-a9304871-aa25-415a-a646-010a9cf3f792" in namespace "containers-5611" to be "Succeeded or Failed"
    May  9 15:35:16.900: INFO: Pod "client-containers-a9304871-aa25-415a-a646-010a9cf3f792": Phase="Pending", Reason="", readiness=false. Elapsed: 7.101373ms
    May  9 15:35:18.905: INFO: Pod "client-containers-a9304871-aa25-415a-a646-010a9cf3f792": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011514316s
    May  9 15:35:20.906: INFO: Pod "client-containers-a9304871-aa25-415a-a646-010a9cf3f792": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012568774s
    May  9 15:35:22.908: INFO: Pod "client-containers-a9304871-aa25-415a-a646-010a9cf3f792": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014575818s
    STEP: Saw pod success 05/09/23 15:35:22.908
    May  9 15:35:22.908: INFO: Pod "client-containers-a9304871-aa25-415a-a646-010a9cf3f792" satisfied condition "Succeeded or Failed"
    May  9 15:35:22.914: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod client-containers-a9304871-aa25-415a-a646-010a9cf3f792 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 15:35:22.972
    May  9 15:35:22.990: INFO: Waiting for pod client-containers-a9304871-aa25-415a-a646-010a9cf3f792 to disappear
    May  9 15:35:22.994: INFO: Pod client-containers-a9304871-aa25-415a-a646-010a9cf3f792 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    May  9 15:35:22.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-5611" for this suite. 05/09/23 15:35:23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:35:23.013
May  9 15:35:23.013: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename webhook 05/09/23 15:35:23.014
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:35:23.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:35:23.033
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/09/23 15:35:23.064
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 15:35:23.723
STEP: Deploying the webhook pod 05/09/23 15:35:23.735
STEP: Wait for the deployment to be ready 05/09/23 15:35:23.749
May  9 15:35:23.756: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 15:35:25.773
STEP: Verifying the service has paired with the endpoint 05/09/23 15:35:25.789
May  9 15:35:26.790: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 05/09/23 15:35:26.796
STEP: create a pod 05/09/23 15:35:26.816
May  9 15:35:26.824: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2151" to be "running"
May  9 15:35:26.829: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.727045ms
May  9 15:35:28.836: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011777694s
May  9 15:35:28.836: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 05/09/23 15:35:28.836
May  9 15:35:28.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=webhook-2151 attach --namespace=webhook-2151 to-be-attached-pod -i -c=container1'
May  9 15:35:28.952: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 15:35:28.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2151" for this suite. 05/09/23 15:35:29.012
STEP: Destroying namespace "webhook-2151-markers" for this suite. 05/09/23 15:35:29.018
------------------------------
â€¢ [SLOW TEST] [6.015 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:35:23.013
    May  9 15:35:23.013: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename webhook 05/09/23 15:35:23.014
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:35:23.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:35:23.033
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/09/23 15:35:23.064
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 15:35:23.723
    STEP: Deploying the webhook pod 05/09/23 15:35:23.735
    STEP: Wait for the deployment to be ready 05/09/23 15:35:23.749
    May  9 15:35:23.756: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 15:35:25.773
    STEP: Verifying the service has paired with the endpoint 05/09/23 15:35:25.789
    May  9 15:35:26.790: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 05/09/23 15:35:26.796
    STEP: create a pod 05/09/23 15:35:26.816
    May  9 15:35:26.824: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2151" to be "running"
    May  9 15:35:26.829: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.727045ms
    May  9 15:35:28.836: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011777694s
    May  9 15:35:28.836: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 05/09/23 15:35:28.836
    May  9 15:35:28.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=webhook-2151 attach --namespace=webhook-2151 to-be-attached-pod -i -c=container1'
    May  9 15:35:28.952: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 15:35:28.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2151" for this suite. 05/09/23 15:35:29.012
    STEP: Destroying namespace "webhook-2151-markers" for this suite. 05/09/23 15:35:29.018
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:35:29.028
May  9 15:35:29.028: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 15:35:29.029
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:35:29.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:35:29.048
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 05/09/23 15:35:29.052
May  9 15:35:29.069: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83bd3442-17e6-4848-a07e-581d1c8b0ba0" in namespace "projected-4236" to be "Succeeded or Failed"
May  9 15:35:29.073: INFO: Pod "downwardapi-volume-83bd3442-17e6-4848-a07e-581d1c8b0ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.784173ms
May  9 15:35:31.080: INFO: Pod "downwardapi-volume-83bd3442-17e6-4848-a07e-581d1c8b0ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010815046s
May  9 15:35:33.079: INFO: Pod "downwardapi-volume-83bd3442-17e6-4848-a07e-581d1c8b0ba0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009924998s
STEP: Saw pod success 05/09/23 15:35:33.079
May  9 15:35:33.079: INFO: Pod "downwardapi-volume-83bd3442-17e6-4848-a07e-581d1c8b0ba0" satisfied condition "Succeeded or Failed"
May  9 15:35:33.084: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-83bd3442-17e6-4848-a07e-581d1c8b0ba0 container client-container: <nil>
STEP: delete the pod 05/09/23 15:35:33.095
May  9 15:35:33.113: INFO: Waiting for pod downwardapi-volume-83bd3442-17e6-4848-a07e-581d1c8b0ba0 to disappear
May  9 15:35:33.119: INFO: Pod downwardapi-volume-83bd3442-17e6-4848-a07e-581d1c8b0ba0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  9 15:35:33.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4236" for this suite. 05/09/23 15:35:33.129
------------------------------
â€¢ [4.109 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:35:29.028
    May  9 15:35:29.028: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 15:35:29.029
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:35:29.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:35:29.048
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 05/09/23 15:35:29.052
    May  9 15:35:29.069: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83bd3442-17e6-4848-a07e-581d1c8b0ba0" in namespace "projected-4236" to be "Succeeded or Failed"
    May  9 15:35:29.073: INFO: Pod "downwardapi-volume-83bd3442-17e6-4848-a07e-581d1c8b0ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.784173ms
    May  9 15:35:31.080: INFO: Pod "downwardapi-volume-83bd3442-17e6-4848-a07e-581d1c8b0ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010815046s
    May  9 15:35:33.079: INFO: Pod "downwardapi-volume-83bd3442-17e6-4848-a07e-581d1c8b0ba0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009924998s
    STEP: Saw pod success 05/09/23 15:35:33.079
    May  9 15:35:33.079: INFO: Pod "downwardapi-volume-83bd3442-17e6-4848-a07e-581d1c8b0ba0" satisfied condition "Succeeded or Failed"
    May  9 15:35:33.084: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-83bd3442-17e6-4848-a07e-581d1c8b0ba0 container client-container: <nil>
    STEP: delete the pod 05/09/23 15:35:33.095
    May  9 15:35:33.113: INFO: Waiting for pod downwardapi-volume-83bd3442-17e6-4848-a07e-581d1c8b0ba0 to disappear
    May  9 15:35:33.119: INFO: Pod downwardapi-volume-83bd3442-17e6-4848-a07e-581d1c8b0ba0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  9 15:35:33.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4236" for this suite. 05/09/23 15:35:33.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:35:33.138
May  9 15:35:33.138: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename endpointslice 05/09/23 15:35:33.139
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:35:33.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:35:33.167
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 05/09/23 15:35:38.248
STEP: referencing matching pods with named port 05/09/23 15:35:43.26
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 05/09/23 15:35:48.272
STEP: recreating EndpointSlices after they've been deleted 05/09/23 15:35:53.283
May  9 15:35:53.322: INFO: EndpointSlice for Service endpointslice-434/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
May  9 15:36:03.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-434" for this suite. 05/09/23 15:36:03.342
------------------------------
â€¢ [SLOW TEST] [30.213 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:35:33.138
    May  9 15:35:33.138: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename endpointslice 05/09/23 15:35:33.139
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:35:33.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:35:33.167
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 05/09/23 15:35:38.248
    STEP: referencing matching pods with named port 05/09/23 15:35:43.26
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 05/09/23 15:35:48.272
    STEP: recreating EndpointSlices after they've been deleted 05/09/23 15:35:53.283
    May  9 15:35:53.322: INFO: EndpointSlice for Service endpointslice-434/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    May  9 15:36:03.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-434" for this suite. 05/09/23 15:36:03.342
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:36:03.352
May  9 15:36:03.352: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename pods 05/09/23 15:36:03.353
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:03.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:03.377
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
May  9 15:36:03.382: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: creating the pod 05/09/23 15:36:03.383
STEP: submitting the pod to kubernetes 05/09/23 15:36:03.383
May  9 15:36:03.392: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-e55bce03-01c9-4098-b573-7674bc9c262e" in namespace "pods-5143" to be "running and ready"
May  9 15:36:03.398: INFO: Pod "pod-logs-websocket-e55bce03-01c9-4098-b573-7674bc9c262e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.797804ms
May  9 15:36:03.398: INFO: The phase of Pod pod-logs-websocket-e55bce03-01c9-4098-b573-7674bc9c262e is Pending, waiting for it to be Running (with Ready = true)
May  9 15:36:05.405: INFO: Pod "pod-logs-websocket-e55bce03-01c9-4098-b573-7674bc9c262e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011499598s
May  9 15:36:05.405: INFO: The phase of Pod pod-logs-websocket-e55bce03-01c9-4098-b573-7674bc9c262e is Pending, waiting for it to be Running (with Ready = true)
May  9 15:36:07.405: INFO: Pod "pod-logs-websocket-e55bce03-01c9-4098-b573-7674bc9c262e": Phase="Running", Reason="", readiness=true. Elapsed: 4.011669105s
May  9 15:36:07.405: INFO: The phase of Pod pod-logs-websocket-e55bce03-01c9-4098-b573-7674bc9c262e is Running (Ready = true)
May  9 15:36:07.405: INFO: Pod "pod-logs-websocket-e55bce03-01c9-4098-b573-7674bc9c262e" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  9 15:36:07.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5143" for this suite. 05/09/23 15:36:07.489
------------------------------
â€¢ [4.147 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:36:03.352
    May  9 15:36:03.352: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename pods 05/09/23 15:36:03.353
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:03.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:03.377
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    May  9 15:36:03.382: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: creating the pod 05/09/23 15:36:03.383
    STEP: submitting the pod to kubernetes 05/09/23 15:36:03.383
    May  9 15:36:03.392: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-e55bce03-01c9-4098-b573-7674bc9c262e" in namespace "pods-5143" to be "running and ready"
    May  9 15:36:03.398: INFO: Pod "pod-logs-websocket-e55bce03-01c9-4098-b573-7674bc9c262e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.797804ms
    May  9 15:36:03.398: INFO: The phase of Pod pod-logs-websocket-e55bce03-01c9-4098-b573-7674bc9c262e is Pending, waiting for it to be Running (with Ready = true)
    May  9 15:36:05.405: INFO: Pod "pod-logs-websocket-e55bce03-01c9-4098-b573-7674bc9c262e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011499598s
    May  9 15:36:05.405: INFO: The phase of Pod pod-logs-websocket-e55bce03-01c9-4098-b573-7674bc9c262e is Pending, waiting for it to be Running (with Ready = true)
    May  9 15:36:07.405: INFO: Pod "pod-logs-websocket-e55bce03-01c9-4098-b573-7674bc9c262e": Phase="Running", Reason="", readiness=true. Elapsed: 4.011669105s
    May  9 15:36:07.405: INFO: The phase of Pod pod-logs-websocket-e55bce03-01c9-4098-b573-7674bc9c262e is Running (Ready = true)
    May  9 15:36:07.405: INFO: Pod "pod-logs-websocket-e55bce03-01c9-4098-b573-7674bc9c262e" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  9 15:36:07.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5143" for this suite. 05/09/23 15:36:07.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:36:07.505
May  9 15:36:07.506: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename security-context 05/09/23 15:36:07.507
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:07.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:07.528
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/09/23 15:36:07.532
May  9 15:36:07.544: INFO: Waiting up to 5m0s for pod "security-context-f640964d-baf6-4507-b97f-3f5779da35d6" in namespace "security-context-9673" to be "Succeeded or Failed"
May  9 15:36:07.550: INFO: Pod "security-context-f640964d-baf6-4507-b97f-3f5779da35d6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.684043ms
May  9 15:36:09.557: INFO: Pod "security-context-f640964d-baf6-4507-b97f-3f5779da35d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012528804s
May  9 15:36:11.560: INFO: Pod "security-context-f640964d-baf6-4507-b97f-3f5779da35d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015271778s
STEP: Saw pod success 05/09/23 15:36:11.56
May  9 15:36:11.560: INFO: Pod "security-context-f640964d-baf6-4507-b97f-3f5779da35d6" satisfied condition "Succeeded or Failed"
May  9 15:36:11.572: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod security-context-f640964d-baf6-4507-b97f-3f5779da35d6 container test-container: <nil>
STEP: delete the pod 05/09/23 15:36:11.616
May  9 15:36:11.635: INFO: Waiting for pod security-context-f640964d-baf6-4507-b97f-3f5779da35d6 to disappear
May  9 15:36:11.640: INFO: Pod security-context-f640964d-baf6-4507-b97f-3f5779da35d6 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May  9 15:36:11.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-9673" for this suite. 05/09/23 15:36:11.647
------------------------------
â€¢ [4.154 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:36:07.505
    May  9 15:36:07.506: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename security-context 05/09/23 15:36:07.507
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:07.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:07.528
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/09/23 15:36:07.532
    May  9 15:36:07.544: INFO: Waiting up to 5m0s for pod "security-context-f640964d-baf6-4507-b97f-3f5779da35d6" in namespace "security-context-9673" to be "Succeeded or Failed"
    May  9 15:36:07.550: INFO: Pod "security-context-f640964d-baf6-4507-b97f-3f5779da35d6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.684043ms
    May  9 15:36:09.557: INFO: Pod "security-context-f640964d-baf6-4507-b97f-3f5779da35d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012528804s
    May  9 15:36:11.560: INFO: Pod "security-context-f640964d-baf6-4507-b97f-3f5779da35d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015271778s
    STEP: Saw pod success 05/09/23 15:36:11.56
    May  9 15:36:11.560: INFO: Pod "security-context-f640964d-baf6-4507-b97f-3f5779da35d6" satisfied condition "Succeeded or Failed"
    May  9 15:36:11.572: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod security-context-f640964d-baf6-4507-b97f-3f5779da35d6 container test-container: <nil>
    STEP: delete the pod 05/09/23 15:36:11.616
    May  9 15:36:11.635: INFO: Waiting for pod security-context-f640964d-baf6-4507-b97f-3f5779da35d6 to disappear
    May  9 15:36:11.640: INFO: Pod security-context-f640964d-baf6-4507-b97f-3f5779da35d6 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May  9 15:36:11.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-9673" for this suite. 05/09/23 15:36:11.647
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:36:11.66
May  9 15:36:11.660: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 15:36:11.661
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:11.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:11.683
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-427270d0-891d-4122-956c-c8407fc2fd91 05/09/23 15:36:11.688
STEP: Creating a pod to test consume secrets 05/09/23 15:36:11.693
May  9 15:36:11.704: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f7b460a6-01de-4e60-8c42-b32dff10ef79" in namespace "projected-4423" to be "Succeeded or Failed"
May  9 15:36:11.708: INFO: Pod "pod-projected-secrets-f7b460a6-01de-4e60-8c42-b32dff10ef79": Phase="Pending", Reason="", readiness=false. Elapsed: 4.384523ms
May  9 15:36:13.715: INFO: Pod "pod-projected-secrets-f7b460a6-01de-4e60-8c42-b32dff10ef79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01115839s
May  9 15:36:15.718: INFO: Pod "pod-projected-secrets-f7b460a6-01de-4e60-8c42-b32dff10ef79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013712638s
STEP: Saw pod success 05/09/23 15:36:15.718
May  9 15:36:15.718: INFO: Pod "pod-projected-secrets-f7b460a6-01de-4e60-8c42-b32dff10ef79" satisfied condition "Succeeded or Failed"
May  9 15:36:15.723: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-secrets-f7b460a6-01de-4e60-8c42-b32dff10ef79 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/09/23 15:36:15.735
May  9 15:36:15.753: INFO: Waiting for pod pod-projected-secrets-f7b460a6-01de-4e60-8c42-b32dff10ef79 to disappear
May  9 15:36:15.758: INFO: Pod pod-projected-secrets-f7b460a6-01de-4e60-8c42-b32dff10ef79 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May  9 15:36:15.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4423" for this suite. 05/09/23 15:36:15.763
------------------------------
â€¢ [4.112 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:36:11.66
    May  9 15:36:11.660: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 15:36:11.661
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:11.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:11.683
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-427270d0-891d-4122-956c-c8407fc2fd91 05/09/23 15:36:11.688
    STEP: Creating a pod to test consume secrets 05/09/23 15:36:11.693
    May  9 15:36:11.704: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f7b460a6-01de-4e60-8c42-b32dff10ef79" in namespace "projected-4423" to be "Succeeded or Failed"
    May  9 15:36:11.708: INFO: Pod "pod-projected-secrets-f7b460a6-01de-4e60-8c42-b32dff10ef79": Phase="Pending", Reason="", readiness=false. Elapsed: 4.384523ms
    May  9 15:36:13.715: INFO: Pod "pod-projected-secrets-f7b460a6-01de-4e60-8c42-b32dff10ef79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01115839s
    May  9 15:36:15.718: INFO: Pod "pod-projected-secrets-f7b460a6-01de-4e60-8c42-b32dff10ef79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013712638s
    STEP: Saw pod success 05/09/23 15:36:15.718
    May  9 15:36:15.718: INFO: Pod "pod-projected-secrets-f7b460a6-01de-4e60-8c42-b32dff10ef79" satisfied condition "Succeeded or Failed"
    May  9 15:36:15.723: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-secrets-f7b460a6-01de-4e60-8c42-b32dff10ef79 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 15:36:15.735
    May  9 15:36:15.753: INFO: Waiting for pod pod-projected-secrets-f7b460a6-01de-4e60-8c42-b32dff10ef79 to disappear
    May  9 15:36:15.758: INFO: Pod pod-projected-secrets-f7b460a6-01de-4e60-8c42-b32dff10ef79 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May  9 15:36:15.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4423" for this suite. 05/09/23 15:36:15.763
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:36:15.773
May  9 15:36:15.773: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 15:36:15.774
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:15.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:15.796
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-f4e47290-eed6-4fdc-83a4-8c80eb1c4d5a 05/09/23 15:36:15.801
STEP: Creating a pod to test consume configMaps 05/09/23 15:36:15.807
May  9 15:36:15.817: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-00b157a2-84a6-418f-9c8a-98901bdc3bb2" in namespace "projected-1725" to be "Succeeded or Failed"
May  9 15:36:15.824: INFO: Pod "pod-projected-configmaps-00b157a2-84a6-418f-9c8a-98901bdc3bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054253ms
May  9 15:36:17.829: INFO: Pod "pod-projected-configmaps-00b157a2-84a6-418f-9c8a-98901bdc3bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011941789s
May  9 15:36:19.831: INFO: Pod "pod-projected-configmaps-00b157a2-84a6-418f-9c8a-98901bdc3bb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013530061s
STEP: Saw pod success 05/09/23 15:36:19.831
May  9 15:36:19.831: INFO: Pod "pod-projected-configmaps-00b157a2-84a6-418f-9c8a-98901bdc3bb2" satisfied condition "Succeeded or Failed"
May  9 15:36:19.836: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-configmaps-00b157a2-84a6-418f-9c8a-98901bdc3bb2 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 15:36:19.845
May  9 15:36:19.860: INFO: Waiting for pod pod-projected-configmaps-00b157a2-84a6-418f-9c8a-98901bdc3bb2 to disappear
May  9 15:36:19.864: INFO: Pod pod-projected-configmaps-00b157a2-84a6-418f-9c8a-98901bdc3bb2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May  9 15:36:19.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1725" for this suite. 05/09/23 15:36:19.871
------------------------------
â€¢ [4.106 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:36:15.773
    May  9 15:36:15.773: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 15:36:15.774
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:15.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:15.796
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-f4e47290-eed6-4fdc-83a4-8c80eb1c4d5a 05/09/23 15:36:15.801
    STEP: Creating a pod to test consume configMaps 05/09/23 15:36:15.807
    May  9 15:36:15.817: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-00b157a2-84a6-418f-9c8a-98901bdc3bb2" in namespace "projected-1725" to be "Succeeded or Failed"
    May  9 15:36:15.824: INFO: Pod "pod-projected-configmaps-00b157a2-84a6-418f-9c8a-98901bdc3bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054253ms
    May  9 15:36:17.829: INFO: Pod "pod-projected-configmaps-00b157a2-84a6-418f-9c8a-98901bdc3bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011941789s
    May  9 15:36:19.831: INFO: Pod "pod-projected-configmaps-00b157a2-84a6-418f-9c8a-98901bdc3bb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013530061s
    STEP: Saw pod success 05/09/23 15:36:19.831
    May  9 15:36:19.831: INFO: Pod "pod-projected-configmaps-00b157a2-84a6-418f-9c8a-98901bdc3bb2" satisfied condition "Succeeded or Failed"
    May  9 15:36:19.836: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-configmaps-00b157a2-84a6-418f-9c8a-98901bdc3bb2 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 15:36:19.845
    May  9 15:36:19.860: INFO: Waiting for pod pod-projected-configmaps-00b157a2-84a6-418f-9c8a-98901bdc3bb2 to disappear
    May  9 15:36:19.864: INFO: Pod pod-projected-configmaps-00b157a2-84a6-418f-9c8a-98901bdc3bb2 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May  9 15:36:19.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1725" for this suite. 05/09/23 15:36:19.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:36:19.88
May  9 15:36:19.880: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename gc 05/09/23 15:36:19.881
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:19.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:19.9
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
May  9 15:36:19.939: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"93eb6d4b-ddf3-4f15-8c79-63d17a0e97a5", Controller:(*bool)(0xc004bb367a), BlockOwnerDeletion:(*bool)(0xc004bb367b)}}
May  9 15:36:19.949: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4a596e44-ad86-4d39-b514-ecd82efe9a78", Controller:(*bool)(0xc00491d94a), BlockOwnerDeletion:(*bool)(0xc00491d94b)}}
May  9 15:36:19.961: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2823433b-168e-4b01-8bf5-6834653389b5", Controller:(*bool)(0xc00491db6a), BlockOwnerDeletion:(*bool)(0xc00491db6b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May  9 15:36:24.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9578" for this suite. 05/09/23 15:36:24.994
------------------------------
â€¢ [SLOW TEST] [5.122 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:36:19.88
    May  9 15:36:19.880: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename gc 05/09/23 15:36:19.881
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:19.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:19.9
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    May  9 15:36:19.939: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"93eb6d4b-ddf3-4f15-8c79-63d17a0e97a5", Controller:(*bool)(0xc004bb367a), BlockOwnerDeletion:(*bool)(0xc004bb367b)}}
    May  9 15:36:19.949: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4a596e44-ad86-4d39-b514-ecd82efe9a78", Controller:(*bool)(0xc00491d94a), BlockOwnerDeletion:(*bool)(0xc00491d94b)}}
    May  9 15:36:19.961: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2823433b-168e-4b01-8bf5-6834653389b5", Controller:(*bool)(0xc00491db6a), BlockOwnerDeletion:(*bool)(0xc00491db6b)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May  9 15:36:24.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9578" for this suite. 05/09/23 15:36:24.994
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:36:25.004
May  9 15:36:25.004: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename podtemplate 05/09/23 15:36:25.005
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:25.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:25.027
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 05/09/23 15:36:25.037
STEP: Replace a pod template 05/09/23 15:36:25.045
May  9 15:36:25.056: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
May  9 15:36:25.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-2992" for this suite. 05/09/23 15:36:25.063
------------------------------
â€¢ [0.068 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:36:25.004
    May  9 15:36:25.004: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename podtemplate 05/09/23 15:36:25.005
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:25.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:25.027
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 05/09/23 15:36:25.037
    STEP: Replace a pod template 05/09/23 15:36:25.045
    May  9 15:36:25.056: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    May  9 15:36:25.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-2992" for this suite. 05/09/23 15:36:25.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:36:25.072
May  9 15:36:25.072: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename namespaces 05/09/23 15:36:25.074
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:25.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:25.097
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 05/09/23 15:36:25.101
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:25.116
STEP: Creating a service in the namespace 05/09/23 15:36:25.122
STEP: Deleting the namespace 05/09/23 15:36:25.134
STEP: Waiting for the namespace to be removed. 05/09/23 15:36:25.152
STEP: Recreating the namespace 05/09/23 15:36:31.159
STEP: Verifying there is no service in the namespace 05/09/23 15:36:31.389
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 15:36:31.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2177" for this suite. 05/09/23 15:36:31.415
STEP: Destroying namespace "nsdeletetest-1967" for this suite. 05/09/23 15:36:31.428
May  9 15:36:31.434: INFO: Namespace nsdeletetest-1967 was already deleted
STEP: Destroying namespace "nsdeletetest-2611" for this suite. 05/09/23 15:36:31.434
------------------------------
â€¢ [SLOW TEST] [6.371 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:36:25.072
    May  9 15:36:25.072: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename namespaces 05/09/23 15:36:25.074
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:25.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:25.097
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 05/09/23 15:36:25.101
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:25.116
    STEP: Creating a service in the namespace 05/09/23 15:36:25.122
    STEP: Deleting the namespace 05/09/23 15:36:25.134
    STEP: Waiting for the namespace to be removed. 05/09/23 15:36:25.152
    STEP: Recreating the namespace 05/09/23 15:36:31.159
    STEP: Verifying there is no service in the namespace 05/09/23 15:36:31.389
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 15:36:31.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2177" for this suite. 05/09/23 15:36:31.415
    STEP: Destroying namespace "nsdeletetest-1967" for this suite. 05/09/23 15:36:31.428
    May  9 15:36:31.434: INFO: Namespace nsdeletetest-1967 was already deleted
    STEP: Destroying namespace "nsdeletetest-2611" for this suite. 05/09/23 15:36:31.434
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:36:31.445
May  9 15:36:31.445: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubelet-test 05/09/23 15:36:31.446
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:31.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:31.468
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May  9 15:36:35.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4905" for this suite. 05/09/23 15:36:35.516
------------------------------
â€¢ [4.080 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:36:31.445
    May  9 15:36:31.445: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubelet-test 05/09/23 15:36:31.446
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:31.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:31.468
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May  9 15:36:35.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4905" for this suite. 05/09/23 15:36:35.516
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:36:35.526
May  9 15:36:35.526: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename replicaset 05/09/23 15:36:35.527
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:35.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:35.598
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 05/09/23 15:36:35.602
May  9 15:36:35.613: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9834" to be "running and ready"
May  9 15:36:35.618: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.520698ms
May  9 15:36:35.618: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May  9 15:36:37.625: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.011594608s
May  9 15:36:37.625: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
May  9 15:36:37.625: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 05/09/23 15:36:37.63
STEP: Then the orphan pod is adopted 05/09/23 15:36:37.638
STEP: When the matched label of one of its pods change 05/09/23 15:36:38.654
May  9 15:36:38.663: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 05/09/23 15:36:38.687
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May  9 15:36:39.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9834" for this suite. 05/09/23 15:36:39.706
------------------------------
â€¢ [4.201 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:36:35.526
    May  9 15:36:35.526: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename replicaset 05/09/23 15:36:35.527
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:35.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:35.598
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 05/09/23 15:36:35.602
    May  9 15:36:35.613: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9834" to be "running and ready"
    May  9 15:36:35.618: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.520698ms
    May  9 15:36:35.618: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    May  9 15:36:37.625: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.011594608s
    May  9 15:36:37.625: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    May  9 15:36:37.625: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 05/09/23 15:36:37.63
    STEP: Then the orphan pod is adopted 05/09/23 15:36:37.638
    STEP: When the matched label of one of its pods change 05/09/23 15:36:38.654
    May  9 15:36:38.663: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 05/09/23 15:36:38.687
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May  9 15:36:39.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9834" for this suite. 05/09/23 15:36:39.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:36:39.727
May  9 15:36:39.727: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 15:36:39.728
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:39.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:39.754
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-d17044c5-0586-4d3d-a593-0d7e7238bf1c 05/09/23 15:36:39.759
STEP: Creating a pod to test consume configMaps 05/09/23 15:36:39.766
May  9 15:36:39.776: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-aafb4072-a7be-4162-adfd-a261ae53e58a" in namespace "projected-3092" to be "Succeeded or Failed"
May  9 15:36:39.784: INFO: Pod "pod-projected-configmaps-aafb4072-a7be-4162-adfd-a261ae53e58a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.74659ms
May  9 15:36:41.794: INFO: Pod "pod-projected-configmaps-aafb4072-a7be-4162-adfd-a261ae53e58a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018279668s
May  9 15:36:43.790: INFO: Pod "pod-projected-configmaps-aafb4072-a7be-4162-adfd-a261ae53e58a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013702057s
STEP: Saw pod success 05/09/23 15:36:43.79
May  9 15:36:43.790: INFO: Pod "pod-projected-configmaps-aafb4072-a7be-4162-adfd-a261ae53e58a" satisfied condition "Succeeded or Failed"
May  9 15:36:43.796: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod pod-projected-configmaps-aafb4072-a7be-4162-adfd-a261ae53e58a container projected-configmap-volume-test: <nil>
STEP: delete the pod 05/09/23 15:36:43.808
May  9 15:36:43.827: INFO: Waiting for pod pod-projected-configmaps-aafb4072-a7be-4162-adfd-a261ae53e58a to disappear
May  9 15:36:43.835: INFO: Pod pod-projected-configmaps-aafb4072-a7be-4162-adfd-a261ae53e58a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May  9 15:36:43.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3092" for this suite. 05/09/23 15:36:43.843
------------------------------
â€¢ [4.124 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:36:39.727
    May  9 15:36:39.727: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 15:36:39.728
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:39.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:39.754
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-d17044c5-0586-4d3d-a593-0d7e7238bf1c 05/09/23 15:36:39.759
    STEP: Creating a pod to test consume configMaps 05/09/23 15:36:39.766
    May  9 15:36:39.776: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-aafb4072-a7be-4162-adfd-a261ae53e58a" in namespace "projected-3092" to be "Succeeded or Failed"
    May  9 15:36:39.784: INFO: Pod "pod-projected-configmaps-aafb4072-a7be-4162-adfd-a261ae53e58a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.74659ms
    May  9 15:36:41.794: INFO: Pod "pod-projected-configmaps-aafb4072-a7be-4162-adfd-a261ae53e58a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018279668s
    May  9 15:36:43.790: INFO: Pod "pod-projected-configmaps-aafb4072-a7be-4162-adfd-a261ae53e58a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013702057s
    STEP: Saw pod success 05/09/23 15:36:43.79
    May  9 15:36:43.790: INFO: Pod "pod-projected-configmaps-aafb4072-a7be-4162-adfd-a261ae53e58a" satisfied condition "Succeeded or Failed"
    May  9 15:36:43.796: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod pod-projected-configmaps-aafb4072-a7be-4162-adfd-a261ae53e58a container projected-configmap-volume-test: <nil>
    STEP: delete the pod 05/09/23 15:36:43.808
    May  9 15:36:43.827: INFO: Waiting for pod pod-projected-configmaps-aafb4072-a7be-4162-adfd-a261ae53e58a to disappear
    May  9 15:36:43.835: INFO: Pod pod-projected-configmaps-aafb4072-a7be-4162-adfd-a261ae53e58a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May  9 15:36:43.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3092" for this suite. 05/09/23 15:36:43.843
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:36:43.852
May  9 15:36:43.852: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename replicaset 05/09/23 15:36:43.853
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:43.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:43.875
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 05/09/23 15:36:43.879
STEP: Verify that the required pods have come up 05/09/23 15:36:43.892
May  9 15:36:43.895: INFO: Pod name sample-pod: Found 0 pods out of 3
May  9 15:36:48.904: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 05/09/23 15:36:48.904
May  9 15:36:48.909: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 05/09/23 15:36:48.909
STEP: DeleteCollection of the ReplicaSets 05/09/23 15:36:48.914
STEP: After DeleteCollection verify that ReplicaSets have been deleted 05/09/23 15:36:48.925
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May  9 15:36:48.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9368" for this suite. 05/09/23 15:36:48.947
------------------------------
â€¢ [SLOW TEST] [5.108 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:36:43.852
    May  9 15:36:43.852: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename replicaset 05/09/23 15:36:43.853
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:43.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:43.875
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 05/09/23 15:36:43.879
    STEP: Verify that the required pods have come up 05/09/23 15:36:43.892
    May  9 15:36:43.895: INFO: Pod name sample-pod: Found 0 pods out of 3
    May  9 15:36:48.904: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 05/09/23 15:36:48.904
    May  9 15:36:48.909: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 05/09/23 15:36:48.909
    STEP: DeleteCollection of the ReplicaSets 05/09/23 15:36:48.914
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 05/09/23 15:36:48.925
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May  9 15:36:48.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9368" for this suite. 05/09/23 15:36:48.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:36:48.961
May  9 15:36:48.961: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename emptydir 05/09/23 15:36:48.962
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:48.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:48.986
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 05/09/23 15:36:48.99
May  9 15:36:49.000: INFO: Waiting up to 5m0s for pod "pod-7cb7449d-a596-4041-b264-9912fbc2539f" in namespace "emptydir-5819" to be "Succeeded or Failed"
May  9 15:36:49.005: INFO: Pod "pod-7cb7449d-a596-4041-b264-9912fbc2539f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.775018ms
May  9 15:36:51.013: INFO: Pod "pod-7cb7449d-a596-4041-b264-9912fbc2539f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013007454s
May  9 15:36:53.012: INFO: Pod "pod-7cb7449d-a596-4041-b264-9912fbc2539f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011441213s
STEP: Saw pod success 05/09/23 15:36:53.012
May  9 15:36:53.012: INFO: Pod "pod-7cb7449d-a596-4041-b264-9912fbc2539f" satisfied condition "Succeeded or Failed"
May  9 15:36:53.016: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-7cb7449d-a596-4041-b264-9912fbc2539f container test-container: <nil>
STEP: delete the pod 05/09/23 15:36:53.03
May  9 15:36:53.056: INFO: Waiting for pod pod-7cb7449d-a596-4041-b264-9912fbc2539f to disappear
May  9 15:36:53.060: INFO: Pod pod-7cb7449d-a596-4041-b264-9912fbc2539f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  9 15:36:53.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5819" for this suite. 05/09/23 15:36:53.07
------------------------------
â€¢ [4.117 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:36:48.961
    May  9 15:36:48.961: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename emptydir 05/09/23 15:36:48.962
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:48.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:48.986
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 05/09/23 15:36:48.99
    May  9 15:36:49.000: INFO: Waiting up to 5m0s for pod "pod-7cb7449d-a596-4041-b264-9912fbc2539f" in namespace "emptydir-5819" to be "Succeeded or Failed"
    May  9 15:36:49.005: INFO: Pod "pod-7cb7449d-a596-4041-b264-9912fbc2539f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.775018ms
    May  9 15:36:51.013: INFO: Pod "pod-7cb7449d-a596-4041-b264-9912fbc2539f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013007454s
    May  9 15:36:53.012: INFO: Pod "pod-7cb7449d-a596-4041-b264-9912fbc2539f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011441213s
    STEP: Saw pod success 05/09/23 15:36:53.012
    May  9 15:36:53.012: INFO: Pod "pod-7cb7449d-a596-4041-b264-9912fbc2539f" satisfied condition "Succeeded or Failed"
    May  9 15:36:53.016: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-7cb7449d-a596-4041-b264-9912fbc2539f container test-container: <nil>
    STEP: delete the pod 05/09/23 15:36:53.03
    May  9 15:36:53.056: INFO: Waiting for pod pod-7cb7449d-a596-4041-b264-9912fbc2539f to disappear
    May  9 15:36:53.060: INFO: Pod pod-7cb7449d-a596-4041-b264-9912fbc2539f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  9 15:36:53.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5819" for this suite. 05/09/23 15:36:53.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:36:53.08
May  9 15:36:53.080: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename events 05/09/23 15:36:53.081
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:53.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:53.102
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 05/09/23 15:36:53.107
May  9 15:36:53.113: INFO: created test-event-1
May  9 15:36:53.120: INFO: created test-event-2
May  9 15:36:53.126: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 05/09/23 15:36:53.126
STEP: delete collection of events 05/09/23 15:36:53.13
May  9 15:36:53.130: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 05/09/23 15:36:53.159
May  9 15:36:53.159: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
May  9 15:36:53.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-8657" for this suite. 05/09/23 15:36:53.17
------------------------------
â€¢ [0.097 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:36:53.08
    May  9 15:36:53.080: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename events 05/09/23 15:36:53.081
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:53.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:53.102
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 05/09/23 15:36:53.107
    May  9 15:36:53.113: INFO: created test-event-1
    May  9 15:36:53.120: INFO: created test-event-2
    May  9 15:36:53.126: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 05/09/23 15:36:53.126
    STEP: delete collection of events 05/09/23 15:36:53.13
    May  9 15:36:53.130: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 05/09/23 15:36:53.159
    May  9 15:36:53.159: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    May  9 15:36:53.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-8657" for this suite. 05/09/23 15:36:53.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:36:53.179
May  9 15:36:53.179: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename proxy 05/09/23 15:36:53.179
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:53.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:53.2
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
May  9 15:36:53.204: INFO: Creating pod...
May  9 15:36:53.216: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8399" to be "running"
May  9 15:36:53.221: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.522096ms
May  9 15:36:55.227: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.011151011s
May  9 15:36:55.227: INFO: Pod "agnhost" satisfied condition "running"
May  9 15:36:55.227: INFO: Creating service...
May  9 15:36:55.247: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/pods/agnhost/proxy?method=DELETE
May  9 15:36:55.261: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  9 15:36:55.261: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/pods/agnhost/proxy?method=OPTIONS
May  9 15:36:55.269: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  9 15:36:55.269: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/pods/agnhost/proxy?method=PATCH
May  9 15:36:55.276: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  9 15:36:55.276: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/pods/agnhost/proxy?method=POST
May  9 15:36:55.282: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  9 15:36:55.282: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/pods/agnhost/proxy?method=PUT
May  9 15:36:55.289: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May  9 15:36:55.289: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/services/e2e-proxy-test-service/proxy?method=DELETE
May  9 15:36:55.330: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  9 15:36:55.330: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/services/e2e-proxy-test-service/proxy?method=OPTIONS
May  9 15:36:55.341: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  9 15:36:55.341: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/services/e2e-proxy-test-service/proxy?method=PATCH
May  9 15:36:55.352: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  9 15:36:55.352: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/services/e2e-proxy-test-service/proxy?method=POST
May  9 15:36:55.364: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  9 15:36:55.364: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/services/e2e-proxy-test-service/proxy?method=PUT
May  9 15:36:55.373: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May  9 15:36:55.373: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/pods/agnhost/proxy?method=GET
May  9 15:36:55.383: INFO: http.Client request:GET StatusCode:301
May  9 15:36:55.383: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/services/e2e-proxy-test-service/proxy?method=GET
May  9 15:36:55.423: INFO: http.Client request:GET StatusCode:301
May  9 15:36:55.423: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/pods/agnhost/proxy?method=HEAD
May  9 15:36:55.436: INFO: http.Client request:HEAD StatusCode:301
May  9 15:36:55.436: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/services/e2e-proxy-test-service/proxy?method=HEAD
May  9 15:36:55.444: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
May  9 15:36:55.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8399" for this suite. 05/09/23 15:36:55.451
------------------------------
â€¢ [2.282 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:36:53.179
    May  9 15:36:53.179: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename proxy 05/09/23 15:36:53.179
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:53.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:53.2
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    May  9 15:36:53.204: INFO: Creating pod...
    May  9 15:36:53.216: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8399" to be "running"
    May  9 15:36:53.221: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.522096ms
    May  9 15:36:55.227: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.011151011s
    May  9 15:36:55.227: INFO: Pod "agnhost" satisfied condition "running"
    May  9 15:36:55.227: INFO: Creating service...
    May  9 15:36:55.247: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/pods/agnhost/proxy?method=DELETE
    May  9 15:36:55.261: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  9 15:36:55.261: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/pods/agnhost/proxy?method=OPTIONS
    May  9 15:36:55.269: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  9 15:36:55.269: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/pods/agnhost/proxy?method=PATCH
    May  9 15:36:55.276: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  9 15:36:55.276: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/pods/agnhost/proxy?method=POST
    May  9 15:36:55.282: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  9 15:36:55.282: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/pods/agnhost/proxy?method=PUT
    May  9 15:36:55.289: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May  9 15:36:55.289: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/services/e2e-proxy-test-service/proxy?method=DELETE
    May  9 15:36:55.330: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  9 15:36:55.330: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/services/e2e-proxy-test-service/proxy?method=OPTIONS
    May  9 15:36:55.341: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  9 15:36:55.341: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/services/e2e-proxy-test-service/proxy?method=PATCH
    May  9 15:36:55.352: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  9 15:36:55.352: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/services/e2e-proxy-test-service/proxy?method=POST
    May  9 15:36:55.364: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  9 15:36:55.364: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/services/e2e-proxy-test-service/proxy?method=PUT
    May  9 15:36:55.373: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May  9 15:36:55.373: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/pods/agnhost/proxy?method=GET
    May  9 15:36:55.383: INFO: http.Client request:GET StatusCode:301
    May  9 15:36:55.383: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/services/e2e-proxy-test-service/proxy?method=GET
    May  9 15:36:55.423: INFO: http.Client request:GET StatusCode:301
    May  9 15:36:55.423: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/pods/agnhost/proxy?method=HEAD
    May  9 15:36:55.436: INFO: http.Client request:HEAD StatusCode:301
    May  9 15:36:55.436: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8399/services/e2e-proxy-test-service/proxy?method=HEAD
    May  9 15:36:55.444: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    May  9 15:36:55.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8399" for this suite. 05/09/23 15:36:55.451
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:36:55.463
May  9 15:36:55.463: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 15:36:55.464
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:55.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:55.499
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-b66c7a36-9ea7-4eb8-a529-31776db76caa 05/09/23 15:36:55.504
STEP: Creating a pod to test consume secrets 05/09/23 15:36:55.513
May  9 15:36:55.524: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2bb109b3-c41c-462f-8fec-b0ab5c37fdd3" in namespace "projected-603" to be "Succeeded or Failed"
May  9 15:36:55.528: INFO: Pod "pod-projected-secrets-2bb109b3-c41c-462f-8fec-b0ab5c37fdd3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.788844ms
May  9 15:36:57.547: INFO: Pod "pod-projected-secrets-2bb109b3-c41c-462f-8fec-b0ab5c37fdd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023216507s
May  9 15:36:59.534: INFO: Pod "pod-projected-secrets-2bb109b3-c41c-462f-8fec-b0ab5c37fdd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010768888s
STEP: Saw pod success 05/09/23 15:36:59.534
May  9 15:36:59.535: INFO: Pod "pod-projected-secrets-2bb109b3-c41c-462f-8fec-b0ab5c37fdd3" satisfied condition "Succeeded or Failed"
May  9 15:36:59.539: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-secrets-2bb109b3-c41c-462f-8fec-b0ab5c37fdd3 container secret-volume-test: <nil>
STEP: delete the pod 05/09/23 15:36:59.551
May  9 15:36:59.567: INFO: Waiting for pod pod-projected-secrets-2bb109b3-c41c-462f-8fec-b0ab5c37fdd3 to disappear
May  9 15:36:59.571: INFO: Pod pod-projected-secrets-2bb109b3-c41c-462f-8fec-b0ab5c37fdd3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May  9 15:36:59.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-603" for this suite. 05/09/23 15:36:59.578
------------------------------
â€¢ [4.126 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:36:55.463
    May  9 15:36:55.463: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 15:36:55.464
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:55.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:55.499
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-b66c7a36-9ea7-4eb8-a529-31776db76caa 05/09/23 15:36:55.504
    STEP: Creating a pod to test consume secrets 05/09/23 15:36:55.513
    May  9 15:36:55.524: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2bb109b3-c41c-462f-8fec-b0ab5c37fdd3" in namespace "projected-603" to be "Succeeded or Failed"
    May  9 15:36:55.528: INFO: Pod "pod-projected-secrets-2bb109b3-c41c-462f-8fec-b0ab5c37fdd3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.788844ms
    May  9 15:36:57.547: INFO: Pod "pod-projected-secrets-2bb109b3-c41c-462f-8fec-b0ab5c37fdd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023216507s
    May  9 15:36:59.534: INFO: Pod "pod-projected-secrets-2bb109b3-c41c-462f-8fec-b0ab5c37fdd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010768888s
    STEP: Saw pod success 05/09/23 15:36:59.534
    May  9 15:36:59.535: INFO: Pod "pod-projected-secrets-2bb109b3-c41c-462f-8fec-b0ab5c37fdd3" satisfied condition "Succeeded or Failed"
    May  9 15:36:59.539: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-secrets-2bb109b3-c41c-462f-8fec-b0ab5c37fdd3 container secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 15:36:59.551
    May  9 15:36:59.567: INFO: Waiting for pod pod-projected-secrets-2bb109b3-c41c-462f-8fec-b0ab5c37fdd3 to disappear
    May  9 15:36:59.571: INFO: Pod pod-projected-secrets-2bb109b3-c41c-462f-8fec-b0ab5c37fdd3 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May  9 15:36:59.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-603" for this suite. 05/09/23 15:36:59.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:36:59.59
May  9 15:36:59.590: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename init-container 05/09/23 15:36:59.591
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:59.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:59.609
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 05/09/23 15:36:59.612
May  9 15:36:59.613: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May  9 15:37:04.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3529" for this suite. 05/09/23 15:37:04.305
------------------------------
â€¢ [4.723 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:36:59.59
    May  9 15:36:59.590: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename init-container 05/09/23 15:36:59.591
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:36:59.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:36:59.609
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 05/09/23 15:36:59.612
    May  9 15:36:59.613: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May  9 15:37:04.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3529" for this suite. 05/09/23 15:37:04.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:37:04.315
May  9 15:37:04.315: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename secrets 05/09/23 15:37:04.316
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:37:04.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:37:04.336
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-5e493600-9419-45c1-9a4c-ee70bd75773f 05/09/23 15:37:04.368
STEP: Creating a pod to test consume secrets 05/09/23 15:37:04.377
May  9 15:37:04.390: INFO: Waiting up to 5m0s for pod "pod-secrets-e1401e50-64ae-40d2-b34b-5daeb6ce8474" in namespace "secrets-4271" to be "Succeeded or Failed"
May  9 15:37:04.395: INFO: Pod "pod-secrets-e1401e50-64ae-40d2-b34b-5daeb6ce8474": Phase="Pending", Reason="", readiness=false. Elapsed: 5.002612ms
May  9 15:37:06.403: INFO: Pod "pod-secrets-e1401e50-64ae-40d2-b34b-5daeb6ce8474": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012846697s
May  9 15:37:08.401: INFO: Pod "pod-secrets-e1401e50-64ae-40d2-b34b-5daeb6ce8474": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010951563s
STEP: Saw pod success 05/09/23 15:37:08.401
May  9 15:37:08.401: INFO: Pod "pod-secrets-e1401e50-64ae-40d2-b34b-5daeb6ce8474" satisfied condition "Succeeded or Failed"
May  9 15:37:08.405: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-secrets-e1401e50-64ae-40d2-b34b-5daeb6ce8474 container secret-volume-test: <nil>
STEP: delete the pod 05/09/23 15:37:08.42
May  9 15:37:08.434: INFO: Waiting for pod pod-secrets-e1401e50-64ae-40d2-b34b-5daeb6ce8474 to disappear
May  9 15:37:08.438: INFO: Pod pod-secrets-e1401e50-64ae-40d2-b34b-5daeb6ce8474 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May  9 15:37:08.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4271" for this suite. 05/09/23 15:37:08.445
STEP: Destroying namespace "secret-namespace-7800" for this suite. 05/09/23 15:37:08.458
------------------------------
â€¢ [4.155 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:37:04.315
    May  9 15:37:04.315: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename secrets 05/09/23 15:37:04.316
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:37:04.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:37:04.336
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-5e493600-9419-45c1-9a4c-ee70bd75773f 05/09/23 15:37:04.368
    STEP: Creating a pod to test consume secrets 05/09/23 15:37:04.377
    May  9 15:37:04.390: INFO: Waiting up to 5m0s for pod "pod-secrets-e1401e50-64ae-40d2-b34b-5daeb6ce8474" in namespace "secrets-4271" to be "Succeeded or Failed"
    May  9 15:37:04.395: INFO: Pod "pod-secrets-e1401e50-64ae-40d2-b34b-5daeb6ce8474": Phase="Pending", Reason="", readiness=false. Elapsed: 5.002612ms
    May  9 15:37:06.403: INFO: Pod "pod-secrets-e1401e50-64ae-40d2-b34b-5daeb6ce8474": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012846697s
    May  9 15:37:08.401: INFO: Pod "pod-secrets-e1401e50-64ae-40d2-b34b-5daeb6ce8474": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010951563s
    STEP: Saw pod success 05/09/23 15:37:08.401
    May  9 15:37:08.401: INFO: Pod "pod-secrets-e1401e50-64ae-40d2-b34b-5daeb6ce8474" satisfied condition "Succeeded or Failed"
    May  9 15:37:08.405: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-secrets-e1401e50-64ae-40d2-b34b-5daeb6ce8474 container secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 15:37:08.42
    May  9 15:37:08.434: INFO: Waiting for pod pod-secrets-e1401e50-64ae-40d2-b34b-5daeb6ce8474 to disappear
    May  9 15:37:08.438: INFO: Pod pod-secrets-e1401e50-64ae-40d2-b34b-5daeb6ce8474 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May  9 15:37:08.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4271" for this suite. 05/09/23 15:37:08.445
    STEP: Destroying namespace "secret-namespace-7800" for this suite. 05/09/23 15:37:08.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:37:08.471
May  9 15:37:08.471: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename container-probe 05/09/23 15:37:08.472
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:37:08.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:37:08.495
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-33fe34ba-306b-492d-b3a4-85b022e9938b in namespace container-probe-6762 05/09/23 15:37:08.499
May  9 15:37:08.519: INFO: Waiting up to 5m0s for pod "liveness-33fe34ba-306b-492d-b3a4-85b022e9938b" in namespace "container-probe-6762" to be "not pending"
May  9 15:37:08.526: INFO: Pod "liveness-33fe34ba-306b-492d-b3a4-85b022e9938b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.697287ms
May  9 15:37:10.535: INFO: Pod "liveness-33fe34ba-306b-492d-b3a4-85b022e9938b": Phase="Running", Reason="", readiness=true. Elapsed: 2.015319181s
May  9 15:37:10.535: INFO: Pod "liveness-33fe34ba-306b-492d-b3a4-85b022e9938b" satisfied condition "not pending"
May  9 15:37:10.535: INFO: Started pod liveness-33fe34ba-306b-492d-b3a4-85b022e9938b in namespace container-probe-6762
STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 15:37:10.535
May  9 15:37:10.540: INFO: Initial restart count of pod liveness-33fe34ba-306b-492d-b3a4-85b022e9938b is 0
May  9 15:37:30.616: INFO: Restart count of pod container-probe-6762/liveness-33fe34ba-306b-492d-b3a4-85b022e9938b is now 1 (20.075841838s elapsed)
May  9 15:37:50.680: INFO: Restart count of pod container-probe-6762/liveness-33fe34ba-306b-492d-b3a4-85b022e9938b is now 2 (40.139705544s elapsed)
May  9 15:38:10.759: INFO: Restart count of pod container-probe-6762/liveness-33fe34ba-306b-492d-b3a4-85b022e9938b is now 3 (1m0.218570447s elapsed)
May  9 15:38:30.864: INFO: Restart count of pod container-probe-6762/liveness-33fe34ba-306b-492d-b3a4-85b022e9938b is now 4 (1m20.323739567s elapsed)
May  9 15:39:35.139: INFO: Restart count of pod container-probe-6762/liveness-33fe34ba-306b-492d-b3a4-85b022e9938b is now 5 (2m24.599053167s elapsed)
STEP: deleting the pod 05/09/23 15:39:35.139
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May  9 15:39:35.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6762" for this suite. 05/09/23 15:39:35.165
------------------------------
â€¢ [SLOW TEST] [146.702 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:37:08.471
    May  9 15:37:08.471: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename container-probe 05/09/23 15:37:08.472
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:37:08.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:37:08.495
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-33fe34ba-306b-492d-b3a4-85b022e9938b in namespace container-probe-6762 05/09/23 15:37:08.499
    May  9 15:37:08.519: INFO: Waiting up to 5m0s for pod "liveness-33fe34ba-306b-492d-b3a4-85b022e9938b" in namespace "container-probe-6762" to be "not pending"
    May  9 15:37:08.526: INFO: Pod "liveness-33fe34ba-306b-492d-b3a4-85b022e9938b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.697287ms
    May  9 15:37:10.535: INFO: Pod "liveness-33fe34ba-306b-492d-b3a4-85b022e9938b": Phase="Running", Reason="", readiness=true. Elapsed: 2.015319181s
    May  9 15:37:10.535: INFO: Pod "liveness-33fe34ba-306b-492d-b3a4-85b022e9938b" satisfied condition "not pending"
    May  9 15:37:10.535: INFO: Started pod liveness-33fe34ba-306b-492d-b3a4-85b022e9938b in namespace container-probe-6762
    STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 15:37:10.535
    May  9 15:37:10.540: INFO: Initial restart count of pod liveness-33fe34ba-306b-492d-b3a4-85b022e9938b is 0
    May  9 15:37:30.616: INFO: Restart count of pod container-probe-6762/liveness-33fe34ba-306b-492d-b3a4-85b022e9938b is now 1 (20.075841838s elapsed)
    May  9 15:37:50.680: INFO: Restart count of pod container-probe-6762/liveness-33fe34ba-306b-492d-b3a4-85b022e9938b is now 2 (40.139705544s elapsed)
    May  9 15:38:10.759: INFO: Restart count of pod container-probe-6762/liveness-33fe34ba-306b-492d-b3a4-85b022e9938b is now 3 (1m0.218570447s elapsed)
    May  9 15:38:30.864: INFO: Restart count of pod container-probe-6762/liveness-33fe34ba-306b-492d-b3a4-85b022e9938b is now 4 (1m20.323739567s elapsed)
    May  9 15:39:35.139: INFO: Restart count of pod container-probe-6762/liveness-33fe34ba-306b-492d-b3a4-85b022e9938b is now 5 (2m24.599053167s elapsed)
    STEP: deleting the pod 05/09/23 15:39:35.139
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May  9 15:39:35.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6762" for this suite. 05/09/23 15:39:35.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:39:35.175
May  9 15:39:35.175: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 15:39:35.176
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:39:35.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:39:35.195
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 05/09/23 15:39:35.199
May  9 15:39:35.200: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 15:39:37.322: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 15:39:45.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2386" for this suite. 05/09/23 15:39:45.714
------------------------------
â€¢ [SLOW TEST] [10.548 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:39:35.175
    May  9 15:39:35.175: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 15:39:35.176
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:39:35.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:39:35.195
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 05/09/23 15:39:35.199
    May  9 15:39:35.200: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 15:39:37.322: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 15:39:45.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2386" for this suite. 05/09/23 15:39:45.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:39:45.724
May  9 15:39:45.724: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename disruption 05/09/23 15:39:45.725
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:39:45.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:39:45.749
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 05/09/23 15:39:45.754
STEP: Waiting for the pdb to be processed 05/09/23 15:39:45.76
STEP: updating the pdb 05/09/23 15:39:47.77
STEP: Waiting for the pdb to be processed 05/09/23 15:39:47.781
STEP: patching the pdb 05/09/23 15:39:49.795
STEP: Waiting for the pdb to be processed 05/09/23 15:39:49.806
STEP: Waiting for the pdb to be deleted 05/09/23 15:39:51.824
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May  9 15:39:51.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5107" for this suite. 05/09/23 15:39:51.845
------------------------------
â€¢ [SLOW TEST] [6.130 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:39:45.724
    May  9 15:39:45.724: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename disruption 05/09/23 15:39:45.725
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:39:45.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:39:45.749
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 05/09/23 15:39:45.754
    STEP: Waiting for the pdb to be processed 05/09/23 15:39:45.76
    STEP: updating the pdb 05/09/23 15:39:47.77
    STEP: Waiting for the pdb to be processed 05/09/23 15:39:47.781
    STEP: patching the pdb 05/09/23 15:39:49.795
    STEP: Waiting for the pdb to be processed 05/09/23 15:39:49.806
    STEP: Waiting for the pdb to be deleted 05/09/23 15:39:51.824
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May  9 15:39:51.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5107" for this suite. 05/09/23 15:39:51.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:39:51.858
May  9 15:39:51.858: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename endpointslice 05/09/23 15:39:51.859
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:39:51.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:39:51.877
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 05/09/23 15:39:51.881
STEP: getting /apis/discovery.k8s.io 05/09/23 15:39:51.885
STEP: getting /apis/discovery.k8s.iov1 05/09/23 15:39:51.887
STEP: creating 05/09/23 15:39:51.889
STEP: getting 05/09/23 15:39:51.909
STEP: listing 05/09/23 15:39:51.914
STEP: watching 05/09/23 15:39:51.92
May  9 15:39:51.920: INFO: starting watch
STEP: cluster-wide listing 05/09/23 15:39:51.922
STEP: cluster-wide watching 05/09/23 15:39:51.926
May  9 15:39:51.927: INFO: starting watch
STEP: patching 05/09/23 15:39:51.929
STEP: updating 05/09/23 15:39:51.945
May  9 15:39:51.956: INFO: waiting for watch events with expected annotations
May  9 15:39:51.956: INFO: saw patched and updated annotations
STEP: deleting 05/09/23 15:39:51.956
STEP: deleting a collection 05/09/23 15:39:51.972
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
May  9 15:39:51.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9485" for this suite. 05/09/23 15:39:52.001
------------------------------
â€¢ [0.153 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:39:51.858
    May  9 15:39:51.858: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename endpointslice 05/09/23 15:39:51.859
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:39:51.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:39:51.877
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 05/09/23 15:39:51.881
    STEP: getting /apis/discovery.k8s.io 05/09/23 15:39:51.885
    STEP: getting /apis/discovery.k8s.iov1 05/09/23 15:39:51.887
    STEP: creating 05/09/23 15:39:51.889
    STEP: getting 05/09/23 15:39:51.909
    STEP: listing 05/09/23 15:39:51.914
    STEP: watching 05/09/23 15:39:51.92
    May  9 15:39:51.920: INFO: starting watch
    STEP: cluster-wide listing 05/09/23 15:39:51.922
    STEP: cluster-wide watching 05/09/23 15:39:51.926
    May  9 15:39:51.927: INFO: starting watch
    STEP: patching 05/09/23 15:39:51.929
    STEP: updating 05/09/23 15:39:51.945
    May  9 15:39:51.956: INFO: waiting for watch events with expected annotations
    May  9 15:39:51.956: INFO: saw patched and updated annotations
    STEP: deleting 05/09/23 15:39:51.956
    STEP: deleting a collection 05/09/23 15:39:51.972
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    May  9 15:39:51.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9485" for this suite. 05/09/23 15:39:52.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:39:52.012
May  9 15:39:52.012: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename daemonsets 05/09/23 15:39:52.013
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:39:52.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:39:52.032
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 05/09/23 15:39:52.065
STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 15:39:52.073
May  9 15:39:52.085: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 15:39:52.085: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 15:39:53.098: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 15:39:53.098: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 15:39:54.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  9 15:39:54.101: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 05/09/23 15:39:54.105
STEP: DeleteCollection of the DaemonSets 05/09/23 15:39:54.111
STEP: Verify that ReplicaSets have been deleted 05/09/23 15:39:54.122
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
May  9 15:39:54.145: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"317739988"},"items":null}

May  9 15:39:54.150: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"317739988"},"items":[{"metadata":{"name":"daemon-set-hdmp9","generateName":"daemon-set-","namespace":"daemonsets-442","uid":"85b65850-fd15-45f2-b606-9c6213a8129d","resourceVersion":"317739876","creationTimestamp":"2023-05-09T15:39:52Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"29689562005b76ad3ee1ceb046bc5d36ba0f8b117b9b0ee8e8dbc8122b781b18","cni.projectcalico.org/podIP":"10.2.0.125/32","cni.projectcalico.org/podIPs":"10.2.0.125/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"30710602-0a63-4b1a-9f94-8211ac7de1cf","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-09T15:39:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-09T15:39:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"30710602-0a63-4b1a-9f94-8211ac7de1cf\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-09T15:39:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.0.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ccbj8","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ccbj8","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"Always","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"nodepool-8cc7f47e-9b0c-4801-88-node-bbade7","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["nodepool-8cc7f47e-9b0c-4801-88-node-bbade7"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:52Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:52Z"}],"hostIP":"51.68.93.170","podIP":"10.2.0.125","podIPs":[{"ip":"10.2.0.125"}],"startTime":"2023-05-09T15:39:52Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-09T15:39:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4f961dd1f1c3622f9bbd258079cc96350329d3e7bb69e5eabdb9ea8d86abc5e5","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-p2mvk","generateName":"daemon-set-","namespace":"daemonsets-442","uid":"b3709ed1-37f4-4ff8-afa2-cd1c7ce8debf","resourceVersion":"317739935","creationTimestamp":"2023-05-09T15:39:52Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"21c39e17cd8979fad7eb7f2e294631d05d6a89ee617023aa70d4196e872c1284","cni.projectcalico.org/podIP":"10.2.1.112/32","cni.projectcalico.org/podIPs":"10.2.1.112/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"30710602-0a63-4b1a-9f94-8211ac7de1cf","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-09T15:39:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-09T15:39:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"30710602-0a63-4b1a-9f94-8211ac7de1cf\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-09T15:39:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-8qk6z","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-8qk6z","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"Always","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"nodepool-8cc7f47e-9b0c-4801-88-node-7ad816","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["nodepool-8cc7f47e-9b0c-4801-88-node-7ad816"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:52Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:52Z"}],"hostIP":"51.68.91.222","podIP":"10.2.1.112","podIPs":[{"ip":"10.2.1.112"}],"startTime":"2023-05-09T15:39:52Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-09T15:39:52Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://804e1dd710ae48dcfae019189564a99c162a77fd20e2d05df5b54706ba8820c9","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vndmj","generateName":"daemon-set-","namespace":"daemonsets-442","uid":"e3db6e13-64f4-4884-baf3-f6cc379b665a","resourceVersion":"317739955","creationTimestamp":"2023-05-09T15:39:52Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"464db6ac3d2f202e04725b9040ae69265b06d011174670240ba167227256ace4","cni.projectcalico.org/podIP":"10.2.2.116/32","cni.projectcalico.org/podIPs":"10.2.2.116/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"30710602-0a63-4b1a-9f94-8211ac7de1cf","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-09T15:39:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-09T15:39:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"30710602-0a63-4b1a-9f94-8211ac7de1cf\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-09T15:39:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.2.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-fcpng","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-fcpng","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"Always","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"nodepool-8cc7f47e-9b0c-4801-88-node-f76f62","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["nodepool-8cc7f47e-9b0c-4801-88-node-f76f62"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:52Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:52Z"}],"hostIP":"51.68.94.118","podIP":"10.2.2.116","podIPs":[{"ip":"10.2.2.116"}],"startTime":"2023-05-09T15:39:52Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-09T15:39:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://c30ebe86e3c8ec4f5065e01d950374bcb3d4223a42eb7ec8ea264eb4213b3ddd","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 15:39:54.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-442" for this suite. 05/09/23 15:39:54.181
------------------------------
â€¢ [2.179 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:39:52.012
    May  9 15:39:52.012: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename daemonsets 05/09/23 15:39:52.013
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:39:52.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:39:52.032
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 05/09/23 15:39:52.065
    STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 15:39:52.073
    May  9 15:39:52.085: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 15:39:52.085: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 15:39:53.098: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 15:39:53.098: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 15:39:54.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  9 15:39:54.101: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 05/09/23 15:39:54.105
    STEP: DeleteCollection of the DaemonSets 05/09/23 15:39:54.111
    STEP: Verify that ReplicaSets have been deleted 05/09/23 15:39:54.122
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    May  9 15:39:54.145: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"317739988"},"items":null}

    May  9 15:39:54.150: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"317739988"},"items":[{"metadata":{"name":"daemon-set-hdmp9","generateName":"daemon-set-","namespace":"daemonsets-442","uid":"85b65850-fd15-45f2-b606-9c6213a8129d","resourceVersion":"317739876","creationTimestamp":"2023-05-09T15:39:52Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"29689562005b76ad3ee1ceb046bc5d36ba0f8b117b9b0ee8e8dbc8122b781b18","cni.projectcalico.org/podIP":"10.2.0.125/32","cni.projectcalico.org/podIPs":"10.2.0.125/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"30710602-0a63-4b1a-9f94-8211ac7de1cf","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-09T15:39:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-09T15:39:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"30710602-0a63-4b1a-9f94-8211ac7de1cf\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-09T15:39:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.0.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ccbj8","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ccbj8","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"Always","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"nodepool-8cc7f47e-9b0c-4801-88-node-bbade7","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["nodepool-8cc7f47e-9b0c-4801-88-node-bbade7"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:52Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:52Z"}],"hostIP":"51.68.93.170","podIP":"10.2.0.125","podIPs":[{"ip":"10.2.0.125"}],"startTime":"2023-05-09T15:39:52Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-09T15:39:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4f961dd1f1c3622f9bbd258079cc96350329d3e7bb69e5eabdb9ea8d86abc5e5","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-p2mvk","generateName":"daemon-set-","namespace":"daemonsets-442","uid":"b3709ed1-37f4-4ff8-afa2-cd1c7ce8debf","resourceVersion":"317739935","creationTimestamp":"2023-05-09T15:39:52Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"21c39e17cd8979fad7eb7f2e294631d05d6a89ee617023aa70d4196e872c1284","cni.projectcalico.org/podIP":"10.2.1.112/32","cni.projectcalico.org/podIPs":"10.2.1.112/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"30710602-0a63-4b1a-9f94-8211ac7de1cf","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-09T15:39:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-09T15:39:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"30710602-0a63-4b1a-9f94-8211ac7de1cf\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-09T15:39:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-8qk6z","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-8qk6z","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"Always","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"nodepool-8cc7f47e-9b0c-4801-88-node-7ad816","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["nodepool-8cc7f47e-9b0c-4801-88-node-7ad816"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:52Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:52Z"}],"hostIP":"51.68.91.222","podIP":"10.2.1.112","podIPs":[{"ip":"10.2.1.112"}],"startTime":"2023-05-09T15:39:52Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-09T15:39:52Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://804e1dd710ae48dcfae019189564a99c162a77fd20e2d05df5b54706ba8820c9","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vndmj","generateName":"daemon-set-","namespace":"daemonsets-442","uid":"e3db6e13-64f4-4884-baf3-f6cc379b665a","resourceVersion":"317739955","creationTimestamp":"2023-05-09T15:39:52Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"464db6ac3d2f202e04725b9040ae69265b06d011174670240ba167227256ace4","cni.projectcalico.org/podIP":"10.2.2.116/32","cni.projectcalico.org/podIPs":"10.2.2.116/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"30710602-0a63-4b1a-9f94-8211ac7de1cf","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-09T15:39:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-09T15:39:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"30710602-0a63-4b1a-9f94-8211ac7de1cf\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-09T15:39:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.2.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-fcpng","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-fcpng","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"Always","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"nodepool-8cc7f47e-9b0c-4801-88-node-f76f62","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["nodepool-8cc7f47e-9b0c-4801-88-node-f76f62"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:52Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-09T15:39:52Z"}],"hostIP":"51.68.94.118","podIP":"10.2.2.116","podIPs":[{"ip":"10.2.2.116"}],"startTime":"2023-05-09T15:39:52Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-09T15:39:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://c30ebe86e3c8ec4f5065e01d950374bcb3d4223a42eb7ec8ea264eb4213b3ddd","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 15:39:54.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-442" for this suite. 05/09/23 15:39:54.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:39:54.193
May  9 15:39:54.193: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename svcaccounts 05/09/23 15:39:54.194
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:39:54.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:39:54.218
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-xqhl9"  05/09/23 15:39:54.222
May  9 15:39:54.229: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-xqhl9"  05/09/23 15:39:54.229
May  9 15:39:54.238: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May  9 15:39:54.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4280" for this suite. 05/09/23 15:39:54.243
------------------------------
â€¢ [0.061 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:39:54.193
    May  9 15:39:54.193: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename svcaccounts 05/09/23 15:39:54.194
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:39:54.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:39:54.218
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-xqhl9"  05/09/23 15:39:54.222
    May  9 15:39:54.229: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-xqhl9"  05/09/23 15:39:54.229
    May  9 15:39:54.238: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May  9 15:39:54.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4280" for this suite. 05/09/23 15:39:54.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:39:54.254
May  9 15:39:54.254: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename var-expansion 05/09/23 15:39:54.255
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:39:54.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:39:54.274
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 05/09/23 15:39:54.279
STEP: waiting for pod running 05/09/23 15:39:54.29
May  9 15:39:54.290: INFO: Waiting up to 2m0s for pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5" in namespace "var-expansion-4795" to be "running"
May  9 15:39:54.296: INFO: Pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.348354ms
May  9 15:39:56.303: INFO: Pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012740969s
May  9 15:39:56.303: INFO: Pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5" satisfied condition "running"
STEP: creating a file in subpath 05/09/23 15:39:56.303
May  9 15:39:56.308: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4795 PodName:var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 15:39:56.308: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 15:39:56.309: INFO: ExecWithOptions: Clientset creation
May  9 15:39:56.309: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/var-expansion-4795/pods/var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 05/09/23 15:39:56.471
May  9 15:39:56.477: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4795 PodName:var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 15:39:56.477: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 15:39:56.477: INFO: ExecWithOptions: Clientset creation
May  9 15:39:56.478: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/var-expansion-4795/pods/var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 05/09/23 15:39:56.613
May  9 15:39:57.135: INFO: Successfully updated pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5"
STEP: waiting for annotated pod running 05/09/23 15:39:57.135
May  9 15:39:57.135: INFO: Waiting up to 2m0s for pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5" in namespace "var-expansion-4795" to be "running"
May  9 15:39:57.142: INFO: Pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5": Phase="Running", Reason="", readiness=true. Elapsed: 6.576237ms
May  9 15:39:57.142: INFO: Pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5" satisfied condition "running"
STEP: deleting the pod gracefully 05/09/23 15:39:57.142
May  9 15:39:57.142: INFO: Deleting pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5" in namespace "var-expansion-4795"
May  9 15:39:57.153: INFO: Wait up to 5m0s for pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May  9 15:40:31.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4795" for this suite. 05/09/23 15:40:31.173
------------------------------
â€¢ [SLOW TEST] [36.928 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:39:54.254
    May  9 15:39:54.254: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename var-expansion 05/09/23 15:39:54.255
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:39:54.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:39:54.274
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 05/09/23 15:39:54.279
    STEP: waiting for pod running 05/09/23 15:39:54.29
    May  9 15:39:54.290: INFO: Waiting up to 2m0s for pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5" in namespace "var-expansion-4795" to be "running"
    May  9 15:39:54.296: INFO: Pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.348354ms
    May  9 15:39:56.303: INFO: Pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012740969s
    May  9 15:39:56.303: INFO: Pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5" satisfied condition "running"
    STEP: creating a file in subpath 05/09/23 15:39:56.303
    May  9 15:39:56.308: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4795 PodName:var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 15:39:56.308: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 15:39:56.309: INFO: ExecWithOptions: Clientset creation
    May  9 15:39:56.309: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/var-expansion-4795/pods/var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 05/09/23 15:39:56.471
    May  9 15:39:56.477: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4795 PodName:var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 15:39:56.477: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 15:39:56.477: INFO: ExecWithOptions: Clientset creation
    May  9 15:39:56.478: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/var-expansion-4795/pods/var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 05/09/23 15:39:56.613
    May  9 15:39:57.135: INFO: Successfully updated pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5"
    STEP: waiting for annotated pod running 05/09/23 15:39:57.135
    May  9 15:39:57.135: INFO: Waiting up to 2m0s for pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5" in namespace "var-expansion-4795" to be "running"
    May  9 15:39:57.142: INFO: Pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5": Phase="Running", Reason="", readiness=true. Elapsed: 6.576237ms
    May  9 15:39:57.142: INFO: Pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5" satisfied condition "running"
    STEP: deleting the pod gracefully 05/09/23 15:39:57.142
    May  9 15:39:57.142: INFO: Deleting pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5" in namespace "var-expansion-4795"
    May  9 15:39:57.153: INFO: Wait up to 5m0s for pod "var-expansion-6bee8ee0-f4df-44ee-98a2-77d0d5d025b5" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May  9 15:40:31.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4795" for this suite. 05/09/23 15:40:31.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:40:31.184
May  9 15:40:31.184: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename prestop 05/09/23 15:40:31.185
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:40:31.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:40:31.206
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-5916 05/09/23 15:40:31.211
STEP: Waiting for pods to come up. 05/09/23 15:40:31.226
May  9 15:40:31.226: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5916" to be "running"
May  9 15:40:31.231: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.205493ms
May  9 15:40:33.238: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.011923562s
May  9 15:40:33.238: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-5916 05/09/23 15:40:33.243
May  9 15:40:33.250: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5916" to be "running"
May  9 15:40:33.255: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.799821ms
May  9 15:40:35.261: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.010817822s
May  9 15:40:35.261: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 05/09/23 15:40:35.261
May  9 15:40:40.279: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 05/09/23 15:40:40.279
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
May  9 15:40:40.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-5916" for this suite. 05/09/23 15:40:40.304
------------------------------
â€¢ [SLOW TEST] [9.130 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:40:31.184
    May  9 15:40:31.184: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename prestop 05/09/23 15:40:31.185
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:40:31.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:40:31.206
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-5916 05/09/23 15:40:31.211
    STEP: Waiting for pods to come up. 05/09/23 15:40:31.226
    May  9 15:40:31.226: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5916" to be "running"
    May  9 15:40:31.231: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.205493ms
    May  9 15:40:33.238: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.011923562s
    May  9 15:40:33.238: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-5916 05/09/23 15:40:33.243
    May  9 15:40:33.250: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5916" to be "running"
    May  9 15:40:33.255: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.799821ms
    May  9 15:40:35.261: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.010817822s
    May  9 15:40:35.261: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 05/09/23 15:40:35.261
    May  9 15:40:40.279: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 05/09/23 15:40:40.279
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    May  9 15:40:40.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-5916" for this suite. 05/09/23 15:40:40.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:40:40.315
May  9 15:40:40.315: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename dns 05/09/23 15:40:40.316
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:40:40.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:40:40.342
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 05/09/23 15:40:40.347
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8227.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8227.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8227.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8227.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8227.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 226.170.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.170.226_udp@PTR;check="$$(dig +tcp +noall +answer +search 226.170.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.170.226_tcp@PTR;sleep 1; done
 05/09/23 15:40:40.367
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8227.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8227.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8227.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8227.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8227.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8227.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 226.170.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.170.226_udp@PTR;check="$$(dig +tcp +noall +answer +search 226.170.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.170.226_tcp@PTR;sleep 1; done
 05/09/23 15:40:40.367
STEP: creating a pod to probe DNS 05/09/23 15:40:40.367
STEP: submitting the pod to kubernetes 05/09/23 15:40:40.367
May  9 15:40:40.380: INFO: Waiting up to 15m0s for pod "dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909" in namespace "dns-8227" to be "running"
May  9 15:40:40.385: INFO: Pod "dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909": Phase="Pending", Reason="", readiness=false. Elapsed: 4.908013ms
May  9 15:40:42.392: INFO: Pod "dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012335582s
May  9 15:40:44.395: INFO: Pod "dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909": Phase="Running", Reason="", readiness=true. Elapsed: 4.01495432s
May  9 15:40:44.395: INFO: Pod "dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909" satisfied condition "running"
STEP: retrieving the pod 05/09/23 15:40:44.395
STEP: looking for the results for each expected name from probers 05/09/23 15:40:44.402
May  9 15:40:44.411: INFO: Unable to read wheezy_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:44.419: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:44.429: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:44.439: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:44.479: INFO: Unable to read jessie_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:44.489: INFO: Unable to read jessie_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:44.499: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:44.509: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:44.539: INFO: Lookups using dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909 failed for: [wheezy_udp@dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_udp@dns-test-service.dns-8227.svc.cluster.local jessie_tcp@dns-test-service.dns-8227.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local]

May  9 15:40:49.550: INFO: Unable to read wheezy_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:49.559: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:49.574: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:49.589: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:49.632: INFO: Unable to read jessie_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:49.639: INFO: Unable to read jessie_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:49.651: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:49.675: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:49.701: INFO: Lookups using dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909 failed for: [wheezy_udp@dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_udp@dns-test-service.dns-8227.svc.cluster.local jessie_tcp@dns-test-service.dns-8227.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local]

May  9 15:40:54.550: INFO: Unable to read wheezy_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:54.559: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:54.565: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:54.585: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:54.620: INFO: Unable to read jessie_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:54.630: INFO: Unable to read jessie_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:54.639: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:54.645: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:54.681: INFO: Lookups using dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909 failed for: [wheezy_udp@dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_udp@dns-test-service.dns-8227.svc.cluster.local jessie_tcp@dns-test-service.dns-8227.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local]

May  9 15:40:59.550: INFO: Unable to read wheezy_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:59.559: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:59.569: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:59.580: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:59.615: INFO: Unable to read jessie_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:59.621: INFO: Unable to read jessie_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:59.630: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:59.639: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:40:59.665: INFO: Lookups using dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909 failed for: [wheezy_udp@dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_udp@dns-test-service.dns-8227.svc.cluster.local jessie_tcp@dns-test-service.dns-8227.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local]

May  9 15:41:04.551: INFO: Unable to read wheezy_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:41:04.562: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:41:04.569: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:41:04.580: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:41:04.620: INFO: Unable to read jessie_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:41:04.630: INFO: Unable to read jessie_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:41:04.638: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:41:04.650: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:41:04.679: INFO: Lookups using dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909 failed for: [wheezy_udp@dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_udp@dns-test-service.dns-8227.svc.cluster.local jessie_tcp@dns-test-service.dns-8227.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local]

May  9 15:41:09.550: INFO: Unable to read wheezy_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:41:09.559: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:41:09.575: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:41:09.583: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:41:09.615: INFO: Unable to read jessie_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:41:09.623: INFO: Unable to read jessie_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:41:09.630: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:41:09.639: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
May  9 15:41:09.679: INFO: Lookups using dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909 failed for: [wheezy_udp@dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_udp@dns-test-service.dns-8227.svc.cluster.local jessie_tcp@dns-test-service.dns-8227.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local]

May  9 15:41:14.679: INFO: DNS probes using dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909 succeeded

STEP: deleting the pod 05/09/23 15:41:14.679
STEP: deleting the test service 05/09/23 15:41:14.696
STEP: deleting the test headless service 05/09/23 15:41:14.721
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May  9 15:41:14.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8227" for this suite. 05/09/23 15:41:14.746
------------------------------
â€¢ [SLOW TEST] [34.444 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:40:40.315
    May  9 15:40:40.315: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename dns 05/09/23 15:40:40.316
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:40:40.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:40:40.342
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 05/09/23 15:40:40.347
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8227.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8227.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8227.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8227.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8227.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 226.170.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.170.226_udp@PTR;check="$$(dig +tcp +noall +answer +search 226.170.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.170.226_tcp@PTR;sleep 1; done
     05/09/23 15:40:40.367
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8227.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8227.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8227.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8227.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8227.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8227.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8227.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 226.170.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.170.226_udp@PTR;check="$$(dig +tcp +noall +answer +search 226.170.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.170.226_tcp@PTR;sleep 1; done
     05/09/23 15:40:40.367
    STEP: creating a pod to probe DNS 05/09/23 15:40:40.367
    STEP: submitting the pod to kubernetes 05/09/23 15:40:40.367
    May  9 15:40:40.380: INFO: Waiting up to 15m0s for pod "dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909" in namespace "dns-8227" to be "running"
    May  9 15:40:40.385: INFO: Pod "dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909": Phase="Pending", Reason="", readiness=false. Elapsed: 4.908013ms
    May  9 15:40:42.392: INFO: Pod "dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012335582s
    May  9 15:40:44.395: INFO: Pod "dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909": Phase="Running", Reason="", readiness=true. Elapsed: 4.01495432s
    May  9 15:40:44.395: INFO: Pod "dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909" satisfied condition "running"
    STEP: retrieving the pod 05/09/23 15:40:44.395
    STEP: looking for the results for each expected name from probers 05/09/23 15:40:44.402
    May  9 15:40:44.411: INFO: Unable to read wheezy_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:44.419: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:44.429: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:44.439: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:44.479: INFO: Unable to read jessie_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:44.489: INFO: Unable to read jessie_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:44.499: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:44.509: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:44.539: INFO: Lookups using dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909 failed for: [wheezy_udp@dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_udp@dns-test-service.dns-8227.svc.cluster.local jessie_tcp@dns-test-service.dns-8227.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local]

    May  9 15:40:49.550: INFO: Unable to read wheezy_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:49.559: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:49.574: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:49.589: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:49.632: INFO: Unable to read jessie_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:49.639: INFO: Unable to read jessie_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:49.651: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:49.675: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:49.701: INFO: Lookups using dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909 failed for: [wheezy_udp@dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_udp@dns-test-service.dns-8227.svc.cluster.local jessie_tcp@dns-test-service.dns-8227.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local]

    May  9 15:40:54.550: INFO: Unable to read wheezy_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:54.559: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:54.565: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:54.585: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:54.620: INFO: Unable to read jessie_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:54.630: INFO: Unable to read jessie_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:54.639: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:54.645: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:54.681: INFO: Lookups using dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909 failed for: [wheezy_udp@dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_udp@dns-test-service.dns-8227.svc.cluster.local jessie_tcp@dns-test-service.dns-8227.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local]

    May  9 15:40:59.550: INFO: Unable to read wheezy_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:59.559: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:59.569: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:59.580: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:59.615: INFO: Unable to read jessie_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:59.621: INFO: Unable to read jessie_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:59.630: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:59.639: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:40:59.665: INFO: Lookups using dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909 failed for: [wheezy_udp@dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_udp@dns-test-service.dns-8227.svc.cluster.local jessie_tcp@dns-test-service.dns-8227.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local]

    May  9 15:41:04.551: INFO: Unable to read wheezy_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:41:04.562: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:41:04.569: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:41:04.580: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:41:04.620: INFO: Unable to read jessie_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:41:04.630: INFO: Unable to read jessie_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:41:04.638: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:41:04.650: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:41:04.679: INFO: Lookups using dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909 failed for: [wheezy_udp@dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_udp@dns-test-service.dns-8227.svc.cluster.local jessie_tcp@dns-test-service.dns-8227.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local]

    May  9 15:41:09.550: INFO: Unable to read wheezy_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:41:09.559: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:41:09.575: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:41:09.583: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:41:09.615: INFO: Unable to read jessie_udp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:41:09.623: INFO: Unable to read jessie_tcp@dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:41:09.630: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:41:09.639: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local from pod dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909: the server could not find the requested resource (get pods dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909)
    May  9 15:41:09.679: INFO: Lookups using dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909 failed for: [wheezy_udp@dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@dns-test-service.dns-8227.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_udp@dns-test-service.dns-8227.svc.cluster.local jessie_tcp@dns-test-service.dns-8227.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8227.svc.cluster.local]

    May  9 15:41:14.679: INFO: DNS probes using dns-8227/dns-test-ceef557b-ee09-4ba9-b09c-5c8726ee7909 succeeded

    STEP: deleting the pod 05/09/23 15:41:14.679
    STEP: deleting the test service 05/09/23 15:41:14.696
    STEP: deleting the test headless service 05/09/23 15:41:14.721
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May  9 15:41:14.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8227" for this suite. 05/09/23 15:41:14.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:41:14.759
May  9 15:41:14.759: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename controllerrevisions 05/09/23 15:41:14.76
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:41:14.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:41:14.778
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-k9xml-daemon-set" 05/09/23 15:41:14.838
STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 15:41:14.845
May  9 15:41:14.855: INFO: Number of nodes with available pods controlled by daemonset e2e-k9xml-daemon-set: 0
May  9 15:41:14.855: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 15:41:15.867: INFO: Number of nodes with available pods controlled by daemonset e2e-k9xml-daemon-set: 0
May  9 15:41:15.867: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 15:41:16.876: INFO: Number of nodes with available pods controlled by daemonset e2e-k9xml-daemon-set: 1
May  9 15:41:16.876: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 15:41:17.870: INFO: Number of nodes with available pods controlled by daemonset e2e-k9xml-daemon-set: 3
May  9 15:41:17.870: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-k9xml-daemon-set
STEP: Confirm DaemonSet "e2e-k9xml-daemon-set" successfully created with "daemonset-name=e2e-k9xml-daemon-set" label 05/09/23 15:41:17.875
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-k9xml-daemon-set" 05/09/23 15:41:17.886
May  9 15:41:17.893: INFO: Located ControllerRevision: "e2e-k9xml-daemon-set-9bcd9bf66"
STEP: Patching ControllerRevision "e2e-k9xml-daemon-set-9bcd9bf66" 05/09/23 15:41:17.898
May  9 15:41:17.905: INFO: e2e-k9xml-daemon-set-9bcd9bf66 has been patched
STEP: Create a new ControllerRevision 05/09/23 15:41:17.905
May  9 15:41:17.911: INFO: Created ControllerRevision: e2e-k9xml-daemon-set-685f5698d4
STEP: Confirm that there are two ControllerRevisions 05/09/23 15:41:17.911
May  9 15:41:17.911: INFO: Requesting list of ControllerRevisions to confirm quantity
May  9 15:41:17.915: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-k9xml-daemon-set-9bcd9bf66" 05/09/23 15:41:17.916
STEP: Confirm that there is only one ControllerRevision 05/09/23 15:41:17.924
May  9 15:41:17.924: INFO: Requesting list of ControllerRevisions to confirm quantity
May  9 15:41:17.928: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-k9xml-daemon-set-685f5698d4" 05/09/23 15:41:17.933
May  9 15:41:17.946: INFO: e2e-k9xml-daemon-set-685f5698d4 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 05/09/23 15:41:17.946
W0509 15:41:17.953937      21 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 05/09/23 15:41:17.953
May  9 15:41:17.954: INFO: Requesting list of ControllerRevisions to confirm quantity
May  9 15:41:18.958: INFO: Requesting list of ControllerRevisions to confirm quantity
May  9 15:41:18.965: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-k9xml-daemon-set-685f5698d4=updated" 05/09/23 15:41:18.965
STEP: Confirm that there is only one ControllerRevision 05/09/23 15:41:18.976
May  9 15:41:18.976: INFO: Requesting list of ControllerRevisions to confirm quantity
May  9 15:41:18.980: INFO: Found 1 ControllerRevisions
May  9 15:41:18.984: INFO: ControllerRevision "e2e-k9xml-daemon-set-bb6d757bf" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-k9xml-daemon-set" 05/09/23 15:41:18.997
STEP: deleting DaemonSet.extensions e2e-k9xml-daemon-set in namespace controllerrevisions-3531, will wait for the garbage collector to delete the pods 05/09/23 15:41:18.997
May  9 15:41:19.061: INFO: Deleting DaemonSet.extensions e2e-k9xml-daemon-set took: 8.398827ms
May  9 15:41:19.162: INFO: Terminating DaemonSet.extensions e2e-k9xml-daemon-set pods took: 100.861428ms
May  9 15:41:20.169: INFO: Number of nodes with available pods controlled by daemonset e2e-k9xml-daemon-set: 0
May  9 15:41:20.169: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-k9xml-daemon-set
May  9 15:41:20.177: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"317749188"},"items":null}

May  9 15:41:20.185: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"317749191"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 15:41:20.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-3531" for this suite. 05/09/23 15:41:20.211
------------------------------
â€¢ [SLOW TEST] [5.458 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:41:14.759
    May  9 15:41:14.759: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename controllerrevisions 05/09/23 15:41:14.76
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:41:14.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:41:14.778
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-k9xml-daemon-set" 05/09/23 15:41:14.838
    STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 15:41:14.845
    May  9 15:41:14.855: INFO: Number of nodes with available pods controlled by daemonset e2e-k9xml-daemon-set: 0
    May  9 15:41:14.855: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 15:41:15.867: INFO: Number of nodes with available pods controlled by daemonset e2e-k9xml-daemon-set: 0
    May  9 15:41:15.867: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 15:41:16.876: INFO: Number of nodes with available pods controlled by daemonset e2e-k9xml-daemon-set: 1
    May  9 15:41:16.876: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 15:41:17.870: INFO: Number of nodes with available pods controlled by daemonset e2e-k9xml-daemon-set: 3
    May  9 15:41:17.870: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-k9xml-daemon-set
    STEP: Confirm DaemonSet "e2e-k9xml-daemon-set" successfully created with "daemonset-name=e2e-k9xml-daemon-set" label 05/09/23 15:41:17.875
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-k9xml-daemon-set" 05/09/23 15:41:17.886
    May  9 15:41:17.893: INFO: Located ControllerRevision: "e2e-k9xml-daemon-set-9bcd9bf66"
    STEP: Patching ControllerRevision "e2e-k9xml-daemon-set-9bcd9bf66" 05/09/23 15:41:17.898
    May  9 15:41:17.905: INFO: e2e-k9xml-daemon-set-9bcd9bf66 has been patched
    STEP: Create a new ControllerRevision 05/09/23 15:41:17.905
    May  9 15:41:17.911: INFO: Created ControllerRevision: e2e-k9xml-daemon-set-685f5698d4
    STEP: Confirm that there are two ControllerRevisions 05/09/23 15:41:17.911
    May  9 15:41:17.911: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  9 15:41:17.915: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-k9xml-daemon-set-9bcd9bf66" 05/09/23 15:41:17.916
    STEP: Confirm that there is only one ControllerRevision 05/09/23 15:41:17.924
    May  9 15:41:17.924: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  9 15:41:17.928: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-k9xml-daemon-set-685f5698d4" 05/09/23 15:41:17.933
    May  9 15:41:17.946: INFO: e2e-k9xml-daemon-set-685f5698d4 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 05/09/23 15:41:17.946
    W0509 15:41:17.953937      21 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 05/09/23 15:41:17.953
    May  9 15:41:17.954: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  9 15:41:18.958: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  9 15:41:18.965: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-k9xml-daemon-set-685f5698d4=updated" 05/09/23 15:41:18.965
    STEP: Confirm that there is only one ControllerRevision 05/09/23 15:41:18.976
    May  9 15:41:18.976: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  9 15:41:18.980: INFO: Found 1 ControllerRevisions
    May  9 15:41:18.984: INFO: ControllerRevision "e2e-k9xml-daemon-set-bb6d757bf" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-k9xml-daemon-set" 05/09/23 15:41:18.997
    STEP: deleting DaemonSet.extensions e2e-k9xml-daemon-set in namespace controllerrevisions-3531, will wait for the garbage collector to delete the pods 05/09/23 15:41:18.997
    May  9 15:41:19.061: INFO: Deleting DaemonSet.extensions e2e-k9xml-daemon-set took: 8.398827ms
    May  9 15:41:19.162: INFO: Terminating DaemonSet.extensions e2e-k9xml-daemon-set pods took: 100.861428ms
    May  9 15:41:20.169: INFO: Number of nodes with available pods controlled by daemonset e2e-k9xml-daemon-set: 0
    May  9 15:41:20.169: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-k9xml-daemon-set
    May  9 15:41:20.177: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"317749188"},"items":null}

    May  9 15:41:20.185: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"317749191"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 15:41:20.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-3531" for this suite. 05/09/23 15:41:20.211
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:41:20.218
May  9 15:41:20.219: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubelet-test 05/09/23 15:41:20.22
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:41:20.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:41:20.239
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
May  9 15:41:20.257: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsedccab41-b79c-4dd5-9dac-8f6311c64449" in namespace "kubelet-test-9682" to be "running and ready"
May  9 15:41:20.263: INFO: Pod "busybox-readonly-fsedccab41-b79c-4dd5-9dac-8f6311c64449": Phase="Pending", Reason="", readiness=false. Elapsed: 5.615203ms
May  9 15:41:20.263: INFO: The phase of Pod busybox-readonly-fsedccab41-b79c-4dd5-9dac-8f6311c64449 is Pending, waiting for it to be Running (with Ready = true)
May  9 15:41:22.269: INFO: Pod "busybox-readonly-fsedccab41-b79c-4dd5-9dac-8f6311c64449": Phase="Running", Reason="", readiness=true. Elapsed: 2.011954972s
May  9 15:41:22.269: INFO: The phase of Pod busybox-readonly-fsedccab41-b79c-4dd5-9dac-8f6311c64449 is Running (Ready = true)
May  9 15:41:22.269: INFO: Pod "busybox-readonly-fsedccab41-b79c-4dd5-9dac-8f6311c64449" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May  9 15:41:22.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9682" for this suite. 05/09/23 15:41:22.36
------------------------------
â€¢ [2.170 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:41:20.218
    May  9 15:41:20.219: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubelet-test 05/09/23 15:41:20.22
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:41:20.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:41:20.239
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    May  9 15:41:20.257: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsedccab41-b79c-4dd5-9dac-8f6311c64449" in namespace "kubelet-test-9682" to be "running and ready"
    May  9 15:41:20.263: INFO: Pod "busybox-readonly-fsedccab41-b79c-4dd5-9dac-8f6311c64449": Phase="Pending", Reason="", readiness=false. Elapsed: 5.615203ms
    May  9 15:41:20.263: INFO: The phase of Pod busybox-readonly-fsedccab41-b79c-4dd5-9dac-8f6311c64449 is Pending, waiting for it to be Running (with Ready = true)
    May  9 15:41:22.269: INFO: Pod "busybox-readonly-fsedccab41-b79c-4dd5-9dac-8f6311c64449": Phase="Running", Reason="", readiness=true. Elapsed: 2.011954972s
    May  9 15:41:22.269: INFO: The phase of Pod busybox-readonly-fsedccab41-b79c-4dd5-9dac-8f6311c64449 is Running (Ready = true)
    May  9 15:41:22.269: INFO: Pod "busybox-readonly-fsedccab41-b79c-4dd5-9dac-8f6311c64449" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May  9 15:41:22.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9682" for this suite. 05/09/23 15:41:22.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:41:22.389
May  9 15:41:22.389: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename secrets 05/09/23 15:41:22.39
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:41:22.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:41:22.412
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 05/09/23 15:41:22.416
STEP: listing secrets in all namespaces to ensure that there are more than zero 05/09/23 15:41:22.422
STEP: patching the secret 05/09/23 15:41:22.428
STEP: deleting the secret using a LabelSelector 05/09/23 15:41:22.439
STEP: listing secrets in all namespaces, searching for label name and value in patch 05/09/23 15:41:22.45
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
May  9 15:41:22.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3941" for this suite. 05/09/23 15:41:22.464
------------------------------
â€¢ [0.086 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:41:22.389
    May  9 15:41:22.389: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename secrets 05/09/23 15:41:22.39
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:41:22.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:41:22.412
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 05/09/23 15:41:22.416
    STEP: listing secrets in all namespaces to ensure that there are more than zero 05/09/23 15:41:22.422
    STEP: patching the secret 05/09/23 15:41:22.428
    STEP: deleting the secret using a LabelSelector 05/09/23 15:41:22.439
    STEP: listing secrets in all namespaces, searching for label name and value in patch 05/09/23 15:41:22.45
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    May  9 15:41:22.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3941" for this suite. 05/09/23 15:41:22.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:41:22.476
May  9 15:41:22.476: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename var-expansion 05/09/23 15:41:22.477
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:41:22.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:41:22.517
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
May  9 15:41:22.539: INFO: Waiting up to 2m0s for pod "var-expansion-542db924-07d9-4d26-aabb-ff8b6e663335" in namespace "var-expansion-7019" to be "container 0 failed with reason CreateContainerConfigError"
May  9 15:41:22.544: INFO: Pod "var-expansion-542db924-07d9-4d26-aabb-ff8b6e663335": Phase="Pending", Reason="", readiness=false. Elapsed: 5.584475ms
May  9 15:41:24.551: INFO: Pod "var-expansion-542db924-07d9-4d26-aabb-ff8b6e663335": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01242085s
May  9 15:41:24.551: INFO: Pod "var-expansion-542db924-07d9-4d26-aabb-ff8b6e663335" satisfied condition "container 0 failed with reason CreateContainerConfigError"
May  9 15:41:24.551: INFO: Deleting pod "var-expansion-542db924-07d9-4d26-aabb-ff8b6e663335" in namespace "var-expansion-7019"
May  9 15:41:24.563: INFO: Wait up to 5m0s for pod "var-expansion-542db924-07d9-4d26-aabb-ff8b6e663335" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May  9 15:41:28.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7019" for this suite. 05/09/23 15:41:28.58
------------------------------
â€¢ [SLOW TEST] [6.118 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:41:22.476
    May  9 15:41:22.476: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename var-expansion 05/09/23 15:41:22.477
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:41:22.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:41:22.517
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    May  9 15:41:22.539: INFO: Waiting up to 2m0s for pod "var-expansion-542db924-07d9-4d26-aabb-ff8b6e663335" in namespace "var-expansion-7019" to be "container 0 failed with reason CreateContainerConfigError"
    May  9 15:41:22.544: INFO: Pod "var-expansion-542db924-07d9-4d26-aabb-ff8b6e663335": Phase="Pending", Reason="", readiness=false. Elapsed: 5.584475ms
    May  9 15:41:24.551: INFO: Pod "var-expansion-542db924-07d9-4d26-aabb-ff8b6e663335": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01242085s
    May  9 15:41:24.551: INFO: Pod "var-expansion-542db924-07d9-4d26-aabb-ff8b6e663335" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    May  9 15:41:24.551: INFO: Deleting pod "var-expansion-542db924-07d9-4d26-aabb-ff8b6e663335" in namespace "var-expansion-7019"
    May  9 15:41:24.563: INFO: Wait up to 5m0s for pod "var-expansion-542db924-07d9-4d26-aabb-ff8b6e663335" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May  9 15:41:28.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7019" for this suite. 05/09/23 15:41:28.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:41:28.595
May  9 15:41:28.595: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename emptydir 05/09/23 15:41:28.596
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:41:28.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:41:28.617
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 05/09/23 15:41:28.623
May  9 15:41:28.657: INFO: Waiting up to 5m0s for pod "pod-d07ced06-0197-4f42-ae81-327b1ebbb88b" in namespace "emptydir-7843" to be "Succeeded or Failed"
May  9 15:41:28.685: INFO: Pod "pod-d07ced06-0197-4f42-ae81-327b1ebbb88b": Phase="Pending", Reason="", readiness=false. Elapsed: 28.8982ms
May  9 15:41:30.693: INFO: Pod "pod-d07ced06-0197-4f42-ae81-327b1ebbb88b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035970617s
May  9 15:41:32.693: INFO: Pod "pod-d07ced06-0197-4f42-ae81-327b1ebbb88b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036303186s
STEP: Saw pod success 05/09/23 15:41:32.693
May  9 15:41:32.693: INFO: Pod "pod-d07ced06-0197-4f42-ae81-327b1ebbb88b" satisfied condition "Succeeded or Failed"
May  9 15:41:32.699: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-d07ced06-0197-4f42-ae81-327b1ebbb88b container test-container: <nil>
STEP: delete the pod 05/09/23 15:41:32.713
May  9 15:41:32.734: INFO: Waiting for pod pod-d07ced06-0197-4f42-ae81-327b1ebbb88b to disappear
May  9 15:41:32.741: INFO: Pod pod-d07ced06-0197-4f42-ae81-327b1ebbb88b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  9 15:41:32.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7843" for this suite. 05/09/23 15:41:32.748
------------------------------
â€¢ [4.162 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:41:28.595
    May  9 15:41:28.595: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename emptydir 05/09/23 15:41:28.596
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:41:28.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:41:28.617
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 05/09/23 15:41:28.623
    May  9 15:41:28.657: INFO: Waiting up to 5m0s for pod "pod-d07ced06-0197-4f42-ae81-327b1ebbb88b" in namespace "emptydir-7843" to be "Succeeded or Failed"
    May  9 15:41:28.685: INFO: Pod "pod-d07ced06-0197-4f42-ae81-327b1ebbb88b": Phase="Pending", Reason="", readiness=false. Elapsed: 28.8982ms
    May  9 15:41:30.693: INFO: Pod "pod-d07ced06-0197-4f42-ae81-327b1ebbb88b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035970617s
    May  9 15:41:32.693: INFO: Pod "pod-d07ced06-0197-4f42-ae81-327b1ebbb88b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036303186s
    STEP: Saw pod success 05/09/23 15:41:32.693
    May  9 15:41:32.693: INFO: Pod "pod-d07ced06-0197-4f42-ae81-327b1ebbb88b" satisfied condition "Succeeded or Failed"
    May  9 15:41:32.699: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-d07ced06-0197-4f42-ae81-327b1ebbb88b container test-container: <nil>
    STEP: delete the pod 05/09/23 15:41:32.713
    May  9 15:41:32.734: INFO: Waiting for pod pod-d07ced06-0197-4f42-ae81-327b1ebbb88b to disappear
    May  9 15:41:32.741: INFO: Pod pod-d07ced06-0197-4f42-ae81-327b1ebbb88b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  9 15:41:32.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7843" for this suite. 05/09/23 15:41:32.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:41:32.759
May  9 15:41:32.759: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename downward-api 05/09/23 15:41:32.76
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:41:32.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:41:32.783
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 05/09/23 15:41:32.788
May  9 15:41:32.799: INFO: Waiting up to 5m0s for pod "downward-api-0414a760-6228-4f7d-86a4-ae1c4c1848a2" in namespace "downward-api-576" to be "Succeeded or Failed"
May  9 15:41:32.804: INFO: Pod "downward-api-0414a760-6228-4f7d-86a4-ae1c4c1848a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.466863ms
May  9 15:41:34.813: INFO: Pod "downward-api-0414a760-6228-4f7d-86a4-ae1c4c1848a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013360895s
May  9 15:41:36.811: INFO: Pod "downward-api-0414a760-6228-4f7d-86a4-ae1c4c1848a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011956403s
STEP: Saw pod success 05/09/23 15:41:36.811
May  9 15:41:36.812: INFO: Pod "downward-api-0414a760-6228-4f7d-86a4-ae1c4c1848a2" satisfied condition "Succeeded or Failed"
May  9 15:41:36.818: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downward-api-0414a760-6228-4f7d-86a4-ae1c4c1848a2 container dapi-container: <nil>
STEP: delete the pod 05/09/23 15:41:36.832
May  9 15:41:36.846: INFO: Waiting for pod downward-api-0414a760-6228-4f7d-86a4-ae1c4c1848a2 to disappear
May  9 15:41:36.852: INFO: Pod downward-api-0414a760-6228-4f7d-86a4-ae1c4c1848a2 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May  9 15:41:36.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-576" for this suite. 05/09/23 15:41:36.863
------------------------------
â€¢ [4.115 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:41:32.759
    May  9 15:41:32.759: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename downward-api 05/09/23 15:41:32.76
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:41:32.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:41:32.783
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 05/09/23 15:41:32.788
    May  9 15:41:32.799: INFO: Waiting up to 5m0s for pod "downward-api-0414a760-6228-4f7d-86a4-ae1c4c1848a2" in namespace "downward-api-576" to be "Succeeded or Failed"
    May  9 15:41:32.804: INFO: Pod "downward-api-0414a760-6228-4f7d-86a4-ae1c4c1848a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.466863ms
    May  9 15:41:34.813: INFO: Pod "downward-api-0414a760-6228-4f7d-86a4-ae1c4c1848a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013360895s
    May  9 15:41:36.811: INFO: Pod "downward-api-0414a760-6228-4f7d-86a4-ae1c4c1848a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011956403s
    STEP: Saw pod success 05/09/23 15:41:36.811
    May  9 15:41:36.812: INFO: Pod "downward-api-0414a760-6228-4f7d-86a4-ae1c4c1848a2" satisfied condition "Succeeded or Failed"
    May  9 15:41:36.818: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downward-api-0414a760-6228-4f7d-86a4-ae1c4c1848a2 container dapi-container: <nil>
    STEP: delete the pod 05/09/23 15:41:36.832
    May  9 15:41:36.846: INFO: Waiting for pod downward-api-0414a760-6228-4f7d-86a4-ae1c4c1848a2 to disappear
    May  9 15:41:36.852: INFO: Pod downward-api-0414a760-6228-4f7d-86a4-ae1c4c1848a2 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May  9 15:41:36.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-576" for this suite. 05/09/23 15:41:36.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:41:36.875
May  9 15:41:36.875: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubectl 05/09/23 15:41:36.877
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:41:36.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:41:36.904
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
May  9 15:41:36.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-699 version'
May  9 15:41:36.991: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
May  9 15:41:36.991: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.4\", GitCommit:\"f89670c3aa4059d6999cb42e23ccb4f0b9a03979\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:13:53Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.4\", GitCommit:\"f89670c3aa4059d6999cb42e23ccb4f0b9a03979\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:05:35Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  9 15:41:36.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-699" for this suite. 05/09/23 15:41:36.997
------------------------------
â€¢ [0.130 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:41:36.875
    May  9 15:41:36.875: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubectl 05/09/23 15:41:36.877
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:41:36.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:41:36.904
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    May  9 15:41:36.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-699 version'
    May  9 15:41:36.991: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    May  9 15:41:36.991: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.4\", GitCommit:\"f89670c3aa4059d6999cb42e23ccb4f0b9a03979\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:13:53Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.4\", GitCommit:\"f89670c3aa4059d6999cb42e23ccb4f0b9a03979\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:05:35Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  9 15:41:36.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-699" for this suite. 05/09/23 15:41:36.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:41:37.007
May  9 15:41:37.008: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename resourcequota 05/09/23 15:41:37.008
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:41:37.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:41:37.032
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-75nt8" 05/09/23 15:41:37.043
May  9 15:41:37.056: INFO: Resource quota "e2e-rq-status-75nt8" reports spec: hard cpu limit of 500m
May  9 15:41:37.056: INFO: Resource quota "e2e-rq-status-75nt8" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-75nt8" /status 05/09/23 15:41:37.056
STEP: Confirm /status for "e2e-rq-status-75nt8" resourceQuota via watch 05/09/23 15:41:37.07
May  9 15:41:37.073: INFO: observed resourceQuota "e2e-rq-status-75nt8" in namespace "resourcequota-686" with hard status: v1.ResourceList(nil)
May  9 15:41:37.073: INFO: Found resourceQuota "e2e-rq-status-75nt8" in namespace "resourcequota-686" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
May  9 15:41:37.073: INFO: ResourceQuota "e2e-rq-status-75nt8" /status was updated
STEP: Patching hard spec values for cpu & memory 05/09/23 15:41:37.077
May  9 15:41:37.084: INFO: Resource quota "e2e-rq-status-75nt8" reports spec: hard cpu limit of 1
May  9 15:41:37.084: INFO: Resource quota "e2e-rq-status-75nt8" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-75nt8" /status 05/09/23 15:41:37.084
STEP: Confirm /status for "e2e-rq-status-75nt8" resourceQuota via watch 05/09/23 15:41:37.091
May  9 15:41:37.093: INFO: observed resourceQuota "e2e-rq-status-75nt8" in namespace "resourcequota-686" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
May  9 15:41:37.093: INFO: Found resourceQuota "e2e-rq-status-75nt8" in namespace "resourcequota-686" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
May  9 15:41:37.093: INFO: ResourceQuota "e2e-rq-status-75nt8" /status was patched
STEP: Get "e2e-rq-status-75nt8" /status 05/09/23 15:41:37.093
May  9 15:41:37.099: INFO: Resourcequota "e2e-rq-status-75nt8" reports status: hard cpu of 1
May  9 15:41:37.099: INFO: Resourcequota "e2e-rq-status-75nt8" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-75nt8" /status before checking Spec is unchanged 05/09/23 15:41:37.104
May  9 15:41:37.111: INFO: Resourcequota "e2e-rq-status-75nt8" reports status: hard cpu of 2
May  9 15:41:37.111: INFO: Resourcequota "e2e-rq-status-75nt8" reports status: hard memory of 2Gi
May  9 15:41:37.113: INFO: Found resourceQuota "e2e-rq-status-75nt8" in namespace "resourcequota-686" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
May  9 15:43:42.130: INFO: ResourceQuota "e2e-rq-status-75nt8" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  9 15:43:42.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-686" for this suite. 05/09/23 15:43:42.142
------------------------------
â€¢ [SLOW TEST] [125.151 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:41:37.007
    May  9 15:41:37.008: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename resourcequota 05/09/23 15:41:37.008
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:41:37.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:41:37.032
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-75nt8" 05/09/23 15:41:37.043
    May  9 15:41:37.056: INFO: Resource quota "e2e-rq-status-75nt8" reports spec: hard cpu limit of 500m
    May  9 15:41:37.056: INFO: Resource quota "e2e-rq-status-75nt8" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-75nt8" /status 05/09/23 15:41:37.056
    STEP: Confirm /status for "e2e-rq-status-75nt8" resourceQuota via watch 05/09/23 15:41:37.07
    May  9 15:41:37.073: INFO: observed resourceQuota "e2e-rq-status-75nt8" in namespace "resourcequota-686" with hard status: v1.ResourceList(nil)
    May  9 15:41:37.073: INFO: Found resourceQuota "e2e-rq-status-75nt8" in namespace "resourcequota-686" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    May  9 15:41:37.073: INFO: ResourceQuota "e2e-rq-status-75nt8" /status was updated
    STEP: Patching hard spec values for cpu & memory 05/09/23 15:41:37.077
    May  9 15:41:37.084: INFO: Resource quota "e2e-rq-status-75nt8" reports spec: hard cpu limit of 1
    May  9 15:41:37.084: INFO: Resource quota "e2e-rq-status-75nt8" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-75nt8" /status 05/09/23 15:41:37.084
    STEP: Confirm /status for "e2e-rq-status-75nt8" resourceQuota via watch 05/09/23 15:41:37.091
    May  9 15:41:37.093: INFO: observed resourceQuota "e2e-rq-status-75nt8" in namespace "resourcequota-686" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    May  9 15:41:37.093: INFO: Found resourceQuota "e2e-rq-status-75nt8" in namespace "resourcequota-686" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    May  9 15:41:37.093: INFO: ResourceQuota "e2e-rq-status-75nt8" /status was patched
    STEP: Get "e2e-rq-status-75nt8" /status 05/09/23 15:41:37.093
    May  9 15:41:37.099: INFO: Resourcequota "e2e-rq-status-75nt8" reports status: hard cpu of 1
    May  9 15:41:37.099: INFO: Resourcequota "e2e-rq-status-75nt8" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-75nt8" /status before checking Spec is unchanged 05/09/23 15:41:37.104
    May  9 15:41:37.111: INFO: Resourcequota "e2e-rq-status-75nt8" reports status: hard cpu of 2
    May  9 15:41:37.111: INFO: Resourcequota "e2e-rq-status-75nt8" reports status: hard memory of 2Gi
    May  9 15:41:37.113: INFO: Found resourceQuota "e2e-rq-status-75nt8" in namespace "resourcequota-686" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    May  9 15:43:42.130: INFO: ResourceQuota "e2e-rq-status-75nt8" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  9 15:43:42.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-686" for this suite. 05/09/23 15:43:42.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:43:42.158
May  9 15:43:42.159: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename watch 05/09/23 15:43:42.159
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:43:42.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:43:42.183
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 05/09/23 15:43:42.188
STEP: modifying the configmap once 05/09/23 15:43:42.193
STEP: modifying the configmap a second time 05/09/23 15:43:42.204
STEP: deleting the configmap 05/09/23 15:43:42.215
STEP: creating a watch on configmaps from the resource version returned by the first update 05/09/23 15:43:42.228
STEP: Expecting to observe notifications for all changes to the configmap after the first update 05/09/23 15:43:42.231
May  9 15:43:42.231: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6028  36c722bf-6ebe-4b5b-b2a7-8c05d4d8f86b 317762611 0 2023-05-09 15:43:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-09 15:43:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 15:43:42.231: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6028  36c722bf-6ebe-4b5b-b2a7-8c05d4d8f86b 317762613 0 2023-05-09 15:43:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-09 15:43:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May  9 15:43:42.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6028" for this suite. 05/09/23 15:43:42.24
------------------------------
â€¢ [0.091 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:43:42.158
    May  9 15:43:42.159: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename watch 05/09/23 15:43:42.159
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:43:42.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:43:42.183
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 05/09/23 15:43:42.188
    STEP: modifying the configmap once 05/09/23 15:43:42.193
    STEP: modifying the configmap a second time 05/09/23 15:43:42.204
    STEP: deleting the configmap 05/09/23 15:43:42.215
    STEP: creating a watch on configmaps from the resource version returned by the first update 05/09/23 15:43:42.228
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 05/09/23 15:43:42.231
    May  9 15:43:42.231: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6028  36c722bf-6ebe-4b5b-b2a7-8c05d4d8f86b 317762611 0 2023-05-09 15:43:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-09 15:43:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 15:43:42.231: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6028  36c722bf-6ebe-4b5b-b2a7-8c05d4d8f86b 317762613 0 2023-05-09 15:43:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-09 15:43:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May  9 15:43:42.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6028" for this suite. 05/09/23 15:43:42.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:43:42.251
May  9 15:43:42.252: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename secrets 05/09/23 15:43:42.252
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:43:42.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:43:42.275
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-424170cc-c5ee-4514-bf7a-04c97fb6cfed 05/09/23 15:43:42.28
STEP: Creating a pod to test consume secrets 05/09/23 15:43:42.286
May  9 15:43:42.297: INFO: Waiting up to 5m0s for pod "pod-secrets-5b5f230a-7f88-47a3-85b3-d9235d16265b" in namespace "secrets-7312" to be "Succeeded or Failed"
May  9 15:43:42.304: INFO: Pod "pod-secrets-5b5f230a-7f88-47a3-85b3-d9235d16265b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.905534ms
May  9 15:43:44.310: INFO: Pod "pod-secrets-5b5f230a-7f88-47a3-85b3-d9235d16265b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012978208s
May  9 15:43:46.314: INFO: Pod "pod-secrets-5b5f230a-7f88-47a3-85b3-d9235d16265b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017068887s
STEP: Saw pod success 05/09/23 15:43:46.314
May  9 15:43:46.315: INFO: Pod "pod-secrets-5b5f230a-7f88-47a3-85b3-d9235d16265b" satisfied condition "Succeeded or Failed"
May  9 15:43:46.319: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-secrets-5b5f230a-7f88-47a3-85b3-d9235d16265b container secret-volume-test: <nil>
STEP: delete the pod 05/09/23 15:43:46.384
May  9 15:43:46.405: INFO: Waiting for pod pod-secrets-5b5f230a-7f88-47a3-85b3-d9235d16265b to disappear
May  9 15:43:46.410: INFO: Pod pod-secrets-5b5f230a-7f88-47a3-85b3-d9235d16265b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May  9 15:43:46.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7312" for this suite. 05/09/23 15:43:46.417
------------------------------
â€¢ [4.175 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:43:42.251
    May  9 15:43:42.252: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename secrets 05/09/23 15:43:42.252
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:43:42.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:43:42.275
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-424170cc-c5ee-4514-bf7a-04c97fb6cfed 05/09/23 15:43:42.28
    STEP: Creating a pod to test consume secrets 05/09/23 15:43:42.286
    May  9 15:43:42.297: INFO: Waiting up to 5m0s for pod "pod-secrets-5b5f230a-7f88-47a3-85b3-d9235d16265b" in namespace "secrets-7312" to be "Succeeded or Failed"
    May  9 15:43:42.304: INFO: Pod "pod-secrets-5b5f230a-7f88-47a3-85b3-d9235d16265b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.905534ms
    May  9 15:43:44.310: INFO: Pod "pod-secrets-5b5f230a-7f88-47a3-85b3-d9235d16265b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012978208s
    May  9 15:43:46.314: INFO: Pod "pod-secrets-5b5f230a-7f88-47a3-85b3-d9235d16265b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017068887s
    STEP: Saw pod success 05/09/23 15:43:46.314
    May  9 15:43:46.315: INFO: Pod "pod-secrets-5b5f230a-7f88-47a3-85b3-d9235d16265b" satisfied condition "Succeeded or Failed"
    May  9 15:43:46.319: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-secrets-5b5f230a-7f88-47a3-85b3-d9235d16265b container secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 15:43:46.384
    May  9 15:43:46.405: INFO: Waiting for pod pod-secrets-5b5f230a-7f88-47a3-85b3-d9235d16265b to disappear
    May  9 15:43:46.410: INFO: Pod pod-secrets-5b5f230a-7f88-47a3-85b3-d9235d16265b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May  9 15:43:46.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7312" for this suite. 05/09/23 15:43:46.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:43:46.429
May  9 15:43:46.429: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename sysctl 05/09/23 15:43:46.43
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:43:46.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:43:46.457
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 05/09/23 15:43:46.461
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May  9 15:43:46.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-8294" for this suite. 05/09/23 15:43:46.473
------------------------------
â€¢ [0.055 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:43:46.429
    May  9 15:43:46.429: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename sysctl 05/09/23 15:43:46.43
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:43:46.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:43:46.457
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 05/09/23 15:43:46.461
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May  9 15:43:46.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-8294" for this suite. 05/09/23 15:43:46.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:43:46.485
May  9 15:43:46.485: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubectl 05/09/23 15:43:46.485
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:43:46.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:43:46.506
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 05/09/23 15:43:46.511
May  9 15:43:46.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 create -f -'
May  9 15:43:47.240: INFO: stderr: ""
May  9 15:43:47.240: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/09/23 15:43:47.24
May  9 15:43:47.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  9 15:43:47.393: INFO: stderr: ""
May  9 15:43:47.393: INFO: stdout: "update-demo-nautilus-7v4sf update-demo-nautilus-cqgg5 "
May  9 15:43:47.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-7v4sf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 15:43:47.668: INFO: stderr: ""
May  9 15:43:47.668: INFO: stdout: ""
May  9 15:43:47.668: INFO: update-demo-nautilus-7v4sf is created but not running
May  9 15:43:52.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  9 15:43:52.755: INFO: stderr: ""
May  9 15:43:52.755: INFO: stdout: "update-demo-nautilus-7v4sf update-demo-nautilus-cqgg5 "
May  9 15:43:52.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-7v4sf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 15:43:52.833: INFO: stderr: ""
May  9 15:43:52.833: INFO: stdout: "true"
May  9 15:43:52.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-7v4sf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  9 15:43:52.910: INFO: stderr: ""
May  9 15:43:52.910: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May  9 15:43:52.910: INFO: validating pod update-demo-nautilus-7v4sf
May  9 15:43:52.919: INFO: got data: {
  "image": "nautilus.jpg"
}

May  9 15:43:52.919: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  9 15:43:52.919: INFO: update-demo-nautilus-7v4sf is verified up and running
May  9 15:43:52.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-cqgg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 15:43:53.002: INFO: stderr: ""
May  9 15:43:53.002: INFO: stdout: "true"
May  9 15:43:53.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-cqgg5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  9 15:43:53.080: INFO: stderr: ""
May  9 15:43:53.080: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May  9 15:43:53.080: INFO: validating pod update-demo-nautilus-cqgg5
May  9 15:43:53.088: INFO: got data: {
  "image": "nautilus.jpg"
}

May  9 15:43:53.088: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  9 15:43:53.088: INFO: update-demo-nautilus-cqgg5 is verified up and running
STEP: scaling down the replication controller 05/09/23 15:43:53.088
May  9 15:43:53.090: INFO: scanned /root for discovery docs: <nil>
May  9 15:43:53.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May  9 15:43:54.190: INFO: stderr: ""
May  9 15:43:54.190: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/09/23 15:43:54.19
May  9 15:43:54.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  9 15:43:54.277: INFO: stderr: ""
May  9 15:43:54.277: INFO: stdout: "update-demo-nautilus-7v4sf update-demo-nautilus-cqgg5 "
STEP: Replicas for name=update-demo: expected=1 actual=2 05/09/23 15:43:54.277
May  9 15:43:59.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  9 15:43:59.355: INFO: stderr: ""
May  9 15:43:59.355: INFO: stdout: "update-demo-nautilus-cqgg5 "
May  9 15:43:59.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-cqgg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 15:43:59.433: INFO: stderr: ""
May  9 15:43:59.433: INFO: stdout: "true"
May  9 15:43:59.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-cqgg5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  9 15:43:59.512: INFO: stderr: ""
May  9 15:43:59.512: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May  9 15:43:59.512: INFO: validating pod update-demo-nautilus-cqgg5
May  9 15:43:59.518: INFO: got data: {
  "image": "nautilus.jpg"
}

May  9 15:43:59.518: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  9 15:43:59.518: INFO: update-demo-nautilus-cqgg5 is verified up and running
STEP: scaling up the replication controller 05/09/23 15:43:59.518
May  9 15:43:59.519: INFO: scanned /root for discovery docs: <nil>
May  9 15:43:59.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May  9 15:44:00.629: INFO: stderr: ""
May  9 15:44:00.629: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/09/23 15:44:00.629
May  9 15:44:00.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  9 15:44:00.721: INFO: stderr: ""
May  9 15:44:00.721: INFO: stdout: "update-demo-nautilus-cqgg5 update-demo-nautilus-pgn2d "
May  9 15:44:00.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-cqgg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 15:44:00.804: INFO: stderr: ""
May  9 15:44:00.804: INFO: stdout: "true"
May  9 15:44:00.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-cqgg5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  9 15:44:00.917: INFO: stderr: ""
May  9 15:44:00.917: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May  9 15:44:00.917: INFO: validating pod update-demo-nautilus-cqgg5
May  9 15:44:00.929: INFO: got data: {
  "image": "nautilus.jpg"
}

May  9 15:44:00.929: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  9 15:44:00.929: INFO: update-demo-nautilus-cqgg5 is verified up and running
May  9 15:44:00.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-pgn2d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 15:44:01.010: INFO: stderr: ""
May  9 15:44:01.010: INFO: stdout: ""
May  9 15:44:01.010: INFO: update-demo-nautilus-pgn2d is created but not running
May  9 15:44:06.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  9 15:44:06.121: INFO: stderr: ""
May  9 15:44:06.121: INFO: stdout: "update-demo-nautilus-cqgg5 update-demo-nautilus-pgn2d "
May  9 15:44:06.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-cqgg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 15:44:06.204: INFO: stderr: ""
May  9 15:44:06.204: INFO: stdout: "true"
May  9 15:44:06.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-cqgg5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  9 15:44:06.285: INFO: stderr: ""
May  9 15:44:06.286: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May  9 15:44:06.286: INFO: validating pod update-demo-nautilus-cqgg5
May  9 15:44:06.294: INFO: got data: {
  "image": "nautilus.jpg"
}

May  9 15:44:06.294: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  9 15:44:06.294: INFO: update-demo-nautilus-cqgg5 is verified up and running
May  9 15:44:06.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-pgn2d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 15:44:06.374: INFO: stderr: ""
May  9 15:44:06.374: INFO: stdout: "true"
May  9 15:44:06.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-pgn2d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  9 15:44:06.452: INFO: stderr: ""
May  9 15:44:06.452: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May  9 15:44:06.452: INFO: validating pod update-demo-nautilus-pgn2d
May  9 15:44:06.461: INFO: got data: {
  "image": "nautilus.jpg"
}

May  9 15:44:06.461: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  9 15:44:06.461: INFO: update-demo-nautilus-pgn2d is verified up and running
STEP: using delete to clean up resources 05/09/23 15:44:06.461
May  9 15:44:06.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 delete --grace-period=0 --force -f -'
May  9 15:44:06.603: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  9 15:44:06.603: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  9 15:44:06.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get rc,svc -l name=update-demo --no-headers'
May  9 15:44:06.709: INFO: stderr: "No resources found in kubectl-8969 namespace.\n"
May  9 15:44:06.710: INFO: stdout: ""
May  9 15:44:06.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  9 15:44:06.801: INFO: stderr: ""
May  9 15:44:06.801: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  9 15:44:06.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8969" for this suite. 05/09/23 15:44:06.808
------------------------------
â€¢ [SLOW TEST] [20.335 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:43:46.485
    May  9 15:43:46.485: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubectl 05/09/23 15:43:46.485
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:43:46.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:43:46.506
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 05/09/23 15:43:46.511
    May  9 15:43:46.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 create -f -'
    May  9 15:43:47.240: INFO: stderr: ""
    May  9 15:43:47.240: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/09/23 15:43:47.24
    May  9 15:43:47.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  9 15:43:47.393: INFO: stderr: ""
    May  9 15:43:47.393: INFO: stdout: "update-demo-nautilus-7v4sf update-demo-nautilus-cqgg5 "
    May  9 15:43:47.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-7v4sf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 15:43:47.668: INFO: stderr: ""
    May  9 15:43:47.668: INFO: stdout: ""
    May  9 15:43:47.668: INFO: update-demo-nautilus-7v4sf is created but not running
    May  9 15:43:52.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  9 15:43:52.755: INFO: stderr: ""
    May  9 15:43:52.755: INFO: stdout: "update-demo-nautilus-7v4sf update-demo-nautilus-cqgg5 "
    May  9 15:43:52.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-7v4sf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 15:43:52.833: INFO: stderr: ""
    May  9 15:43:52.833: INFO: stdout: "true"
    May  9 15:43:52.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-7v4sf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  9 15:43:52.910: INFO: stderr: ""
    May  9 15:43:52.910: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May  9 15:43:52.910: INFO: validating pod update-demo-nautilus-7v4sf
    May  9 15:43:52.919: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  9 15:43:52.919: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  9 15:43:52.919: INFO: update-demo-nautilus-7v4sf is verified up and running
    May  9 15:43:52.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-cqgg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 15:43:53.002: INFO: stderr: ""
    May  9 15:43:53.002: INFO: stdout: "true"
    May  9 15:43:53.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-cqgg5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  9 15:43:53.080: INFO: stderr: ""
    May  9 15:43:53.080: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May  9 15:43:53.080: INFO: validating pod update-demo-nautilus-cqgg5
    May  9 15:43:53.088: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  9 15:43:53.088: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  9 15:43:53.088: INFO: update-demo-nautilus-cqgg5 is verified up and running
    STEP: scaling down the replication controller 05/09/23 15:43:53.088
    May  9 15:43:53.090: INFO: scanned /root for discovery docs: <nil>
    May  9 15:43:53.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    May  9 15:43:54.190: INFO: stderr: ""
    May  9 15:43:54.190: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/09/23 15:43:54.19
    May  9 15:43:54.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  9 15:43:54.277: INFO: stderr: ""
    May  9 15:43:54.277: INFO: stdout: "update-demo-nautilus-7v4sf update-demo-nautilus-cqgg5 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 05/09/23 15:43:54.277
    May  9 15:43:59.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  9 15:43:59.355: INFO: stderr: ""
    May  9 15:43:59.355: INFO: stdout: "update-demo-nautilus-cqgg5 "
    May  9 15:43:59.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-cqgg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 15:43:59.433: INFO: stderr: ""
    May  9 15:43:59.433: INFO: stdout: "true"
    May  9 15:43:59.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-cqgg5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  9 15:43:59.512: INFO: stderr: ""
    May  9 15:43:59.512: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May  9 15:43:59.512: INFO: validating pod update-demo-nautilus-cqgg5
    May  9 15:43:59.518: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  9 15:43:59.518: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  9 15:43:59.518: INFO: update-demo-nautilus-cqgg5 is verified up and running
    STEP: scaling up the replication controller 05/09/23 15:43:59.518
    May  9 15:43:59.519: INFO: scanned /root for discovery docs: <nil>
    May  9 15:43:59.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    May  9 15:44:00.629: INFO: stderr: ""
    May  9 15:44:00.629: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/09/23 15:44:00.629
    May  9 15:44:00.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  9 15:44:00.721: INFO: stderr: ""
    May  9 15:44:00.721: INFO: stdout: "update-demo-nautilus-cqgg5 update-demo-nautilus-pgn2d "
    May  9 15:44:00.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-cqgg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 15:44:00.804: INFO: stderr: ""
    May  9 15:44:00.804: INFO: stdout: "true"
    May  9 15:44:00.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-cqgg5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  9 15:44:00.917: INFO: stderr: ""
    May  9 15:44:00.917: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May  9 15:44:00.917: INFO: validating pod update-demo-nautilus-cqgg5
    May  9 15:44:00.929: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  9 15:44:00.929: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  9 15:44:00.929: INFO: update-demo-nautilus-cqgg5 is verified up and running
    May  9 15:44:00.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-pgn2d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 15:44:01.010: INFO: stderr: ""
    May  9 15:44:01.010: INFO: stdout: ""
    May  9 15:44:01.010: INFO: update-demo-nautilus-pgn2d is created but not running
    May  9 15:44:06.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  9 15:44:06.121: INFO: stderr: ""
    May  9 15:44:06.121: INFO: stdout: "update-demo-nautilus-cqgg5 update-demo-nautilus-pgn2d "
    May  9 15:44:06.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-cqgg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 15:44:06.204: INFO: stderr: ""
    May  9 15:44:06.204: INFO: stdout: "true"
    May  9 15:44:06.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-cqgg5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  9 15:44:06.285: INFO: stderr: ""
    May  9 15:44:06.286: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May  9 15:44:06.286: INFO: validating pod update-demo-nautilus-cqgg5
    May  9 15:44:06.294: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  9 15:44:06.294: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  9 15:44:06.294: INFO: update-demo-nautilus-cqgg5 is verified up and running
    May  9 15:44:06.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-pgn2d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 15:44:06.374: INFO: stderr: ""
    May  9 15:44:06.374: INFO: stdout: "true"
    May  9 15:44:06.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods update-demo-nautilus-pgn2d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  9 15:44:06.452: INFO: stderr: ""
    May  9 15:44:06.452: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May  9 15:44:06.452: INFO: validating pod update-demo-nautilus-pgn2d
    May  9 15:44:06.461: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  9 15:44:06.461: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  9 15:44:06.461: INFO: update-demo-nautilus-pgn2d is verified up and running
    STEP: using delete to clean up resources 05/09/23 15:44:06.461
    May  9 15:44:06.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 delete --grace-period=0 --force -f -'
    May  9 15:44:06.603: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  9 15:44:06.603: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    May  9 15:44:06.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get rc,svc -l name=update-demo --no-headers'
    May  9 15:44:06.709: INFO: stderr: "No resources found in kubectl-8969 namespace.\n"
    May  9 15:44:06.710: INFO: stdout: ""
    May  9 15:44:06.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-8969 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May  9 15:44:06.801: INFO: stderr: ""
    May  9 15:44:06.801: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  9 15:44:06.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8969" for this suite. 05/09/23 15:44:06.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:44:06.822
May  9 15:44:06.822: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename svcaccounts 05/09/23 15:44:06.824
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:44:06.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:44:06.845
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
May  9 15:44:06.876: INFO: created pod pod-service-account-defaultsa
May  9 15:44:06.876: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May  9 15:44:06.884: INFO: created pod pod-service-account-mountsa
May  9 15:44:06.884: INFO: pod pod-service-account-mountsa service account token volume mount: true
May  9 15:44:06.891: INFO: created pod pod-service-account-nomountsa
May  9 15:44:06.891: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May  9 15:44:06.898: INFO: created pod pod-service-account-defaultsa-mountspec
May  9 15:44:06.898: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May  9 15:44:06.908: INFO: created pod pod-service-account-mountsa-mountspec
May  9 15:44:06.908: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May  9 15:44:06.917: INFO: created pod pod-service-account-nomountsa-mountspec
May  9 15:44:06.917: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May  9 15:44:06.931: INFO: created pod pod-service-account-defaultsa-nomountspec
May  9 15:44:06.931: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May  9 15:44:06.965: INFO: created pod pod-service-account-mountsa-nomountspec
May  9 15:44:06.965: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May  9 15:44:06.975: INFO: created pod pod-service-account-nomountsa-nomountspec
May  9 15:44:06.976: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May  9 15:44:06.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5624" for this suite. 05/09/23 15:44:06.983
------------------------------
â€¢ [0.169 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:44:06.822
    May  9 15:44:06.822: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename svcaccounts 05/09/23 15:44:06.824
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:44:06.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:44:06.845
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    May  9 15:44:06.876: INFO: created pod pod-service-account-defaultsa
    May  9 15:44:06.876: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    May  9 15:44:06.884: INFO: created pod pod-service-account-mountsa
    May  9 15:44:06.884: INFO: pod pod-service-account-mountsa service account token volume mount: true
    May  9 15:44:06.891: INFO: created pod pod-service-account-nomountsa
    May  9 15:44:06.891: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    May  9 15:44:06.898: INFO: created pod pod-service-account-defaultsa-mountspec
    May  9 15:44:06.898: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    May  9 15:44:06.908: INFO: created pod pod-service-account-mountsa-mountspec
    May  9 15:44:06.908: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    May  9 15:44:06.917: INFO: created pod pod-service-account-nomountsa-mountspec
    May  9 15:44:06.917: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    May  9 15:44:06.931: INFO: created pod pod-service-account-defaultsa-nomountspec
    May  9 15:44:06.931: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    May  9 15:44:06.965: INFO: created pod pod-service-account-mountsa-nomountspec
    May  9 15:44:06.965: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    May  9 15:44:06.975: INFO: created pod pod-service-account-nomountsa-nomountspec
    May  9 15:44:06.976: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May  9 15:44:06.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5624" for this suite. 05/09/23 15:44:06.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:44:06.992
May  9 15:44:06.992: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename container-runtime 05/09/23 15:44:06.993
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:44:07.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:44:07.014
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 05/09/23 15:44:07.062
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 05/09/23 15:44:23.184
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 05/09/23 15:44:23.19
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 05/09/23 15:44:23.203
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 05/09/23 15:44:23.203
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 05/09/23 15:44:23.228
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 05/09/23 15:44:26.256
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 05/09/23 15:44:27.269
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 05/09/23 15:44:27.283
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 05/09/23 15:44:27.283
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 05/09/23 15:44:27.35
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 05/09/23 15:44:28.363
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 05/09/23 15:44:32.428
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 05/09/23 15:44:32.44
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 05/09/23 15:44:32.44
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May  9 15:44:32.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-986" for this suite. 05/09/23 15:44:32.486
------------------------------
â€¢ [SLOW TEST] [25.505 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:44:06.992
    May  9 15:44:06.992: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename container-runtime 05/09/23 15:44:06.993
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:44:07.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:44:07.014
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 05/09/23 15:44:07.062
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 05/09/23 15:44:23.184
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 05/09/23 15:44:23.19
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 05/09/23 15:44:23.203
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 05/09/23 15:44:23.203
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 05/09/23 15:44:23.228
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 05/09/23 15:44:26.256
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 05/09/23 15:44:27.269
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 05/09/23 15:44:27.283
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 05/09/23 15:44:27.283
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 05/09/23 15:44:27.35
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 05/09/23 15:44:28.363
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 05/09/23 15:44:32.428
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 05/09/23 15:44:32.44
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 05/09/23 15:44:32.44
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May  9 15:44:32.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-986" for this suite. 05/09/23 15:44:32.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:44:32.498
May  9 15:44:32.498: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename resourcequota 05/09/23 15:44:32.499
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:44:32.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:44:32.523
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 05/09/23 15:44:32.527
STEP: Creating a ResourceQuota 05/09/23 15:44:37.532
STEP: Ensuring resource quota status is calculated 05/09/23 15:44:37.54
STEP: Creating a Service 05/09/23 15:44:39.546
STEP: Creating a NodePort Service 05/09/23 15:44:39.565
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 05/09/23 15:44:39.59
STEP: Ensuring resource quota status captures service creation 05/09/23 15:44:39.664
STEP: Deleting Services 05/09/23 15:44:41.669
STEP: Ensuring resource quota status released usage 05/09/23 15:44:41.709
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  9 15:44:43.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4409" for this suite. 05/09/23 15:44:43.722
------------------------------
â€¢ [SLOW TEST] [11.234 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:44:32.498
    May  9 15:44:32.498: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename resourcequota 05/09/23 15:44:32.499
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:44:32.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:44:32.523
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 05/09/23 15:44:32.527
    STEP: Creating a ResourceQuota 05/09/23 15:44:37.532
    STEP: Ensuring resource quota status is calculated 05/09/23 15:44:37.54
    STEP: Creating a Service 05/09/23 15:44:39.546
    STEP: Creating a NodePort Service 05/09/23 15:44:39.565
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 05/09/23 15:44:39.59
    STEP: Ensuring resource quota status captures service creation 05/09/23 15:44:39.664
    STEP: Deleting Services 05/09/23 15:44:41.669
    STEP: Ensuring resource quota status released usage 05/09/23 15:44:41.709
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  9 15:44:43.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4409" for this suite. 05/09/23 15:44:43.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:44:43.734
May  9 15:44:43.734: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename secrets 05/09/23 15:44:43.735
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:44:43.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:44:43.76
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-360426ea-f38c-4376-8318-ccbe3d77dad7 05/09/23 15:44:43.764
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
May  9 15:44:43.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4754" for this suite. 05/09/23 15:44:43.773
------------------------------
â€¢ [0.047 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:44:43.734
    May  9 15:44:43.734: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename secrets 05/09/23 15:44:43.735
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:44:43.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:44:43.76
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-360426ea-f38c-4376-8318-ccbe3d77dad7 05/09/23 15:44:43.764
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    May  9 15:44:43.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4754" for this suite. 05/09/23 15:44:43.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:44:43.782
May  9 15:44:43.782: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename configmap 05/09/23 15:44:43.783
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:44:43.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:44:43.805
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-5850/configmap-test-5eb6f56d-5b9c-4a48-b814-cf8d4b3dc09c 05/09/23 15:44:43.81
STEP: Creating a pod to test consume configMaps 05/09/23 15:44:43.816
May  9 15:44:43.827: INFO: Waiting up to 5m0s for pod "pod-configmaps-24bcaa88-1154-49ba-8222-a98eb47f87ab" in namespace "configmap-5850" to be "Succeeded or Failed"
May  9 15:44:43.835: INFO: Pod "pod-configmaps-24bcaa88-1154-49ba-8222-a98eb47f87ab": Phase="Pending", Reason="", readiness=false. Elapsed: 7.848122ms
May  9 15:44:45.842: INFO: Pod "pod-configmaps-24bcaa88-1154-49ba-8222-a98eb47f87ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015176393s
May  9 15:44:47.843: INFO: Pod "pod-configmaps-24bcaa88-1154-49ba-8222-a98eb47f87ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015604477s
STEP: Saw pod success 05/09/23 15:44:47.843
May  9 15:44:47.843: INFO: Pod "pod-configmaps-24bcaa88-1154-49ba-8222-a98eb47f87ab" satisfied condition "Succeeded or Failed"
May  9 15:44:47.847: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 pod pod-configmaps-24bcaa88-1154-49ba-8222-a98eb47f87ab container env-test: <nil>
STEP: delete the pod 05/09/23 15:44:47.903
May  9 15:44:47.919: INFO: Waiting for pod pod-configmaps-24bcaa88-1154-49ba-8222-a98eb47f87ab to disappear
May  9 15:44:47.924: INFO: Pod pod-configmaps-24bcaa88-1154-49ba-8222-a98eb47f87ab no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  9 15:44:47.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5850" for this suite. 05/09/23 15:44:47.962
------------------------------
â€¢ [4.190 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:44:43.782
    May  9 15:44:43.782: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename configmap 05/09/23 15:44:43.783
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:44:43.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:44:43.805
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-5850/configmap-test-5eb6f56d-5b9c-4a48-b814-cf8d4b3dc09c 05/09/23 15:44:43.81
    STEP: Creating a pod to test consume configMaps 05/09/23 15:44:43.816
    May  9 15:44:43.827: INFO: Waiting up to 5m0s for pod "pod-configmaps-24bcaa88-1154-49ba-8222-a98eb47f87ab" in namespace "configmap-5850" to be "Succeeded or Failed"
    May  9 15:44:43.835: INFO: Pod "pod-configmaps-24bcaa88-1154-49ba-8222-a98eb47f87ab": Phase="Pending", Reason="", readiness=false. Elapsed: 7.848122ms
    May  9 15:44:45.842: INFO: Pod "pod-configmaps-24bcaa88-1154-49ba-8222-a98eb47f87ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015176393s
    May  9 15:44:47.843: INFO: Pod "pod-configmaps-24bcaa88-1154-49ba-8222-a98eb47f87ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015604477s
    STEP: Saw pod success 05/09/23 15:44:47.843
    May  9 15:44:47.843: INFO: Pod "pod-configmaps-24bcaa88-1154-49ba-8222-a98eb47f87ab" satisfied condition "Succeeded or Failed"
    May  9 15:44:47.847: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 pod pod-configmaps-24bcaa88-1154-49ba-8222-a98eb47f87ab container env-test: <nil>
    STEP: delete the pod 05/09/23 15:44:47.903
    May  9 15:44:47.919: INFO: Waiting for pod pod-configmaps-24bcaa88-1154-49ba-8222-a98eb47f87ab to disappear
    May  9 15:44:47.924: INFO: Pod pod-configmaps-24bcaa88-1154-49ba-8222-a98eb47f87ab no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  9 15:44:47.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5850" for this suite. 05/09/23 15:44:47.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:44:47.973
May  9 15:44:47.973: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename statefulset 05/09/23 15:44:47.974
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:44:47.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:44:47.995
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3365 05/09/23 15:44:48
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 05/09/23 15:44:48.007
May  9 15:44:48.021: INFO: Found 0 stateful pods, waiting for 3
May  9 15:44:58.029: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  9 15:44:58.029: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  9 15:44:58.029: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 05/09/23 15:44:58.05
May  9 15:44:58.083: INFO: Updating stateful set ss2
STEP: Creating a new revision 05/09/23 15:44:58.083
STEP: Not applying an update when the partition is greater than the number of replicas 05/09/23 15:45:08.107
STEP: Performing a canary update 05/09/23 15:45:08.107
May  9 15:45:08.133: INFO: Updating stateful set ss2
May  9 15:45:08.144: INFO: Waiting for Pod statefulset-3365/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 05/09/23 15:45:18.157
May  9 15:45:18.194: INFO: Found 2 stateful pods, waiting for 3
May  9 15:45:28.203: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  9 15:45:28.203: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  9 15:45:28.203: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 05/09/23 15:45:28.216
May  9 15:45:28.252: INFO: Updating stateful set ss2
May  9 15:45:28.273: INFO: Waiting for Pod statefulset-3365/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
May  9 15:45:38.598: INFO: Updating stateful set ss2
May  9 15:45:38.609: INFO: Waiting for StatefulSet statefulset-3365/ss2 to complete update
May  9 15:45:38.610: INFO: Waiting for Pod statefulset-3365/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May  9 15:45:48.626: INFO: Deleting all statefulset in ns statefulset-3365
May  9 15:45:48.631: INFO: Scaling statefulset ss2 to 0
May  9 15:45:58.661: INFO: Waiting for statefulset status.replicas updated to 0
May  9 15:45:58.666: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May  9 15:45:58.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3365" for this suite. 05/09/23 15:45:58.697
------------------------------
â€¢ [SLOW TEST] [70.733 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:44:47.973
    May  9 15:44:47.973: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename statefulset 05/09/23 15:44:47.974
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:44:47.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:44:47.995
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3365 05/09/23 15:44:48
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 05/09/23 15:44:48.007
    May  9 15:44:48.021: INFO: Found 0 stateful pods, waiting for 3
    May  9 15:44:58.029: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May  9 15:44:58.029: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May  9 15:44:58.029: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 05/09/23 15:44:58.05
    May  9 15:44:58.083: INFO: Updating stateful set ss2
    STEP: Creating a new revision 05/09/23 15:44:58.083
    STEP: Not applying an update when the partition is greater than the number of replicas 05/09/23 15:45:08.107
    STEP: Performing a canary update 05/09/23 15:45:08.107
    May  9 15:45:08.133: INFO: Updating stateful set ss2
    May  9 15:45:08.144: INFO: Waiting for Pod statefulset-3365/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 05/09/23 15:45:18.157
    May  9 15:45:18.194: INFO: Found 2 stateful pods, waiting for 3
    May  9 15:45:28.203: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May  9 15:45:28.203: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May  9 15:45:28.203: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 05/09/23 15:45:28.216
    May  9 15:45:28.252: INFO: Updating stateful set ss2
    May  9 15:45:28.273: INFO: Waiting for Pod statefulset-3365/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    May  9 15:45:38.598: INFO: Updating stateful set ss2
    May  9 15:45:38.609: INFO: Waiting for StatefulSet statefulset-3365/ss2 to complete update
    May  9 15:45:38.610: INFO: Waiting for Pod statefulset-3365/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May  9 15:45:48.626: INFO: Deleting all statefulset in ns statefulset-3365
    May  9 15:45:48.631: INFO: Scaling statefulset ss2 to 0
    May  9 15:45:58.661: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 15:45:58.666: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May  9 15:45:58.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3365" for this suite. 05/09/23 15:45:58.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:45:58.706
May  9 15:45:58.706: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename configmap 05/09/23 15:45:58.707
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:45:58.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:45:58.731
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-10d28bd3-a7f0-41f3-afad-4786a4c5fc1a 05/09/23 15:45:58.735
STEP: Creating a pod to test consume configMaps 05/09/23 15:45:58.74
May  9 15:45:58.751: INFO: Waiting up to 5m0s for pod "pod-configmaps-ecc883a8-dc96-4a35-8c05-1ca9dcacfb48" in namespace "configmap-5739" to be "Succeeded or Failed"
May  9 15:45:58.755: INFO: Pod "pod-configmaps-ecc883a8-dc96-4a35-8c05-1ca9dcacfb48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.197788ms
May  9 15:46:00.761: INFO: Pod "pod-configmaps-ecc883a8-dc96-4a35-8c05-1ca9dcacfb48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010028259s
May  9 15:46:02.766: INFO: Pod "pod-configmaps-ecc883a8-dc96-4a35-8c05-1ca9dcacfb48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014427534s
STEP: Saw pod success 05/09/23 15:46:02.766
May  9 15:46:02.766: INFO: Pod "pod-configmaps-ecc883a8-dc96-4a35-8c05-1ca9dcacfb48" satisfied condition "Succeeded or Failed"
May  9 15:46:02.773: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 pod pod-configmaps-ecc883a8-dc96-4a35-8c05-1ca9dcacfb48 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 15:46:02.784
May  9 15:46:02.800: INFO: Waiting for pod pod-configmaps-ecc883a8-dc96-4a35-8c05-1ca9dcacfb48 to disappear
May  9 15:46:02.805: INFO: Pod pod-configmaps-ecc883a8-dc96-4a35-8c05-1ca9dcacfb48 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  9 15:46:02.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5739" for this suite. 05/09/23 15:46:02.811
------------------------------
â€¢ [4.117 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:45:58.706
    May  9 15:45:58.706: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename configmap 05/09/23 15:45:58.707
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:45:58.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:45:58.731
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-10d28bd3-a7f0-41f3-afad-4786a4c5fc1a 05/09/23 15:45:58.735
    STEP: Creating a pod to test consume configMaps 05/09/23 15:45:58.74
    May  9 15:45:58.751: INFO: Waiting up to 5m0s for pod "pod-configmaps-ecc883a8-dc96-4a35-8c05-1ca9dcacfb48" in namespace "configmap-5739" to be "Succeeded or Failed"
    May  9 15:45:58.755: INFO: Pod "pod-configmaps-ecc883a8-dc96-4a35-8c05-1ca9dcacfb48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.197788ms
    May  9 15:46:00.761: INFO: Pod "pod-configmaps-ecc883a8-dc96-4a35-8c05-1ca9dcacfb48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010028259s
    May  9 15:46:02.766: INFO: Pod "pod-configmaps-ecc883a8-dc96-4a35-8c05-1ca9dcacfb48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014427534s
    STEP: Saw pod success 05/09/23 15:46:02.766
    May  9 15:46:02.766: INFO: Pod "pod-configmaps-ecc883a8-dc96-4a35-8c05-1ca9dcacfb48" satisfied condition "Succeeded or Failed"
    May  9 15:46:02.773: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 pod pod-configmaps-ecc883a8-dc96-4a35-8c05-1ca9dcacfb48 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 15:46:02.784
    May  9 15:46:02.800: INFO: Waiting for pod pod-configmaps-ecc883a8-dc96-4a35-8c05-1ca9dcacfb48 to disappear
    May  9 15:46:02.805: INFO: Pod pod-configmaps-ecc883a8-dc96-4a35-8c05-1ca9dcacfb48 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  9 15:46:02.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5739" for this suite. 05/09/23 15:46:02.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:46:02.825
May  9 15:46:02.825: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename dns 05/09/23 15:46:02.826
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:46:02.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:46:02.846
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 05/09/23 15:46:02.856
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2697 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2697;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2697 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2697;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2697.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2697.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2697.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2697.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2697.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2697.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2697.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2697.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2697.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2697.svc;check="$$(dig +notcp +noall +answer +search 14.38.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.38.14_udp@PTR;check="$$(dig +tcp +noall +answer +search 14.38.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.38.14_tcp@PTR;sleep 1; done
 05/09/23 15:46:02.885
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2697 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2697;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2697 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2697;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2697.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2697.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2697.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2697.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2697.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2697.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2697.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2697.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2697.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2697.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2697.svc;check="$$(dig +notcp +noall +answer +search 14.38.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.38.14_udp@PTR;check="$$(dig +tcp +noall +answer +search 14.38.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.38.14_tcp@PTR;sleep 1; done
 05/09/23 15:46:02.885
STEP: creating a pod to probe DNS 05/09/23 15:46:02.885
STEP: submitting the pod to kubernetes 05/09/23 15:46:02.885
May  9 15:46:02.898: INFO: Waiting up to 15m0s for pod "dns-test-4a483388-79ab-449b-8604-911bbb9d6d84" in namespace "dns-2697" to be "running"
May  9 15:46:02.902: INFO: Pod "dns-test-4a483388-79ab-449b-8604-911bbb9d6d84": Phase="Pending", Reason="", readiness=false. Elapsed: 4.237928ms
May  9 15:46:04.909: INFO: Pod "dns-test-4a483388-79ab-449b-8604-911bbb9d6d84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011181367s
May  9 15:46:06.927: INFO: Pod "dns-test-4a483388-79ab-449b-8604-911bbb9d6d84": Phase="Running", Reason="", readiness=true. Elapsed: 4.029321656s
May  9 15:46:06.927: INFO: Pod "dns-test-4a483388-79ab-449b-8604-911bbb9d6d84" satisfied condition "running"
STEP: retrieving the pod 05/09/23 15:46:06.927
STEP: looking for the results for each expected name from probers 05/09/23 15:46:06.933
May  9 15:46:06.939: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:06.944: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:06.951: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:06.957: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:06.962: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:06.969: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:06.975: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:06.981: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:07.018: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:07.025: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:07.034: INFO: Unable to read jessie_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:07.043: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:07.050: INFO: Unable to read jessie_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:07.056: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:07.063: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:07.069: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:07.100: INFO: Lookups using dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2697 wheezy_tcp@dns-test-service.dns-2697 wheezy_udp@dns-test-service.dns-2697.svc wheezy_tcp@dns-test-service.dns-2697.svc wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2697 jessie_tcp@dns-test-service.dns-2697 jessie_udp@dns-test-service.dns-2697.svc jessie_tcp@dns-test-service.dns-2697.svc jessie_udp@_http._tcp.dns-test-service.dns-2697.svc jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc]

May  9 15:46:12.112: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:12.120: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:12.141: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:12.149: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:12.156: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:12.163: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:12.170: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:12.177: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:12.222: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:12.248: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:12.255: INFO: Unable to read jessie_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:12.262: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:12.268: INFO: Unable to read jessie_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:12.275: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:12.281: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:12.288: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:12.315: INFO: Lookups using dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2697 wheezy_tcp@dns-test-service.dns-2697 wheezy_udp@dns-test-service.dns-2697.svc wheezy_tcp@dns-test-service.dns-2697.svc wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2697 jessie_tcp@dns-test-service.dns-2697 jessie_udp@dns-test-service.dns-2697.svc jessie_tcp@dns-test-service.dns-2697.svc jessie_udp@_http._tcp.dns-test-service.dns-2697.svc jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc]

May  9 15:46:17.111: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:17.118: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:17.125: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:17.131: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:17.138: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:17.144: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:17.150: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:17.155: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:17.184: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:17.190: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:17.196: INFO: Unable to read jessie_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:17.203: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:17.209: INFO: Unable to read jessie_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:17.216: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:17.222: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:17.228: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:17.257: INFO: Lookups using dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2697 wheezy_tcp@dns-test-service.dns-2697 wheezy_udp@dns-test-service.dns-2697.svc wheezy_tcp@dns-test-service.dns-2697.svc wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2697 jessie_tcp@dns-test-service.dns-2697 jessie_udp@dns-test-service.dns-2697.svc jessie_tcp@dns-test-service.dns-2697.svc jessie_udp@_http._tcp.dns-test-service.dns-2697.svc jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc]

May  9 15:46:22.107: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:22.115: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:22.121: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:22.128: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:22.134: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:22.140: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:22.147: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:22.153: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:22.183: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:22.189: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:22.196: INFO: Unable to read jessie_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:22.204: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:22.209: INFO: Unable to read jessie_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:22.215: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:22.222: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:22.230: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:22.295: INFO: Lookups using dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2697 wheezy_tcp@dns-test-service.dns-2697 wheezy_udp@dns-test-service.dns-2697.svc wheezy_tcp@dns-test-service.dns-2697.svc wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2697 jessie_tcp@dns-test-service.dns-2697 jessie_udp@dns-test-service.dns-2697.svc jessie_tcp@dns-test-service.dns-2697.svc jessie_udp@_http._tcp.dns-test-service.dns-2697.svc jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc]

May  9 15:46:27.108: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:27.113: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:27.129: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:27.135: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:27.143: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:27.150: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:27.157: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:27.163: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:27.202: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:27.209: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:27.218: INFO: Unable to read jessie_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:27.224: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:27.231: INFO: Unable to read jessie_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:27.237: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:27.244: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:27.250: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:27.277: INFO: Lookups using dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2697 wheezy_tcp@dns-test-service.dns-2697 wheezy_udp@dns-test-service.dns-2697.svc wheezy_tcp@dns-test-service.dns-2697.svc wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2697 jessie_tcp@dns-test-service.dns-2697 jessie_udp@dns-test-service.dns-2697.svc jessie_tcp@dns-test-service.dns-2697.svc jessie_udp@_http._tcp.dns-test-service.dns-2697.svc jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc]

May  9 15:46:32.111: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:32.118: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:32.125: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:32.132: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:32.138: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:32.143: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:32.149: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:32.156: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:32.189: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:32.196: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:32.202: INFO: Unable to read jessie_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:32.209: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:32.215: INFO: Unable to read jessie_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:32.221: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:32.226: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:32.232: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
May  9 15:46:32.257: INFO: Lookups using dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2697 wheezy_tcp@dns-test-service.dns-2697 wheezy_udp@dns-test-service.dns-2697.svc wheezy_tcp@dns-test-service.dns-2697.svc wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2697 jessie_tcp@dns-test-service.dns-2697 jessie_udp@dns-test-service.dns-2697.svc jessie_tcp@dns-test-service.dns-2697.svc jessie_udp@_http._tcp.dns-test-service.dns-2697.svc jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc]

May  9 15:46:37.259: INFO: DNS probes using dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84 succeeded

STEP: deleting the pod 05/09/23 15:46:37.259
STEP: deleting the test service 05/09/23 15:46:37.282
STEP: deleting the test headless service 05/09/23 15:46:37.314
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May  9 15:46:37.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2697" for this suite. 05/09/23 15:46:37.335
------------------------------
â€¢ [SLOW TEST] [34.519 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:46:02.825
    May  9 15:46:02.825: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename dns 05/09/23 15:46:02.826
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:46:02.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:46:02.846
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 05/09/23 15:46:02.856
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2697 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2697;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2697 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2697;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2697.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2697.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2697.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2697.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2697.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2697.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2697.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2697.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2697.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2697.svc;check="$$(dig +notcp +noall +answer +search 14.38.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.38.14_udp@PTR;check="$$(dig +tcp +noall +answer +search 14.38.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.38.14_tcp@PTR;sleep 1; done
     05/09/23 15:46:02.885
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2697 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2697;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2697 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2697;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2697.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2697.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2697.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2697.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2697.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2697.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2697.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2697.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2697.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2697.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2697.svc;check="$$(dig +notcp +noall +answer +search 14.38.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.38.14_udp@PTR;check="$$(dig +tcp +noall +answer +search 14.38.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.38.14_tcp@PTR;sleep 1; done
     05/09/23 15:46:02.885
    STEP: creating a pod to probe DNS 05/09/23 15:46:02.885
    STEP: submitting the pod to kubernetes 05/09/23 15:46:02.885
    May  9 15:46:02.898: INFO: Waiting up to 15m0s for pod "dns-test-4a483388-79ab-449b-8604-911bbb9d6d84" in namespace "dns-2697" to be "running"
    May  9 15:46:02.902: INFO: Pod "dns-test-4a483388-79ab-449b-8604-911bbb9d6d84": Phase="Pending", Reason="", readiness=false. Elapsed: 4.237928ms
    May  9 15:46:04.909: INFO: Pod "dns-test-4a483388-79ab-449b-8604-911bbb9d6d84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011181367s
    May  9 15:46:06.927: INFO: Pod "dns-test-4a483388-79ab-449b-8604-911bbb9d6d84": Phase="Running", Reason="", readiness=true. Elapsed: 4.029321656s
    May  9 15:46:06.927: INFO: Pod "dns-test-4a483388-79ab-449b-8604-911bbb9d6d84" satisfied condition "running"
    STEP: retrieving the pod 05/09/23 15:46:06.927
    STEP: looking for the results for each expected name from probers 05/09/23 15:46:06.933
    May  9 15:46:06.939: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:06.944: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:06.951: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:06.957: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:06.962: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:06.969: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:06.975: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:06.981: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:07.018: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:07.025: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:07.034: INFO: Unable to read jessie_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:07.043: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:07.050: INFO: Unable to read jessie_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:07.056: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:07.063: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:07.069: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:07.100: INFO: Lookups using dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2697 wheezy_tcp@dns-test-service.dns-2697 wheezy_udp@dns-test-service.dns-2697.svc wheezy_tcp@dns-test-service.dns-2697.svc wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2697 jessie_tcp@dns-test-service.dns-2697 jessie_udp@dns-test-service.dns-2697.svc jessie_tcp@dns-test-service.dns-2697.svc jessie_udp@_http._tcp.dns-test-service.dns-2697.svc jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc]

    May  9 15:46:12.112: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:12.120: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:12.141: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:12.149: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:12.156: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:12.163: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:12.170: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:12.177: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:12.222: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:12.248: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:12.255: INFO: Unable to read jessie_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:12.262: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:12.268: INFO: Unable to read jessie_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:12.275: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:12.281: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:12.288: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:12.315: INFO: Lookups using dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2697 wheezy_tcp@dns-test-service.dns-2697 wheezy_udp@dns-test-service.dns-2697.svc wheezy_tcp@dns-test-service.dns-2697.svc wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2697 jessie_tcp@dns-test-service.dns-2697 jessie_udp@dns-test-service.dns-2697.svc jessie_tcp@dns-test-service.dns-2697.svc jessie_udp@_http._tcp.dns-test-service.dns-2697.svc jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc]

    May  9 15:46:17.111: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:17.118: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:17.125: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:17.131: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:17.138: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:17.144: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:17.150: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:17.155: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:17.184: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:17.190: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:17.196: INFO: Unable to read jessie_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:17.203: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:17.209: INFO: Unable to read jessie_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:17.216: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:17.222: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:17.228: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:17.257: INFO: Lookups using dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2697 wheezy_tcp@dns-test-service.dns-2697 wheezy_udp@dns-test-service.dns-2697.svc wheezy_tcp@dns-test-service.dns-2697.svc wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2697 jessie_tcp@dns-test-service.dns-2697 jessie_udp@dns-test-service.dns-2697.svc jessie_tcp@dns-test-service.dns-2697.svc jessie_udp@_http._tcp.dns-test-service.dns-2697.svc jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc]

    May  9 15:46:22.107: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:22.115: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:22.121: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:22.128: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:22.134: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:22.140: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:22.147: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:22.153: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:22.183: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:22.189: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:22.196: INFO: Unable to read jessie_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:22.204: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:22.209: INFO: Unable to read jessie_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:22.215: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:22.222: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:22.230: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:22.295: INFO: Lookups using dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2697 wheezy_tcp@dns-test-service.dns-2697 wheezy_udp@dns-test-service.dns-2697.svc wheezy_tcp@dns-test-service.dns-2697.svc wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2697 jessie_tcp@dns-test-service.dns-2697 jessie_udp@dns-test-service.dns-2697.svc jessie_tcp@dns-test-service.dns-2697.svc jessie_udp@_http._tcp.dns-test-service.dns-2697.svc jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc]

    May  9 15:46:27.108: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:27.113: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:27.129: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:27.135: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:27.143: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:27.150: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:27.157: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:27.163: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:27.202: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:27.209: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:27.218: INFO: Unable to read jessie_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:27.224: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:27.231: INFO: Unable to read jessie_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:27.237: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:27.244: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:27.250: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:27.277: INFO: Lookups using dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2697 wheezy_tcp@dns-test-service.dns-2697 wheezy_udp@dns-test-service.dns-2697.svc wheezy_tcp@dns-test-service.dns-2697.svc wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2697 jessie_tcp@dns-test-service.dns-2697 jessie_udp@dns-test-service.dns-2697.svc jessie_tcp@dns-test-service.dns-2697.svc jessie_udp@_http._tcp.dns-test-service.dns-2697.svc jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc]

    May  9 15:46:32.111: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:32.118: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:32.125: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:32.132: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:32.138: INFO: Unable to read wheezy_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:32.143: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:32.149: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:32.156: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:32.189: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:32.196: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:32.202: INFO: Unable to read jessie_udp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:32.209: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697 from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:32.215: INFO: Unable to read jessie_udp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:32.221: INFO: Unable to read jessie_tcp@dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:32.226: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:32.232: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc from pod dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84: the server could not find the requested resource (get pods dns-test-4a483388-79ab-449b-8604-911bbb9d6d84)
    May  9 15:46:32.257: INFO: Lookups using dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2697 wheezy_tcp@dns-test-service.dns-2697 wheezy_udp@dns-test-service.dns-2697.svc wheezy_tcp@dns-test-service.dns-2697.svc wheezy_udp@_http._tcp.dns-test-service.dns-2697.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2697.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2697 jessie_tcp@dns-test-service.dns-2697 jessie_udp@dns-test-service.dns-2697.svc jessie_tcp@dns-test-service.dns-2697.svc jessie_udp@_http._tcp.dns-test-service.dns-2697.svc jessie_tcp@_http._tcp.dns-test-service.dns-2697.svc]

    May  9 15:46:37.259: INFO: DNS probes using dns-2697/dns-test-4a483388-79ab-449b-8604-911bbb9d6d84 succeeded

    STEP: deleting the pod 05/09/23 15:46:37.259
    STEP: deleting the test service 05/09/23 15:46:37.282
    STEP: deleting the test headless service 05/09/23 15:46:37.314
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May  9 15:46:37.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2697" for this suite. 05/09/23 15:46:37.335
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:46:37.345
May  9 15:46:37.345: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename configmap 05/09/23 15:46:37.346
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:46:37.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:46:37.379
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-e6a2f948-c16f-489c-aafa-5f28d78e62b3 05/09/23 15:46:37.384
STEP: Creating a pod to test consume configMaps 05/09/23 15:46:37.391
May  9 15:46:37.402: INFO: Waiting up to 5m0s for pod "pod-configmaps-deabeb22-b2f6-4ef5-b0cd-c1e488e0c17d" in namespace "configmap-277" to be "Succeeded or Failed"
May  9 15:46:37.408: INFO: Pod "pod-configmaps-deabeb22-b2f6-4ef5-b0cd-c1e488e0c17d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.163778ms
May  9 15:46:39.419: INFO: Pod "pod-configmaps-deabeb22-b2f6-4ef5-b0cd-c1e488e0c17d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01675425s
May  9 15:46:41.417: INFO: Pod "pod-configmaps-deabeb22-b2f6-4ef5-b0cd-c1e488e0c17d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015205136s
STEP: Saw pod success 05/09/23 15:46:41.417
May  9 15:46:41.417: INFO: Pod "pod-configmaps-deabeb22-b2f6-4ef5-b0cd-c1e488e0c17d" satisfied condition "Succeeded or Failed"
May  9 15:46:41.426: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-configmaps-deabeb22-b2f6-4ef5-b0cd-c1e488e0c17d container agnhost-container: <nil>
STEP: delete the pod 05/09/23 15:46:41.512
May  9 15:46:41.528: INFO: Waiting for pod pod-configmaps-deabeb22-b2f6-4ef5-b0cd-c1e488e0c17d to disappear
May  9 15:46:41.533: INFO: Pod pod-configmaps-deabeb22-b2f6-4ef5-b0cd-c1e488e0c17d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  9 15:46:41.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-277" for this suite. 05/09/23 15:46:41.54
------------------------------
â€¢ [4.203 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:46:37.345
    May  9 15:46:37.345: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename configmap 05/09/23 15:46:37.346
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:46:37.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:46:37.379
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-e6a2f948-c16f-489c-aafa-5f28d78e62b3 05/09/23 15:46:37.384
    STEP: Creating a pod to test consume configMaps 05/09/23 15:46:37.391
    May  9 15:46:37.402: INFO: Waiting up to 5m0s for pod "pod-configmaps-deabeb22-b2f6-4ef5-b0cd-c1e488e0c17d" in namespace "configmap-277" to be "Succeeded or Failed"
    May  9 15:46:37.408: INFO: Pod "pod-configmaps-deabeb22-b2f6-4ef5-b0cd-c1e488e0c17d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.163778ms
    May  9 15:46:39.419: INFO: Pod "pod-configmaps-deabeb22-b2f6-4ef5-b0cd-c1e488e0c17d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01675425s
    May  9 15:46:41.417: INFO: Pod "pod-configmaps-deabeb22-b2f6-4ef5-b0cd-c1e488e0c17d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015205136s
    STEP: Saw pod success 05/09/23 15:46:41.417
    May  9 15:46:41.417: INFO: Pod "pod-configmaps-deabeb22-b2f6-4ef5-b0cd-c1e488e0c17d" satisfied condition "Succeeded or Failed"
    May  9 15:46:41.426: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-configmaps-deabeb22-b2f6-4ef5-b0cd-c1e488e0c17d container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 15:46:41.512
    May  9 15:46:41.528: INFO: Waiting for pod pod-configmaps-deabeb22-b2f6-4ef5-b0cd-c1e488e0c17d to disappear
    May  9 15:46:41.533: INFO: Pod pod-configmaps-deabeb22-b2f6-4ef5-b0cd-c1e488e0c17d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  9 15:46:41.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-277" for this suite. 05/09/23 15:46:41.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:46:41.549
May  9 15:46:41.549: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename resourcequota 05/09/23 15:46:41.55
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:46:41.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:46:41.572
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 05/09/23 15:46:41.576
STEP: Getting a ResourceQuota 05/09/23 15:46:41.582
STEP: Updating a ResourceQuota 05/09/23 15:46:41.588
STEP: Verifying a ResourceQuota was modified 05/09/23 15:46:41.594
STEP: Deleting a ResourceQuota 05/09/23 15:46:41.599
STEP: Verifying the deleted ResourceQuota 05/09/23 15:46:41.606
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  9 15:46:41.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7457" for this suite. 05/09/23 15:46:41.618
------------------------------
â€¢ [0.079 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:46:41.549
    May  9 15:46:41.549: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename resourcequota 05/09/23 15:46:41.55
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:46:41.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:46:41.572
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 05/09/23 15:46:41.576
    STEP: Getting a ResourceQuota 05/09/23 15:46:41.582
    STEP: Updating a ResourceQuota 05/09/23 15:46:41.588
    STEP: Verifying a ResourceQuota was modified 05/09/23 15:46:41.594
    STEP: Deleting a ResourceQuota 05/09/23 15:46:41.599
    STEP: Verifying the deleted ResourceQuota 05/09/23 15:46:41.606
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  9 15:46:41.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7457" for this suite. 05/09/23 15:46:41.618
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:46:41.628
May  9 15:46:41.629: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubectl 05/09/23 15:46:41.629
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:46:41.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:46:41.649
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/09/23 15:46:41.653
May  9 15:46:41.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4552 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May  9 15:46:41.737: INFO: stderr: ""
May  9 15:46:41.737: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 05/09/23 15:46:41.737
May  9 15:46:41.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4552 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
May  9 15:46:41.954: INFO: stderr: ""
May  9 15:46:41.954: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/09/23 15:46:41.954
May  9 15:46:41.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4552 delete pods e2e-test-httpd-pod'
May  9 15:46:44.738: INFO: stderr: ""
May  9 15:46:44.738: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  9 15:46:44.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4552" for this suite. 05/09/23 15:46:44.745
------------------------------
â€¢ [3.124 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:46:41.628
    May  9 15:46:41.629: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubectl 05/09/23 15:46:41.629
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:46:41.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:46:41.649
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/09/23 15:46:41.653
    May  9 15:46:41.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4552 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    May  9 15:46:41.737: INFO: stderr: ""
    May  9 15:46:41.737: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 05/09/23 15:46:41.737
    May  9 15:46:41.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4552 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    May  9 15:46:41.954: INFO: stderr: ""
    May  9 15:46:41.954: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/09/23 15:46:41.954
    May  9 15:46:41.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4552 delete pods e2e-test-httpd-pod'
    May  9 15:46:44.738: INFO: stderr: ""
    May  9 15:46:44.738: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  9 15:46:44.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4552" for this suite. 05/09/23 15:46:44.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:46:44.754
May  9 15:46:44.754: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename webhook 05/09/23 15:46:44.755
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:46:44.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:46:44.774
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/09/23 15:46:44.799
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 15:46:45.248
STEP: Deploying the webhook pod 05/09/23 15:46:45.258
STEP: Wait for the deployment to be ready 05/09/23 15:46:45.273
May  9 15:46:45.283: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 15:46:47.302
STEP: Verifying the service has paired with the endpoint 05/09/23 15:46:47.316
May  9 15:46:48.317: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 05/09/23 15:46:48.322
STEP: create a pod that should be updated by the webhook 05/09/23 15:46:48.348
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 15:46:48.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2100" for this suite. 05/09/23 15:46:48.485
STEP: Destroying namespace "webhook-2100-markers" for this suite. 05/09/23 15:46:48.494
------------------------------
â€¢ [3.749 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:46:44.754
    May  9 15:46:44.754: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename webhook 05/09/23 15:46:44.755
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:46:44.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:46:44.774
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/09/23 15:46:44.799
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 15:46:45.248
    STEP: Deploying the webhook pod 05/09/23 15:46:45.258
    STEP: Wait for the deployment to be ready 05/09/23 15:46:45.273
    May  9 15:46:45.283: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 15:46:47.302
    STEP: Verifying the service has paired with the endpoint 05/09/23 15:46:47.316
    May  9 15:46:48.317: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 05/09/23 15:46:48.322
    STEP: create a pod that should be updated by the webhook 05/09/23 15:46:48.348
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 15:46:48.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2100" for this suite. 05/09/23 15:46:48.485
    STEP: Destroying namespace "webhook-2100-markers" for this suite. 05/09/23 15:46:48.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:46:48.503
May  9 15:46:48.503: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename downward-api 05/09/23 15:46:48.504
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:46:48.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:46:48.534
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 05/09/23 15:46:48.539
May  9 15:46:48.549: INFO: Waiting up to 5m0s for pod "annotationupdatec09615d7-482e-4f27-852c-1598669e6f7a" in namespace "downward-api-7091" to be "running and ready"
May  9 15:46:48.556: INFO: Pod "annotationupdatec09615d7-482e-4f27-852c-1598669e6f7a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.988349ms
May  9 15:46:48.557: INFO: The phase of Pod annotationupdatec09615d7-482e-4f27-852c-1598669e6f7a is Pending, waiting for it to be Running (with Ready = true)
May  9 15:46:50.564: INFO: Pod "annotationupdatec09615d7-482e-4f27-852c-1598669e6f7a": Phase="Running", Reason="", readiness=true. Elapsed: 2.014881469s
May  9 15:46:50.564: INFO: The phase of Pod annotationupdatec09615d7-482e-4f27-852c-1598669e6f7a is Running (Ready = true)
May  9 15:46:50.564: INFO: Pod "annotationupdatec09615d7-482e-4f27-852c-1598669e6f7a" satisfied condition "running and ready"
May  9 15:46:51.105: INFO: Successfully updated pod "annotationupdatec09615d7-482e-4f27-852c-1598669e6f7a"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  9 15:46:55.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7091" for this suite. 05/09/23 15:46:55.155
------------------------------
â€¢ [SLOW TEST] [6.661 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:46:48.503
    May  9 15:46:48.503: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename downward-api 05/09/23 15:46:48.504
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:46:48.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:46:48.534
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 05/09/23 15:46:48.539
    May  9 15:46:48.549: INFO: Waiting up to 5m0s for pod "annotationupdatec09615d7-482e-4f27-852c-1598669e6f7a" in namespace "downward-api-7091" to be "running and ready"
    May  9 15:46:48.556: INFO: Pod "annotationupdatec09615d7-482e-4f27-852c-1598669e6f7a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.988349ms
    May  9 15:46:48.557: INFO: The phase of Pod annotationupdatec09615d7-482e-4f27-852c-1598669e6f7a is Pending, waiting for it to be Running (with Ready = true)
    May  9 15:46:50.564: INFO: Pod "annotationupdatec09615d7-482e-4f27-852c-1598669e6f7a": Phase="Running", Reason="", readiness=true. Elapsed: 2.014881469s
    May  9 15:46:50.564: INFO: The phase of Pod annotationupdatec09615d7-482e-4f27-852c-1598669e6f7a is Running (Ready = true)
    May  9 15:46:50.564: INFO: Pod "annotationupdatec09615d7-482e-4f27-852c-1598669e6f7a" satisfied condition "running and ready"
    May  9 15:46:51.105: INFO: Successfully updated pod "annotationupdatec09615d7-482e-4f27-852c-1598669e6f7a"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  9 15:46:55.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7091" for this suite. 05/09/23 15:46:55.155
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:46:55.165
May  9 15:46:55.165: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename emptydir 05/09/23 15:46:55.166
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:46:55.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:46:55.189
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 05/09/23 15:46:55.193
May  9 15:46:55.204: INFO: Waiting up to 5m0s for pod "pod-d4efd132-d4c5-4a64-a205-c9998bafc350" in namespace "emptydir-3480" to be "Succeeded or Failed"
May  9 15:46:55.207: INFO: Pod "pod-d4efd132-d4c5-4a64-a205-c9998bafc350": Phase="Pending", Reason="", readiness=false. Elapsed: 3.798617ms
May  9 15:46:57.214: INFO: Pod "pod-d4efd132-d4c5-4a64-a205-c9998bafc350": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010145578s
May  9 15:46:59.213: INFO: Pod "pod-d4efd132-d4c5-4a64-a205-c9998bafc350": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009625712s
STEP: Saw pod success 05/09/23 15:46:59.213
May  9 15:46:59.214: INFO: Pod "pod-d4efd132-d4c5-4a64-a205-c9998bafc350" satisfied condition "Succeeded or Failed"
May  9 15:46:59.221: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 pod pod-d4efd132-d4c5-4a64-a205-c9998bafc350 container test-container: <nil>
STEP: delete the pod 05/09/23 15:46:59.231
May  9 15:46:59.245: INFO: Waiting for pod pod-d4efd132-d4c5-4a64-a205-c9998bafc350 to disappear
May  9 15:46:59.250: INFO: Pod pod-d4efd132-d4c5-4a64-a205-c9998bafc350 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  9 15:46:59.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3480" for this suite. 05/09/23 15:46:59.258
------------------------------
â€¢ [4.101 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:46:55.165
    May  9 15:46:55.165: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename emptydir 05/09/23 15:46:55.166
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:46:55.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:46:55.189
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 05/09/23 15:46:55.193
    May  9 15:46:55.204: INFO: Waiting up to 5m0s for pod "pod-d4efd132-d4c5-4a64-a205-c9998bafc350" in namespace "emptydir-3480" to be "Succeeded or Failed"
    May  9 15:46:55.207: INFO: Pod "pod-d4efd132-d4c5-4a64-a205-c9998bafc350": Phase="Pending", Reason="", readiness=false. Elapsed: 3.798617ms
    May  9 15:46:57.214: INFO: Pod "pod-d4efd132-d4c5-4a64-a205-c9998bafc350": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010145578s
    May  9 15:46:59.213: INFO: Pod "pod-d4efd132-d4c5-4a64-a205-c9998bafc350": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009625712s
    STEP: Saw pod success 05/09/23 15:46:59.213
    May  9 15:46:59.214: INFO: Pod "pod-d4efd132-d4c5-4a64-a205-c9998bafc350" satisfied condition "Succeeded or Failed"
    May  9 15:46:59.221: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 pod pod-d4efd132-d4c5-4a64-a205-c9998bafc350 container test-container: <nil>
    STEP: delete the pod 05/09/23 15:46:59.231
    May  9 15:46:59.245: INFO: Waiting for pod pod-d4efd132-d4c5-4a64-a205-c9998bafc350 to disappear
    May  9 15:46:59.250: INFO: Pod pod-d4efd132-d4c5-4a64-a205-c9998bafc350 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  9 15:46:59.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3480" for this suite. 05/09/23 15:46:59.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:46:59.269
May  9 15:46:59.270: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename container-lifecycle-hook 05/09/23 15:46:59.271
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:46:59.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:46:59.299
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 05/09/23 15:46:59.31
May  9 15:46:59.320: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1972" to be "running and ready"
May  9 15:46:59.328: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.501425ms
May  9 15:46:59.328: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  9 15:47:01.334: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013102438s
May  9 15:47:01.334: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  9 15:47:01.334: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 05/09/23 15:47:01.338
May  9 15:47:01.346: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-1972" to be "running and ready"
May  9 15:47:01.355: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.062697ms
May  9 15:47:01.355: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
May  9 15:47:03.361: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.013914816s
May  9 15:47:03.361: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
May  9 15:47:03.361: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 05/09/23 15:47:03.366
STEP: delete the pod with lifecycle hook 05/09/23 15:47:03.379
May  9 15:47:03.388: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  9 15:47:03.392: INFO: Pod pod-with-poststart-http-hook still exists
May  9 15:47:05.393: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  9 15:47:05.400: INFO: Pod pod-with-poststart-http-hook still exists
May  9 15:47:07.393: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  9 15:47:07.399: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
May  9 15:47:07.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1972" for this suite. 05/09/23 15:47:07.405
------------------------------
â€¢ [SLOW TEST] [8.146 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:46:59.269
    May  9 15:46:59.270: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/09/23 15:46:59.271
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:46:59.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:46:59.299
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 05/09/23 15:46:59.31
    May  9 15:46:59.320: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1972" to be "running and ready"
    May  9 15:46:59.328: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.501425ms
    May  9 15:46:59.328: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  9 15:47:01.334: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013102438s
    May  9 15:47:01.334: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  9 15:47:01.334: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 05/09/23 15:47:01.338
    May  9 15:47:01.346: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-1972" to be "running and ready"
    May  9 15:47:01.355: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.062697ms
    May  9 15:47:01.355: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    May  9 15:47:03.361: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.013914816s
    May  9 15:47:03.361: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    May  9 15:47:03.361: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 05/09/23 15:47:03.366
    STEP: delete the pod with lifecycle hook 05/09/23 15:47:03.379
    May  9 15:47:03.388: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May  9 15:47:03.392: INFO: Pod pod-with-poststart-http-hook still exists
    May  9 15:47:05.393: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May  9 15:47:05.400: INFO: Pod pod-with-poststart-http-hook still exists
    May  9 15:47:07.393: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May  9 15:47:07.399: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    May  9 15:47:07.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1972" for this suite. 05/09/23 15:47:07.405
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:47:07.418
May  9 15:47:07.418: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename webhook 05/09/23 15:47:07.419
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:47:07.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:47:07.44
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/09/23 15:47:07.476
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 15:47:08.285
STEP: Deploying the webhook pod 05/09/23 15:47:08.291
STEP: Wait for the deployment to be ready 05/09/23 15:47:08.305
May  9 15:47:08.315: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 15:47:10.333
STEP: Verifying the service has paired with the endpoint 05/09/23 15:47:10.35
May  9 15:47:11.350: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
May  9 15:47:11.358: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3004-crds.webhook.example.com via the AdmissionRegistration API 05/09/23 15:47:11.881
STEP: Creating a custom resource that should be mutated by the webhook 05/09/23 15:47:11.916
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 15:47:14.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6919" for this suite. 05/09/23 15:47:14.929
STEP: Destroying namespace "webhook-6919-markers" for this suite. 05/09/23 15:47:14.941
------------------------------
â€¢ [SLOW TEST] [7.532 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:47:07.418
    May  9 15:47:07.418: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename webhook 05/09/23 15:47:07.419
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:47:07.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:47:07.44
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/09/23 15:47:07.476
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 15:47:08.285
    STEP: Deploying the webhook pod 05/09/23 15:47:08.291
    STEP: Wait for the deployment to be ready 05/09/23 15:47:08.305
    May  9 15:47:08.315: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 15:47:10.333
    STEP: Verifying the service has paired with the endpoint 05/09/23 15:47:10.35
    May  9 15:47:11.350: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    May  9 15:47:11.358: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3004-crds.webhook.example.com via the AdmissionRegistration API 05/09/23 15:47:11.881
    STEP: Creating a custom resource that should be mutated by the webhook 05/09/23 15:47:11.916
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 15:47:14.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6919" for this suite. 05/09/23 15:47:14.929
    STEP: Destroying namespace "webhook-6919-markers" for this suite. 05/09/23 15:47:14.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:47:14.952
May  9 15:47:14.952: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename sched-pred 05/09/23 15:47:14.953
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:47:14.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:47:14.984
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
May  9 15:47:14.999: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  9 15:47:15.009: INFO: Waiting for terminating namespaces to be deleted...
May  9 15:47:15.015: INFO: 
Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 before test
May  9 15:47:15.025: INFO: canal-zcmck from kube-system started at 2023-05-09 08:48:05 +0000 UTC (2 container statuses recorded)
May  9 15:47:15.025: INFO: 	Container calico-node ready: true, restart count 2
May  9 15:47:15.025: INFO: 	Container kube-flannel ready: true, restart count 0
May  9 15:47:15.025: INFO: coredns-996c5dbc5-4cqdb from kube-system started at 2023-05-09 12:58:07 +0000 UTC (1 container statuses recorded)
May  9 15:47:15.025: INFO: 	Container coredns ready: true, restart count 0
May  9 15:47:15.025: INFO: kube-proxy-q8nhz from kube-system started at 2023-05-09 08:48:05 +0000 UTC (1 container statuses recorded)
May  9 15:47:15.025: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 15:47:15.025: INFO: wormhole-xj4hj from kube-system started at 2023-05-09 08:48:05 +0000 UTC (1 container statuses recorded)
May  9 15:47:15.025: INFO: 	Container wormhole ready: true, restart count 0
May  9 15:47:15.025: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-69c6h from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
May  9 15:47:15.025: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 15:47:15.025: INFO: 	Container systemd-logs ready: true, restart count 0
May  9 15:47:15.025: INFO: webhook-to-be-mutated from webhook-2100 started at 2023-05-09 15:46:48 +0000 UTC (1 container statuses recorded)
May  9 15:47:15.025: INFO: 	Container example ready: false, restart count 0
May  9 15:47:15.025: INFO: 
Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 before test
May  9 15:47:15.036: INFO: calico-kube-controllers-6cffbf7894-zrprz from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
May  9 15:47:15.036: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  9 15:47:15.036: INFO: canal-22bj2 from kube-system started at 2023-05-09 08:47:59 +0000 UTC (2 container statuses recorded)
May  9 15:47:15.036: INFO: 	Container calico-node ready: true, restart count 2
May  9 15:47:15.036: INFO: 	Container kube-flannel ready: true, restart count 0
May  9 15:47:15.036: INFO: coredns-996c5dbc5-24wst from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
May  9 15:47:15.036: INFO: 	Container coredns ready: true, restart count 0
May  9 15:47:15.036: INFO: kube-dns-autoscaler-789d47d664-hhx85 from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
May  9 15:47:15.036: INFO: 	Container autoscaler ready: true, restart count 0
May  9 15:47:15.036: INFO: kube-proxy-r9g6q from kube-system started at 2023-05-09 08:47:59 +0000 UTC (1 container statuses recorded)
May  9 15:47:15.036: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 15:47:15.036: INFO: metrics-server-5f9c95d78-hk5zz from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
May  9 15:47:15.036: INFO: 	Container metrics-server ready: true, restart count 0
May  9 15:47:15.036: INFO: wormhole-hnkjk from kube-system started at 2023-05-09 08:47:59 +0000 UTC (1 container statuses recorded)
May  9 15:47:15.036: INFO: 	Container wormhole ready: true, restart count 0
May  9 15:47:15.036: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-2x6mg from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
May  9 15:47:15.036: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 15:47:15.036: INFO: 	Container systemd-logs ready: true, restart count 0
May  9 15:47:15.036: INFO: 
Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 before test
May  9 15:47:15.046: INFO: canal-jl6h7 from kube-system started at 2023-05-09 08:48:03 +0000 UTC (2 container statuses recorded)
May  9 15:47:15.046: INFO: 	Container calico-node ready: true, restart count 2
May  9 15:47:15.046: INFO: 	Container kube-flannel ready: true, restart count 0
May  9 15:47:15.046: INFO: kube-proxy-qqc8l from kube-system started at 2023-05-09 08:48:03 +0000 UTC (1 container statuses recorded)
May  9 15:47:15.046: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 15:47:15.046: INFO: wormhole-825jz from kube-system started at 2023-05-09 08:48:03 +0000 UTC (1 container statuses recorded)
May  9 15:47:15.046: INFO: 	Container wormhole ready: true, restart count 0
May  9 15:47:15.046: INFO: sonobuoy from sonobuoy started at 2023-05-09 15:24:58 +0000 UTC (1 container statuses recorded)
May  9 15:47:15.047: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  9 15:47:15.047: INFO: sonobuoy-e2e-job-d96e230b174b4cad from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
May  9 15:47:15.047: INFO: 	Container e2e ready: true, restart count 0
May  9 15:47:15.047: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 15:47:15.047: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-zvq2k from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
May  9 15:47:15.047: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 15:47:15.047: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/09/23 15:47:15.047
May  9 15:47:15.065: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3061" to be "running"
May  9 15:47:15.071: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.118073ms
May  9 15:47:17.077: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011191367s
May  9 15:47:17.077: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/09/23 15:47:17.082
STEP: Trying to apply a random label on the found node. 05/09/23 15:47:17.101
STEP: verifying the node has the label kubernetes.io/e2e-379c7466-a89b-43fc-a355-406ef9baa841 42 05/09/23 15:47:17.111
STEP: Trying to relaunch the pod, now with labels. 05/09/23 15:47:17.117
May  9 15:47:17.123: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-3061" to be "not pending"
May  9 15:47:17.127: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.266676ms
May  9 15:47:19.136: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.012479331s
May  9 15:47:19.136: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-379c7466-a89b-43fc-a355-406ef9baa841 off the node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 05/09/23 15:47:19.142
STEP: verifying the node doesn't have the label kubernetes.io/e2e-379c7466-a89b-43fc-a355-406ef9baa841 05/09/23 15:47:19.158
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 15:47:19.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-3061" for this suite. 05/09/23 15:47:19.171
------------------------------
â€¢ [4.227 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:47:14.952
    May  9 15:47:14.952: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename sched-pred 05/09/23 15:47:14.953
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:47:14.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:47:14.984
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    May  9 15:47:14.999: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  9 15:47:15.009: INFO: Waiting for terminating namespaces to be deleted...
    May  9 15:47:15.015: INFO: 
    Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 before test
    May  9 15:47:15.025: INFO: canal-zcmck from kube-system started at 2023-05-09 08:48:05 +0000 UTC (2 container statuses recorded)
    May  9 15:47:15.025: INFO: 	Container calico-node ready: true, restart count 2
    May  9 15:47:15.025: INFO: 	Container kube-flannel ready: true, restart count 0
    May  9 15:47:15.025: INFO: coredns-996c5dbc5-4cqdb from kube-system started at 2023-05-09 12:58:07 +0000 UTC (1 container statuses recorded)
    May  9 15:47:15.025: INFO: 	Container coredns ready: true, restart count 0
    May  9 15:47:15.025: INFO: kube-proxy-q8nhz from kube-system started at 2023-05-09 08:48:05 +0000 UTC (1 container statuses recorded)
    May  9 15:47:15.025: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 15:47:15.025: INFO: wormhole-xj4hj from kube-system started at 2023-05-09 08:48:05 +0000 UTC (1 container statuses recorded)
    May  9 15:47:15.025: INFO: 	Container wormhole ready: true, restart count 0
    May  9 15:47:15.025: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-69c6h from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
    May  9 15:47:15.025: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 15:47:15.025: INFO: 	Container systemd-logs ready: true, restart count 0
    May  9 15:47:15.025: INFO: webhook-to-be-mutated from webhook-2100 started at 2023-05-09 15:46:48 +0000 UTC (1 container statuses recorded)
    May  9 15:47:15.025: INFO: 	Container example ready: false, restart count 0
    May  9 15:47:15.025: INFO: 
    Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 before test
    May  9 15:47:15.036: INFO: calico-kube-controllers-6cffbf7894-zrprz from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
    May  9 15:47:15.036: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  9 15:47:15.036: INFO: canal-22bj2 from kube-system started at 2023-05-09 08:47:59 +0000 UTC (2 container statuses recorded)
    May  9 15:47:15.036: INFO: 	Container calico-node ready: true, restart count 2
    May  9 15:47:15.036: INFO: 	Container kube-flannel ready: true, restart count 0
    May  9 15:47:15.036: INFO: coredns-996c5dbc5-24wst from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
    May  9 15:47:15.036: INFO: 	Container coredns ready: true, restart count 0
    May  9 15:47:15.036: INFO: kube-dns-autoscaler-789d47d664-hhx85 from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
    May  9 15:47:15.036: INFO: 	Container autoscaler ready: true, restart count 0
    May  9 15:47:15.036: INFO: kube-proxy-r9g6q from kube-system started at 2023-05-09 08:47:59 +0000 UTC (1 container statuses recorded)
    May  9 15:47:15.036: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 15:47:15.036: INFO: metrics-server-5f9c95d78-hk5zz from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
    May  9 15:47:15.036: INFO: 	Container metrics-server ready: true, restart count 0
    May  9 15:47:15.036: INFO: wormhole-hnkjk from kube-system started at 2023-05-09 08:47:59 +0000 UTC (1 container statuses recorded)
    May  9 15:47:15.036: INFO: 	Container wormhole ready: true, restart count 0
    May  9 15:47:15.036: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-2x6mg from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
    May  9 15:47:15.036: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 15:47:15.036: INFO: 	Container systemd-logs ready: true, restart count 0
    May  9 15:47:15.036: INFO: 
    Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 before test
    May  9 15:47:15.046: INFO: canal-jl6h7 from kube-system started at 2023-05-09 08:48:03 +0000 UTC (2 container statuses recorded)
    May  9 15:47:15.046: INFO: 	Container calico-node ready: true, restart count 2
    May  9 15:47:15.046: INFO: 	Container kube-flannel ready: true, restart count 0
    May  9 15:47:15.046: INFO: kube-proxy-qqc8l from kube-system started at 2023-05-09 08:48:03 +0000 UTC (1 container statuses recorded)
    May  9 15:47:15.046: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 15:47:15.046: INFO: wormhole-825jz from kube-system started at 2023-05-09 08:48:03 +0000 UTC (1 container statuses recorded)
    May  9 15:47:15.046: INFO: 	Container wormhole ready: true, restart count 0
    May  9 15:47:15.046: INFO: sonobuoy from sonobuoy started at 2023-05-09 15:24:58 +0000 UTC (1 container statuses recorded)
    May  9 15:47:15.047: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  9 15:47:15.047: INFO: sonobuoy-e2e-job-d96e230b174b4cad from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
    May  9 15:47:15.047: INFO: 	Container e2e ready: true, restart count 0
    May  9 15:47:15.047: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 15:47:15.047: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-zvq2k from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
    May  9 15:47:15.047: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 15:47:15.047: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/09/23 15:47:15.047
    May  9 15:47:15.065: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3061" to be "running"
    May  9 15:47:15.071: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.118073ms
    May  9 15:47:17.077: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011191367s
    May  9 15:47:17.077: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/09/23 15:47:17.082
    STEP: Trying to apply a random label on the found node. 05/09/23 15:47:17.101
    STEP: verifying the node has the label kubernetes.io/e2e-379c7466-a89b-43fc-a355-406ef9baa841 42 05/09/23 15:47:17.111
    STEP: Trying to relaunch the pod, now with labels. 05/09/23 15:47:17.117
    May  9 15:47:17.123: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-3061" to be "not pending"
    May  9 15:47:17.127: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.266676ms
    May  9 15:47:19.136: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.012479331s
    May  9 15:47:19.136: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-379c7466-a89b-43fc-a355-406ef9baa841 off the node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 05/09/23 15:47:19.142
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-379c7466-a89b-43fc-a355-406ef9baa841 05/09/23 15:47:19.158
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 15:47:19.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-3061" for this suite. 05/09/23 15:47:19.171
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:47:19.179
May  9 15:47:19.179: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename security-context 05/09/23 15:47:19.18
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:47:19.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:47:19.202
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/09/23 15:47:19.208
May  9 15:47:19.218: INFO: Waiting up to 5m0s for pod "security-context-77356b0f-8952-4c80-b08c-43a6bb6727e1" in namespace "security-context-859" to be "Succeeded or Failed"
May  9 15:47:19.223: INFO: Pod "security-context-77356b0f-8952-4c80-b08c-43a6bb6727e1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.110527ms
May  9 15:47:21.230: INFO: Pod "security-context-77356b0f-8952-4c80-b08c-43a6bb6727e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012613704s
May  9 15:47:23.230: INFO: Pod "security-context-77356b0f-8952-4c80-b08c-43a6bb6727e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012230315s
STEP: Saw pod success 05/09/23 15:47:23.23
May  9 15:47:23.230: INFO: Pod "security-context-77356b0f-8952-4c80-b08c-43a6bb6727e1" satisfied condition "Succeeded or Failed"
May  9 15:47:23.236: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod security-context-77356b0f-8952-4c80-b08c-43a6bb6727e1 container test-container: <nil>
STEP: delete the pod 05/09/23 15:47:23.29
May  9 15:47:23.313: INFO: Waiting for pod security-context-77356b0f-8952-4c80-b08c-43a6bb6727e1 to disappear
May  9 15:47:23.318: INFO: Pod security-context-77356b0f-8952-4c80-b08c-43a6bb6727e1 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May  9 15:47:23.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-859" for this suite. 05/09/23 15:47:23.324
------------------------------
â€¢ [4.190 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:47:19.179
    May  9 15:47:19.179: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename security-context 05/09/23 15:47:19.18
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:47:19.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:47:19.202
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/09/23 15:47:19.208
    May  9 15:47:19.218: INFO: Waiting up to 5m0s for pod "security-context-77356b0f-8952-4c80-b08c-43a6bb6727e1" in namespace "security-context-859" to be "Succeeded or Failed"
    May  9 15:47:19.223: INFO: Pod "security-context-77356b0f-8952-4c80-b08c-43a6bb6727e1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.110527ms
    May  9 15:47:21.230: INFO: Pod "security-context-77356b0f-8952-4c80-b08c-43a6bb6727e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012613704s
    May  9 15:47:23.230: INFO: Pod "security-context-77356b0f-8952-4c80-b08c-43a6bb6727e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012230315s
    STEP: Saw pod success 05/09/23 15:47:23.23
    May  9 15:47:23.230: INFO: Pod "security-context-77356b0f-8952-4c80-b08c-43a6bb6727e1" satisfied condition "Succeeded or Failed"
    May  9 15:47:23.236: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod security-context-77356b0f-8952-4c80-b08c-43a6bb6727e1 container test-container: <nil>
    STEP: delete the pod 05/09/23 15:47:23.29
    May  9 15:47:23.313: INFO: Waiting for pod security-context-77356b0f-8952-4c80-b08c-43a6bb6727e1 to disappear
    May  9 15:47:23.318: INFO: Pod security-context-77356b0f-8952-4c80-b08c-43a6bb6727e1 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May  9 15:47:23.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-859" for this suite. 05/09/23 15:47:23.324
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:47:23.372
May  9 15:47:23.372: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename replication-controller 05/09/23 15:47:23.373
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:47:23.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:47:23.394
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
May  9 15:47:23.399: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 05/09/23 15:47:24.415
STEP: Checking rc "condition-test" has the desired failure condition set 05/09/23 15:47:24.422
STEP: Scaling down rc "condition-test" to satisfy pod quota 05/09/23 15:47:25.434
May  9 15:47:25.474: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 05/09/23 15:47:25.474
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May  9 15:47:25.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1404" for this suite. 05/09/23 15:47:25.487
------------------------------
â€¢ [2.126 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:47:23.372
    May  9 15:47:23.372: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename replication-controller 05/09/23 15:47:23.373
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:47:23.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:47:23.394
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    May  9 15:47:23.399: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 05/09/23 15:47:24.415
    STEP: Checking rc "condition-test" has the desired failure condition set 05/09/23 15:47:24.422
    STEP: Scaling down rc "condition-test" to satisfy pod quota 05/09/23 15:47:25.434
    May  9 15:47:25.474: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 05/09/23 15:47:25.474
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May  9 15:47:25.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1404" for this suite. 05/09/23 15:47:25.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:47:25.5
May  9 15:47:25.500: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename downward-api 05/09/23 15:47:25.501
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:47:25.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:47:25.521
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 05/09/23 15:47:25.525
May  9 15:47:25.534: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d2d0dd11-d2fc-4e2a-8453-b95cc990b759" in namespace "downward-api-1655" to be "Succeeded or Failed"
May  9 15:47:25.539: INFO: Pod "downwardapi-volume-d2d0dd11-d2fc-4e2a-8453-b95cc990b759": Phase="Pending", Reason="", readiness=false. Elapsed: 4.863085ms
May  9 15:47:27.547: INFO: Pod "downwardapi-volume-d2d0dd11-d2fc-4e2a-8453-b95cc990b759": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012920573s
May  9 15:47:29.546: INFO: Pod "downwardapi-volume-d2d0dd11-d2fc-4e2a-8453-b95cc990b759": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011402398s
STEP: Saw pod success 05/09/23 15:47:29.546
May  9 15:47:29.546: INFO: Pod "downwardapi-volume-d2d0dd11-d2fc-4e2a-8453-b95cc990b759" satisfied condition "Succeeded or Failed"
May  9 15:47:29.552: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-d2d0dd11-d2fc-4e2a-8453-b95cc990b759 container client-container: <nil>
STEP: delete the pod 05/09/23 15:47:29.577
May  9 15:47:29.619: INFO: Waiting for pod downwardapi-volume-d2d0dd11-d2fc-4e2a-8453-b95cc990b759 to disappear
May  9 15:47:29.625: INFO: Pod downwardapi-volume-d2d0dd11-d2fc-4e2a-8453-b95cc990b759 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  9 15:47:29.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1655" for this suite. 05/09/23 15:47:29.632
------------------------------
â€¢ [4.141 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:47:25.5
    May  9 15:47:25.500: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename downward-api 05/09/23 15:47:25.501
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:47:25.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:47:25.521
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 05/09/23 15:47:25.525
    May  9 15:47:25.534: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d2d0dd11-d2fc-4e2a-8453-b95cc990b759" in namespace "downward-api-1655" to be "Succeeded or Failed"
    May  9 15:47:25.539: INFO: Pod "downwardapi-volume-d2d0dd11-d2fc-4e2a-8453-b95cc990b759": Phase="Pending", Reason="", readiness=false. Elapsed: 4.863085ms
    May  9 15:47:27.547: INFO: Pod "downwardapi-volume-d2d0dd11-d2fc-4e2a-8453-b95cc990b759": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012920573s
    May  9 15:47:29.546: INFO: Pod "downwardapi-volume-d2d0dd11-d2fc-4e2a-8453-b95cc990b759": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011402398s
    STEP: Saw pod success 05/09/23 15:47:29.546
    May  9 15:47:29.546: INFO: Pod "downwardapi-volume-d2d0dd11-d2fc-4e2a-8453-b95cc990b759" satisfied condition "Succeeded or Failed"
    May  9 15:47:29.552: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-d2d0dd11-d2fc-4e2a-8453-b95cc990b759 container client-container: <nil>
    STEP: delete the pod 05/09/23 15:47:29.577
    May  9 15:47:29.619: INFO: Waiting for pod downwardapi-volume-d2d0dd11-d2fc-4e2a-8453-b95cc990b759 to disappear
    May  9 15:47:29.625: INFO: Pod downwardapi-volume-d2d0dd11-d2fc-4e2a-8453-b95cc990b759 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  9 15:47:29.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1655" for this suite. 05/09/23 15:47:29.632
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:47:29.642
May  9 15:47:29.642: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubectl 05/09/23 15:47:29.643
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:47:29.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:47:29.668
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 05/09/23 15:47:29.673
May  9 15:47:29.673: INFO: namespace kubectl-3119
May  9 15:47:29.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3119 create -f -'
May  9 15:47:31.213: INFO: stderr: ""
May  9 15:47:31.213: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/09/23 15:47:31.213
May  9 15:47:32.218: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 15:47:32.218: INFO: Found 0 / 1
May  9 15:47:33.219: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 15:47:33.219: INFO: Found 1 / 1
May  9 15:47:33.219: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  9 15:47:33.224: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 15:47:33.224: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  9 15:47:33.224: INFO: wait on agnhost-primary startup in kubectl-3119 
May  9 15:47:33.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3119 logs agnhost-primary-4gn9t agnhost-primary'
May  9 15:47:33.332: INFO: stderr: ""
May  9 15:47:33.332: INFO: stdout: "Paused\n"
STEP: exposing RC 05/09/23 15:47:33.332
May  9 15:47:33.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3119 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May  9 15:47:33.432: INFO: stderr: ""
May  9 15:47:33.432: INFO: stdout: "service/rm2 exposed\n"
May  9 15:47:33.437: INFO: Service rm2 in namespace kubectl-3119 found.
STEP: exposing service 05/09/23 15:47:35.449
May  9 15:47:35.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3119 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May  9 15:47:35.567: INFO: stderr: ""
May  9 15:47:35.567: INFO: stdout: "service/rm3 exposed\n"
May  9 15:47:35.572: INFO: Service rm3 in namespace kubectl-3119 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  9 15:47:37.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3119" for this suite. 05/09/23 15:47:37.589
------------------------------
â€¢ [SLOW TEST] [7.957 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:47:29.642
    May  9 15:47:29.642: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubectl 05/09/23 15:47:29.643
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:47:29.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:47:29.668
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 05/09/23 15:47:29.673
    May  9 15:47:29.673: INFO: namespace kubectl-3119
    May  9 15:47:29.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3119 create -f -'
    May  9 15:47:31.213: INFO: stderr: ""
    May  9 15:47:31.213: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/09/23 15:47:31.213
    May  9 15:47:32.218: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 15:47:32.218: INFO: Found 0 / 1
    May  9 15:47:33.219: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 15:47:33.219: INFO: Found 1 / 1
    May  9 15:47:33.219: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    May  9 15:47:33.224: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 15:47:33.224: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May  9 15:47:33.224: INFO: wait on agnhost-primary startup in kubectl-3119 
    May  9 15:47:33.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3119 logs agnhost-primary-4gn9t agnhost-primary'
    May  9 15:47:33.332: INFO: stderr: ""
    May  9 15:47:33.332: INFO: stdout: "Paused\n"
    STEP: exposing RC 05/09/23 15:47:33.332
    May  9 15:47:33.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3119 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    May  9 15:47:33.432: INFO: stderr: ""
    May  9 15:47:33.432: INFO: stdout: "service/rm2 exposed\n"
    May  9 15:47:33.437: INFO: Service rm2 in namespace kubectl-3119 found.
    STEP: exposing service 05/09/23 15:47:35.449
    May  9 15:47:35.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3119 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    May  9 15:47:35.567: INFO: stderr: ""
    May  9 15:47:35.567: INFO: stdout: "service/rm3 exposed\n"
    May  9 15:47:35.572: INFO: Service rm3 in namespace kubectl-3119 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  9 15:47:37.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3119" for this suite. 05/09/23 15:47:37.589
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:47:37.6
May  9 15:47:37.600: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename resourcequota 05/09/23 15:47:37.601
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:47:37.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:47:37.621
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 05/09/23 15:47:37.626
STEP: Creating a ResourceQuota 05/09/23 15:47:42.63
STEP: Ensuring resource quota status is calculated 05/09/23 15:47:42.636
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  9 15:47:44.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8872" for this suite. 05/09/23 15:47:44.65
------------------------------
â€¢ [SLOW TEST] [7.058 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:47:37.6
    May  9 15:47:37.600: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename resourcequota 05/09/23 15:47:37.601
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:47:37.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:47:37.621
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 05/09/23 15:47:37.626
    STEP: Creating a ResourceQuota 05/09/23 15:47:42.63
    STEP: Ensuring resource quota status is calculated 05/09/23 15:47:42.636
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  9 15:47:44.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8872" for this suite. 05/09/23 15:47:44.65
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:47:44.658
May  9 15:47:44.658: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename container-probe 05/09/23 15:47:44.659
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:47:44.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:47:44.681
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-6066546d-8041-4ced-95a4-ab3dd3da6f59 in namespace container-probe-5326 05/09/23 15:47:44.686
May  9 15:47:44.697: INFO: Waiting up to 5m0s for pod "liveness-6066546d-8041-4ced-95a4-ab3dd3da6f59" in namespace "container-probe-5326" to be "not pending"
May  9 15:47:44.704: INFO: Pod "liveness-6066546d-8041-4ced-95a4-ab3dd3da6f59": Phase="Pending", Reason="", readiness=false. Elapsed: 6.363595ms
May  9 15:47:46.714: INFO: Pod "liveness-6066546d-8041-4ced-95a4-ab3dd3da6f59": Phase="Running", Reason="", readiness=true. Elapsed: 2.016307301s
May  9 15:47:46.714: INFO: Pod "liveness-6066546d-8041-4ced-95a4-ab3dd3da6f59" satisfied condition "not pending"
May  9 15:47:46.714: INFO: Started pod liveness-6066546d-8041-4ced-95a4-ab3dd3da6f59 in namespace container-probe-5326
STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 15:47:46.714
May  9 15:47:46.719: INFO: Initial restart count of pod liveness-6066546d-8041-4ced-95a4-ab3dd3da6f59 is 0
STEP: deleting the pod 05/09/23 15:51:47.724
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May  9 15:51:47.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5326" for this suite. 05/09/23 15:51:47.774
------------------------------
â€¢ [SLOW TEST] [243.124 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:47:44.658
    May  9 15:47:44.658: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename container-probe 05/09/23 15:47:44.659
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:47:44.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:47:44.681
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-6066546d-8041-4ced-95a4-ab3dd3da6f59 in namespace container-probe-5326 05/09/23 15:47:44.686
    May  9 15:47:44.697: INFO: Waiting up to 5m0s for pod "liveness-6066546d-8041-4ced-95a4-ab3dd3da6f59" in namespace "container-probe-5326" to be "not pending"
    May  9 15:47:44.704: INFO: Pod "liveness-6066546d-8041-4ced-95a4-ab3dd3da6f59": Phase="Pending", Reason="", readiness=false. Elapsed: 6.363595ms
    May  9 15:47:46.714: INFO: Pod "liveness-6066546d-8041-4ced-95a4-ab3dd3da6f59": Phase="Running", Reason="", readiness=true. Elapsed: 2.016307301s
    May  9 15:47:46.714: INFO: Pod "liveness-6066546d-8041-4ced-95a4-ab3dd3da6f59" satisfied condition "not pending"
    May  9 15:47:46.714: INFO: Started pod liveness-6066546d-8041-4ced-95a4-ab3dd3da6f59 in namespace container-probe-5326
    STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 15:47:46.714
    May  9 15:47:46.719: INFO: Initial restart count of pod liveness-6066546d-8041-4ced-95a4-ab3dd3da6f59 is 0
    STEP: deleting the pod 05/09/23 15:51:47.724
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May  9 15:51:47.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5326" for this suite. 05/09/23 15:51:47.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:51:47.784
May  9 15:51:47.785: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename emptydir 05/09/23 15:51:47.785
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:51:47.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:51:47.803
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 05/09/23 15:51:47.807
May  9 15:51:47.826: INFO: Waiting up to 5m0s for pod "pod-49a22509-4405-42b9-946a-e71765898384" in namespace "emptydir-431" to be "Succeeded or Failed"
May  9 15:51:47.833: INFO: Pod "pod-49a22509-4405-42b9-946a-e71765898384": Phase="Pending", Reason="", readiness=false. Elapsed: 7.131259ms
May  9 15:51:49.841: INFO: Pod "pod-49a22509-4405-42b9-946a-e71765898384": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014843016s
May  9 15:51:51.847: INFO: Pod "pod-49a22509-4405-42b9-946a-e71765898384": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020708552s
STEP: Saw pod success 05/09/23 15:51:51.847
May  9 15:51:51.847: INFO: Pod "pod-49a22509-4405-42b9-946a-e71765898384" satisfied condition "Succeeded or Failed"
May  9 15:51:51.853: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-49a22509-4405-42b9-946a-e71765898384 container test-container: <nil>
STEP: delete the pod 05/09/23 15:51:51.918
May  9 15:51:51.939: INFO: Waiting for pod pod-49a22509-4405-42b9-946a-e71765898384 to disappear
May  9 15:51:51.944: INFO: Pod pod-49a22509-4405-42b9-946a-e71765898384 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  9 15:51:51.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-431" for this suite. 05/09/23 15:51:51.95
------------------------------
â€¢ [4.175 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:51:47.784
    May  9 15:51:47.785: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename emptydir 05/09/23 15:51:47.785
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:51:47.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:51:47.803
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 05/09/23 15:51:47.807
    May  9 15:51:47.826: INFO: Waiting up to 5m0s for pod "pod-49a22509-4405-42b9-946a-e71765898384" in namespace "emptydir-431" to be "Succeeded or Failed"
    May  9 15:51:47.833: INFO: Pod "pod-49a22509-4405-42b9-946a-e71765898384": Phase="Pending", Reason="", readiness=false. Elapsed: 7.131259ms
    May  9 15:51:49.841: INFO: Pod "pod-49a22509-4405-42b9-946a-e71765898384": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014843016s
    May  9 15:51:51.847: INFO: Pod "pod-49a22509-4405-42b9-946a-e71765898384": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020708552s
    STEP: Saw pod success 05/09/23 15:51:51.847
    May  9 15:51:51.847: INFO: Pod "pod-49a22509-4405-42b9-946a-e71765898384" satisfied condition "Succeeded or Failed"
    May  9 15:51:51.853: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-49a22509-4405-42b9-946a-e71765898384 container test-container: <nil>
    STEP: delete the pod 05/09/23 15:51:51.918
    May  9 15:51:51.939: INFO: Waiting for pod pod-49a22509-4405-42b9-946a-e71765898384 to disappear
    May  9 15:51:51.944: INFO: Pod pod-49a22509-4405-42b9-946a-e71765898384 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  9 15:51:51.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-431" for this suite. 05/09/23 15:51:51.95
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:51:51.96
May  9 15:51:51.960: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename downward-api 05/09/23 15:51:51.961
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:51:51.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:51:51.98
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 05/09/23 15:51:51.984
May  9 15:51:51.997: INFO: Waiting up to 5m0s for pod "labelsupdatebb8e472b-b1e7-4502-acf7-395d5685fc07" in namespace "downward-api-3224" to be "running and ready"
May  9 15:51:52.004: INFO: Pod "labelsupdatebb8e472b-b1e7-4502-acf7-395d5685fc07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.96406ms
May  9 15:51:52.004: INFO: The phase of Pod labelsupdatebb8e472b-b1e7-4502-acf7-395d5685fc07 is Pending, waiting for it to be Running (with Ready = true)
May  9 15:51:54.014: INFO: Pod "labelsupdatebb8e472b-b1e7-4502-acf7-395d5685fc07": Phase="Running", Reason="", readiness=true. Elapsed: 2.017081899s
May  9 15:51:54.014: INFO: The phase of Pod labelsupdatebb8e472b-b1e7-4502-acf7-395d5685fc07 is Running (Ready = true)
May  9 15:51:54.014: INFO: Pod "labelsupdatebb8e472b-b1e7-4502-acf7-395d5685fc07" satisfied condition "running and ready"
May  9 15:51:54.550: INFO: Successfully updated pod "labelsupdatebb8e472b-b1e7-4502-acf7-395d5685fc07"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  9 15:51:58.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3224" for this suite. 05/09/23 15:51:58.603
------------------------------
â€¢ [SLOW TEST] [6.654 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:51:51.96
    May  9 15:51:51.960: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename downward-api 05/09/23 15:51:51.961
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:51:51.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:51:51.98
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 05/09/23 15:51:51.984
    May  9 15:51:51.997: INFO: Waiting up to 5m0s for pod "labelsupdatebb8e472b-b1e7-4502-acf7-395d5685fc07" in namespace "downward-api-3224" to be "running and ready"
    May  9 15:51:52.004: INFO: Pod "labelsupdatebb8e472b-b1e7-4502-acf7-395d5685fc07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.96406ms
    May  9 15:51:52.004: INFO: The phase of Pod labelsupdatebb8e472b-b1e7-4502-acf7-395d5685fc07 is Pending, waiting for it to be Running (with Ready = true)
    May  9 15:51:54.014: INFO: Pod "labelsupdatebb8e472b-b1e7-4502-acf7-395d5685fc07": Phase="Running", Reason="", readiness=true. Elapsed: 2.017081899s
    May  9 15:51:54.014: INFO: The phase of Pod labelsupdatebb8e472b-b1e7-4502-acf7-395d5685fc07 is Running (Ready = true)
    May  9 15:51:54.014: INFO: Pod "labelsupdatebb8e472b-b1e7-4502-acf7-395d5685fc07" satisfied condition "running and ready"
    May  9 15:51:54.550: INFO: Successfully updated pod "labelsupdatebb8e472b-b1e7-4502-acf7-395d5685fc07"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  9 15:51:58.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3224" for this suite. 05/09/23 15:51:58.603
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:51:58.614
May  9 15:51:58.614: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename dns 05/09/23 15:51:58.616
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:51:58.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:51:58.634
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 05/09/23 15:51:58.639
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8667.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local; sleep 1; done
 05/09/23 15:51:58.646
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8667.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local; sleep 1; done
 05/09/23 15:51:58.646
STEP: creating a pod to probe DNS 05/09/23 15:51:58.646
STEP: submitting the pod to kubernetes 05/09/23 15:51:58.646
May  9 15:51:58.656: INFO: Waiting up to 15m0s for pod "dns-test-ccaf3ded-df89-4224-ac40-071537d654c5" in namespace "dns-8667" to be "running"
May  9 15:51:58.664: INFO: Pod "dns-test-ccaf3ded-df89-4224-ac40-071537d654c5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.555931ms
May  9 15:52:00.670: INFO: Pod "dns-test-ccaf3ded-df89-4224-ac40-071537d654c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014276534s
May  9 15:52:02.671: INFO: Pod "dns-test-ccaf3ded-df89-4224-ac40-071537d654c5": Phase="Running", Reason="", readiness=true. Elapsed: 4.014470323s
May  9 15:52:02.671: INFO: Pod "dns-test-ccaf3ded-df89-4224-ac40-071537d654c5" satisfied condition "running"
STEP: retrieving the pod 05/09/23 15:52:02.671
STEP: looking for the results for each expected name from probers 05/09/23 15:52:02.676
May  9 15:52:02.689: INFO: DNS probes using dns-test-ccaf3ded-df89-4224-ac40-071537d654c5 succeeded

STEP: deleting the pod 05/09/23 15:52:02.689
STEP: changing the externalName to bar.example.com 05/09/23 15:52:02.708
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8667.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local; sleep 1; done
 05/09/23 15:52:02.723
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8667.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local; sleep 1; done
 05/09/23 15:52:02.723
STEP: creating a second pod to probe DNS 05/09/23 15:52:02.724
STEP: submitting the pod to kubernetes 05/09/23 15:52:02.724
May  9 15:52:02.732: INFO: Waiting up to 15m0s for pod "dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f" in namespace "dns-8667" to be "running"
May  9 15:52:02.738: INFO: Pod "dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.764945ms
May  9 15:52:04.745: INFO: Pod "dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f": Phase="Running", Reason="", readiness=true. Elapsed: 2.013358588s
May  9 15:52:04.745: INFO: Pod "dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f" satisfied condition "running"
STEP: retrieving the pod 05/09/23 15:52:04.745
STEP: looking for the results for each expected name from probers 05/09/23 15:52:04.751
May  9 15:52:04.760: INFO: File wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 15:52:04.770: INFO: File jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 15:52:04.770: INFO: Lookups using dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f failed for: [wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local]

May  9 15:52:09.780: INFO: File wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 15:52:09.788: INFO: File jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 15:52:09.788: INFO: Lookups using dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f failed for: [wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local]

May  9 15:52:14.781: INFO: File wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 15:52:14.797: INFO: File jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 15:52:14.797: INFO: Lookups using dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f failed for: [wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local]

May  9 15:52:19.781: INFO: File wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 15:52:19.789: INFO: File jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 15:52:19.789: INFO: Lookups using dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f failed for: [wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local]

May  9 15:52:24.779: INFO: File wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 15:52:24.788: INFO: File jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 15:52:24.788: INFO: Lookups using dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f failed for: [wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local]

May  9 15:52:29.780: INFO: File wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 15:52:29.788: INFO: File jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
' instead of 'bar.example.com.'
May  9 15:52:29.788: INFO: Lookups using dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f failed for: [wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local]

May  9 15:52:34.788: INFO: DNS probes using dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f succeeded

STEP: deleting the pod 05/09/23 15:52:34.788
STEP: changing the service to type=ClusterIP 05/09/23 15:52:34.809
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8667.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local; sleep 1; done
 05/09/23 15:52:34.833
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8667.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local; sleep 1; done
 05/09/23 15:52:34.834
STEP: creating a third pod to probe DNS 05/09/23 15:52:34.834
STEP: submitting the pod to kubernetes 05/09/23 15:52:34.838
May  9 15:52:34.850: INFO: Waiting up to 15m0s for pod "dns-test-966a885c-7145-4187-a003-8b40a10543d7" in namespace "dns-8667" to be "running"
May  9 15:52:34.855: INFO: Pod "dns-test-966a885c-7145-4187-a003-8b40a10543d7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.115899ms
May  9 15:52:36.862: INFO: Pod "dns-test-966a885c-7145-4187-a003-8b40a10543d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.0119561s
May  9 15:52:36.862: INFO: Pod "dns-test-966a885c-7145-4187-a003-8b40a10543d7" satisfied condition "running"
STEP: retrieving the pod 05/09/23 15:52:36.862
STEP: looking for the results for each expected name from probers 05/09/23 15:52:36.867
May  9 15:52:36.892: INFO: DNS probes using dns-test-966a885c-7145-4187-a003-8b40a10543d7 succeeded

STEP: deleting the pod 05/09/23 15:52:36.892
STEP: deleting the test externalName service 05/09/23 15:52:36.907
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May  9 15:52:36.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8667" for this suite. 05/09/23 15:52:36.933
------------------------------
â€¢ [SLOW TEST] [38.326 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:51:58.614
    May  9 15:51:58.614: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename dns 05/09/23 15:51:58.616
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:51:58.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:51:58.634
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 05/09/23 15:51:58.639
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8667.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local; sleep 1; done
     05/09/23 15:51:58.646
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8667.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local; sleep 1; done
     05/09/23 15:51:58.646
    STEP: creating a pod to probe DNS 05/09/23 15:51:58.646
    STEP: submitting the pod to kubernetes 05/09/23 15:51:58.646
    May  9 15:51:58.656: INFO: Waiting up to 15m0s for pod "dns-test-ccaf3ded-df89-4224-ac40-071537d654c5" in namespace "dns-8667" to be "running"
    May  9 15:51:58.664: INFO: Pod "dns-test-ccaf3ded-df89-4224-ac40-071537d654c5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.555931ms
    May  9 15:52:00.670: INFO: Pod "dns-test-ccaf3ded-df89-4224-ac40-071537d654c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014276534s
    May  9 15:52:02.671: INFO: Pod "dns-test-ccaf3ded-df89-4224-ac40-071537d654c5": Phase="Running", Reason="", readiness=true. Elapsed: 4.014470323s
    May  9 15:52:02.671: INFO: Pod "dns-test-ccaf3ded-df89-4224-ac40-071537d654c5" satisfied condition "running"
    STEP: retrieving the pod 05/09/23 15:52:02.671
    STEP: looking for the results for each expected name from probers 05/09/23 15:52:02.676
    May  9 15:52:02.689: INFO: DNS probes using dns-test-ccaf3ded-df89-4224-ac40-071537d654c5 succeeded

    STEP: deleting the pod 05/09/23 15:52:02.689
    STEP: changing the externalName to bar.example.com 05/09/23 15:52:02.708
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8667.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local; sleep 1; done
     05/09/23 15:52:02.723
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8667.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local; sleep 1; done
     05/09/23 15:52:02.723
    STEP: creating a second pod to probe DNS 05/09/23 15:52:02.724
    STEP: submitting the pod to kubernetes 05/09/23 15:52:02.724
    May  9 15:52:02.732: INFO: Waiting up to 15m0s for pod "dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f" in namespace "dns-8667" to be "running"
    May  9 15:52:02.738: INFO: Pod "dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.764945ms
    May  9 15:52:04.745: INFO: Pod "dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f": Phase="Running", Reason="", readiness=true. Elapsed: 2.013358588s
    May  9 15:52:04.745: INFO: Pod "dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f" satisfied condition "running"
    STEP: retrieving the pod 05/09/23 15:52:04.745
    STEP: looking for the results for each expected name from probers 05/09/23 15:52:04.751
    May  9 15:52:04.760: INFO: File wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 15:52:04.770: INFO: File jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 15:52:04.770: INFO: Lookups using dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f failed for: [wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local]

    May  9 15:52:09.780: INFO: File wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 15:52:09.788: INFO: File jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 15:52:09.788: INFO: Lookups using dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f failed for: [wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local]

    May  9 15:52:14.781: INFO: File wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 15:52:14.797: INFO: File jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 15:52:14.797: INFO: Lookups using dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f failed for: [wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local]

    May  9 15:52:19.781: INFO: File wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 15:52:19.789: INFO: File jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 15:52:19.789: INFO: Lookups using dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f failed for: [wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local]

    May  9 15:52:24.779: INFO: File wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 15:52:24.788: INFO: File jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 15:52:24.788: INFO: Lookups using dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f failed for: [wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local]

    May  9 15:52:29.780: INFO: File wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 15:52:29.788: INFO: File jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local from pod  dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  9 15:52:29.788: INFO: Lookups using dns-8667/dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f failed for: [wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local]

    May  9 15:52:34.788: INFO: DNS probes using dns-test-850d67d3-7dc1-49a0-9143-99d5b448e74f succeeded

    STEP: deleting the pod 05/09/23 15:52:34.788
    STEP: changing the service to type=ClusterIP 05/09/23 15:52:34.809
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8667.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8667.svc.cluster.local; sleep 1; done
     05/09/23 15:52:34.833
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8667.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8667.svc.cluster.local; sleep 1; done
     05/09/23 15:52:34.834
    STEP: creating a third pod to probe DNS 05/09/23 15:52:34.834
    STEP: submitting the pod to kubernetes 05/09/23 15:52:34.838
    May  9 15:52:34.850: INFO: Waiting up to 15m0s for pod "dns-test-966a885c-7145-4187-a003-8b40a10543d7" in namespace "dns-8667" to be "running"
    May  9 15:52:34.855: INFO: Pod "dns-test-966a885c-7145-4187-a003-8b40a10543d7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.115899ms
    May  9 15:52:36.862: INFO: Pod "dns-test-966a885c-7145-4187-a003-8b40a10543d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.0119561s
    May  9 15:52:36.862: INFO: Pod "dns-test-966a885c-7145-4187-a003-8b40a10543d7" satisfied condition "running"
    STEP: retrieving the pod 05/09/23 15:52:36.862
    STEP: looking for the results for each expected name from probers 05/09/23 15:52:36.867
    May  9 15:52:36.892: INFO: DNS probes using dns-test-966a885c-7145-4187-a003-8b40a10543d7 succeeded

    STEP: deleting the pod 05/09/23 15:52:36.892
    STEP: deleting the test externalName service 05/09/23 15:52:36.907
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May  9 15:52:36.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8667" for this suite. 05/09/23 15:52:36.933
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:52:36.942
May  9 15:52:36.942: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename resourcequota 05/09/23 15:52:36.943
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:52:36.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:52:36.967
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 05/09/23 15:52:36.971
STEP: Getting a ResourceQuota 05/09/23 15:52:36.977
STEP: Listing all ResourceQuotas with LabelSelector 05/09/23 15:52:36.982
STEP: Patching the ResourceQuota 05/09/23 15:52:36.987
STEP: Deleting a Collection of ResourceQuotas 05/09/23 15:52:36.994
STEP: Verifying the deleted ResourceQuota 05/09/23 15:52:37.004
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  9 15:52:37.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9142" for this suite. 05/09/23 15:52:37.013
------------------------------
â€¢ [0.081 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:52:36.942
    May  9 15:52:36.942: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename resourcequota 05/09/23 15:52:36.943
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:52:36.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:52:36.967
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 05/09/23 15:52:36.971
    STEP: Getting a ResourceQuota 05/09/23 15:52:36.977
    STEP: Listing all ResourceQuotas with LabelSelector 05/09/23 15:52:36.982
    STEP: Patching the ResourceQuota 05/09/23 15:52:36.987
    STEP: Deleting a Collection of ResourceQuotas 05/09/23 15:52:36.994
    STEP: Verifying the deleted ResourceQuota 05/09/23 15:52:37.004
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  9 15:52:37.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9142" for this suite. 05/09/23 15:52:37.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:52:37.023
May  9 15:52:37.023: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename daemonsets 05/09/23 15:52:37.024
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:52:37.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:52:37.046
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
May  9 15:52:37.076: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 15:52:37.083
May  9 15:52:37.094: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 15:52:37.094: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 15:52:38.108: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 15:52:38.108: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 15:52:39.108: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 15:52:39.108: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 is running 0 daemon pod, expected 1
May  9 15:52:40.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  9 15:52:40.109: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 05/09/23 15:52:40.131
STEP: Check that daemon pods images are updated. 05/09/23 15:52:40.144
May  9 15:52:40.149: INFO: Wrong image for pod: daemon-set-2s66r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  9 15:52:40.149: INFO: Wrong image for pod: daemon-set-6rvvt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  9 15:52:40.149: INFO: Wrong image for pod: daemon-set-ts85n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  9 15:52:41.162: INFO: Wrong image for pod: daemon-set-2s66r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  9 15:52:41.162: INFO: Wrong image for pod: daemon-set-ts85n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  9 15:52:42.160: INFO: Wrong image for pod: daemon-set-2s66r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  9 15:52:42.160: INFO: Wrong image for pod: daemon-set-ts85n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  9 15:52:42.160: INFO: Pod daemon-set-tvs5j is not available
May  9 15:52:43.170: INFO: Wrong image for pod: daemon-set-2s66r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  9 15:52:43.170: INFO: Wrong image for pod: daemon-set-ts85n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  9 15:52:43.170: INFO: Pod daemon-set-tvs5j is not available
May  9 15:52:44.169: INFO: Wrong image for pod: daemon-set-ts85n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  9 15:52:45.162: INFO: Wrong image for pod: daemon-set-ts85n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  9 15:52:45.162: INFO: Pod daemon-set-vfwd8 is not available
May  9 15:52:47.162: INFO: Pod daemon-set-llx87 is not available
STEP: Check that daemon pods are still running on every node of the cluster. 05/09/23 15:52:47.17
May  9 15:52:47.181: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 15:52:47.181: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 is running 0 daemon pod, expected 1
May  9 15:52:48.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  9 15:52:48.196: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/09/23 15:52:48.222
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2550, will wait for the garbage collector to delete the pods 05/09/23 15:52:48.223
May  9 15:52:48.287: INFO: Deleting DaemonSet.extensions daemon-set took: 9.08584ms
May  9 15:52:48.388: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.094723ms
May  9 15:52:51.294: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 15:52:51.294: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  9 15:52:51.299: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"317816235"},"items":null}

May  9 15:52:51.303: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"317816236"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 15:52:51.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2550" for this suite. 05/09/23 15:52:51.335
------------------------------
â€¢ [SLOW TEST] [14.321 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:52:37.023
    May  9 15:52:37.023: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename daemonsets 05/09/23 15:52:37.024
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:52:37.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:52:37.046
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    May  9 15:52:37.076: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 15:52:37.083
    May  9 15:52:37.094: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 15:52:37.094: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 15:52:38.108: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 15:52:38.108: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 15:52:39.108: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 15:52:39.108: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 is running 0 daemon pod, expected 1
    May  9 15:52:40.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  9 15:52:40.109: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 05/09/23 15:52:40.131
    STEP: Check that daemon pods images are updated. 05/09/23 15:52:40.144
    May  9 15:52:40.149: INFO: Wrong image for pod: daemon-set-2s66r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  9 15:52:40.149: INFO: Wrong image for pod: daemon-set-6rvvt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  9 15:52:40.149: INFO: Wrong image for pod: daemon-set-ts85n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  9 15:52:41.162: INFO: Wrong image for pod: daemon-set-2s66r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  9 15:52:41.162: INFO: Wrong image for pod: daemon-set-ts85n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  9 15:52:42.160: INFO: Wrong image for pod: daemon-set-2s66r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  9 15:52:42.160: INFO: Wrong image for pod: daemon-set-ts85n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  9 15:52:42.160: INFO: Pod daemon-set-tvs5j is not available
    May  9 15:52:43.170: INFO: Wrong image for pod: daemon-set-2s66r. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  9 15:52:43.170: INFO: Wrong image for pod: daemon-set-ts85n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  9 15:52:43.170: INFO: Pod daemon-set-tvs5j is not available
    May  9 15:52:44.169: INFO: Wrong image for pod: daemon-set-ts85n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  9 15:52:45.162: INFO: Wrong image for pod: daemon-set-ts85n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  9 15:52:45.162: INFO: Pod daemon-set-vfwd8 is not available
    May  9 15:52:47.162: INFO: Pod daemon-set-llx87 is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 05/09/23 15:52:47.17
    May  9 15:52:47.181: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 15:52:47.181: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 is running 0 daemon pod, expected 1
    May  9 15:52:48.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  9 15:52:48.196: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/09/23 15:52:48.222
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2550, will wait for the garbage collector to delete the pods 05/09/23 15:52:48.223
    May  9 15:52:48.287: INFO: Deleting DaemonSet.extensions daemon-set took: 9.08584ms
    May  9 15:52:48.388: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.094723ms
    May  9 15:52:51.294: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 15:52:51.294: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  9 15:52:51.299: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"317816235"},"items":null}

    May  9 15:52:51.303: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"317816236"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 15:52:51.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2550" for this suite. 05/09/23 15:52:51.335
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:52:51.345
May  9 15:52:51.345: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename configmap 05/09/23 15:52:51.346
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:52:51.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:52:51.369
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  9 15:52:51.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1231" for this suite. 05/09/23 15:52:51.437
------------------------------
â€¢ [0.102 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:52:51.345
    May  9 15:52:51.345: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename configmap 05/09/23 15:52:51.346
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:52:51.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:52:51.369
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  9 15:52:51.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1231" for this suite. 05/09/23 15:52:51.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:52:51.449
May  9 15:52:51.449: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename cronjob 05/09/23 15:52:51.45
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:52:51.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:52:51.481
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 05/09/23 15:52:51.485
STEP: Ensuring more than one job is running at a time 05/09/23 15:52:51.493
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 05/09/23 15:54:01.5
STEP: Removing cronjob 05/09/23 15:54:01.505
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May  9 15:54:01.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8826" for this suite. 05/09/23 15:54:01.527
------------------------------
â€¢ [SLOW TEST] [70.093 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:52:51.449
    May  9 15:52:51.449: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename cronjob 05/09/23 15:52:51.45
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:52:51.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:52:51.481
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 05/09/23 15:52:51.485
    STEP: Ensuring more than one job is running at a time 05/09/23 15:52:51.493
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 05/09/23 15:54:01.5
    STEP: Removing cronjob 05/09/23 15:54:01.505
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May  9 15:54:01.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8826" for this suite. 05/09/23 15:54:01.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:54:01.543
May  9 15:54:01.543: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename secrets 05/09/23 15:54:01.544
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:54:01.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:54:01.581
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May  9 15:54:01.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9037" for this suite. 05/09/23 15:54:01.649
------------------------------
â€¢ [0.114 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:54:01.543
    May  9 15:54:01.543: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename secrets 05/09/23 15:54:01.544
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:54:01.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:54:01.581
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May  9 15:54:01.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9037" for this suite. 05/09/23 15:54:01.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:54:01.657
May  9 15:54:01.657: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename pod-network-test 05/09/23 15:54:01.658
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:54:01.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:54:01.691
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-2348 05/09/23 15:54:01.696
STEP: creating a selector 05/09/23 15:54:01.696
STEP: Creating the service pods in kubernetes 05/09/23 15:54:01.696
May  9 15:54:01.696: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  9 15:54:01.731: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2348" to be "running and ready"
May  9 15:54:01.740: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.326792ms
May  9 15:54:01.740: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  9 15:54:03.747: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015385181s
May  9 15:54:03.747: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  9 15:54:05.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01605177s
May  9 15:54:05.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 15:54:07.749: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017677089s
May  9 15:54:07.749: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 15:54:09.746: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014465523s
May  9 15:54:09.746: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 15:54:11.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015354807s
May  9 15:54:11.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 15:54:13.748: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.017079256s
May  9 15:54:13.748: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 15:54:15.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.016080968s
May  9 15:54:15.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 15:54:17.746: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01477871s
May  9 15:54:17.746: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 15:54:19.768: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.03687453s
May  9 15:54:19.768: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 15:54:21.748: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.016403685s
May  9 15:54:21.748: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 15:54:23.746: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.01469609s
May  9 15:54:23.746: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  9 15:54:23.746: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  9 15:54:23.752: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2348" to be "running and ready"
May  9 15:54:23.758: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.719958ms
May  9 15:54:23.758: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  9 15:54:23.758: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  9 15:54:23.762: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2348" to be "running and ready"
May  9 15:54:23.767: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.046769ms
May  9 15:54:23.767: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  9 15:54:23.767: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/09/23 15:54:23.773
May  9 15:54:23.780: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2348" to be "running"
May  9 15:54:23.785: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.063297ms
May  9 15:54:25.791: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011074003s
May  9 15:54:25.791: INFO: Pod "test-container-pod" satisfied condition "running"
May  9 15:54:25.795: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May  9 15:54:25.795: INFO: Breadth first check of 10.2.1.150 on host 51.68.91.222...
May  9 15:54:25.802: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.2.132:9080/dial?request=hostname&protocol=http&host=10.2.1.150&port=8083&tries=1'] Namespace:pod-network-test-2348 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 15:54:25.802: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 15:54:25.802: INFO: ExecWithOptions: Clientset creation
May  9 15:54:25.802: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-2348/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.2.132%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.2.1.150%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  9 15:54:25.935: INFO: Waiting for responses: map[]
May  9 15:54:25.935: INFO: reached 10.2.1.150 after 0/1 tries
May  9 15:54:25.935: INFO: Breadth first check of 10.2.0.137 on host 51.68.93.170...
May  9 15:54:25.943: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.2.132:9080/dial?request=hostname&protocol=http&host=10.2.0.137&port=8083&tries=1'] Namespace:pod-network-test-2348 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 15:54:25.943: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 15:54:25.944: INFO: ExecWithOptions: Clientset creation
May  9 15:54:25.944: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-2348/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.2.132%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.2.0.137%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  9 15:54:26.096: INFO: Waiting for responses: map[]
May  9 15:54:26.096: INFO: reached 10.2.0.137 after 0/1 tries
May  9 15:54:26.096: INFO: Breadth first check of 10.2.2.131 on host 51.68.94.118...
May  9 15:54:26.104: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.2.132:9080/dial?request=hostname&protocol=http&host=10.2.2.131&port=8083&tries=1'] Namespace:pod-network-test-2348 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 15:54:26.104: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 15:54:26.105: INFO: ExecWithOptions: Clientset creation
May  9 15:54:26.105: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-2348/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.2.132%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.2.2.131%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  9 15:54:26.221: INFO: Waiting for responses: map[]
May  9 15:54:26.221: INFO: reached 10.2.2.131 after 0/1 tries
May  9 15:54:26.221: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
May  9 15:54:26.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2348" for this suite. 05/09/23 15:54:26.227
------------------------------
â€¢ [SLOW TEST] [24.579 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:54:01.657
    May  9 15:54:01.657: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename pod-network-test 05/09/23 15:54:01.658
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:54:01.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:54:01.691
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-2348 05/09/23 15:54:01.696
    STEP: creating a selector 05/09/23 15:54:01.696
    STEP: Creating the service pods in kubernetes 05/09/23 15:54:01.696
    May  9 15:54:01.696: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  9 15:54:01.731: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2348" to be "running and ready"
    May  9 15:54:01.740: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.326792ms
    May  9 15:54:01.740: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  9 15:54:03.747: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015385181s
    May  9 15:54:03.747: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  9 15:54:05.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01605177s
    May  9 15:54:05.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 15:54:07.749: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017677089s
    May  9 15:54:07.749: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 15:54:09.746: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014465523s
    May  9 15:54:09.746: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 15:54:11.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015354807s
    May  9 15:54:11.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 15:54:13.748: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.017079256s
    May  9 15:54:13.748: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 15:54:15.747: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.016080968s
    May  9 15:54:15.747: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 15:54:17.746: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01477871s
    May  9 15:54:17.746: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 15:54:19.768: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.03687453s
    May  9 15:54:19.768: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 15:54:21.748: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.016403685s
    May  9 15:54:21.748: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 15:54:23.746: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.01469609s
    May  9 15:54:23.746: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  9 15:54:23.746: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  9 15:54:23.752: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2348" to be "running and ready"
    May  9 15:54:23.758: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.719958ms
    May  9 15:54:23.758: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  9 15:54:23.758: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  9 15:54:23.762: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2348" to be "running and ready"
    May  9 15:54:23.767: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.046769ms
    May  9 15:54:23.767: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  9 15:54:23.767: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/09/23 15:54:23.773
    May  9 15:54:23.780: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2348" to be "running"
    May  9 15:54:23.785: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.063297ms
    May  9 15:54:25.791: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011074003s
    May  9 15:54:25.791: INFO: Pod "test-container-pod" satisfied condition "running"
    May  9 15:54:25.795: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May  9 15:54:25.795: INFO: Breadth first check of 10.2.1.150 on host 51.68.91.222...
    May  9 15:54:25.802: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.2.132:9080/dial?request=hostname&protocol=http&host=10.2.1.150&port=8083&tries=1'] Namespace:pod-network-test-2348 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 15:54:25.802: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 15:54:25.802: INFO: ExecWithOptions: Clientset creation
    May  9 15:54:25.802: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-2348/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.2.132%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.2.1.150%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  9 15:54:25.935: INFO: Waiting for responses: map[]
    May  9 15:54:25.935: INFO: reached 10.2.1.150 after 0/1 tries
    May  9 15:54:25.935: INFO: Breadth first check of 10.2.0.137 on host 51.68.93.170...
    May  9 15:54:25.943: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.2.132:9080/dial?request=hostname&protocol=http&host=10.2.0.137&port=8083&tries=1'] Namespace:pod-network-test-2348 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 15:54:25.943: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 15:54:25.944: INFO: ExecWithOptions: Clientset creation
    May  9 15:54:25.944: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-2348/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.2.132%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.2.0.137%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  9 15:54:26.096: INFO: Waiting for responses: map[]
    May  9 15:54:26.096: INFO: reached 10.2.0.137 after 0/1 tries
    May  9 15:54:26.096: INFO: Breadth first check of 10.2.2.131 on host 51.68.94.118...
    May  9 15:54:26.104: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.2.132:9080/dial?request=hostname&protocol=http&host=10.2.2.131&port=8083&tries=1'] Namespace:pod-network-test-2348 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 15:54:26.104: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 15:54:26.105: INFO: ExecWithOptions: Clientset creation
    May  9 15:54:26.105: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-2348/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.2.132%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.2.2.131%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  9 15:54:26.221: INFO: Waiting for responses: map[]
    May  9 15:54:26.221: INFO: reached 10.2.2.131 after 0/1 tries
    May  9 15:54:26.221: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    May  9 15:54:26.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2348" for this suite. 05/09/23 15:54:26.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:54:26.239
May  9 15:54:26.240: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubelet-test 05/09/23 15:54:26.24
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:54:26.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:54:26.262
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
May  9 15:54:26.278: INFO: Waiting up to 5m0s for pod "busybox-scheduling-4b84512d-d349-4394-a3b8-33ca6c6bea17" in namespace "kubelet-test-5165" to be "running and ready"
May  9 15:54:26.284: INFO: Pod "busybox-scheduling-4b84512d-d349-4394-a3b8-33ca6c6bea17": Phase="Pending", Reason="", readiness=false. Elapsed: 6.360383ms
May  9 15:54:26.284: INFO: The phase of Pod busybox-scheduling-4b84512d-d349-4394-a3b8-33ca6c6bea17 is Pending, waiting for it to be Running (with Ready = true)
May  9 15:54:28.325: INFO: Pod "busybox-scheduling-4b84512d-d349-4394-a3b8-33ca6c6bea17": Phase="Running", Reason="", readiness=true. Elapsed: 2.047266904s
May  9 15:54:28.325: INFO: The phase of Pod busybox-scheduling-4b84512d-d349-4394-a3b8-33ca6c6bea17 is Running (Ready = true)
May  9 15:54:28.325: INFO: Pod "busybox-scheduling-4b84512d-d349-4394-a3b8-33ca6c6bea17" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May  9 15:54:28.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5165" for this suite. 05/09/23 15:54:28.4
------------------------------
â€¢ [2.169 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:54:26.239
    May  9 15:54:26.240: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubelet-test 05/09/23 15:54:26.24
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:54:26.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:54:26.262
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    May  9 15:54:26.278: INFO: Waiting up to 5m0s for pod "busybox-scheduling-4b84512d-d349-4394-a3b8-33ca6c6bea17" in namespace "kubelet-test-5165" to be "running and ready"
    May  9 15:54:26.284: INFO: Pod "busybox-scheduling-4b84512d-d349-4394-a3b8-33ca6c6bea17": Phase="Pending", Reason="", readiness=false. Elapsed: 6.360383ms
    May  9 15:54:26.284: INFO: The phase of Pod busybox-scheduling-4b84512d-d349-4394-a3b8-33ca6c6bea17 is Pending, waiting for it to be Running (with Ready = true)
    May  9 15:54:28.325: INFO: Pod "busybox-scheduling-4b84512d-d349-4394-a3b8-33ca6c6bea17": Phase="Running", Reason="", readiness=true. Elapsed: 2.047266904s
    May  9 15:54:28.325: INFO: The phase of Pod busybox-scheduling-4b84512d-d349-4394-a3b8-33ca6c6bea17 is Running (Ready = true)
    May  9 15:54:28.325: INFO: Pod "busybox-scheduling-4b84512d-d349-4394-a3b8-33ca6c6bea17" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May  9 15:54:28.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5165" for this suite. 05/09/23 15:54:28.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:54:28.409
May  9 15:54:28.409: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename disruption 05/09/23 15:54:28.41
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:54:28.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:54:28.463
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 05/09/23 15:54:28.478
STEP: Updating PodDisruptionBudget status 05/09/23 15:54:30.493
STEP: Waiting for all pods to be running 05/09/23 15:54:30.582
May  9 15:54:30.589: INFO: running pods: 0 < 1
STEP: locating a running pod 05/09/23 15:54:32.596
STEP: Waiting for the pdb to be processed 05/09/23 15:54:32.615
STEP: Patching PodDisruptionBudget status 05/09/23 15:54:32.627
STEP: Waiting for the pdb to be processed 05/09/23 15:54:32.64
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May  9 15:54:32.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2596" for this suite. 05/09/23 15:54:32.654
------------------------------
â€¢ [4.255 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:54:28.409
    May  9 15:54:28.409: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename disruption 05/09/23 15:54:28.41
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:54:28.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:54:28.463
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 05/09/23 15:54:28.478
    STEP: Updating PodDisruptionBudget status 05/09/23 15:54:30.493
    STEP: Waiting for all pods to be running 05/09/23 15:54:30.582
    May  9 15:54:30.589: INFO: running pods: 0 < 1
    STEP: locating a running pod 05/09/23 15:54:32.596
    STEP: Waiting for the pdb to be processed 05/09/23 15:54:32.615
    STEP: Patching PodDisruptionBudget status 05/09/23 15:54:32.627
    STEP: Waiting for the pdb to be processed 05/09/23 15:54:32.64
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May  9 15:54:32.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2596" for this suite. 05/09/23 15:54:32.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:54:32.664
May  9 15:54:32.664: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename downward-api 05/09/23 15:54:32.665
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:54:32.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:54:32.686
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 05/09/23 15:54:32.69
May  9 15:54:32.702: INFO: Waiting up to 5m0s for pod "downwardapi-volume-95369ce4-aaa9-4823-82c7-df1e845a7359" in namespace "downward-api-3512" to be "Succeeded or Failed"
May  9 15:54:32.707: INFO: Pod "downwardapi-volume-95369ce4-aaa9-4823-82c7-df1e845a7359": Phase="Pending", Reason="", readiness=false. Elapsed: 4.617091ms
May  9 15:54:34.714: INFO: Pod "downwardapi-volume-95369ce4-aaa9-4823-82c7-df1e845a7359": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01213487s
May  9 15:54:36.714: INFO: Pod "downwardapi-volume-95369ce4-aaa9-4823-82c7-df1e845a7359": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011574635s
STEP: Saw pod success 05/09/23 15:54:36.714
May  9 15:54:36.714: INFO: Pod "downwardapi-volume-95369ce4-aaa9-4823-82c7-df1e845a7359" satisfied condition "Succeeded or Failed"
May  9 15:54:36.719: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod downwardapi-volume-95369ce4-aaa9-4823-82c7-df1e845a7359 container client-container: <nil>
STEP: delete the pod 05/09/23 15:54:36.792
May  9 15:54:36.809: INFO: Waiting for pod downwardapi-volume-95369ce4-aaa9-4823-82c7-df1e845a7359 to disappear
May  9 15:54:36.812: INFO: Pod downwardapi-volume-95369ce4-aaa9-4823-82c7-df1e845a7359 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  9 15:54:36.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3512" for this suite. 05/09/23 15:54:36.818
------------------------------
â€¢ [4.161 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:54:32.664
    May  9 15:54:32.664: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename downward-api 05/09/23 15:54:32.665
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:54:32.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:54:32.686
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 05/09/23 15:54:32.69
    May  9 15:54:32.702: INFO: Waiting up to 5m0s for pod "downwardapi-volume-95369ce4-aaa9-4823-82c7-df1e845a7359" in namespace "downward-api-3512" to be "Succeeded or Failed"
    May  9 15:54:32.707: INFO: Pod "downwardapi-volume-95369ce4-aaa9-4823-82c7-df1e845a7359": Phase="Pending", Reason="", readiness=false. Elapsed: 4.617091ms
    May  9 15:54:34.714: INFO: Pod "downwardapi-volume-95369ce4-aaa9-4823-82c7-df1e845a7359": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01213487s
    May  9 15:54:36.714: INFO: Pod "downwardapi-volume-95369ce4-aaa9-4823-82c7-df1e845a7359": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011574635s
    STEP: Saw pod success 05/09/23 15:54:36.714
    May  9 15:54:36.714: INFO: Pod "downwardapi-volume-95369ce4-aaa9-4823-82c7-df1e845a7359" satisfied condition "Succeeded or Failed"
    May  9 15:54:36.719: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod downwardapi-volume-95369ce4-aaa9-4823-82c7-df1e845a7359 container client-container: <nil>
    STEP: delete the pod 05/09/23 15:54:36.792
    May  9 15:54:36.809: INFO: Waiting for pod downwardapi-volume-95369ce4-aaa9-4823-82c7-df1e845a7359 to disappear
    May  9 15:54:36.812: INFO: Pod downwardapi-volume-95369ce4-aaa9-4823-82c7-df1e845a7359 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  9 15:54:36.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3512" for this suite. 05/09/23 15:54:36.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:54:36.826
May  9 15:54:36.826: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename taint-single-pod 05/09/23 15:54:36.827
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:54:36.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:54:36.872
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
May  9 15:54:36.876: INFO: Waiting up to 1m0s for all nodes to be ready
May  9 15:55:36.934: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
May  9 15:55:36.940: INFO: Starting informer...
STEP: Starting pod... 05/09/23 15:55:36.94
May  9 15:55:37.162: INFO: Pod is running on nodepool-8cc7f47e-9b0c-4801-88-node-7ad816. Tainting Node
STEP: Trying to apply a taint on the Node 05/09/23 15:55:37.162
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/09/23 15:55:37.175
STEP: Waiting short time to make sure Pod is queued for deletion 05/09/23 15:55:37.182
May  9 15:55:37.182: INFO: Pod wasn't evicted. Proceeding
May  9 15:55:37.182: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/09/23 15:55:37.198
STEP: Waiting some time to make sure that toleration time passed. 05/09/23 15:55:37.205
May  9 15:56:52.205: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 15:56:52.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-37" for this suite. 05/09/23 15:56:52.233
------------------------------
â€¢ [SLOW TEST] [135.417 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:54:36.826
    May  9 15:54:36.826: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename taint-single-pod 05/09/23 15:54:36.827
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:54:36.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:54:36.872
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    May  9 15:54:36.876: INFO: Waiting up to 1m0s for all nodes to be ready
    May  9 15:55:36.934: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    May  9 15:55:36.940: INFO: Starting informer...
    STEP: Starting pod... 05/09/23 15:55:36.94
    May  9 15:55:37.162: INFO: Pod is running on nodepool-8cc7f47e-9b0c-4801-88-node-7ad816. Tainting Node
    STEP: Trying to apply a taint on the Node 05/09/23 15:55:37.162
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/09/23 15:55:37.175
    STEP: Waiting short time to make sure Pod is queued for deletion 05/09/23 15:55:37.182
    May  9 15:55:37.182: INFO: Pod wasn't evicted. Proceeding
    May  9 15:55:37.182: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/09/23 15:55:37.198
    STEP: Waiting some time to make sure that toleration time passed. 05/09/23 15:55:37.205
    May  9 15:56:52.205: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 15:56:52.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-37" for this suite. 05/09/23 15:56:52.233
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:56:52.244
May  9 15:56:52.244: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubectl 05/09/23 15:56:52.245
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:56:52.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:56:52.269
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 05/09/23 15:56:52.273
May  9 15:56:52.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 create -f -'
May  9 15:56:52.474: INFO: stderr: ""
May  9 15:56:52.474: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/09/23 15:56:52.474
May  9 15:56:52.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  9 15:56:52.559: INFO: stderr: ""
May  9 15:56:52.559: INFO: stdout: "update-demo-nautilus-p9lbr update-demo-nautilus-r48qr "
May  9 15:56:52.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 get pods update-demo-nautilus-p9lbr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 15:56:52.663: INFO: stderr: ""
May  9 15:56:52.663: INFO: stdout: ""
May  9 15:56:52.663: INFO: update-demo-nautilus-p9lbr is created but not running
May  9 15:56:57.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  9 15:56:57.755: INFO: stderr: ""
May  9 15:56:57.755: INFO: stdout: "update-demo-nautilus-p9lbr update-demo-nautilus-r48qr "
May  9 15:56:57.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 get pods update-demo-nautilus-p9lbr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 15:56:57.833: INFO: stderr: ""
May  9 15:56:57.833: INFO: stdout: "true"
May  9 15:56:57.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 get pods update-demo-nautilus-p9lbr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  9 15:56:57.911: INFO: stderr: ""
May  9 15:56:57.911: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May  9 15:56:57.911: INFO: validating pod update-demo-nautilus-p9lbr
May  9 15:56:57.921: INFO: got data: {
  "image": "nautilus.jpg"
}

May  9 15:56:57.921: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  9 15:56:57.921: INFO: update-demo-nautilus-p9lbr is verified up and running
May  9 15:56:57.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 get pods update-demo-nautilus-r48qr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  9 15:56:58.007: INFO: stderr: ""
May  9 15:56:58.007: INFO: stdout: "true"
May  9 15:56:58.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 get pods update-demo-nautilus-r48qr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  9 15:56:58.097: INFO: stderr: ""
May  9 15:56:58.097: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May  9 15:56:58.097: INFO: validating pod update-demo-nautilus-r48qr
May  9 15:56:58.109: INFO: got data: {
  "image": "nautilus.jpg"
}

May  9 15:56:58.109: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  9 15:56:58.109: INFO: update-demo-nautilus-r48qr is verified up and running
STEP: using delete to clean up resources 05/09/23 15:56:58.109
May  9 15:56:58.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 delete --grace-period=0 --force -f -'
May  9 15:56:58.195: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  9 15:56:58.195: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  9 15:56:58.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 get rc,svc -l name=update-demo --no-headers'
May  9 15:56:58.288: INFO: stderr: "No resources found in kubectl-2935 namespace.\n"
May  9 15:56:58.288: INFO: stdout: ""
May  9 15:56:58.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  9 15:56:58.372: INFO: stderr: ""
May  9 15:56:58.372: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  9 15:56:58.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2935" for this suite. 05/09/23 15:56:58.378
------------------------------
â€¢ [SLOW TEST] [6.145 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:56:52.244
    May  9 15:56:52.244: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubectl 05/09/23 15:56:52.245
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:56:52.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:56:52.269
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 05/09/23 15:56:52.273
    May  9 15:56:52.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 create -f -'
    May  9 15:56:52.474: INFO: stderr: ""
    May  9 15:56:52.474: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/09/23 15:56:52.474
    May  9 15:56:52.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  9 15:56:52.559: INFO: stderr: ""
    May  9 15:56:52.559: INFO: stdout: "update-demo-nautilus-p9lbr update-demo-nautilus-r48qr "
    May  9 15:56:52.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 get pods update-demo-nautilus-p9lbr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 15:56:52.663: INFO: stderr: ""
    May  9 15:56:52.663: INFO: stdout: ""
    May  9 15:56:52.663: INFO: update-demo-nautilus-p9lbr is created but not running
    May  9 15:56:57.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  9 15:56:57.755: INFO: stderr: ""
    May  9 15:56:57.755: INFO: stdout: "update-demo-nautilus-p9lbr update-demo-nautilus-r48qr "
    May  9 15:56:57.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 get pods update-demo-nautilus-p9lbr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 15:56:57.833: INFO: stderr: ""
    May  9 15:56:57.833: INFO: stdout: "true"
    May  9 15:56:57.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 get pods update-demo-nautilus-p9lbr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  9 15:56:57.911: INFO: stderr: ""
    May  9 15:56:57.911: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May  9 15:56:57.911: INFO: validating pod update-demo-nautilus-p9lbr
    May  9 15:56:57.921: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  9 15:56:57.921: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  9 15:56:57.921: INFO: update-demo-nautilus-p9lbr is verified up and running
    May  9 15:56:57.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 get pods update-demo-nautilus-r48qr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  9 15:56:58.007: INFO: stderr: ""
    May  9 15:56:58.007: INFO: stdout: "true"
    May  9 15:56:58.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 get pods update-demo-nautilus-r48qr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  9 15:56:58.097: INFO: stderr: ""
    May  9 15:56:58.097: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May  9 15:56:58.097: INFO: validating pod update-demo-nautilus-r48qr
    May  9 15:56:58.109: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  9 15:56:58.109: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  9 15:56:58.109: INFO: update-demo-nautilus-r48qr is verified up and running
    STEP: using delete to clean up resources 05/09/23 15:56:58.109
    May  9 15:56:58.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 delete --grace-period=0 --force -f -'
    May  9 15:56:58.195: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  9 15:56:58.195: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    May  9 15:56:58.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 get rc,svc -l name=update-demo --no-headers'
    May  9 15:56:58.288: INFO: stderr: "No resources found in kubectl-2935 namespace.\n"
    May  9 15:56:58.288: INFO: stdout: ""
    May  9 15:56:58.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-2935 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May  9 15:56:58.372: INFO: stderr: ""
    May  9 15:56:58.372: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  9 15:56:58.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2935" for this suite. 05/09/23 15:56:58.378
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:56:58.389
May  9 15:56:58.389: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename statefulset 05/09/23 15:56:58.39
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:56:58.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:56:58.426
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7432 05/09/23 15:56:58.431
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 05/09/23 15:56:58.438
STEP: Creating stateful set ss in namespace statefulset-7432 05/09/23 15:56:58.46
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7432 05/09/23 15:56:58.469
May  9 15:56:58.475: INFO: Found 0 stateful pods, waiting for 1
May  9 15:57:08.482: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 05/09/23 15:57:08.482
May  9 15:57:08.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-7432 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 15:57:08.704: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 15:57:08.704: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 15:57:08.704: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  9 15:57:08.711: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  9 15:57:18.718: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  9 15:57:18.719: INFO: Waiting for statefulset status.replicas updated to 0
May  9 15:57:18.745: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999778s
May  9 15:57:19.751: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.9939253s
May  9 15:57:20.759: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987827303s
May  9 15:57:21.767: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.980169654s
May  9 15:57:22.777: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.971611113s
May  9 15:57:23.783: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.962703646s
May  9 15:57:24.793: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.956366372s
May  9 15:57:25.801: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.946613268s
May  9 15:57:26.809: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.937793614s
May  9 15:57:27.818: INFO: Verifying statefulset ss doesn't scale past 1 for another 929.40434ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7432 05/09/23 15:57:28.818
May  9 15:57:28.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-7432 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  9 15:57:29.034: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  9 15:57:29.034: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  9 15:57:29.034: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  9 15:57:29.042: INFO: Found 1 stateful pods, waiting for 3
May  9 15:57:39.050: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  9 15:57:39.050: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  9 15:57:39.050: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 05/09/23 15:57:39.05
STEP: Scale down will halt with unhealthy stateful pod 05/09/23 15:57:39.05
May  9 15:57:39.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-7432 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 15:57:39.331: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 15:57:39.331: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 15:57:39.331: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  9 15:57:39.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-7432 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 15:57:39.533: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 15:57:39.533: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 15:57:39.533: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  9 15:57:39.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-7432 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 15:57:39.785: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 15:57:39.785: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 15:57:39.785: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  9 15:57:39.785: INFO: Waiting for statefulset status.replicas updated to 0
May  9 15:57:39.792: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May  9 15:57:49.807: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  9 15:57:49.807: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  9 15:57:49.807: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  9 15:57:49.830: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999562s
May  9 15:57:50.837: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993515562s
May  9 15:57:51.844: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986813385s
May  9 15:57:52.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979783568s
May  9 15:57:53.858: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972981138s
May  9 15:57:54.864: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.965022556s
May  9 15:57:55.872: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.959744809s
May  9 15:57:56.881: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.951151455s
May  9 15:57:57.889: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.941813565s
May  9 15:57:58.897: INFO: Verifying statefulset ss doesn't scale past 3 for another 934.346555ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7432 05/09/23 15:57:59.897
May  9 15:57:59.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-7432 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  9 15:58:00.130: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  9 15:58:00.130: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  9 15:58:00.130: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  9 15:58:00.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-7432 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  9 15:58:00.345: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  9 15:58:00.345: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  9 15:58:00.345: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  9 15:58:00.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-7432 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  9 15:58:00.558: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  9 15:58:00.558: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  9 15:58:00.558: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  9 15:58:00.558: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 05/09/23 15:58:10.597
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May  9 15:58:10.597: INFO: Deleting all statefulset in ns statefulset-7432
May  9 15:58:10.602: INFO: Scaling statefulset ss to 0
May  9 15:58:10.617: INFO: Waiting for statefulset status.replicas updated to 0
May  9 15:58:10.621: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May  9 15:58:10.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7432" for this suite. 05/09/23 15:58:10.694
------------------------------
â€¢ [SLOW TEST] [72.315 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:56:58.389
    May  9 15:56:58.389: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename statefulset 05/09/23 15:56:58.39
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:56:58.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:56:58.426
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7432 05/09/23 15:56:58.431
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 05/09/23 15:56:58.438
    STEP: Creating stateful set ss in namespace statefulset-7432 05/09/23 15:56:58.46
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7432 05/09/23 15:56:58.469
    May  9 15:56:58.475: INFO: Found 0 stateful pods, waiting for 1
    May  9 15:57:08.482: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 05/09/23 15:57:08.482
    May  9 15:57:08.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-7432 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 15:57:08.704: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 15:57:08.704: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 15:57:08.704: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  9 15:57:08.711: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    May  9 15:57:18.718: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  9 15:57:18.719: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 15:57:18.745: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999778s
    May  9 15:57:19.751: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.9939253s
    May  9 15:57:20.759: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987827303s
    May  9 15:57:21.767: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.980169654s
    May  9 15:57:22.777: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.971611113s
    May  9 15:57:23.783: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.962703646s
    May  9 15:57:24.793: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.956366372s
    May  9 15:57:25.801: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.946613268s
    May  9 15:57:26.809: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.937793614s
    May  9 15:57:27.818: INFO: Verifying statefulset ss doesn't scale past 1 for another 929.40434ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7432 05/09/23 15:57:28.818
    May  9 15:57:28.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-7432 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  9 15:57:29.034: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  9 15:57:29.034: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  9 15:57:29.034: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  9 15:57:29.042: INFO: Found 1 stateful pods, waiting for 3
    May  9 15:57:39.050: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May  9 15:57:39.050: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    May  9 15:57:39.050: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 05/09/23 15:57:39.05
    STEP: Scale down will halt with unhealthy stateful pod 05/09/23 15:57:39.05
    May  9 15:57:39.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-7432 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 15:57:39.331: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 15:57:39.331: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 15:57:39.331: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  9 15:57:39.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-7432 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 15:57:39.533: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 15:57:39.533: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 15:57:39.533: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  9 15:57:39.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-7432 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 15:57:39.785: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 15:57:39.785: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 15:57:39.785: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  9 15:57:39.785: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 15:57:39.792: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    May  9 15:57:49.807: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  9 15:57:49.807: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    May  9 15:57:49.807: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    May  9 15:57:49.830: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999562s
    May  9 15:57:50.837: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993515562s
    May  9 15:57:51.844: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986813385s
    May  9 15:57:52.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979783568s
    May  9 15:57:53.858: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972981138s
    May  9 15:57:54.864: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.965022556s
    May  9 15:57:55.872: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.959744809s
    May  9 15:57:56.881: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.951151455s
    May  9 15:57:57.889: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.941813565s
    May  9 15:57:58.897: INFO: Verifying statefulset ss doesn't scale past 3 for another 934.346555ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7432 05/09/23 15:57:59.897
    May  9 15:57:59.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-7432 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  9 15:58:00.130: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  9 15:58:00.130: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  9 15:58:00.130: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  9 15:58:00.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-7432 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  9 15:58:00.345: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  9 15:58:00.345: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  9 15:58:00.345: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  9 15:58:00.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-7432 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  9 15:58:00.558: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  9 15:58:00.558: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  9 15:58:00.558: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  9 15:58:00.558: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 05/09/23 15:58:10.597
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May  9 15:58:10.597: INFO: Deleting all statefulset in ns statefulset-7432
    May  9 15:58:10.602: INFO: Scaling statefulset ss to 0
    May  9 15:58:10.617: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 15:58:10.621: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May  9 15:58:10.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7432" for this suite. 05/09/23 15:58:10.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:58:10.708
May  9 15:58:10.708: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename gc 05/09/23 15:58:10.709
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:58:10.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:58:10.751
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 05/09/23 15:58:10.757
STEP: delete the rc 05/09/23 15:58:15.775
STEP: wait for all pods to be garbage collected 05/09/23 15:58:15.783
STEP: Gathering metrics 05/09/23 15:58:20.794
W0509 15:58:20.807338      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  9 15:58:20.807: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May  9 15:58:20.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6514" for this suite. 05/09/23 15:58:20.812
------------------------------
â€¢ [SLOW TEST] [10.114 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:58:10.708
    May  9 15:58:10.708: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename gc 05/09/23 15:58:10.709
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:58:10.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:58:10.751
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 05/09/23 15:58:10.757
    STEP: delete the rc 05/09/23 15:58:15.775
    STEP: wait for all pods to be garbage collected 05/09/23 15:58:15.783
    STEP: Gathering metrics 05/09/23 15:58:20.794
    W0509 15:58:20.807338      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May  9 15:58:20.807: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May  9 15:58:20.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6514" for this suite. 05/09/23 15:58:20.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:58:20.822
May  9 15:58:20.822: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename subpath 05/09/23 15:58:20.823
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:58:20.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:58:20.855
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/09/23 15:58:20.862
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-fjgv 05/09/23 15:58:20.886
STEP: Creating a pod to test atomic-volume-subpath 05/09/23 15:58:20.886
May  9 15:58:20.896: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-fjgv" in namespace "subpath-8259" to be "Succeeded or Failed"
May  9 15:58:20.903: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.875367ms
May  9 15:58:22.909: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 2.013336151s
May  9 15:58:24.910: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 4.013429821s
May  9 15:58:26.911: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 6.01476236s
May  9 15:58:28.912: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 8.015603561s
May  9 15:58:30.911: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 10.015274287s
May  9 15:58:32.909: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 12.012931151s
May  9 15:58:34.910: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 14.013574316s
May  9 15:58:36.909: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 16.013164867s
May  9 15:58:38.909: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 18.013379601s
May  9 15:58:40.911: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 20.014622158s
May  9 15:58:42.912: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=false. Elapsed: 22.015969287s
May  9 15:58:44.912: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015681508s
STEP: Saw pod success 05/09/23 15:58:44.912
May  9 15:58:44.912: INFO: Pod "pod-subpath-test-secret-fjgv" satisfied condition "Succeeded or Failed"
May  9 15:58:44.918: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-subpath-test-secret-fjgv container test-container-subpath-secret-fjgv: <nil>
STEP: delete the pod 05/09/23 15:58:44.974
May  9 15:58:44.987: INFO: Waiting for pod pod-subpath-test-secret-fjgv to disappear
May  9 15:58:44.995: INFO: Pod pod-subpath-test-secret-fjgv no longer exists
STEP: Deleting pod pod-subpath-test-secret-fjgv 05/09/23 15:58:44.995
May  9 15:58:44.996: INFO: Deleting pod "pod-subpath-test-secret-fjgv" in namespace "subpath-8259"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May  9 15:58:45.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8259" for this suite. 05/09/23 15:58:45.012
------------------------------
â€¢ [SLOW TEST] [24.201 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:58:20.822
    May  9 15:58:20.822: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename subpath 05/09/23 15:58:20.823
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:58:20.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:58:20.855
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/09/23 15:58:20.862
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-fjgv 05/09/23 15:58:20.886
    STEP: Creating a pod to test atomic-volume-subpath 05/09/23 15:58:20.886
    May  9 15:58:20.896: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-fjgv" in namespace "subpath-8259" to be "Succeeded or Failed"
    May  9 15:58:20.903: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.875367ms
    May  9 15:58:22.909: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 2.013336151s
    May  9 15:58:24.910: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 4.013429821s
    May  9 15:58:26.911: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 6.01476236s
    May  9 15:58:28.912: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 8.015603561s
    May  9 15:58:30.911: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 10.015274287s
    May  9 15:58:32.909: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 12.012931151s
    May  9 15:58:34.910: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 14.013574316s
    May  9 15:58:36.909: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 16.013164867s
    May  9 15:58:38.909: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 18.013379601s
    May  9 15:58:40.911: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=true. Elapsed: 20.014622158s
    May  9 15:58:42.912: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Running", Reason="", readiness=false. Elapsed: 22.015969287s
    May  9 15:58:44.912: INFO: Pod "pod-subpath-test-secret-fjgv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015681508s
    STEP: Saw pod success 05/09/23 15:58:44.912
    May  9 15:58:44.912: INFO: Pod "pod-subpath-test-secret-fjgv" satisfied condition "Succeeded or Failed"
    May  9 15:58:44.918: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-subpath-test-secret-fjgv container test-container-subpath-secret-fjgv: <nil>
    STEP: delete the pod 05/09/23 15:58:44.974
    May  9 15:58:44.987: INFO: Waiting for pod pod-subpath-test-secret-fjgv to disappear
    May  9 15:58:44.995: INFO: Pod pod-subpath-test-secret-fjgv no longer exists
    STEP: Deleting pod pod-subpath-test-secret-fjgv 05/09/23 15:58:44.995
    May  9 15:58:44.996: INFO: Deleting pod "pod-subpath-test-secret-fjgv" in namespace "subpath-8259"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May  9 15:58:45.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8259" for this suite. 05/09/23 15:58:45.012
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:58:45.023
May  9 15:58:45.023: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename downward-api 05/09/23 15:58:45.024
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:58:45.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:58:45.046
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 05/09/23 15:58:45.051
May  9 15:58:45.063: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2acab292-5f48-4c02-baba-f1fb21fb54d0" in namespace "downward-api-4887" to be "Succeeded or Failed"
May  9 15:58:45.074: INFO: Pod "downwardapi-volume-2acab292-5f48-4c02-baba-f1fb21fb54d0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.9003ms
May  9 15:58:47.081: INFO: Pod "downwardapi-volume-2acab292-5f48-4c02-baba-f1fb21fb54d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018559034s
May  9 15:58:49.086: INFO: Pod "downwardapi-volume-2acab292-5f48-4c02-baba-f1fb21fb54d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023016399s
STEP: Saw pod success 05/09/23 15:58:49.086
May  9 15:58:49.086: INFO: Pod "downwardapi-volume-2acab292-5f48-4c02-baba-f1fb21fb54d0" satisfied condition "Succeeded or Failed"
May  9 15:58:49.092: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-2acab292-5f48-4c02-baba-f1fb21fb54d0 container client-container: <nil>
STEP: delete the pod 05/09/23 15:58:49.103
May  9 15:58:49.121: INFO: Waiting for pod downwardapi-volume-2acab292-5f48-4c02-baba-f1fb21fb54d0 to disappear
May  9 15:58:49.132: INFO: Pod downwardapi-volume-2acab292-5f48-4c02-baba-f1fb21fb54d0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  9 15:58:49.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4887" for this suite. 05/09/23 15:58:49.141
------------------------------
â€¢ [4.126 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:58:45.023
    May  9 15:58:45.023: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename downward-api 05/09/23 15:58:45.024
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:58:45.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:58:45.046
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 05/09/23 15:58:45.051
    May  9 15:58:45.063: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2acab292-5f48-4c02-baba-f1fb21fb54d0" in namespace "downward-api-4887" to be "Succeeded or Failed"
    May  9 15:58:45.074: INFO: Pod "downwardapi-volume-2acab292-5f48-4c02-baba-f1fb21fb54d0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.9003ms
    May  9 15:58:47.081: INFO: Pod "downwardapi-volume-2acab292-5f48-4c02-baba-f1fb21fb54d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018559034s
    May  9 15:58:49.086: INFO: Pod "downwardapi-volume-2acab292-5f48-4c02-baba-f1fb21fb54d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023016399s
    STEP: Saw pod success 05/09/23 15:58:49.086
    May  9 15:58:49.086: INFO: Pod "downwardapi-volume-2acab292-5f48-4c02-baba-f1fb21fb54d0" satisfied condition "Succeeded or Failed"
    May  9 15:58:49.092: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-2acab292-5f48-4c02-baba-f1fb21fb54d0 container client-container: <nil>
    STEP: delete the pod 05/09/23 15:58:49.103
    May  9 15:58:49.121: INFO: Waiting for pod downwardapi-volume-2acab292-5f48-4c02-baba-f1fb21fb54d0 to disappear
    May  9 15:58:49.132: INFO: Pod downwardapi-volume-2acab292-5f48-4c02-baba-f1fb21fb54d0 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  9 15:58:49.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4887" for this suite. 05/09/23 15:58:49.141
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:58:49.15
May  9 15:58:49.150: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubectl 05/09/23 15:58:49.151
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:58:49.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:58:49.17
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/09/23 15:58:49.174
May  9 15:58:49.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3467 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
May  9 15:58:49.266: INFO: stderr: ""
May  9 15:58:49.266: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 05/09/23 15:58:49.266
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
May  9 15:58:49.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3467 delete pods e2e-test-httpd-pod'
May  9 15:58:51.472: INFO: stderr: ""
May  9 15:58:51.472: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  9 15:58:51.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3467" for this suite. 05/09/23 15:58:51.479
------------------------------
â€¢ [2.339 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:58:49.15
    May  9 15:58:49.150: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubectl 05/09/23 15:58:49.151
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:58:49.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:58:49.17
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/09/23 15:58:49.174
    May  9 15:58:49.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3467 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    May  9 15:58:49.266: INFO: stderr: ""
    May  9 15:58:49.266: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 05/09/23 15:58:49.266
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    May  9 15:58:49.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-3467 delete pods e2e-test-httpd-pod'
    May  9 15:58:51.472: INFO: stderr: ""
    May  9 15:58:51.472: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  9 15:58:51.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3467" for this suite. 05/09/23 15:58:51.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:58:51.491
May  9 15:58:51.491: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename resourcequota 05/09/23 15:58:51.492
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:58:51.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:58:51.512
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 05/09/23 15:59:08.523
STEP: Creating a ResourceQuota 05/09/23 15:59:13.531
STEP: Ensuring resource quota status is calculated 05/09/23 15:59:13.537
STEP: Creating a ConfigMap 05/09/23 15:59:15.544
STEP: Ensuring resource quota status captures configMap creation 05/09/23 15:59:15.557
STEP: Deleting a ConfigMap 05/09/23 15:59:17.567
STEP: Ensuring resource quota status released usage 05/09/23 15:59:17.576
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  9 15:59:19.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6080" for this suite. 05/09/23 15:59:19.588
------------------------------
â€¢ [SLOW TEST] [28.107 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:58:51.491
    May  9 15:58:51.491: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename resourcequota 05/09/23 15:58:51.492
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:58:51.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:58:51.512
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 05/09/23 15:59:08.523
    STEP: Creating a ResourceQuota 05/09/23 15:59:13.531
    STEP: Ensuring resource quota status is calculated 05/09/23 15:59:13.537
    STEP: Creating a ConfigMap 05/09/23 15:59:15.544
    STEP: Ensuring resource quota status captures configMap creation 05/09/23 15:59:15.557
    STEP: Deleting a ConfigMap 05/09/23 15:59:17.567
    STEP: Ensuring resource quota status released usage 05/09/23 15:59:17.576
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  9 15:59:19.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6080" for this suite. 05/09/23 15:59:19.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:59:19.599
May  9 15:59:19.599: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename var-expansion 05/09/23 15:59:19.6
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:59:19.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:59:19.626
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 05/09/23 15:59:19.63
May  9 15:59:19.644: INFO: Waiting up to 5m0s for pod "var-expansion-e2a613b9-e7b6-4f3d-a439-5ce4ee333d18" in namespace "var-expansion-2209" to be "Succeeded or Failed"
May  9 15:59:19.650: INFO: Pod "var-expansion-e2a613b9-e7b6-4f3d-a439-5ce4ee333d18": Phase="Pending", Reason="", readiness=false. Elapsed: 5.995214ms
May  9 15:59:21.657: INFO: Pod "var-expansion-e2a613b9-e7b6-4f3d-a439-5ce4ee333d18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01330384s
May  9 15:59:23.660: INFO: Pod "var-expansion-e2a613b9-e7b6-4f3d-a439-5ce4ee333d18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01658046s
STEP: Saw pod success 05/09/23 15:59:23.66
May  9 15:59:23.660: INFO: Pod "var-expansion-e2a613b9-e7b6-4f3d-a439-5ce4ee333d18" satisfied condition "Succeeded or Failed"
May  9 15:59:23.665: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod var-expansion-e2a613b9-e7b6-4f3d-a439-5ce4ee333d18 container dapi-container: <nil>
STEP: delete the pod 05/09/23 15:59:23.678
May  9 15:59:23.694: INFO: Waiting for pod var-expansion-e2a613b9-e7b6-4f3d-a439-5ce4ee333d18 to disappear
May  9 15:59:23.701: INFO: Pod var-expansion-e2a613b9-e7b6-4f3d-a439-5ce4ee333d18 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May  9 15:59:23.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2209" for this suite. 05/09/23 15:59:23.707
------------------------------
â€¢ [4.117 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:59:19.599
    May  9 15:59:19.599: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename var-expansion 05/09/23 15:59:19.6
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:59:19.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:59:19.626
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 05/09/23 15:59:19.63
    May  9 15:59:19.644: INFO: Waiting up to 5m0s for pod "var-expansion-e2a613b9-e7b6-4f3d-a439-5ce4ee333d18" in namespace "var-expansion-2209" to be "Succeeded or Failed"
    May  9 15:59:19.650: INFO: Pod "var-expansion-e2a613b9-e7b6-4f3d-a439-5ce4ee333d18": Phase="Pending", Reason="", readiness=false. Elapsed: 5.995214ms
    May  9 15:59:21.657: INFO: Pod "var-expansion-e2a613b9-e7b6-4f3d-a439-5ce4ee333d18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01330384s
    May  9 15:59:23.660: INFO: Pod "var-expansion-e2a613b9-e7b6-4f3d-a439-5ce4ee333d18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01658046s
    STEP: Saw pod success 05/09/23 15:59:23.66
    May  9 15:59:23.660: INFO: Pod "var-expansion-e2a613b9-e7b6-4f3d-a439-5ce4ee333d18" satisfied condition "Succeeded or Failed"
    May  9 15:59:23.665: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod var-expansion-e2a613b9-e7b6-4f3d-a439-5ce4ee333d18 container dapi-container: <nil>
    STEP: delete the pod 05/09/23 15:59:23.678
    May  9 15:59:23.694: INFO: Waiting for pod var-expansion-e2a613b9-e7b6-4f3d-a439-5ce4ee333d18 to disappear
    May  9 15:59:23.701: INFO: Pod var-expansion-e2a613b9-e7b6-4f3d-a439-5ce4ee333d18 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May  9 15:59:23.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2209" for this suite. 05/09/23 15:59:23.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:59:23.716
May  9 15:59:23.716: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename runtimeclass 05/09/23 15:59:23.717
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:59:23.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:59:23.737
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May  9 15:59:23.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6675" for this suite. 05/09/23 15:59:23.757
------------------------------
â€¢ [0.052 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:59:23.716
    May  9 15:59:23.716: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename runtimeclass 05/09/23 15:59:23.717
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:59:23.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:59:23.737
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May  9 15:59:23.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6675" for this suite. 05/09/23 15:59:23.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:59:23.77
May  9 15:59:23.770: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 15:59:23.771
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:59:23.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:59:23.792
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
May  9 15:59:23.796: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 15:59:24.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2594" for this suite. 05/09/23 15:59:24.835
------------------------------
â€¢ [1.072 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:59:23.77
    May  9 15:59:23.770: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 15:59:23.771
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:59:23.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:59:23.792
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    May  9 15:59:23.796: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 15:59:24.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2594" for this suite. 05/09/23 15:59:24.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:59:24.844
May  9 15:59:24.844: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename replication-controller 05/09/23 15:59:24.845
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:59:24.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:59:24.885
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d 05/09/23 15:59:24.961
May  9 15:59:25.068: INFO: Pod name my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d: Found 0 pods out of 1
May  9 15:59:30.075: INFO: Pod name my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d: Found 1 pods out of 1
May  9 15:59:30.075: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d" are running
May  9 15:59:30.075: INFO: Waiting up to 5m0s for pod "my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d-sfs29" in namespace "replication-controller-6042" to be "running"
May  9 15:59:30.080: INFO: Pod "my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d-sfs29": Phase="Running", Reason="", readiness=true. Elapsed: 5.080962ms
May  9 15:59:30.080: INFO: Pod "my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d-sfs29" satisfied condition "running"
May  9 15:59:30.080: INFO: Pod "my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d-sfs29" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 15:59:25 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 15:59:26 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 15:59:26 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 15:59:25 +0000 UTC Reason: Message:}])
May  9 15:59:30.080: INFO: Trying to dial the pod
May  9 15:59:35.100: INFO: Controller my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d: Got expected result from replica 1 [my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d-sfs29]: "my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d-sfs29", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May  9 15:59:35.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6042" for this suite. 05/09/23 15:59:35.107
------------------------------
â€¢ [SLOW TEST] [10.273 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:59:24.844
    May  9 15:59:24.844: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename replication-controller 05/09/23 15:59:24.845
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:59:24.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:59:24.885
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d 05/09/23 15:59:24.961
    May  9 15:59:25.068: INFO: Pod name my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d: Found 0 pods out of 1
    May  9 15:59:30.075: INFO: Pod name my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d: Found 1 pods out of 1
    May  9 15:59:30.075: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d" are running
    May  9 15:59:30.075: INFO: Waiting up to 5m0s for pod "my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d-sfs29" in namespace "replication-controller-6042" to be "running"
    May  9 15:59:30.080: INFO: Pod "my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d-sfs29": Phase="Running", Reason="", readiness=true. Elapsed: 5.080962ms
    May  9 15:59:30.080: INFO: Pod "my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d-sfs29" satisfied condition "running"
    May  9 15:59:30.080: INFO: Pod "my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d-sfs29" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 15:59:25 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 15:59:26 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 15:59:26 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 15:59:25 +0000 UTC Reason: Message:}])
    May  9 15:59:30.080: INFO: Trying to dial the pod
    May  9 15:59:35.100: INFO: Controller my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d: Got expected result from replica 1 [my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d-sfs29]: "my-hostname-basic-e4410a5f-fd57-4e8b-9953-359ca5782c9d-sfs29", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May  9 15:59:35.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6042" for this suite. 05/09/23 15:59:35.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:59:35.118
May  9 15:59:35.118: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename services 05/09/23 15:59:35.119
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:59:35.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:59:35.139
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-8667 05/09/23 15:59:35.144
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8667 to expose endpoints map[] 05/09/23 15:59:35.155
May  9 15:59:35.162: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
May  9 15:59:36.173: INFO: successfully validated that service endpoint-test2 in namespace services-8667 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8667 05/09/23 15:59:36.173
May  9 15:59:36.185: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8667" to be "running and ready"
May  9 15:59:36.192: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.122253ms
May  9 15:59:36.192: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  9 15:59:38.198: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013572194s
May  9 15:59:38.198: INFO: The phase of Pod pod1 is Running (Ready = true)
May  9 15:59:38.198: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8667 to expose endpoints map[pod1:[80]] 05/09/23 15:59:38.203
May  9 15:59:38.220: INFO: successfully validated that service endpoint-test2 in namespace services-8667 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 05/09/23 15:59:38.22
May  9 15:59:38.220: INFO: Creating new exec pod
May  9 15:59:38.226: INFO: Waiting up to 5m0s for pod "execpod79vdd" in namespace "services-8667" to be "running"
May  9 15:59:38.231: INFO: Pod "execpod79vdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.786964ms
May  9 15:59:40.236: INFO: Pod "execpod79vdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009970075s
May  9 15:59:42.239: INFO: Pod "execpod79vdd": Phase="Running", Reason="", readiness=true. Elapsed: 4.013349244s
May  9 15:59:42.239: INFO: Pod "execpod79vdd" satisfied condition "running"
May  9 15:59:43.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-8667 exec execpod79vdd -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
May  9 15:59:43.459: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May  9 15:59:43.459: INFO: stdout: ""
May  9 15:59:43.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-8667 exec execpod79vdd -- /bin/sh -x -c nc -v -z -w 2 10.3.106.104 80'
May  9 15:59:43.666: INFO: stderr: "+ nc -v -z -w 2 10.3.106.104 80\nConnection to 10.3.106.104 80 port [tcp/http] succeeded!\n"
May  9 15:59:43.666: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-8667 05/09/23 15:59:43.666
May  9 15:59:43.674: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8667" to be "running and ready"
May  9 15:59:43.681: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.175657ms
May  9 15:59:43.681: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  9 15:59:45.688: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014513031s
May  9 15:59:45.688: INFO: The phase of Pod pod2 is Running (Ready = true)
May  9 15:59:45.688: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8667 to expose endpoints map[pod1:[80] pod2:[80]] 05/09/23 15:59:45.693
May  9 15:59:45.716: INFO: successfully validated that service endpoint-test2 in namespace services-8667 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 05/09/23 15:59:45.716
May  9 15:59:46.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-8667 exec execpod79vdd -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
May  9 15:59:46.942: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May  9 15:59:46.942: INFO: stdout: ""
May  9 15:59:46.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-8667 exec execpod79vdd -- /bin/sh -x -c nc -v -z -w 2 10.3.106.104 80'
May  9 15:59:47.161: INFO: stderr: "+ nc -v -z -w 2 10.3.106.104 80\nConnection to 10.3.106.104 80 port [tcp/http] succeeded!\n"
May  9 15:59:47.161: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-8667 05/09/23 15:59:47.161
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8667 to expose endpoints map[pod2:[80]] 05/09/23 15:59:47.18
May  9 15:59:48.213: INFO: successfully validated that service endpoint-test2 in namespace services-8667 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 05/09/23 15:59:48.213
May  9 15:59:49.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-8667 exec execpod79vdd -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
May  9 15:59:49.429: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May  9 15:59:49.429: INFO: stdout: ""
May  9 15:59:49.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-8667 exec execpod79vdd -- /bin/sh -x -c nc -v -z -w 2 10.3.106.104 80'
May  9 15:59:49.629: INFO: stderr: "+ nc -v -z -w 2 10.3.106.104 80\nConnection to 10.3.106.104 80 port [tcp/http] succeeded!\n"
May  9 15:59:49.629: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-8667 05/09/23 15:59:49.629
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8667 to expose endpoints map[] 05/09/23 15:59:49.656
May  9 15:59:49.674: INFO: successfully validated that service endpoint-test2 in namespace services-8667 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  9 15:59:49.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8667" for this suite. 05/09/23 15:59:49.702
------------------------------
â€¢ [SLOW TEST] [14.594 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:59:35.118
    May  9 15:59:35.118: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename services 05/09/23 15:59:35.119
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:59:35.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:59:35.139
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-8667 05/09/23 15:59:35.144
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8667 to expose endpoints map[] 05/09/23 15:59:35.155
    May  9 15:59:35.162: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    May  9 15:59:36.173: INFO: successfully validated that service endpoint-test2 in namespace services-8667 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-8667 05/09/23 15:59:36.173
    May  9 15:59:36.185: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8667" to be "running and ready"
    May  9 15:59:36.192: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.122253ms
    May  9 15:59:36.192: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  9 15:59:38.198: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013572194s
    May  9 15:59:38.198: INFO: The phase of Pod pod1 is Running (Ready = true)
    May  9 15:59:38.198: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8667 to expose endpoints map[pod1:[80]] 05/09/23 15:59:38.203
    May  9 15:59:38.220: INFO: successfully validated that service endpoint-test2 in namespace services-8667 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 05/09/23 15:59:38.22
    May  9 15:59:38.220: INFO: Creating new exec pod
    May  9 15:59:38.226: INFO: Waiting up to 5m0s for pod "execpod79vdd" in namespace "services-8667" to be "running"
    May  9 15:59:38.231: INFO: Pod "execpod79vdd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.786964ms
    May  9 15:59:40.236: INFO: Pod "execpod79vdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009970075s
    May  9 15:59:42.239: INFO: Pod "execpod79vdd": Phase="Running", Reason="", readiness=true. Elapsed: 4.013349244s
    May  9 15:59:42.239: INFO: Pod "execpod79vdd" satisfied condition "running"
    May  9 15:59:43.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-8667 exec execpod79vdd -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    May  9 15:59:43.459: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May  9 15:59:43.459: INFO: stdout: ""
    May  9 15:59:43.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-8667 exec execpod79vdd -- /bin/sh -x -c nc -v -z -w 2 10.3.106.104 80'
    May  9 15:59:43.666: INFO: stderr: "+ nc -v -z -w 2 10.3.106.104 80\nConnection to 10.3.106.104 80 port [tcp/http] succeeded!\n"
    May  9 15:59:43.666: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-8667 05/09/23 15:59:43.666
    May  9 15:59:43.674: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8667" to be "running and ready"
    May  9 15:59:43.681: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.175657ms
    May  9 15:59:43.681: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  9 15:59:45.688: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014513031s
    May  9 15:59:45.688: INFO: The phase of Pod pod2 is Running (Ready = true)
    May  9 15:59:45.688: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8667 to expose endpoints map[pod1:[80] pod2:[80]] 05/09/23 15:59:45.693
    May  9 15:59:45.716: INFO: successfully validated that service endpoint-test2 in namespace services-8667 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 05/09/23 15:59:45.716
    May  9 15:59:46.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-8667 exec execpod79vdd -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    May  9 15:59:46.942: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May  9 15:59:46.942: INFO: stdout: ""
    May  9 15:59:46.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-8667 exec execpod79vdd -- /bin/sh -x -c nc -v -z -w 2 10.3.106.104 80'
    May  9 15:59:47.161: INFO: stderr: "+ nc -v -z -w 2 10.3.106.104 80\nConnection to 10.3.106.104 80 port [tcp/http] succeeded!\n"
    May  9 15:59:47.161: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-8667 05/09/23 15:59:47.161
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8667 to expose endpoints map[pod2:[80]] 05/09/23 15:59:47.18
    May  9 15:59:48.213: INFO: successfully validated that service endpoint-test2 in namespace services-8667 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 05/09/23 15:59:48.213
    May  9 15:59:49.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-8667 exec execpod79vdd -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    May  9 15:59:49.429: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May  9 15:59:49.429: INFO: stdout: ""
    May  9 15:59:49.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-8667 exec execpod79vdd -- /bin/sh -x -c nc -v -z -w 2 10.3.106.104 80'
    May  9 15:59:49.629: INFO: stderr: "+ nc -v -z -w 2 10.3.106.104 80\nConnection to 10.3.106.104 80 port [tcp/http] succeeded!\n"
    May  9 15:59:49.629: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-8667 05/09/23 15:59:49.629
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8667 to expose endpoints map[] 05/09/23 15:59:49.656
    May  9 15:59:49.674: INFO: successfully validated that service endpoint-test2 in namespace services-8667 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  9 15:59:49.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8667" for this suite. 05/09/23 15:59:49.702
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:59:49.711
May  9 15:59:49.711: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename deployment 05/09/23 15:59:49.712
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:59:49.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:59:49.732
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
May  9 15:59:49.751: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May  9 15:59:54.760: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/09/23 15:59:54.76
May  9 15:59:54.760: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 05/09/23 15:59:54.786
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  9 15:59:54.802: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1585  0b60d2bf-de71-4b3b-81ad-db92d29cf668 317859488 1 2023-05-09 15:59:54 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-05-09 15:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040d9f38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

May  9 15:59:54.808: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-1585  95ecbbde-d484-4be5-8449-38dd155300c4 317859494 1 2023-05-09 15:59:54 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 0b60d2bf-de71-4b3b-81ad-db92d29cf668 0xc0014183d7 0xc0014183d8}] [] [{kube-controller-manager Update apps/v1 2023-05-09 15:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b60d2bf-de71-4b3b-81ad-db92d29cf668\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001418468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  9 15:59:54.808: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May  9 15:59:54.808: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1585  63fe8792-274c-46e8-a2a6-98b0cb6e4d76 317859491 1 2023-05-09 15:59:49 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 0b60d2bf-de71-4b3b-81ad-db92d29cf668 0xc0014182a7 0xc0014182a8}] [] [{e2e.test Update apps/v1 2023-05-09 15:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 15:59:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-09 15:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"0b60d2bf-de71-4b3b-81ad-db92d29cf668\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001418368 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  9 15:59:54.818: INFO: Pod "test-cleanup-controller-n5trp" is available:
&Pod{ObjectMeta:{test-cleanup-controller-n5trp test-cleanup-controller- deployment-1585  39349d0a-ea6d-41c8-8fdb-33c341a4b8f8 317859204 0 2023-05-09 15:59:49 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:59a58e93ad37840607fd5b21a93a392d17e79478bfed3c370af8f1742c1c9c2f cni.projectcalico.org/podIP:10.2.1.163/32 cni.projectcalico.org/podIPs:10.2.1.163/32] [{apps/v1 ReplicaSet test-cleanup-controller 63fe8792-274c-46e8-a2a6-98b0cb6e4d76 0xc00461f1d7 0xc00461f1d8}] [] [{kube-controller-manager Update v1 2023-05-09 15:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"63fe8792-274c-46e8-a2a6-98b0cb6e4d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:59:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:59:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ddfk4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ddfk4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:59:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:59:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:59:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:59:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:10.2.1.163,StartTime:2023-05-09 15:59:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 15:59:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e95c5d775459d965edfe43d9eca6861b27b32f425d4a226f692e0d824ca7685e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.163,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 15:59:54.818: INFO: Pod "test-cleanup-deployment-7698ff6f6b-mwr9q" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-mwr9q test-cleanup-deployment-7698ff6f6b- deployment-1585  bb654524-a582-4b1d-ad04-c94fe12ed854 317859498 0 2023-05-09 15:59:54 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 95ecbbde-d484-4be5-8449-38dd155300c4 0xc00461f3e7 0xc00461f3e8}] [] [{kube-controller-manager Update v1 2023-05-09 15:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95ecbbde-d484-4be5-8449-38dd155300c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rxrgl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rxrgl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May  9 15:59:54.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1585" for this suite. 05/09/23 15:59:54.825
------------------------------
â€¢ [SLOW TEST] [5.124 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:59:49.711
    May  9 15:59:49.711: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename deployment 05/09/23 15:59:49.712
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:59:49.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:59:49.732
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    May  9 15:59:49.751: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    May  9 15:59:54.760: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/09/23 15:59:54.76
    May  9 15:59:54.760: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 05/09/23 15:59:54.786
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  9 15:59:54.802: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1585  0b60d2bf-de71-4b3b-81ad-db92d29cf668 317859488 1 2023-05-09 15:59:54 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-05-09 15:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040d9f38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    May  9 15:59:54.808: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-1585  95ecbbde-d484-4be5-8449-38dd155300c4 317859494 1 2023-05-09 15:59:54 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 0b60d2bf-de71-4b3b-81ad-db92d29cf668 0xc0014183d7 0xc0014183d8}] [] [{kube-controller-manager Update apps/v1 2023-05-09 15:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b60d2bf-de71-4b3b-81ad-db92d29cf668\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001418468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  9 15:59:54.808: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    May  9 15:59:54.808: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1585  63fe8792-274c-46e8-a2a6-98b0cb6e4d76 317859491 1 2023-05-09 15:59:49 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 0b60d2bf-de71-4b3b-81ad-db92d29cf668 0xc0014182a7 0xc0014182a8}] [] [{e2e.test Update apps/v1 2023-05-09 15:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 15:59:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-09 15:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"0b60d2bf-de71-4b3b-81ad-db92d29cf668\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001418368 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  9 15:59:54.818: INFO: Pod "test-cleanup-controller-n5trp" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-n5trp test-cleanup-controller- deployment-1585  39349d0a-ea6d-41c8-8fdb-33c341a4b8f8 317859204 0 2023-05-09 15:59:49 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:59a58e93ad37840607fd5b21a93a392d17e79478bfed3c370af8f1742c1c9c2f cni.projectcalico.org/podIP:10.2.1.163/32 cni.projectcalico.org/podIPs:10.2.1.163/32] [{apps/v1 ReplicaSet test-cleanup-controller 63fe8792-274c-46e8-a2a6-98b0cb6e4d76 0xc00461f1d7 0xc00461f1d8}] [] [{kube-controller-manager Update v1 2023-05-09 15:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"63fe8792-274c-46e8-a2a6-98b0cb6e4d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 15:59:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 15:59:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ddfk4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ddfk4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:59:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:59:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:59:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 15:59:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:10.2.1.163,StartTime:2023-05-09 15:59:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 15:59:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e95c5d775459d965edfe43d9eca6861b27b32f425d4a226f692e0d824ca7685e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.163,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 15:59:54.818: INFO: Pod "test-cleanup-deployment-7698ff6f6b-mwr9q" is not available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-mwr9q test-cleanup-deployment-7698ff6f6b- deployment-1585  bb654524-a582-4b1d-ad04-c94fe12ed854 317859498 0 2023-05-09 15:59:54 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 95ecbbde-d484-4be5-8449-38dd155300c4 0xc00461f3e7 0xc00461f3e8}] [] [{kube-controller-manager Update v1 2023-05-09 15:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95ecbbde-d484-4be5-8449-38dd155300c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rxrgl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rxrgl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May  9 15:59:54.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1585" for this suite. 05/09/23 15:59:54.825
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 15:59:54.836
May  9 15:59:54.836: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename resourcequota 05/09/23 15:59:54.837
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:59:54.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:59:54.885
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 05/09/23 15:59:54.889
STEP: Creating a ResourceQuota 05/09/23 15:59:59.896
STEP: Ensuring resource quota status is calculated 05/09/23 15:59:59.902
STEP: Creating a ReplicaSet 05/09/23 16:00:01.909
STEP: Ensuring resource quota status captures replicaset creation 05/09/23 16:00:01.923
STEP: Deleting a ReplicaSet 05/09/23 16:00:03.93
STEP: Ensuring resource quota status released usage 05/09/23 16:00:03.939
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  9 16:00:05.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7793" for this suite. 05/09/23 16:00:05.954
------------------------------
â€¢ [SLOW TEST] [11.126 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 15:59:54.836
    May  9 15:59:54.836: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename resourcequota 05/09/23 15:59:54.837
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 15:59:54.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 15:59:54.885
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 05/09/23 15:59:54.889
    STEP: Creating a ResourceQuota 05/09/23 15:59:59.896
    STEP: Ensuring resource quota status is calculated 05/09/23 15:59:59.902
    STEP: Creating a ReplicaSet 05/09/23 16:00:01.909
    STEP: Ensuring resource quota status captures replicaset creation 05/09/23 16:00:01.923
    STEP: Deleting a ReplicaSet 05/09/23 16:00:03.93
    STEP: Ensuring resource quota status released usage 05/09/23 16:00:03.939
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  9 16:00:05.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7793" for this suite. 05/09/23 16:00:05.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:00:05.964
May  9 16:00:05.964: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename services 05/09/23 16:00:05.965
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:00:05.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:00:05.987
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 05/09/23 16:00:06.001
STEP: watching for the Service to be added 05/09/23 16:00:06.018
May  9 16:00:06.021: INFO: Found Service test-service-gwcpk in namespace services-94 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
May  9 16:00:06.021: INFO: Service test-service-gwcpk created
STEP: Getting /status 05/09/23 16:00:06.021
May  9 16:00:06.027: INFO: Service test-service-gwcpk has LoadBalancer: {[]}
STEP: patching the ServiceStatus 05/09/23 16:00:06.027
STEP: watching for the Service to be patched 05/09/23 16:00:06.036
May  9 16:00:06.039: INFO: observed Service test-service-gwcpk in namespace services-94 with annotations: map[] & LoadBalancer: {[]}
May  9 16:00:06.039: INFO: Found Service test-service-gwcpk in namespace services-94 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
May  9 16:00:06.039: INFO: Service test-service-gwcpk has service status patched
STEP: updating the ServiceStatus 05/09/23 16:00:06.039
May  9 16:00:06.067: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 05/09/23 16:00:06.067
May  9 16:00:06.070: INFO: Observed Service test-service-gwcpk in namespace services-94 with annotations: map[] & Conditions: {[]}
May  9 16:00:06.070: INFO: Observed event: &Service{ObjectMeta:{test-service-gwcpk  services-94  719fce6d-29bf-41a5-807d-0d7e5ff66274 317861271 0 2023-05-09 16:00:06 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-09 16:00:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-09 16:00:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.3.8.6,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.3.8.6],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
May  9 16:00:06.070: INFO: Found Service test-service-gwcpk in namespace services-94 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  9 16:00:06.070: INFO: Service test-service-gwcpk has service status updated
STEP: patching the service 05/09/23 16:00:06.07
STEP: watching for the Service to be patched 05/09/23 16:00:06.079
May  9 16:00:06.081: INFO: observed Service test-service-gwcpk in namespace services-94 with labels: map[test-service-static:true]
May  9 16:00:06.081: INFO: observed Service test-service-gwcpk in namespace services-94 with labels: map[test-service-static:true]
May  9 16:00:06.081: INFO: observed Service test-service-gwcpk in namespace services-94 with labels: map[test-service-static:true]
May  9 16:00:06.081: INFO: Found Service test-service-gwcpk in namespace services-94 with labels: map[test-service:patched test-service-static:true]
May  9 16:00:06.081: INFO: Service test-service-gwcpk patched
STEP: deleting the service 05/09/23 16:00:06.081
STEP: watching for the Service to be deleted 05/09/23 16:00:06.098
May  9 16:00:06.100: INFO: Observed event: ADDED
May  9 16:00:06.100: INFO: Observed event: MODIFIED
May  9 16:00:06.100: INFO: Observed event: MODIFIED
May  9 16:00:06.100: INFO: Observed event: MODIFIED
May  9 16:00:06.100: INFO: Found Service test-service-gwcpk in namespace services-94 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
May  9 16:00:06.100: INFO: Service test-service-gwcpk deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  9 16:00:06.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-94" for this suite. 05/09/23 16:00:06.106
------------------------------
â€¢ [0.150 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:00:05.964
    May  9 16:00:05.964: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename services 05/09/23 16:00:05.965
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:00:05.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:00:05.987
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 05/09/23 16:00:06.001
    STEP: watching for the Service to be added 05/09/23 16:00:06.018
    May  9 16:00:06.021: INFO: Found Service test-service-gwcpk in namespace services-94 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    May  9 16:00:06.021: INFO: Service test-service-gwcpk created
    STEP: Getting /status 05/09/23 16:00:06.021
    May  9 16:00:06.027: INFO: Service test-service-gwcpk has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 05/09/23 16:00:06.027
    STEP: watching for the Service to be patched 05/09/23 16:00:06.036
    May  9 16:00:06.039: INFO: observed Service test-service-gwcpk in namespace services-94 with annotations: map[] & LoadBalancer: {[]}
    May  9 16:00:06.039: INFO: Found Service test-service-gwcpk in namespace services-94 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    May  9 16:00:06.039: INFO: Service test-service-gwcpk has service status patched
    STEP: updating the ServiceStatus 05/09/23 16:00:06.039
    May  9 16:00:06.067: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 05/09/23 16:00:06.067
    May  9 16:00:06.070: INFO: Observed Service test-service-gwcpk in namespace services-94 with annotations: map[] & Conditions: {[]}
    May  9 16:00:06.070: INFO: Observed event: &Service{ObjectMeta:{test-service-gwcpk  services-94  719fce6d-29bf-41a5-807d-0d7e5ff66274 317861271 0 2023-05-09 16:00:06 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-09 16:00:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-09 16:00:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.3.8.6,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.3.8.6],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    May  9 16:00:06.070: INFO: Found Service test-service-gwcpk in namespace services-94 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  9 16:00:06.070: INFO: Service test-service-gwcpk has service status updated
    STEP: patching the service 05/09/23 16:00:06.07
    STEP: watching for the Service to be patched 05/09/23 16:00:06.079
    May  9 16:00:06.081: INFO: observed Service test-service-gwcpk in namespace services-94 with labels: map[test-service-static:true]
    May  9 16:00:06.081: INFO: observed Service test-service-gwcpk in namespace services-94 with labels: map[test-service-static:true]
    May  9 16:00:06.081: INFO: observed Service test-service-gwcpk in namespace services-94 with labels: map[test-service-static:true]
    May  9 16:00:06.081: INFO: Found Service test-service-gwcpk in namespace services-94 with labels: map[test-service:patched test-service-static:true]
    May  9 16:00:06.081: INFO: Service test-service-gwcpk patched
    STEP: deleting the service 05/09/23 16:00:06.081
    STEP: watching for the Service to be deleted 05/09/23 16:00:06.098
    May  9 16:00:06.100: INFO: Observed event: ADDED
    May  9 16:00:06.100: INFO: Observed event: MODIFIED
    May  9 16:00:06.100: INFO: Observed event: MODIFIED
    May  9 16:00:06.100: INFO: Observed event: MODIFIED
    May  9 16:00:06.100: INFO: Found Service test-service-gwcpk in namespace services-94 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    May  9 16:00:06.100: INFO: Service test-service-gwcpk deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  9 16:00:06.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-94" for this suite. 05/09/23 16:00:06.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:00:06.114
May  9 16:00:06.114: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename container-runtime 05/09/23 16:00:06.115
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:00:06.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:00:06.135
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 05/09/23 16:00:06.14
STEP: wait for the container to reach Failed 05/09/23 16:00:06.156
STEP: get the container status 05/09/23 16:00:10.284
STEP: the container should be terminated 05/09/23 16:00:10.293
STEP: the termination message should be set 05/09/23 16:00:10.294
May  9 16:00:10.294: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 05/09/23 16:00:10.294
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May  9 16:00:10.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5185" for this suite. 05/09/23 16:00:10.333
------------------------------
â€¢ [4.227 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:00:06.114
    May  9 16:00:06.114: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename container-runtime 05/09/23 16:00:06.115
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:00:06.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:00:06.135
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 05/09/23 16:00:06.14
    STEP: wait for the container to reach Failed 05/09/23 16:00:06.156
    STEP: get the container status 05/09/23 16:00:10.284
    STEP: the container should be terminated 05/09/23 16:00:10.293
    STEP: the termination message should be set 05/09/23 16:00:10.294
    May  9 16:00:10.294: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 05/09/23 16:00:10.294
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May  9 16:00:10.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5185" for this suite. 05/09/23 16:00:10.333
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:00:10.342
May  9 16:00:10.342: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename containers 05/09/23 16:00:10.344
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:00:10.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:00:10.366
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 05/09/23 16:00:10.37
May  9 16:00:10.379: INFO: Waiting up to 5m0s for pod "client-containers-e89dca82-00e4-40d8-83f8-197216f26ec8" in namespace "containers-2938" to be "Succeeded or Failed"
May  9 16:00:10.384: INFO: Pod "client-containers-e89dca82-00e4-40d8-83f8-197216f26ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.02749ms
May  9 16:00:12.390: INFO: Pod "client-containers-e89dca82-00e4-40d8-83f8-197216f26ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011048301s
May  9 16:00:14.391: INFO: Pod "client-containers-e89dca82-00e4-40d8-83f8-197216f26ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01260142s
STEP: Saw pod success 05/09/23 16:00:14.391
May  9 16:00:14.391: INFO: Pod "client-containers-e89dca82-00e4-40d8-83f8-197216f26ec8" satisfied condition "Succeeded or Failed"
May  9 16:00:14.397: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod client-containers-e89dca82-00e4-40d8-83f8-197216f26ec8 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 16:00:14.411
May  9 16:00:14.427: INFO: Waiting for pod client-containers-e89dca82-00e4-40d8-83f8-197216f26ec8 to disappear
May  9 16:00:14.434: INFO: Pod client-containers-e89dca82-00e4-40d8-83f8-197216f26ec8 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
May  9 16:00:14.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2938" for this suite. 05/09/23 16:00:14.442
------------------------------
â€¢ [4.110 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:00:10.342
    May  9 16:00:10.342: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename containers 05/09/23 16:00:10.344
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:00:10.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:00:10.366
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 05/09/23 16:00:10.37
    May  9 16:00:10.379: INFO: Waiting up to 5m0s for pod "client-containers-e89dca82-00e4-40d8-83f8-197216f26ec8" in namespace "containers-2938" to be "Succeeded or Failed"
    May  9 16:00:10.384: INFO: Pod "client-containers-e89dca82-00e4-40d8-83f8-197216f26ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.02749ms
    May  9 16:00:12.390: INFO: Pod "client-containers-e89dca82-00e4-40d8-83f8-197216f26ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011048301s
    May  9 16:00:14.391: INFO: Pod "client-containers-e89dca82-00e4-40d8-83f8-197216f26ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01260142s
    STEP: Saw pod success 05/09/23 16:00:14.391
    May  9 16:00:14.391: INFO: Pod "client-containers-e89dca82-00e4-40d8-83f8-197216f26ec8" satisfied condition "Succeeded or Failed"
    May  9 16:00:14.397: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod client-containers-e89dca82-00e4-40d8-83f8-197216f26ec8 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 16:00:14.411
    May  9 16:00:14.427: INFO: Waiting for pod client-containers-e89dca82-00e4-40d8-83f8-197216f26ec8 to disappear
    May  9 16:00:14.434: INFO: Pod client-containers-e89dca82-00e4-40d8-83f8-197216f26ec8 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    May  9 16:00:14.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2938" for this suite. 05/09/23 16:00:14.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:00:14.452
May  9 16:00:14.453: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename lease-test 05/09/23 16:00:14.453
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:00:14.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:00:14.513
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
May  9 16:00:14.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-4574" for this suite. 05/09/23 16:00:14.616
------------------------------
â€¢ [0.173 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:00:14.452
    May  9 16:00:14.453: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename lease-test 05/09/23 16:00:14.453
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:00:14.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:00:14.513
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    May  9 16:00:14.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-4574" for this suite. 05/09/23 16:00:14.616
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:00:14.626
May  9 16:00:14.626: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename init-container 05/09/23 16:00:14.626
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:00:14.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:00:14.648
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 05/09/23 16:00:14.653
May  9 16:00:14.653: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May  9 16:00:19.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-9337" for this suite. 05/09/23 16:00:19.693
------------------------------
â€¢ [SLOW TEST] [5.075 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:00:14.626
    May  9 16:00:14.626: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename init-container 05/09/23 16:00:14.626
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:00:14.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:00:14.648
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 05/09/23 16:00:14.653
    May  9 16:00:14.653: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May  9 16:00:19.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-9337" for this suite. 05/09/23 16:00:19.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:00:19.701
May  9 16:00:19.701: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 16:00:19.702
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:00:19.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:00:19.73
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 05/09/23 16:00:19.735
May  9 16:00:19.746: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39f9e008-78b5-461d-ab5e-69f890618e7f" in namespace "projected-2034" to be "Succeeded or Failed"
May  9 16:00:19.759: INFO: Pod "downwardapi-volume-39f9e008-78b5-461d-ab5e-69f890618e7f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.77417ms
May  9 16:00:21.766: INFO: Pod "downwardapi-volume-39f9e008-78b5-461d-ab5e-69f890618e7f": Phase="Running", Reason="", readiness=false. Elapsed: 2.020336913s
May  9 16:00:23.766: INFO: Pod "downwardapi-volume-39f9e008-78b5-461d-ab5e-69f890618e7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020656865s
STEP: Saw pod success 05/09/23 16:00:23.766
May  9 16:00:23.766: INFO: Pod "downwardapi-volume-39f9e008-78b5-461d-ab5e-69f890618e7f" satisfied condition "Succeeded or Failed"
May  9 16:00:23.772: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-39f9e008-78b5-461d-ab5e-69f890618e7f container client-container: <nil>
STEP: delete the pod 05/09/23 16:00:23.783
May  9 16:00:23.803: INFO: Waiting for pod downwardapi-volume-39f9e008-78b5-461d-ab5e-69f890618e7f to disappear
May  9 16:00:23.808: INFO: Pod downwardapi-volume-39f9e008-78b5-461d-ab5e-69f890618e7f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  9 16:00:23.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2034" for this suite. 05/09/23 16:00:23.814
------------------------------
â€¢ [4.122 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:00:19.701
    May  9 16:00:19.701: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 16:00:19.702
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:00:19.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:00:19.73
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 05/09/23 16:00:19.735
    May  9 16:00:19.746: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39f9e008-78b5-461d-ab5e-69f890618e7f" in namespace "projected-2034" to be "Succeeded or Failed"
    May  9 16:00:19.759: INFO: Pod "downwardapi-volume-39f9e008-78b5-461d-ab5e-69f890618e7f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.77417ms
    May  9 16:00:21.766: INFO: Pod "downwardapi-volume-39f9e008-78b5-461d-ab5e-69f890618e7f": Phase="Running", Reason="", readiness=false. Elapsed: 2.020336913s
    May  9 16:00:23.766: INFO: Pod "downwardapi-volume-39f9e008-78b5-461d-ab5e-69f890618e7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020656865s
    STEP: Saw pod success 05/09/23 16:00:23.766
    May  9 16:00:23.766: INFO: Pod "downwardapi-volume-39f9e008-78b5-461d-ab5e-69f890618e7f" satisfied condition "Succeeded or Failed"
    May  9 16:00:23.772: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-39f9e008-78b5-461d-ab5e-69f890618e7f container client-container: <nil>
    STEP: delete the pod 05/09/23 16:00:23.783
    May  9 16:00:23.803: INFO: Waiting for pod downwardapi-volume-39f9e008-78b5-461d-ab5e-69f890618e7f to disappear
    May  9 16:00:23.808: INFO: Pod downwardapi-volume-39f9e008-78b5-461d-ab5e-69f890618e7f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  9 16:00:23.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2034" for this suite. 05/09/23 16:00:23.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:00:23.824
May  9 16:00:23.824: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename sched-pred 05/09/23 16:00:23.825
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:00:23.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:00:23.846
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
May  9 16:00:23.850: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  9 16:00:23.865: INFO: Waiting for terminating namespaces to be deleted...
May  9 16:00:23.871: INFO: 
Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 before test
May  9 16:00:23.881: INFO: pod-init-93e651ac-90c4-4e36-85ca-b9911452efe6 from init-container-9337 started at 2023-05-09 16:00:14 +0000 UTC (1 container statuses recorded)
May  9 16:00:23.881: INFO: 	Container run1 ready: false, restart count 0
May  9 16:00:23.881: INFO: canal-zcmck from kube-system started at 2023-05-09 08:48:05 +0000 UTC (2 container statuses recorded)
May  9 16:00:23.881: INFO: 	Container calico-node ready: true, restart count 2
May  9 16:00:23.881: INFO: 	Container kube-flannel ready: true, restart count 0
May  9 16:00:23.881: INFO: coredns-996c5dbc5-kcx4b from kube-system started at 2023-05-09 15:55:37 +0000 UTC (1 container statuses recorded)
May  9 16:00:23.881: INFO: 	Container coredns ready: true, restart count 0
May  9 16:00:23.881: INFO: kube-proxy-q8nhz from kube-system started at 2023-05-09 08:48:05 +0000 UTC (1 container statuses recorded)
May  9 16:00:23.881: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 16:00:23.881: INFO: wormhole-xj4hj from kube-system started at 2023-05-09 08:48:05 +0000 UTC (1 container statuses recorded)
May  9 16:00:23.881: INFO: 	Container wormhole ready: true, restart count 0
May  9 16:00:23.881: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-69c6h from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
May  9 16:00:23.881: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 16:00:23.881: INFO: 	Container systemd-logs ready: true, restart count 0
May  9 16:00:23.881: INFO: 
Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 before test
May  9 16:00:23.893: INFO: calico-kube-controllers-6cffbf7894-zrprz from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
May  9 16:00:23.894: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  9 16:00:23.894: INFO: canal-22bj2 from kube-system started at 2023-05-09 08:47:59 +0000 UTC (2 container statuses recorded)
May  9 16:00:23.894: INFO: 	Container calico-node ready: true, restart count 2
May  9 16:00:23.894: INFO: 	Container kube-flannel ready: true, restart count 0
May  9 16:00:23.894: INFO: coredns-996c5dbc5-24wst from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
May  9 16:00:23.894: INFO: 	Container coredns ready: true, restart count 0
May  9 16:00:23.894: INFO: kube-dns-autoscaler-789d47d664-hhx85 from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
May  9 16:00:23.894: INFO: 	Container autoscaler ready: true, restart count 0
May  9 16:00:23.894: INFO: kube-proxy-r9g6q from kube-system started at 2023-05-09 08:47:59 +0000 UTC (1 container statuses recorded)
May  9 16:00:23.894: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 16:00:23.894: INFO: metrics-server-5f9c95d78-hk5zz from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
May  9 16:00:23.894: INFO: 	Container metrics-server ready: true, restart count 0
May  9 16:00:23.894: INFO: wormhole-hnkjk from kube-system started at 2023-05-09 08:47:59 +0000 UTC (1 container statuses recorded)
May  9 16:00:23.894: INFO: 	Container wormhole ready: true, restart count 0
May  9 16:00:23.894: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-2x6mg from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
May  9 16:00:23.894: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 16:00:23.894: INFO: 	Container systemd-logs ready: true, restart count 0
May  9 16:00:23.894: INFO: 
Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 before test
May  9 16:00:23.906: INFO: canal-jl6h7 from kube-system started at 2023-05-09 08:48:03 +0000 UTC (2 container statuses recorded)
May  9 16:00:23.906: INFO: 	Container calico-node ready: true, restart count 2
May  9 16:00:23.906: INFO: 	Container kube-flannel ready: true, restart count 0
May  9 16:00:23.906: INFO: kube-proxy-qqc8l from kube-system started at 2023-05-09 08:48:03 +0000 UTC (1 container statuses recorded)
May  9 16:00:23.906: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 16:00:23.906: INFO: wormhole-825jz from kube-system started at 2023-05-09 08:48:03 +0000 UTC (1 container statuses recorded)
May  9 16:00:23.906: INFO: 	Container wormhole ready: true, restart count 0
May  9 16:00:23.906: INFO: sonobuoy from sonobuoy started at 2023-05-09 15:24:58 +0000 UTC (1 container statuses recorded)
May  9 16:00:23.906: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  9 16:00:23.906: INFO: sonobuoy-e2e-job-d96e230b174b4cad from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
May  9 16:00:23.906: INFO: 	Container e2e ready: true, restart count 0
May  9 16:00:23.906: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 16:00:23.906: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-zvq2k from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
May  9 16:00:23.906: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 16:00:23.906: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/09/23 16:00:23.906
May  9 16:00:23.916: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-620" to be "running"
May  9 16:00:23.920: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.883374ms
May  9 16:00:25.928: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011409533s
May  9 16:00:25.928: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/09/23 16:00:25.933
STEP: Trying to apply a random label on the found node. 05/09/23 16:00:25.955
STEP: verifying the node has the label kubernetes.io/e2e-473e8c7d-3eaf-4771-8300-2e5b948845ad 95 05/09/23 16:00:25.967
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 05/09/23 16:00:25.975
May  9 16:00:26.006: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-620" to be "not pending"
May  9 16:00:26.013: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.360138ms
May  9 16:00:28.027: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.021215776s
May  9 16:00:28.027: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 51.68.91.222 on the node which pod4 resides and expect not scheduled 05/09/23 16:00:28.027
May  9 16:00:28.044: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-620" to be "not pending"
May  9 16:00:28.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.855234ms
May  9 16:00:30.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01964183s
May  9 16:00:32.061: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016798266s
May  9 16:00:34.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018663127s
May  9 16:00:36.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019585212s
May  9 16:00:38.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.028079687s
May  9 16:00:40.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.018786307s
May  9 16:00:42.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.019487518s
May  9 16:00:44.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.019288177s
May  9 16:00:46.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.020252957s
May  9 16:00:48.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.018500541s
May  9 16:00:50.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01871346s
May  9 16:00:52.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.017948454s
May  9 16:00:54.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.019065982s
May  9 16:00:56.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.028380427s
May  9 16:00:58.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.017861929s
May  9 16:01:00.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.018490326s
May  9 16:01:02.070: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.025913952s
May  9 16:01:04.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.051583746s
May  9 16:01:06.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.01978339s
May  9 16:01:08.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.018307483s
May  9 16:01:10.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.020244302s
May  9 16:01:12.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.017331774s
May  9 16:01:14.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.021946969s
May  9 16:01:16.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.018606619s
May  9 16:01:18.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.018135822s
May  9 16:01:20.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.017499851s
May  9 16:01:22.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.020556441s
May  9 16:01:24.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.01918971s
May  9 16:01:26.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.020107442s
May  9 16:01:28.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.021957397s
May  9 16:01:30.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.021274122s
May  9 16:01:32.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.017920637s
May  9 16:01:34.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.018757611s
May  9 16:01:36.068: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.023330748s
May  9 16:01:38.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.020291786s
May  9 16:01:40.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.018147265s
May  9 16:01:42.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.017474001s
May  9 16:01:44.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.019943473s
May  9 16:01:46.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.019042919s
May  9 16:01:48.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.019619061s
May  9 16:01:50.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.01856792s
May  9 16:01:52.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.018381849s
May  9 16:01:54.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.019917462s
May  9 16:01:56.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.017247288s
May  9 16:01:58.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.017719255s
May  9 16:02:00.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.019698364s
May  9 16:02:02.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.018194947s
May  9 16:02:04.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.02055s
May  9 16:02:06.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.01920506s
May  9 16:02:08.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.019183448s
May  9 16:02:10.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.019922144s
May  9 16:02:12.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.017881743s
May  9 16:02:14.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.019217896s
May  9 16:02:16.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.021240404s
May  9 16:02:18.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.019740654s
May  9 16:02:20.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.019028096s
May  9 16:02:22.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.019085444s
May  9 16:02:24.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.018979796s
May  9 16:02:26.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.020975161s
May  9 16:02:28.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.017889158s
May  9 16:02:30.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.019510136s
May  9 16:02:32.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.01914123s
May  9 16:02:34.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.021561489s
May  9 16:02:36.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.018324211s
May  9 16:02:38.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.017696464s
May  9 16:02:40.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.019997441s
May  9 16:02:42.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.020102484s
May  9 16:02:44.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.019669382s
May  9 16:02:46.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.018201073s
May  9 16:02:48.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.021860461s
May  9 16:02:50.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.017336648s
May  9 16:02:52.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.018266114s
May  9 16:02:54.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.019951981s
May  9 16:02:56.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.019985734s
May  9 16:02:58.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.021444425s
May  9 16:03:00.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.018065087s
May  9 16:03:02.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.01770451s
May  9 16:03:04.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.018065905s
May  9 16:03:06.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.018052597s
May  9 16:03:08.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.017575847s
May  9 16:03:10.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.027087132s
May  9 16:03:12.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.018315024s
May  9 16:03:14.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.020861731s
May  9 16:03:16.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.019256073s
May  9 16:03:18.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.020517595s
May  9 16:03:20.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.019109987s
May  9 16:03:22.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.019448442s
May  9 16:03:24.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.01884432s
May  9 16:03:26.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.020239598s
May  9 16:03:28.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.018133138s
May  9 16:03:30.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.019854999s
May  9 16:03:32.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.019100632s
May  9 16:03:34.068: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.02302801s
May  9 16:03:36.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.0177613s
May  9 16:03:38.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.019185369s
May  9 16:03:40.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.018228335s
May  9 16:03:42.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.018379739s
May  9 16:03:44.071: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.02648646s
May  9 16:03:46.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.018934118s
May  9 16:03:48.067: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.022078461s
May  9 16:03:50.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.019809793s
May  9 16:03:52.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.019690469s
May  9 16:03:54.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.019096983s
May  9 16:03:56.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.018535716s
May  9 16:03:58.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.019227066s
May  9 16:04:00.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.018609284s
May  9 16:04:02.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.019047294s
May  9 16:04:04.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.01937313s
May  9 16:04:06.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.017433958s
May  9 16:04:08.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.019257319s
May  9 16:04:10.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.018491683s
May  9 16:04:12.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.019649123s
May  9 16:04:14.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.049688378s
May  9 16:04:16.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.019005037s
May  9 16:04:18.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.018267795s
May  9 16:04:20.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.02105065s
May  9 16:04:22.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.019377571s
May  9 16:04:24.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.018088267s
May  9 16:04:26.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.019467723s
May  9 16:04:28.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.018407859s
May  9 16:04:30.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.019727572s
May  9 16:04:32.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.021695869s
May  9 16:04:34.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.019948784s
May  9 16:04:36.068: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.02318955s
May  9 16:04:38.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.018763869s
May  9 16:04:40.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.019603542s
May  9 16:04:42.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.018717616s
May  9 16:04:44.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.017855965s
May  9 16:04:46.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.0198648s
May  9 16:04:48.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.019615878s
May  9 16:04:50.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.018954713s
May  9 16:04:52.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.018675738s
May  9 16:04:54.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.019448551s
May  9 16:04:56.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.018305511s
May  9 16:04:58.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.018104502s
May  9 16:05:00.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.017845292s
May  9 16:05:02.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.018305492s
May  9 16:05:04.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.018270822s
May  9 16:05:06.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.01809454s
May  9 16:05:08.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.021655625s
May  9 16:05:10.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.017879608s
May  9 16:05:12.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.019731432s
May  9 16:05:14.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.018042021s
May  9 16:05:16.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.018099421s
May  9 16:05:18.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.017977348s
May  9 16:05:20.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.020707082s
May  9 16:05:22.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.018536899s
May  9 16:05:24.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.034313687s
May  9 16:05:26.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.019564784s
May  9 16:05:28.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.017706962s
May  9 16:05:28.067: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.022448513s
STEP: removing the label kubernetes.io/e2e-473e8c7d-3eaf-4771-8300-2e5b948845ad off the node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 05/09/23 16:05:28.067
STEP: verifying the node doesn't have the label kubernetes.io/e2e-473e8c7d-3eaf-4771-8300-2e5b948845ad 05/09/23 16:05:28.088
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 16:05:28.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-620" for this suite. 05/09/23 16:05:28.098
------------------------------
â€¢ [SLOW TEST] [304.282 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:00:23.824
    May  9 16:00:23.824: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename sched-pred 05/09/23 16:00:23.825
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:00:23.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:00:23.846
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    May  9 16:00:23.850: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  9 16:00:23.865: INFO: Waiting for terminating namespaces to be deleted...
    May  9 16:00:23.871: INFO: 
    Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 before test
    May  9 16:00:23.881: INFO: pod-init-93e651ac-90c4-4e36-85ca-b9911452efe6 from init-container-9337 started at 2023-05-09 16:00:14 +0000 UTC (1 container statuses recorded)
    May  9 16:00:23.881: INFO: 	Container run1 ready: false, restart count 0
    May  9 16:00:23.881: INFO: canal-zcmck from kube-system started at 2023-05-09 08:48:05 +0000 UTC (2 container statuses recorded)
    May  9 16:00:23.881: INFO: 	Container calico-node ready: true, restart count 2
    May  9 16:00:23.881: INFO: 	Container kube-flannel ready: true, restart count 0
    May  9 16:00:23.881: INFO: coredns-996c5dbc5-kcx4b from kube-system started at 2023-05-09 15:55:37 +0000 UTC (1 container statuses recorded)
    May  9 16:00:23.881: INFO: 	Container coredns ready: true, restart count 0
    May  9 16:00:23.881: INFO: kube-proxy-q8nhz from kube-system started at 2023-05-09 08:48:05 +0000 UTC (1 container statuses recorded)
    May  9 16:00:23.881: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 16:00:23.881: INFO: wormhole-xj4hj from kube-system started at 2023-05-09 08:48:05 +0000 UTC (1 container statuses recorded)
    May  9 16:00:23.881: INFO: 	Container wormhole ready: true, restart count 0
    May  9 16:00:23.881: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-69c6h from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
    May  9 16:00:23.881: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 16:00:23.881: INFO: 	Container systemd-logs ready: true, restart count 0
    May  9 16:00:23.881: INFO: 
    Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 before test
    May  9 16:00:23.893: INFO: calico-kube-controllers-6cffbf7894-zrprz from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
    May  9 16:00:23.894: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  9 16:00:23.894: INFO: canal-22bj2 from kube-system started at 2023-05-09 08:47:59 +0000 UTC (2 container statuses recorded)
    May  9 16:00:23.894: INFO: 	Container calico-node ready: true, restart count 2
    May  9 16:00:23.894: INFO: 	Container kube-flannel ready: true, restart count 0
    May  9 16:00:23.894: INFO: coredns-996c5dbc5-24wst from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
    May  9 16:00:23.894: INFO: 	Container coredns ready: true, restart count 0
    May  9 16:00:23.894: INFO: kube-dns-autoscaler-789d47d664-hhx85 from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
    May  9 16:00:23.894: INFO: 	Container autoscaler ready: true, restart count 0
    May  9 16:00:23.894: INFO: kube-proxy-r9g6q from kube-system started at 2023-05-09 08:47:59 +0000 UTC (1 container statuses recorded)
    May  9 16:00:23.894: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 16:00:23.894: INFO: metrics-server-5f9c95d78-hk5zz from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
    May  9 16:00:23.894: INFO: 	Container metrics-server ready: true, restart count 0
    May  9 16:00:23.894: INFO: wormhole-hnkjk from kube-system started at 2023-05-09 08:47:59 +0000 UTC (1 container statuses recorded)
    May  9 16:00:23.894: INFO: 	Container wormhole ready: true, restart count 0
    May  9 16:00:23.894: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-2x6mg from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
    May  9 16:00:23.894: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 16:00:23.894: INFO: 	Container systemd-logs ready: true, restart count 0
    May  9 16:00:23.894: INFO: 
    Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 before test
    May  9 16:00:23.906: INFO: canal-jl6h7 from kube-system started at 2023-05-09 08:48:03 +0000 UTC (2 container statuses recorded)
    May  9 16:00:23.906: INFO: 	Container calico-node ready: true, restart count 2
    May  9 16:00:23.906: INFO: 	Container kube-flannel ready: true, restart count 0
    May  9 16:00:23.906: INFO: kube-proxy-qqc8l from kube-system started at 2023-05-09 08:48:03 +0000 UTC (1 container statuses recorded)
    May  9 16:00:23.906: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 16:00:23.906: INFO: wormhole-825jz from kube-system started at 2023-05-09 08:48:03 +0000 UTC (1 container statuses recorded)
    May  9 16:00:23.906: INFO: 	Container wormhole ready: true, restart count 0
    May  9 16:00:23.906: INFO: sonobuoy from sonobuoy started at 2023-05-09 15:24:58 +0000 UTC (1 container statuses recorded)
    May  9 16:00:23.906: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  9 16:00:23.906: INFO: sonobuoy-e2e-job-d96e230b174b4cad from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
    May  9 16:00:23.906: INFO: 	Container e2e ready: true, restart count 0
    May  9 16:00:23.906: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 16:00:23.906: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-zvq2k from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
    May  9 16:00:23.906: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 16:00:23.906: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/09/23 16:00:23.906
    May  9 16:00:23.916: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-620" to be "running"
    May  9 16:00:23.920: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.883374ms
    May  9 16:00:25.928: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011409533s
    May  9 16:00:25.928: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/09/23 16:00:25.933
    STEP: Trying to apply a random label on the found node. 05/09/23 16:00:25.955
    STEP: verifying the node has the label kubernetes.io/e2e-473e8c7d-3eaf-4771-8300-2e5b948845ad 95 05/09/23 16:00:25.967
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 05/09/23 16:00:25.975
    May  9 16:00:26.006: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-620" to be "not pending"
    May  9 16:00:26.013: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.360138ms
    May  9 16:00:28.027: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.021215776s
    May  9 16:00:28.027: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 51.68.91.222 on the node which pod4 resides and expect not scheduled 05/09/23 16:00:28.027
    May  9 16:00:28.044: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-620" to be "not pending"
    May  9 16:00:28.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.855234ms
    May  9 16:00:30.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01964183s
    May  9 16:00:32.061: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016798266s
    May  9 16:00:34.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018663127s
    May  9 16:00:36.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019585212s
    May  9 16:00:38.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.028079687s
    May  9 16:00:40.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.018786307s
    May  9 16:00:42.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.019487518s
    May  9 16:00:44.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.019288177s
    May  9 16:00:46.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.020252957s
    May  9 16:00:48.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.018500541s
    May  9 16:00:50.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01871346s
    May  9 16:00:52.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.017948454s
    May  9 16:00:54.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.019065982s
    May  9 16:00:56.073: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.028380427s
    May  9 16:00:58.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.017861929s
    May  9 16:01:00.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.018490326s
    May  9 16:01:02.070: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.025913952s
    May  9 16:01:04.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.051583746s
    May  9 16:01:06.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.01978339s
    May  9 16:01:08.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.018307483s
    May  9 16:01:10.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.020244302s
    May  9 16:01:12.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.017331774s
    May  9 16:01:14.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.021946969s
    May  9 16:01:16.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.018606619s
    May  9 16:01:18.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.018135822s
    May  9 16:01:20.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.017499851s
    May  9 16:01:22.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.020556441s
    May  9 16:01:24.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.01918971s
    May  9 16:01:26.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.020107442s
    May  9 16:01:28.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.021957397s
    May  9 16:01:30.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.021274122s
    May  9 16:01:32.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.017920637s
    May  9 16:01:34.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.018757611s
    May  9 16:01:36.068: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.023330748s
    May  9 16:01:38.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.020291786s
    May  9 16:01:40.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.018147265s
    May  9 16:01:42.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.017474001s
    May  9 16:01:44.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.019943473s
    May  9 16:01:46.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.019042919s
    May  9 16:01:48.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.019619061s
    May  9 16:01:50.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.01856792s
    May  9 16:01:52.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.018381849s
    May  9 16:01:54.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.019917462s
    May  9 16:01:56.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.017247288s
    May  9 16:01:58.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.017719255s
    May  9 16:02:00.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.019698364s
    May  9 16:02:02.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.018194947s
    May  9 16:02:04.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.02055s
    May  9 16:02:06.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.01920506s
    May  9 16:02:08.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.019183448s
    May  9 16:02:10.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.019922144s
    May  9 16:02:12.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.017881743s
    May  9 16:02:14.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.019217896s
    May  9 16:02:16.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.021240404s
    May  9 16:02:18.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.019740654s
    May  9 16:02:20.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.019028096s
    May  9 16:02:22.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.019085444s
    May  9 16:02:24.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.018979796s
    May  9 16:02:26.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.020975161s
    May  9 16:02:28.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.017889158s
    May  9 16:02:30.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.019510136s
    May  9 16:02:32.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.01914123s
    May  9 16:02:34.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.021561489s
    May  9 16:02:36.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.018324211s
    May  9 16:02:38.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.017696464s
    May  9 16:02:40.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.019997441s
    May  9 16:02:42.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.020102484s
    May  9 16:02:44.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.019669382s
    May  9 16:02:46.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.018201073s
    May  9 16:02:48.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.021860461s
    May  9 16:02:50.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.017336648s
    May  9 16:02:52.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.018266114s
    May  9 16:02:54.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.019951981s
    May  9 16:02:56.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.019985734s
    May  9 16:02:58.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.021444425s
    May  9 16:03:00.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.018065087s
    May  9 16:03:02.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.01770451s
    May  9 16:03:04.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.018065905s
    May  9 16:03:06.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.018052597s
    May  9 16:03:08.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.017575847s
    May  9 16:03:10.072: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.027087132s
    May  9 16:03:12.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.018315024s
    May  9 16:03:14.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.020861731s
    May  9 16:03:16.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.019256073s
    May  9 16:03:18.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.020517595s
    May  9 16:03:20.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.019109987s
    May  9 16:03:22.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.019448442s
    May  9 16:03:24.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.01884432s
    May  9 16:03:26.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.020239598s
    May  9 16:03:28.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.018133138s
    May  9 16:03:30.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.019854999s
    May  9 16:03:32.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.019100632s
    May  9 16:03:34.068: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.02302801s
    May  9 16:03:36.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.0177613s
    May  9 16:03:38.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.019185369s
    May  9 16:03:40.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.018228335s
    May  9 16:03:42.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.018379739s
    May  9 16:03:44.071: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.02648646s
    May  9 16:03:46.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.018934118s
    May  9 16:03:48.067: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.022078461s
    May  9 16:03:50.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.019809793s
    May  9 16:03:52.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.019690469s
    May  9 16:03:54.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.019096983s
    May  9 16:03:56.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.018535716s
    May  9 16:03:58.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.019227066s
    May  9 16:04:00.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.018609284s
    May  9 16:04:02.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.019047294s
    May  9 16:04:04.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.01937313s
    May  9 16:04:06.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.017433958s
    May  9 16:04:08.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.019257319s
    May  9 16:04:10.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.018491683s
    May  9 16:04:12.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.019649123s
    May  9 16:04:14.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.049688378s
    May  9 16:04:16.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.019005037s
    May  9 16:04:18.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.018267795s
    May  9 16:04:20.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.02105065s
    May  9 16:04:22.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.019377571s
    May  9 16:04:24.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.018088267s
    May  9 16:04:26.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.019467723s
    May  9 16:04:28.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.018407859s
    May  9 16:04:30.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.019727572s
    May  9 16:04:32.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.021695869s
    May  9 16:04:34.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.019948784s
    May  9 16:04:36.068: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.02318955s
    May  9 16:04:38.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.018763869s
    May  9 16:04:40.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.019603542s
    May  9 16:04:42.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.018717616s
    May  9 16:04:44.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.017855965s
    May  9 16:04:46.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.0198648s
    May  9 16:04:48.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.019615878s
    May  9 16:04:50.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.018954713s
    May  9 16:04:52.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.018675738s
    May  9 16:04:54.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.019448551s
    May  9 16:04:56.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.018305511s
    May  9 16:04:58.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.018104502s
    May  9 16:05:00.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.017845292s
    May  9 16:05:02.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.018305492s
    May  9 16:05:04.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.018270822s
    May  9 16:05:06.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.01809454s
    May  9 16:05:08.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.021655625s
    May  9 16:05:10.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.017879608s
    May  9 16:05:12.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.019731432s
    May  9 16:05:14.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.018042021s
    May  9 16:05:16.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.018099421s
    May  9 16:05:18.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.017977348s
    May  9 16:05:20.065: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.020707082s
    May  9 16:05:22.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.018536899s
    May  9 16:05:24.079: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.034313687s
    May  9 16:05:26.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.019564784s
    May  9 16:05:28.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.017706962s
    May  9 16:05:28.067: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.022448513s
    STEP: removing the label kubernetes.io/e2e-473e8c7d-3eaf-4771-8300-2e5b948845ad off the node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 05/09/23 16:05:28.067
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-473e8c7d-3eaf-4771-8300-2e5b948845ad 05/09/23 16:05:28.088
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 16:05:28.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-620" for this suite. 05/09/23 16:05:28.098
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:05:28.107
May  9 16:05:28.107: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename secrets 05/09/23 16:05:28.108
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:05:28.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:05:28.131
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-34670ed6-03bc-49d8-8a5a-c89bb7fb401d 05/09/23 16:05:28.135
STEP: Creating a pod to test consume secrets 05/09/23 16:05:28.142
May  9 16:05:28.152: INFO: Waiting up to 5m0s for pod "pod-secrets-eaf93895-02a0-4faa-9d1a-c4195ffb55ae" in namespace "secrets-9243" to be "Succeeded or Failed"
May  9 16:05:28.157: INFO: Pod "pod-secrets-eaf93895-02a0-4faa-9d1a-c4195ffb55ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.914824ms
May  9 16:05:30.164: INFO: Pod "pod-secrets-eaf93895-02a0-4faa-9d1a-c4195ffb55ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011190045s
May  9 16:05:32.163: INFO: Pod "pod-secrets-eaf93895-02a0-4faa-9d1a-c4195ffb55ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01058075s
STEP: Saw pod success 05/09/23 16:05:32.163
May  9 16:05:32.163: INFO: Pod "pod-secrets-eaf93895-02a0-4faa-9d1a-c4195ffb55ae" satisfied condition "Succeeded or Failed"
May  9 16:05:32.168: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-secrets-eaf93895-02a0-4faa-9d1a-c4195ffb55ae container secret-env-test: <nil>
STEP: delete the pod 05/09/23 16:05:32.229
May  9 16:05:32.246: INFO: Waiting for pod pod-secrets-eaf93895-02a0-4faa-9d1a-c4195ffb55ae to disappear
May  9 16:05:32.251: INFO: Pod pod-secrets-eaf93895-02a0-4faa-9d1a-c4195ffb55ae no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
May  9 16:05:32.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9243" for this suite. 05/09/23 16:05:32.257
------------------------------
â€¢ [4.159 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:05:28.107
    May  9 16:05:28.107: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename secrets 05/09/23 16:05:28.108
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:05:28.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:05:28.131
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-34670ed6-03bc-49d8-8a5a-c89bb7fb401d 05/09/23 16:05:28.135
    STEP: Creating a pod to test consume secrets 05/09/23 16:05:28.142
    May  9 16:05:28.152: INFO: Waiting up to 5m0s for pod "pod-secrets-eaf93895-02a0-4faa-9d1a-c4195ffb55ae" in namespace "secrets-9243" to be "Succeeded or Failed"
    May  9 16:05:28.157: INFO: Pod "pod-secrets-eaf93895-02a0-4faa-9d1a-c4195ffb55ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.914824ms
    May  9 16:05:30.164: INFO: Pod "pod-secrets-eaf93895-02a0-4faa-9d1a-c4195ffb55ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011190045s
    May  9 16:05:32.163: INFO: Pod "pod-secrets-eaf93895-02a0-4faa-9d1a-c4195ffb55ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01058075s
    STEP: Saw pod success 05/09/23 16:05:32.163
    May  9 16:05:32.163: INFO: Pod "pod-secrets-eaf93895-02a0-4faa-9d1a-c4195ffb55ae" satisfied condition "Succeeded or Failed"
    May  9 16:05:32.168: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-secrets-eaf93895-02a0-4faa-9d1a-c4195ffb55ae container secret-env-test: <nil>
    STEP: delete the pod 05/09/23 16:05:32.229
    May  9 16:05:32.246: INFO: Waiting for pod pod-secrets-eaf93895-02a0-4faa-9d1a-c4195ffb55ae to disappear
    May  9 16:05:32.251: INFO: Pod pod-secrets-eaf93895-02a0-4faa-9d1a-c4195ffb55ae no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    May  9 16:05:32.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9243" for this suite. 05/09/23 16:05:32.257
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:05:32.269
May  9 16:05:32.269: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename init-container 05/09/23 16:05:32.271
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:05:32.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:05:32.289
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 05/09/23 16:05:32.294
May  9 16:05:32.294: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May  9 16:05:35.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3673" for this suite. 05/09/23 16:05:35.388
------------------------------
â€¢ [3.127 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:05:32.269
    May  9 16:05:32.269: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename init-container 05/09/23 16:05:32.271
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:05:32.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:05:32.289
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 05/09/23 16:05:32.294
    May  9 16:05:32.294: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May  9 16:05:35.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3673" for this suite. 05/09/23 16:05:35.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:05:35.399
May  9 16:05:35.399: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename downward-api 05/09/23 16:05:35.4
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:05:35.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:05:35.421
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 05/09/23 16:05:35.429
May  9 16:05:35.440: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b2de8f72-d4dd-4621-9e96-0d94cc3c0db1" in namespace "downward-api-3907" to be "Succeeded or Failed"
May  9 16:05:35.446: INFO: Pod "downwardapi-volume-b2de8f72-d4dd-4621-9e96-0d94cc3c0db1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.452626ms
May  9 16:05:37.452: INFO: Pod "downwardapi-volume-b2de8f72-d4dd-4621-9e96-0d94cc3c0db1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012574726s
May  9 16:05:39.453: INFO: Pod "downwardapi-volume-b2de8f72-d4dd-4621-9e96-0d94cc3c0db1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01336548s
STEP: Saw pod success 05/09/23 16:05:39.453
May  9 16:05:39.453: INFO: Pod "downwardapi-volume-b2de8f72-d4dd-4621-9e96-0d94cc3c0db1" satisfied condition "Succeeded or Failed"
May  9 16:05:39.459: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-b2de8f72-d4dd-4621-9e96-0d94cc3c0db1 container client-container: <nil>
STEP: delete the pod 05/09/23 16:05:39.471
May  9 16:05:39.486: INFO: Waiting for pod downwardapi-volume-b2de8f72-d4dd-4621-9e96-0d94cc3c0db1 to disappear
May  9 16:05:39.491: INFO: Pod downwardapi-volume-b2de8f72-d4dd-4621-9e96-0d94cc3c0db1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  9 16:05:39.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3907" for this suite. 05/09/23 16:05:39.539
------------------------------
â€¢ [4.155 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:05:35.399
    May  9 16:05:35.399: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename downward-api 05/09/23 16:05:35.4
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:05:35.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:05:35.421
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 05/09/23 16:05:35.429
    May  9 16:05:35.440: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b2de8f72-d4dd-4621-9e96-0d94cc3c0db1" in namespace "downward-api-3907" to be "Succeeded or Failed"
    May  9 16:05:35.446: INFO: Pod "downwardapi-volume-b2de8f72-d4dd-4621-9e96-0d94cc3c0db1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.452626ms
    May  9 16:05:37.452: INFO: Pod "downwardapi-volume-b2de8f72-d4dd-4621-9e96-0d94cc3c0db1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012574726s
    May  9 16:05:39.453: INFO: Pod "downwardapi-volume-b2de8f72-d4dd-4621-9e96-0d94cc3c0db1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01336548s
    STEP: Saw pod success 05/09/23 16:05:39.453
    May  9 16:05:39.453: INFO: Pod "downwardapi-volume-b2de8f72-d4dd-4621-9e96-0d94cc3c0db1" satisfied condition "Succeeded or Failed"
    May  9 16:05:39.459: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-b2de8f72-d4dd-4621-9e96-0d94cc3c0db1 container client-container: <nil>
    STEP: delete the pod 05/09/23 16:05:39.471
    May  9 16:05:39.486: INFO: Waiting for pod downwardapi-volume-b2de8f72-d4dd-4621-9e96-0d94cc3c0db1 to disappear
    May  9 16:05:39.491: INFO: Pod downwardapi-volume-b2de8f72-d4dd-4621-9e96-0d94cc3c0db1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  9 16:05:39.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3907" for this suite. 05/09/23 16:05:39.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:05:39.556
May  9 16:05:39.556: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename resourcequota 05/09/23 16:05:39.557
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:05:39.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:05:39.587
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 05/09/23 16:05:39.591
STEP: Counting existing ResourceQuota 05/09/23 16:05:44.597
STEP: Creating a ResourceQuota 05/09/23 16:05:49.603
STEP: Ensuring resource quota status is calculated 05/09/23 16:05:49.611
STEP: Creating a Secret 05/09/23 16:05:51.618
STEP: Ensuring resource quota status captures secret creation 05/09/23 16:05:51.63
STEP: Deleting a secret 05/09/23 16:05:53.636
STEP: Ensuring resource quota status released usage 05/09/23 16:05:53.645
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  9 16:05:55.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6491" for this suite. 05/09/23 16:05:55.657
------------------------------
â€¢ [SLOW TEST] [16.110 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:05:39.556
    May  9 16:05:39.556: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename resourcequota 05/09/23 16:05:39.557
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:05:39.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:05:39.587
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 05/09/23 16:05:39.591
    STEP: Counting existing ResourceQuota 05/09/23 16:05:44.597
    STEP: Creating a ResourceQuota 05/09/23 16:05:49.603
    STEP: Ensuring resource quota status is calculated 05/09/23 16:05:49.611
    STEP: Creating a Secret 05/09/23 16:05:51.618
    STEP: Ensuring resource quota status captures secret creation 05/09/23 16:05:51.63
    STEP: Deleting a secret 05/09/23 16:05:53.636
    STEP: Ensuring resource quota status released usage 05/09/23 16:05:53.645
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  9 16:05:55.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6491" for this suite. 05/09/23 16:05:55.657
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:05:55.666
May  9 16:05:55.666: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubectl 05/09/23 16:05:55.667
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:05:55.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:05:55.688
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/09/23 16:05:55.692
May  9 16:05:55.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4883 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May  9 16:05:55.794: INFO: stderr: ""
May  9 16:05:55.794: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 05/09/23 16:05:55.794
STEP: verifying the pod e2e-test-httpd-pod was created 05/09/23 16:06:00.849
May  9 16:06:00.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4883 get pod e2e-test-httpd-pod -o json'
May  9 16:06:00.943: INFO: stderr: ""
May  9 16:06:00.943: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"5e240571364f6e29ed0197028914991d42e3ce7cc3530838d626181a8a285519\",\n            \"cni.projectcalico.org/podIP\": \"10.2.1.173/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.2.1.173/32\"\n        },\n        \"creationTimestamp\": \"2023-05-09T16:05:55Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4883\",\n        \"resourceVersion\": \"317895853\",\n        \"uid\": \"3333fe97-f7f8-41fc-9124-114909a1394a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"Always\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-f4692\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"nodepool-8cc7f47e-9b0c-4801-88-node-7ad816\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-f4692\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-09T16:05:55Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-09T16:05:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-09T16:05:57Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-09T16:05:55Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://620f634a5326e71f8fa9e2bb6e2a28453c25ed27cfc45a4ba6234e7be31f5947\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-09T16:05:56Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"51.68.91.222\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.2.1.173\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.2.1.173\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-09T16:05:55Z\"\n    }\n}\n"
STEP: replace the image in the pod 05/09/23 16:06:00.943
May  9 16:06:00.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4883 replace -f -'
May  9 16:06:01.170: INFO: stderr: ""
May  9 16:06:01.170: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 05/09/23 16:06:01.17
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
May  9 16:06:01.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4883 delete pods e2e-test-httpd-pod'
May  9 16:06:03.483: INFO: stderr: ""
May  9 16:06:03.483: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  9 16:06:03.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4883" for this suite. 05/09/23 16:06:03.572
------------------------------
â€¢ [SLOW TEST] [7.996 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:05:55.666
    May  9 16:05:55.666: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubectl 05/09/23 16:05:55.667
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:05:55.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:05:55.688
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/09/23 16:05:55.692
    May  9 16:05:55.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4883 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    May  9 16:05:55.794: INFO: stderr: ""
    May  9 16:05:55.794: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 05/09/23 16:05:55.794
    STEP: verifying the pod e2e-test-httpd-pod was created 05/09/23 16:06:00.849
    May  9 16:06:00.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4883 get pod e2e-test-httpd-pod -o json'
    May  9 16:06:00.943: INFO: stderr: ""
    May  9 16:06:00.943: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"5e240571364f6e29ed0197028914991d42e3ce7cc3530838d626181a8a285519\",\n            \"cni.projectcalico.org/podIP\": \"10.2.1.173/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.2.1.173/32\"\n        },\n        \"creationTimestamp\": \"2023-05-09T16:05:55Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4883\",\n        \"resourceVersion\": \"317895853\",\n        \"uid\": \"3333fe97-f7f8-41fc-9124-114909a1394a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"Always\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-f4692\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"nodepool-8cc7f47e-9b0c-4801-88-node-7ad816\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-f4692\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-09T16:05:55Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-09T16:05:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-09T16:05:57Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-09T16:05:55Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://620f634a5326e71f8fa9e2bb6e2a28453c25ed27cfc45a4ba6234e7be31f5947\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-09T16:05:56Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"51.68.91.222\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.2.1.173\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.2.1.173\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-09T16:05:55Z\"\n    }\n}\n"
    STEP: replace the image in the pod 05/09/23 16:06:00.943
    May  9 16:06:00.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4883 replace -f -'
    May  9 16:06:01.170: INFO: stderr: ""
    May  9 16:06:01.170: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 05/09/23 16:06:01.17
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    May  9 16:06:01.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4883 delete pods e2e-test-httpd-pod'
    May  9 16:06:03.483: INFO: stderr: ""
    May  9 16:06:03.483: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  9 16:06:03.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4883" for this suite. 05/09/23 16:06:03.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:06:03.663
May  9 16:06:03.663: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 16:06:03.664
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:06:03.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:06:03.69
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-3db01775-f8e5-48ed-9b5f-d9af06f15494 05/09/23 16:06:03.759
STEP: Creating secret with name s-test-opt-upd-820673a1-92df-4c8c-b4ca-8d47ce1e849f 05/09/23 16:06:03.765
STEP: Creating the pod 05/09/23 16:06:03.771
May  9 16:06:03.785: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-295015eb-1f8c-42bd-ba6b-c455c877f85f" in namespace "projected-7108" to be "running and ready"
May  9 16:06:03.794: INFO: Pod "pod-projected-secrets-295015eb-1f8c-42bd-ba6b-c455c877f85f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.283462ms
May  9 16:06:03.794: INFO: The phase of Pod pod-projected-secrets-295015eb-1f8c-42bd-ba6b-c455c877f85f is Pending, waiting for it to be Running (with Ready = true)
May  9 16:06:05.802: INFO: Pod "pod-projected-secrets-295015eb-1f8c-42bd-ba6b-c455c877f85f": Phase="Running", Reason="", readiness=true. Elapsed: 2.016930739s
May  9 16:06:05.802: INFO: The phase of Pod pod-projected-secrets-295015eb-1f8c-42bd-ba6b-c455c877f85f is Running (Ready = true)
May  9 16:06:05.802: INFO: Pod "pod-projected-secrets-295015eb-1f8c-42bd-ba6b-c455c877f85f" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-3db01775-f8e5-48ed-9b5f-d9af06f15494 05/09/23 16:06:05.85
STEP: Updating secret s-test-opt-upd-820673a1-92df-4c8c-b4ca-8d47ce1e849f 05/09/23 16:06:05.858
STEP: Creating secret with name s-test-opt-create-b77b489e-12df-49cb-8002-78aa094b1a6d 05/09/23 16:06:05.868
STEP: waiting to observe update in volume 05/09/23 16:06:05.874
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May  9 16:06:07.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7108" for this suite. 05/09/23 16:06:07.937
------------------------------
â€¢ [4.282 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:06:03.663
    May  9 16:06:03.663: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 16:06:03.664
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:06:03.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:06:03.69
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-3db01775-f8e5-48ed-9b5f-d9af06f15494 05/09/23 16:06:03.759
    STEP: Creating secret with name s-test-opt-upd-820673a1-92df-4c8c-b4ca-8d47ce1e849f 05/09/23 16:06:03.765
    STEP: Creating the pod 05/09/23 16:06:03.771
    May  9 16:06:03.785: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-295015eb-1f8c-42bd-ba6b-c455c877f85f" in namespace "projected-7108" to be "running and ready"
    May  9 16:06:03.794: INFO: Pod "pod-projected-secrets-295015eb-1f8c-42bd-ba6b-c455c877f85f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.283462ms
    May  9 16:06:03.794: INFO: The phase of Pod pod-projected-secrets-295015eb-1f8c-42bd-ba6b-c455c877f85f is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:06:05.802: INFO: Pod "pod-projected-secrets-295015eb-1f8c-42bd-ba6b-c455c877f85f": Phase="Running", Reason="", readiness=true. Elapsed: 2.016930739s
    May  9 16:06:05.802: INFO: The phase of Pod pod-projected-secrets-295015eb-1f8c-42bd-ba6b-c455c877f85f is Running (Ready = true)
    May  9 16:06:05.802: INFO: Pod "pod-projected-secrets-295015eb-1f8c-42bd-ba6b-c455c877f85f" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-3db01775-f8e5-48ed-9b5f-d9af06f15494 05/09/23 16:06:05.85
    STEP: Updating secret s-test-opt-upd-820673a1-92df-4c8c-b4ca-8d47ce1e849f 05/09/23 16:06:05.858
    STEP: Creating secret with name s-test-opt-create-b77b489e-12df-49cb-8002-78aa094b1a6d 05/09/23 16:06:05.868
    STEP: waiting to observe update in volume 05/09/23 16:06:05.874
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May  9 16:06:07.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7108" for this suite. 05/09/23 16:06:07.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:06:07.946
May  9 16:06:07.946: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename daemonsets 05/09/23 16:06:07.947
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:06:07.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:06:07.968
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 05/09/23 16:06:08.003
STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 16:06:08.01
May  9 16:06:08.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:06:08.022: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 16:06:09.037: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:06:09.037: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 16:06:10.037: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  9 16:06:10.037: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 is running 0 daemon pod, expected 1
May  9 16:06:11.041: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  9 16:06:11.041: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 05/09/23 16:06:11.046
May  9 16:06:11.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  9 16:06:11.074: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 05/09/23 16:06:11.074
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/09/23 16:06:11.099
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5664, will wait for the garbage collector to delete the pods 05/09/23 16:06:11.1
May  9 16:06:11.165: INFO: Deleting DaemonSet.extensions daemon-set took: 8.889382ms
May  9 16:06:11.265: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.528343ms
May  9 16:06:14.171: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:06:14.171: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  9 16:06:14.176: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"317897781"},"items":null}

May  9 16:06:14.180: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"317897781"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 16:06:14.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5664" for this suite. 05/09/23 16:06:14.212
------------------------------
â€¢ [SLOW TEST] [6.274 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:06:07.946
    May  9 16:06:07.946: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename daemonsets 05/09/23 16:06:07.947
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:06:07.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:06:07.968
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 05/09/23 16:06:08.003
    STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 16:06:08.01
    May  9 16:06:08.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:06:08.022: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 16:06:09.037: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:06:09.037: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 16:06:10.037: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  9 16:06:10.037: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 is running 0 daemon pod, expected 1
    May  9 16:06:11.041: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  9 16:06:11.041: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 05/09/23 16:06:11.046
    May  9 16:06:11.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  9 16:06:11.074: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 05/09/23 16:06:11.074
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/09/23 16:06:11.099
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5664, will wait for the garbage collector to delete the pods 05/09/23 16:06:11.1
    May  9 16:06:11.165: INFO: Deleting DaemonSet.extensions daemon-set took: 8.889382ms
    May  9 16:06:11.265: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.528343ms
    May  9 16:06:14.171: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:06:14.171: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  9 16:06:14.176: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"317897781"},"items":null}

    May  9 16:06:14.180: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"317897781"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 16:06:14.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5664" for this suite. 05/09/23 16:06:14.212
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:06:14.222
May  9 16:06:14.222: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 16:06:14.223
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:06:14.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:06:14.264
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 05/09/23 16:06:14.27
May  9 16:06:14.284: INFO: Waiting up to 5m0s for pod "labelsupdatebb26c586-242c-47d5-a23c-72b5af2f2a2f" in namespace "projected-9283" to be "running and ready"
May  9 16:06:14.292: INFO: Pod "labelsupdatebb26c586-242c-47d5-a23c-72b5af2f2a2f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.279752ms
May  9 16:06:14.292: INFO: The phase of Pod labelsupdatebb26c586-242c-47d5-a23c-72b5af2f2a2f is Pending, waiting for it to be Running (with Ready = true)
May  9 16:06:16.318: INFO: Pod "labelsupdatebb26c586-242c-47d5-a23c-72b5af2f2a2f": Phase="Running", Reason="", readiness=true. Elapsed: 2.034173287s
May  9 16:06:16.318: INFO: The phase of Pod labelsupdatebb26c586-242c-47d5-a23c-72b5af2f2a2f is Running (Ready = true)
May  9 16:06:16.318: INFO: Pod "labelsupdatebb26c586-242c-47d5-a23c-72b5af2f2a2f" satisfied condition "running and ready"
May  9 16:06:16.906: INFO: Successfully updated pod "labelsupdatebb26c586-242c-47d5-a23c-72b5af2f2a2f"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  9 16:06:18.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9283" for this suite. 05/09/23 16:06:18.937
------------------------------
â€¢ [4.723 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:06:14.222
    May  9 16:06:14.222: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 16:06:14.223
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:06:14.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:06:14.264
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 05/09/23 16:06:14.27
    May  9 16:06:14.284: INFO: Waiting up to 5m0s for pod "labelsupdatebb26c586-242c-47d5-a23c-72b5af2f2a2f" in namespace "projected-9283" to be "running and ready"
    May  9 16:06:14.292: INFO: Pod "labelsupdatebb26c586-242c-47d5-a23c-72b5af2f2a2f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.279752ms
    May  9 16:06:14.292: INFO: The phase of Pod labelsupdatebb26c586-242c-47d5-a23c-72b5af2f2a2f is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:06:16.318: INFO: Pod "labelsupdatebb26c586-242c-47d5-a23c-72b5af2f2a2f": Phase="Running", Reason="", readiness=true. Elapsed: 2.034173287s
    May  9 16:06:16.318: INFO: The phase of Pod labelsupdatebb26c586-242c-47d5-a23c-72b5af2f2a2f is Running (Ready = true)
    May  9 16:06:16.318: INFO: Pod "labelsupdatebb26c586-242c-47d5-a23c-72b5af2f2a2f" satisfied condition "running and ready"
    May  9 16:06:16.906: INFO: Successfully updated pod "labelsupdatebb26c586-242c-47d5-a23c-72b5af2f2a2f"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  9 16:06:18.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9283" for this suite. 05/09/23 16:06:18.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:06:18.947
May  9 16:06:18.947: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename replication-controller 05/09/23 16:06:18.948
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:06:18.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:06:18.968
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 05/09/23 16:06:18.973
STEP: When the matched label of one of its pods change 05/09/23 16:06:18.979
May  9 16:06:18.987: INFO: Pod name pod-release: Found 0 pods out of 1
May  9 16:06:23.994: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 05/09/23 16:06:24.011
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May  9 16:06:24.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8195" for this suite. 05/09/23 16:06:24.024
------------------------------
â€¢ [SLOW TEST] [5.090 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:06:18.947
    May  9 16:06:18.947: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename replication-controller 05/09/23 16:06:18.948
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:06:18.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:06:18.968
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 05/09/23 16:06:18.973
    STEP: When the matched label of one of its pods change 05/09/23 16:06:18.979
    May  9 16:06:18.987: INFO: Pod name pod-release: Found 0 pods out of 1
    May  9 16:06:23.994: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 05/09/23 16:06:24.011
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May  9 16:06:24.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8195" for this suite. 05/09/23 16:06:24.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:06:24.038
May  9 16:06:24.038: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename deployment 05/09/23 16:06:24.039
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:06:24.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:06:24.072
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 05/09/23 16:06:24.095
May  9 16:06:24.095: INFO: Creating simple deployment test-deployment-2r4l9
May  9 16:06:24.115: INFO: deployment "test-deployment-2r4l9" doesn't have the required revision set
STEP: Getting /status 05/09/23 16:06:26.138
May  9 16:06:26.143: INFO: Deployment test-deployment-2r4l9 has Conditions: [{Available True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2r4l9-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 05/09/23 16:06:26.144
May  9 16:06:26.157: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 6, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 6, 24, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-2r4l9-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 05/09/23 16:06:26.157
May  9 16:06:26.160: INFO: Observed &Deployment event: ADDED
May  9 16:06:26.160: INFO: Observed Deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2r4l9-54bc444df"}
May  9 16:06:26.160: INFO: Observed &Deployment event: MODIFIED
May  9 16:06:26.160: INFO: Observed Deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2r4l9-54bc444df"}
May  9 16:06:26.160: INFO: Observed Deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  9 16:06:26.160: INFO: Observed &Deployment event: MODIFIED
May  9 16:06:26.160: INFO: Observed Deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  9 16:06:26.160: INFO: Observed Deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-2r4l9-54bc444df" is progressing.}
May  9 16:06:26.160: INFO: Observed &Deployment event: MODIFIED
May  9 16:06:26.160: INFO: Observed Deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  9 16:06:26.160: INFO: Observed Deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2r4l9-54bc444df" has successfully progressed.}
May  9 16:06:26.160: INFO: Observed &Deployment event: MODIFIED
May  9 16:06:26.160: INFO: Observed Deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  9 16:06:26.160: INFO: Observed Deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2r4l9-54bc444df" has successfully progressed.}
May  9 16:06:26.160: INFO: Found Deployment test-deployment-2r4l9 in namespace deployment-7607 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  9 16:06:26.160: INFO: Deployment test-deployment-2r4l9 has an updated status
STEP: patching the Statefulset Status 05/09/23 16:06:26.16
May  9 16:06:26.161: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May  9 16:06:26.168: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 05/09/23 16:06:26.168
May  9 16:06:26.171: INFO: Observed &Deployment event: ADDED
May  9 16:06:26.171: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2r4l9-54bc444df"}
May  9 16:06:26.171: INFO: Observed &Deployment event: MODIFIED
May  9 16:06:26.171: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2r4l9-54bc444df"}
May  9 16:06:26.171: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  9 16:06:26.171: INFO: Observed &Deployment event: MODIFIED
May  9 16:06:26.171: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  9 16:06:26.171: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-2r4l9-54bc444df" is progressing.}
May  9 16:06:26.172: INFO: Observed &Deployment event: MODIFIED
May  9 16:06:26.172: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  9 16:06:26.172: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2r4l9-54bc444df" has successfully progressed.}
May  9 16:06:26.172: INFO: Observed &Deployment event: MODIFIED
May  9 16:06:26.172: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  9 16:06:26.172: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2r4l9-54bc444df" has successfully progressed.}
May  9 16:06:26.172: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  9 16:06:26.172: INFO: Observed &Deployment event: MODIFIED
May  9 16:06:26.172: INFO: Found deployment test-deployment-2r4l9 in namespace deployment-7607 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
May  9 16:06:26.172: INFO: Deployment test-deployment-2r4l9 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  9 16:06:26.178: INFO: Deployment "test-deployment-2r4l9":
&Deployment{ObjectMeta:{test-deployment-2r4l9  deployment-7607  a910c8ce-74c2-4afd-95fa-b459149e0378 317898955 1 2023-05-09 16:06:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-09 16:06:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-05-09 16:06:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-05-09 16:06:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0011b5358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-2r4l9-54bc444df",LastUpdateTime:2023-05-09 16:06:26 +0000 UTC,LastTransitionTime:2023-05-09 16:06:26 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  9 16:06:26.185: INFO: New ReplicaSet "test-deployment-2r4l9-54bc444df" of Deployment "test-deployment-2r4l9":
&ReplicaSet{ObjectMeta:{test-deployment-2r4l9-54bc444df  deployment-7607  faa431bb-2747-4f17-aba8-050028bb17d6 317898871 1 2023-05-09 16:06:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-2r4l9 a910c8ce-74c2-4afd-95fa-b459149e0378 0xc0011b59d7 0xc0011b59d8}] [] [{kube-controller-manager Update apps/v1 2023-05-09 16:06:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a910c8ce-74c2-4afd-95fa-b459149e0378\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:06:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0011b5a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  9 16:06:26.193: INFO: Pod "test-deployment-2r4l9-54bc444df-pbwfs" is available:
&Pod{ObjectMeta:{test-deployment-2r4l9-54bc444df-pbwfs test-deployment-2r4l9-54bc444df- deployment-7607  83a31c3c-cd2a-4b75-8958-05f32dc0cb61 317898869 0 2023-05-09 16:06:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:2eb918bef2705879e29c2e35f8fba653c997e478b05628326923fac0303e9b81 cni.projectcalico.org/podIP:10.2.1.177/32 cni.projectcalico.org/podIPs:10.2.1.177/32] [{apps/v1 ReplicaSet test-deployment-2r4l9-54bc444df faa431bb-2747-4f17-aba8-050028bb17d6 0xc0011b5e67 0xc0011b5e68}] [] [{calico Update v1 2023-05-09 16:06:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 16:06:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"faa431bb-2747-4f17-aba8-050028bb17d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 16:06:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c6wqh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c6wqh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:06:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:06:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:06:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:06:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:10.2.1.177,StartTime:2023-05-09 16:06:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 16:06:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://be5198a35084451c65c432d4cc69859ecb2954fbac371dd25db7560a5e167531,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May  9 16:06:26.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7607" for this suite. 05/09/23 16:06:26.2
------------------------------
â€¢ [2.173 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:06:24.038
    May  9 16:06:24.038: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename deployment 05/09/23 16:06:24.039
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:06:24.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:06:24.072
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 05/09/23 16:06:24.095
    May  9 16:06:24.095: INFO: Creating simple deployment test-deployment-2r4l9
    May  9 16:06:24.115: INFO: deployment "test-deployment-2r4l9" doesn't have the required revision set
    STEP: Getting /status 05/09/23 16:06:26.138
    May  9 16:06:26.143: INFO: Deployment test-deployment-2r4l9 has Conditions: [{Available True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2r4l9-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 05/09/23 16:06:26.144
    May  9 16:06:26.157: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 6, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 6, 24, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-2r4l9-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 05/09/23 16:06:26.157
    May  9 16:06:26.160: INFO: Observed &Deployment event: ADDED
    May  9 16:06:26.160: INFO: Observed Deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2r4l9-54bc444df"}
    May  9 16:06:26.160: INFO: Observed &Deployment event: MODIFIED
    May  9 16:06:26.160: INFO: Observed Deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2r4l9-54bc444df"}
    May  9 16:06:26.160: INFO: Observed Deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  9 16:06:26.160: INFO: Observed &Deployment event: MODIFIED
    May  9 16:06:26.160: INFO: Observed Deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  9 16:06:26.160: INFO: Observed Deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-2r4l9-54bc444df" is progressing.}
    May  9 16:06:26.160: INFO: Observed &Deployment event: MODIFIED
    May  9 16:06:26.160: INFO: Observed Deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  9 16:06:26.160: INFO: Observed Deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2r4l9-54bc444df" has successfully progressed.}
    May  9 16:06:26.160: INFO: Observed &Deployment event: MODIFIED
    May  9 16:06:26.160: INFO: Observed Deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  9 16:06:26.160: INFO: Observed Deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2r4l9-54bc444df" has successfully progressed.}
    May  9 16:06:26.160: INFO: Found Deployment test-deployment-2r4l9 in namespace deployment-7607 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  9 16:06:26.160: INFO: Deployment test-deployment-2r4l9 has an updated status
    STEP: patching the Statefulset Status 05/09/23 16:06:26.16
    May  9 16:06:26.161: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May  9 16:06:26.168: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 05/09/23 16:06:26.168
    May  9 16:06:26.171: INFO: Observed &Deployment event: ADDED
    May  9 16:06:26.171: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2r4l9-54bc444df"}
    May  9 16:06:26.171: INFO: Observed &Deployment event: MODIFIED
    May  9 16:06:26.171: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2r4l9-54bc444df"}
    May  9 16:06:26.171: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  9 16:06:26.171: INFO: Observed &Deployment event: MODIFIED
    May  9 16:06:26.171: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  9 16:06:26.171: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:24 +0000 UTC 2023-05-09 16:06:24 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-2r4l9-54bc444df" is progressing.}
    May  9 16:06:26.172: INFO: Observed &Deployment event: MODIFIED
    May  9 16:06:26.172: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  9 16:06:26.172: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2r4l9-54bc444df" has successfully progressed.}
    May  9 16:06:26.172: INFO: Observed &Deployment event: MODIFIED
    May  9 16:06:26.172: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  9 16:06:26.172: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-09 16:06:25 +0000 UTC 2023-05-09 16:06:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2r4l9-54bc444df" has successfully progressed.}
    May  9 16:06:26.172: INFO: Observed deployment test-deployment-2r4l9 in namespace deployment-7607 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  9 16:06:26.172: INFO: Observed &Deployment event: MODIFIED
    May  9 16:06:26.172: INFO: Found deployment test-deployment-2r4l9 in namespace deployment-7607 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    May  9 16:06:26.172: INFO: Deployment test-deployment-2r4l9 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  9 16:06:26.178: INFO: Deployment "test-deployment-2r4l9":
    &Deployment{ObjectMeta:{test-deployment-2r4l9  deployment-7607  a910c8ce-74c2-4afd-95fa-b459149e0378 317898955 1 2023-05-09 16:06:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-09 16:06:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-05-09 16:06:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-05-09 16:06:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0011b5358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-2r4l9-54bc444df",LastUpdateTime:2023-05-09 16:06:26 +0000 UTC,LastTransitionTime:2023-05-09 16:06:26 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  9 16:06:26.185: INFO: New ReplicaSet "test-deployment-2r4l9-54bc444df" of Deployment "test-deployment-2r4l9":
    &ReplicaSet{ObjectMeta:{test-deployment-2r4l9-54bc444df  deployment-7607  faa431bb-2747-4f17-aba8-050028bb17d6 317898871 1 2023-05-09 16:06:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-2r4l9 a910c8ce-74c2-4afd-95fa-b459149e0378 0xc0011b59d7 0xc0011b59d8}] [] [{kube-controller-manager Update apps/v1 2023-05-09 16:06:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a910c8ce-74c2-4afd-95fa-b459149e0378\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:06:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0011b5a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  9 16:06:26.193: INFO: Pod "test-deployment-2r4l9-54bc444df-pbwfs" is available:
    &Pod{ObjectMeta:{test-deployment-2r4l9-54bc444df-pbwfs test-deployment-2r4l9-54bc444df- deployment-7607  83a31c3c-cd2a-4b75-8958-05f32dc0cb61 317898869 0 2023-05-09 16:06:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:2eb918bef2705879e29c2e35f8fba653c997e478b05628326923fac0303e9b81 cni.projectcalico.org/podIP:10.2.1.177/32 cni.projectcalico.org/podIPs:10.2.1.177/32] [{apps/v1 ReplicaSet test-deployment-2r4l9-54bc444df faa431bb-2747-4f17-aba8-050028bb17d6 0xc0011b5e67 0xc0011b5e68}] [] [{calico Update v1 2023-05-09 16:06:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 16:06:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"faa431bb-2747-4f17-aba8-050028bb17d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 16:06:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c6wqh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c6wqh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:06:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:06:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:06:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:06:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:10.2.1.177,StartTime:2023-05-09 16:06:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 16:06:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://be5198a35084451c65c432d4cc69859ecb2954fbac371dd25db7560a5e167531,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May  9 16:06:26.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7607" for this suite. 05/09/23 16:06:26.2
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:06:26.211
May  9 16:06:26.211: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename configmap 05/09/23 16:06:26.212
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:06:26.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:06:26.271
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-5734/configmap-test-d85a892c-77e0-4f85-8450-7eb7b6cae199 05/09/23 16:06:26.289
STEP: Creating a pod to test consume configMaps 05/09/23 16:06:26.296
May  9 16:06:26.309: INFO: Waiting up to 5m0s for pod "pod-configmaps-7f432c9c-52a4-46fe-be78-8d5cf7d38a32" in namespace "configmap-5734" to be "Succeeded or Failed"
May  9 16:06:26.316: INFO: Pod "pod-configmaps-7f432c9c-52a4-46fe-be78-8d5cf7d38a32": Phase="Pending", Reason="", readiness=false. Elapsed: 6.904465ms
May  9 16:06:28.323: INFO: Pod "pod-configmaps-7f432c9c-52a4-46fe-be78-8d5cf7d38a32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013523801s
May  9 16:06:30.325: INFO: Pod "pod-configmaps-7f432c9c-52a4-46fe-be78-8d5cf7d38a32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015530141s
STEP: Saw pod success 05/09/23 16:06:30.325
May  9 16:06:30.325: INFO: Pod "pod-configmaps-7f432c9c-52a4-46fe-be78-8d5cf7d38a32" satisfied condition "Succeeded or Failed"
May  9 16:06:30.331: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod pod-configmaps-7f432c9c-52a4-46fe-be78-8d5cf7d38a32 container env-test: <nil>
STEP: delete the pod 05/09/23 16:06:30.386
May  9 16:06:30.405: INFO: Waiting for pod pod-configmaps-7f432c9c-52a4-46fe-be78-8d5cf7d38a32 to disappear
May  9 16:06:30.410: INFO: Pod pod-configmaps-7f432c9c-52a4-46fe-be78-8d5cf7d38a32 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  9 16:06:30.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5734" for this suite. 05/09/23 16:06:30.417
------------------------------
â€¢ [4.216 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:06:26.211
    May  9 16:06:26.211: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename configmap 05/09/23 16:06:26.212
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:06:26.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:06:26.271
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-5734/configmap-test-d85a892c-77e0-4f85-8450-7eb7b6cae199 05/09/23 16:06:26.289
    STEP: Creating a pod to test consume configMaps 05/09/23 16:06:26.296
    May  9 16:06:26.309: INFO: Waiting up to 5m0s for pod "pod-configmaps-7f432c9c-52a4-46fe-be78-8d5cf7d38a32" in namespace "configmap-5734" to be "Succeeded or Failed"
    May  9 16:06:26.316: INFO: Pod "pod-configmaps-7f432c9c-52a4-46fe-be78-8d5cf7d38a32": Phase="Pending", Reason="", readiness=false. Elapsed: 6.904465ms
    May  9 16:06:28.323: INFO: Pod "pod-configmaps-7f432c9c-52a4-46fe-be78-8d5cf7d38a32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013523801s
    May  9 16:06:30.325: INFO: Pod "pod-configmaps-7f432c9c-52a4-46fe-be78-8d5cf7d38a32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015530141s
    STEP: Saw pod success 05/09/23 16:06:30.325
    May  9 16:06:30.325: INFO: Pod "pod-configmaps-7f432c9c-52a4-46fe-be78-8d5cf7d38a32" satisfied condition "Succeeded or Failed"
    May  9 16:06:30.331: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod pod-configmaps-7f432c9c-52a4-46fe-be78-8d5cf7d38a32 container env-test: <nil>
    STEP: delete the pod 05/09/23 16:06:30.386
    May  9 16:06:30.405: INFO: Waiting for pod pod-configmaps-7f432c9c-52a4-46fe-be78-8d5cf7d38a32 to disappear
    May  9 16:06:30.410: INFO: Pod pod-configmaps-7f432c9c-52a4-46fe-be78-8d5cf7d38a32 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  9 16:06:30.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5734" for this suite. 05/09/23 16:06:30.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:06:30.428
May  9 16:06:30.428: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename daemonsets 05/09/23 16:06:30.429
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:06:30.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:06:30.45
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 05/09/23 16:06:30.493
STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 16:06:30.499
May  9 16:06:30.510: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:06:30.510: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 16:06:31.522: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:06:31.523: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 16:06:32.527: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  9 16:06:32.527: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 05/09/23 16:06:32.531
May  9 16:06:32.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 16:06:32.562: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 16:06:33.574: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 16:06:33.574: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 16:06:34.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 16:06:34.575: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 16:06:35.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  9 16:06:35.575: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 16:06:36.576: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  9 16:06:36.576: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/09/23 16:06:36.582
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2661, will wait for the garbage collector to delete the pods 05/09/23 16:06:36.582
May  9 16:06:36.649: INFO: Deleting DaemonSet.extensions daemon-set took: 11.731545ms
May  9 16:06:36.751: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.111039ms
May  9 16:06:38.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:06:38.657: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  9 16:06:38.662: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"317900202"},"items":null}

May  9 16:06:38.666: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"317900204"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 16:06:38.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2661" for this suite. 05/09/23 16:06:38.698
------------------------------
â€¢ [SLOW TEST] [8.280 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:06:30.428
    May  9 16:06:30.428: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename daemonsets 05/09/23 16:06:30.429
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:06:30.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:06:30.45
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 05/09/23 16:06:30.493
    STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 16:06:30.499
    May  9 16:06:30.510: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:06:30.510: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 16:06:31.522: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:06:31.523: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 16:06:32.527: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  9 16:06:32.527: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 05/09/23 16:06:32.531
    May  9 16:06:32.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 16:06:32.562: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 16:06:33.574: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 16:06:33.574: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 16:06:34.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 16:06:34.575: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 16:06:35.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  9 16:06:35.575: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 16:06:36.576: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  9 16:06:36.576: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/09/23 16:06:36.582
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2661, will wait for the garbage collector to delete the pods 05/09/23 16:06:36.582
    May  9 16:06:36.649: INFO: Deleting DaemonSet.extensions daemon-set took: 11.731545ms
    May  9 16:06:36.751: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.111039ms
    May  9 16:06:38.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:06:38.657: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  9 16:06:38.662: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"317900202"},"items":null}

    May  9 16:06:38.666: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"317900204"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 16:06:38.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2661" for this suite. 05/09/23 16:06:38.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:06:38.711
May  9 16:06:38.711: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename daemonsets 05/09/23 16:06:38.712
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:06:38.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:06:38.739
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
May  9 16:06:38.785: INFO: Create a RollingUpdate DaemonSet
May  9 16:06:38.793: INFO: Check that daemon pods launch on every node of the cluster
May  9 16:06:38.803: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:06:38.804: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 16:06:39.817: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:06:39.817: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 16:06:40.818: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  9 16:06:40.818: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
May  9 16:06:40.818: INFO: Update the DaemonSet to trigger a rollout
May  9 16:06:40.831: INFO: Updating DaemonSet daemon-set
May  9 16:06:43.863: INFO: Roll back the DaemonSet before rollout is complete
May  9 16:06:43.877: INFO: Updating DaemonSet daemon-set
May  9 16:06:43.877: INFO: Make sure DaemonSet rollback is complete
May  9 16:06:43.882: INFO: Wrong image for pod: daemon-set-4hg7n. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
May  9 16:06:43.882: INFO: Pod daemon-set-4hg7n is not available
May  9 16:06:46.894: INFO: Pod daemon-set-mgvpp is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/09/23 16:06:46.918
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9022, will wait for the garbage collector to delete the pods 05/09/23 16:06:46.918
May  9 16:06:46.984: INFO: Deleting DaemonSet.extensions daemon-set took: 9.051879ms
May  9 16:06:47.085: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.799171ms
May  9 16:07:18.790: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:07:18.790: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  9 16:07:18.796: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"317904290"},"items":null}

May  9 16:07:18.801: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"317904294"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 16:07:18.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9022" for this suite. 05/09/23 16:07:18.827
------------------------------
â€¢ [SLOW TEST] [40.124 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:06:38.711
    May  9 16:06:38.711: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename daemonsets 05/09/23 16:06:38.712
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:06:38.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:06:38.739
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    May  9 16:06:38.785: INFO: Create a RollingUpdate DaemonSet
    May  9 16:06:38.793: INFO: Check that daemon pods launch on every node of the cluster
    May  9 16:06:38.803: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:06:38.804: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 16:06:39.817: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:06:39.817: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 16:06:40.818: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  9 16:06:40.818: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    May  9 16:06:40.818: INFO: Update the DaemonSet to trigger a rollout
    May  9 16:06:40.831: INFO: Updating DaemonSet daemon-set
    May  9 16:06:43.863: INFO: Roll back the DaemonSet before rollout is complete
    May  9 16:06:43.877: INFO: Updating DaemonSet daemon-set
    May  9 16:06:43.877: INFO: Make sure DaemonSet rollback is complete
    May  9 16:06:43.882: INFO: Wrong image for pod: daemon-set-4hg7n. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    May  9 16:06:43.882: INFO: Pod daemon-set-4hg7n is not available
    May  9 16:06:46.894: INFO: Pod daemon-set-mgvpp is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/09/23 16:06:46.918
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9022, will wait for the garbage collector to delete the pods 05/09/23 16:06:46.918
    May  9 16:06:46.984: INFO: Deleting DaemonSet.extensions daemon-set took: 9.051879ms
    May  9 16:06:47.085: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.799171ms
    May  9 16:07:18.790: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:07:18.790: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  9 16:07:18.796: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"317904290"},"items":null}

    May  9 16:07:18.801: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"317904294"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 16:07:18.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9022" for this suite. 05/09/23 16:07:18.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:07:18.839
May  9 16:07:18.839: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename ephemeral-containers-test 05/09/23 16:07:18.84
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:07:18.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:07:18.866
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 05/09/23 16:07:18.873
May  9 16:07:18.883: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7041" to be "running and ready"
May  9 16:07:18.889: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.580511ms
May  9 16:07:18.889: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
May  9 16:07:20.896: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012810259s
May  9 16:07:20.896: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
May  9 16:07:20.896: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 05/09/23 16:07:20.902
May  9 16:07:20.911: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7041" to be "container debugger running"
May  9 16:07:20.917: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.434443ms
May  9 16:07:22.926: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015078382s
May  9 16:07:24.926: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014886929s
May  9 16:07:24.926: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 05/09/23 16:07:24.926
May  9 16:07:24.926: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-7041 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:07:24.926: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:07:24.927: INFO: ExecWithOptions: Clientset creation
May  9 16:07:24.927: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/ephemeral-containers-test-7041/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
May  9 16:07:25.070: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May  9 16:07:25.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-7041" for this suite. 05/09/23 16:07:25.087
------------------------------
â€¢ [SLOW TEST] [6.256 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:07:18.839
    May  9 16:07:18.839: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename ephemeral-containers-test 05/09/23 16:07:18.84
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:07:18.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:07:18.866
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 05/09/23 16:07:18.873
    May  9 16:07:18.883: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7041" to be "running and ready"
    May  9 16:07:18.889: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.580511ms
    May  9 16:07:18.889: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:07:20.896: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012810259s
    May  9 16:07:20.896: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    May  9 16:07:20.896: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 05/09/23 16:07:20.902
    May  9 16:07:20.911: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7041" to be "container debugger running"
    May  9 16:07:20.917: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.434443ms
    May  9 16:07:22.926: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015078382s
    May  9 16:07:24.926: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014886929s
    May  9 16:07:24.926: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 05/09/23 16:07:24.926
    May  9 16:07:24.926: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-7041 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:07:24.926: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:07:24.927: INFO: ExecWithOptions: Clientset creation
    May  9 16:07:24.927: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/ephemeral-containers-test-7041/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    May  9 16:07:25.070: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May  9 16:07:25.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-7041" for this suite. 05/09/23 16:07:25.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:07:25.097
May  9 16:07:25.097: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename subpath 05/09/23 16:07:25.098
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:07:25.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:07:25.118
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/09/23 16:07:25.121
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-gdbq 05/09/23 16:07:25.133
STEP: Creating a pod to test atomic-volume-subpath 05/09/23 16:07:25.133
May  9 16:07:25.144: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-gdbq" in namespace "subpath-6139" to be "Succeeded or Failed"
May  9 16:07:25.148: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.162597ms
May  9 16:07:27.154: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 2.010095565s
May  9 16:07:29.155: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 4.010861535s
May  9 16:07:31.159: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 6.015100915s
May  9 16:07:33.156: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 8.011979703s
May  9 16:07:35.156: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 10.011990772s
May  9 16:07:37.154: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 12.010533102s
May  9 16:07:39.157: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 14.01280249s
May  9 16:07:41.156: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 16.011796517s
May  9 16:07:43.156: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 18.012379266s
May  9 16:07:45.239: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 20.095514354s
May  9 16:07:47.155: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=false. Elapsed: 22.011166655s
May  9 16:07:49.156: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.01235956s
STEP: Saw pod success 05/09/23 16:07:49.156
May  9 16:07:49.157: INFO: Pod "pod-subpath-test-configmap-gdbq" satisfied condition "Succeeded or Failed"
May  9 16:07:49.162: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-subpath-test-configmap-gdbq container test-container-subpath-configmap-gdbq: <nil>
STEP: delete the pod 05/09/23 16:07:49.18
May  9 16:07:49.197: INFO: Waiting for pod pod-subpath-test-configmap-gdbq to disappear
May  9 16:07:49.202: INFO: Pod pod-subpath-test-configmap-gdbq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-gdbq 05/09/23 16:07:49.202
May  9 16:07:49.202: INFO: Deleting pod "pod-subpath-test-configmap-gdbq" in namespace "subpath-6139"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May  9 16:07:49.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6139" for this suite. 05/09/23 16:07:49.213
------------------------------
â€¢ [SLOW TEST] [24.127 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:07:25.097
    May  9 16:07:25.097: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename subpath 05/09/23 16:07:25.098
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:07:25.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:07:25.118
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/09/23 16:07:25.121
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-gdbq 05/09/23 16:07:25.133
    STEP: Creating a pod to test atomic-volume-subpath 05/09/23 16:07:25.133
    May  9 16:07:25.144: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-gdbq" in namespace "subpath-6139" to be "Succeeded or Failed"
    May  9 16:07:25.148: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.162597ms
    May  9 16:07:27.154: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 2.010095565s
    May  9 16:07:29.155: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 4.010861535s
    May  9 16:07:31.159: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 6.015100915s
    May  9 16:07:33.156: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 8.011979703s
    May  9 16:07:35.156: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 10.011990772s
    May  9 16:07:37.154: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 12.010533102s
    May  9 16:07:39.157: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 14.01280249s
    May  9 16:07:41.156: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 16.011796517s
    May  9 16:07:43.156: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 18.012379266s
    May  9 16:07:45.239: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=true. Elapsed: 20.095514354s
    May  9 16:07:47.155: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Running", Reason="", readiness=false. Elapsed: 22.011166655s
    May  9 16:07:49.156: INFO: Pod "pod-subpath-test-configmap-gdbq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.01235956s
    STEP: Saw pod success 05/09/23 16:07:49.156
    May  9 16:07:49.157: INFO: Pod "pod-subpath-test-configmap-gdbq" satisfied condition "Succeeded or Failed"
    May  9 16:07:49.162: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-subpath-test-configmap-gdbq container test-container-subpath-configmap-gdbq: <nil>
    STEP: delete the pod 05/09/23 16:07:49.18
    May  9 16:07:49.197: INFO: Waiting for pod pod-subpath-test-configmap-gdbq to disappear
    May  9 16:07:49.202: INFO: Pod pod-subpath-test-configmap-gdbq no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-gdbq 05/09/23 16:07:49.202
    May  9 16:07:49.202: INFO: Deleting pod "pod-subpath-test-configmap-gdbq" in namespace "subpath-6139"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May  9 16:07:49.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6139" for this suite. 05/09/23 16:07:49.213
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:07:49.225
May  9 16:07:49.225: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename crd-webhook 05/09/23 16:07:49.226
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:07:49.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:07:49.254
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 05/09/23 16:07:49.258
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/09/23 16:07:50.015
STEP: Deploying the custom resource conversion webhook pod 05/09/23 16:07:50.026
STEP: Wait for the deployment to be ready 05/09/23 16:07:50.041
May  9 16:07:50.052: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 16:07:52.074
STEP: Verifying the service has paired with the endpoint 05/09/23 16:07:52.088
May  9 16:07:53.089: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
May  9 16:07:53.095: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Creating a v1 custom resource 05/09/23 16:07:56.157
STEP: v2 custom resource should be converted 05/09/23 16:07:56.164
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:07:56.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-8693" for this suite. 05/09/23 16:07:56.773
------------------------------
â€¢ [SLOW TEST] [7.557 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:07:49.225
    May  9 16:07:49.225: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename crd-webhook 05/09/23 16:07:49.226
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:07:49.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:07:49.254
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 05/09/23 16:07:49.258
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/09/23 16:07:50.015
    STEP: Deploying the custom resource conversion webhook pod 05/09/23 16:07:50.026
    STEP: Wait for the deployment to be ready 05/09/23 16:07:50.041
    May  9 16:07:50.052: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 16:07:52.074
    STEP: Verifying the service has paired with the endpoint 05/09/23 16:07:52.088
    May  9 16:07:53.089: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    May  9 16:07:53.095: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Creating a v1 custom resource 05/09/23 16:07:56.157
    STEP: v2 custom resource should be converted 05/09/23 16:07:56.164
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:07:56.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-8693" for this suite. 05/09/23 16:07:56.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:07:56.784
May  9 16:07:56.784: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename container-probe 05/09/23 16:07:56.785
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:07:56.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:07:56.818
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May  9 16:08:56.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5433" for this suite. 05/09/23 16:08:56.85
------------------------------
â€¢ [SLOW TEST] [60.075 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:07:56.784
    May  9 16:07:56.784: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename container-probe 05/09/23 16:07:56.785
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:07:56.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:07:56.818
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May  9 16:08:56.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5433" for this suite. 05/09/23 16:08:56.85
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:08:56.859
May  9 16:08:56.859: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename configmap 05/09/23 16:08:56.86
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:08:56.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:08:56.883
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-cc608a1f-cf4b-4af0-9221-ea401ef7a34d 05/09/23 16:08:56.892
STEP: Creating configMap with name cm-test-opt-upd-69173ae4-5d85-4cbb-819c-6d1a48480ada 05/09/23 16:08:56.899
STEP: Creating the pod 05/09/23 16:08:56.905
May  9 16:08:56.917: INFO: Waiting up to 5m0s for pod "pod-configmaps-bc6858ae-f650-47d7-a058-5e7f5c6122a6" in namespace "configmap-5722" to be "running and ready"
May  9 16:08:56.921: INFO: Pod "pod-configmaps-bc6858ae-f650-47d7-a058-5e7f5c6122a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009644ms
May  9 16:08:56.921: INFO: The phase of Pod pod-configmaps-bc6858ae-f650-47d7-a058-5e7f5c6122a6 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:08:58.929: INFO: Pod "pod-configmaps-bc6858ae-f650-47d7-a058-5e7f5c6122a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011835578s
May  9 16:08:58.929: INFO: The phase of Pod pod-configmaps-bc6858ae-f650-47d7-a058-5e7f5c6122a6 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:09:00.929: INFO: Pod "pod-configmaps-bc6858ae-f650-47d7-a058-5e7f5c6122a6": Phase="Running", Reason="", readiness=true. Elapsed: 4.012607028s
May  9 16:09:00.929: INFO: The phase of Pod pod-configmaps-bc6858ae-f650-47d7-a058-5e7f5c6122a6 is Running (Ready = true)
May  9 16:09:00.929: INFO: Pod "pod-configmaps-bc6858ae-f650-47d7-a058-5e7f5c6122a6" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-cc608a1f-cf4b-4af0-9221-ea401ef7a34d 05/09/23 16:09:00.974
STEP: Updating configmap cm-test-opt-upd-69173ae4-5d85-4cbb-819c-6d1a48480ada 05/09/23 16:09:00.985
STEP: Creating configMap with name cm-test-opt-create-2aa178e9-10e6-40c0-ad01-e9a96deb9080 05/09/23 16:09:00.991
STEP: waiting to observe update in volume 05/09/23 16:09:00.997
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  9 16:10:13.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5722" for this suite. 05/09/23 16:10:13.766
------------------------------
â€¢ [SLOW TEST] [76.917 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:08:56.859
    May  9 16:08:56.859: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename configmap 05/09/23 16:08:56.86
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:08:56.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:08:56.883
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-cc608a1f-cf4b-4af0-9221-ea401ef7a34d 05/09/23 16:08:56.892
    STEP: Creating configMap with name cm-test-opt-upd-69173ae4-5d85-4cbb-819c-6d1a48480ada 05/09/23 16:08:56.899
    STEP: Creating the pod 05/09/23 16:08:56.905
    May  9 16:08:56.917: INFO: Waiting up to 5m0s for pod "pod-configmaps-bc6858ae-f650-47d7-a058-5e7f5c6122a6" in namespace "configmap-5722" to be "running and ready"
    May  9 16:08:56.921: INFO: Pod "pod-configmaps-bc6858ae-f650-47d7-a058-5e7f5c6122a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009644ms
    May  9 16:08:56.921: INFO: The phase of Pod pod-configmaps-bc6858ae-f650-47d7-a058-5e7f5c6122a6 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:08:58.929: INFO: Pod "pod-configmaps-bc6858ae-f650-47d7-a058-5e7f5c6122a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011835578s
    May  9 16:08:58.929: INFO: The phase of Pod pod-configmaps-bc6858ae-f650-47d7-a058-5e7f5c6122a6 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:09:00.929: INFO: Pod "pod-configmaps-bc6858ae-f650-47d7-a058-5e7f5c6122a6": Phase="Running", Reason="", readiness=true. Elapsed: 4.012607028s
    May  9 16:09:00.929: INFO: The phase of Pod pod-configmaps-bc6858ae-f650-47d7-a058-5e7f5c6122a6 is Running (Ready = true)
    May  9 16:09:00.929: INFO: Pod "pod-configmaps-bc6858ae-f650-47d7-a058-5e7f5c6122a6" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-cc608a1f-cf4b-4af0-9221-ea401ef7a34d 05/09/23 16:09:00.974
    STEP: Updating configmap cm-test-opt-upd-69173ae4-5d85-4cbb-819c-6d1a48480ada 05/09/23 16:09:00.985
    STEP: Creating configMap with name cm-test-opt-create-2aa178e9-10e6-40c0-ad01-e9a96deb9080 05/09/23 16:09:00.991
    STEP: waiting to observe update in volume 05/09/23 16:09:00.997
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  9 16:10:13.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5722" for this suite. 05/09/23 16:10:13.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:10:13.776
May  9 16:10:13.776: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename configmap 05/09/23 16:10:13.777
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:10:13.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:10:13.8
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-8884a1d5-b76b-4770-9657-74fa714552d6 05/09/23 16:10:13.804
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  9 16:10:13.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2869" for this suite. 05/09/23 16:10:13.812
------------------------------
â€¢ [0.045 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:10:13.776
    May  9 16:10:13.776: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename configmap 05/09/23 16:10:13.777
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:10:13.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:10:13.8
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-8884a1d5-b76b-4770-9657-74fa714552d6 05/09/23 16:10:13.804
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  9 16:10:13.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2869" for this suite. 05/09/23 16:10:13.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:10:13.822
May  9 16:10:13.822: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename services 05/09/23 16:10:13.822
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:10:13.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:10:13.847
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-562 05/09/23 16:10:13.851
STEP: creating replication controller nodeport-test in namespace services-562 05/09/23 16:10:13.872
I0509 16:10:13.880546      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-562, replica count: 2
I0509 16:10:16.931878      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 16:10:16.931: INFO: Creating new exec pod
May  9 16:10:16.971: INFO: Waiting up to 5m0s for pod "execpodh2lrk" in namespace "services-562" to be "running"
May  9 16:10:16.984: INFO: Pod "execpodh2lrk": Phase="Pending", Reason="", readiness=false. Elapsed: 12.899938ms
May  9 16:10:18.990: INFO: Pod "execpodh2lrk": Phase="Running", Reason="", readiness=true. Elapsed: 2.018633297s
May  9 16:10:18.990: INFO: Pod "execpodh2lrk" satisfied condition "running"
May  9 16:10:19.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-562 exec execpodh2lrk -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
May  9 16:10:20.219: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May  9 16:10:20.219: INFO: stdout: ""
May  9 16:10:20.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-562 exec execpodh2lrk -- /bin/sh -x -c nc -v -z -w 2 10.3.238.132 80'
May  9 16:10:20.426: INFO: stderr: "+ nc -v -z -w 2 10.3.238.132 80\nConnection to 10.3.238.132 80 port [tcp/http] succeeded!\n"
May  9 16:10:20.426: INFO: stdout: ""
May  9 16:10:20.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-562 exec execpodh2lrk -- /bin/sh -x -c nc -v -z -w 2 51.68.93.170 30797'
May  9 16:10:20.611: INFO: stderr: "+ nc -v -z -w 2 51.68.93.170 30797\nConnection to 51.68.93.170 30797 port [tcp/*] succeeded!\n"
May  9 16:10:20.611: INFO: stdout: ""
May  9 16:10:20.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-562 exec execpodh2lrk -- /bin/sh -x -c nc -v -z -w 2 51.68.94.118 30797'
May  9 16:10:20.854: INFO: stderr: "+ nc -v -z -w 2 51.68.94.118 30797\nConnection to 51.68.94.118 30797 port [tcp/*] succeeded!\n"
May  9 16:10:20.855: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  9 16:10:20.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-562" for this suite. 05/09/23 16:10:20.88
------------------------------
â€¢ [SLOW TEST] [7.073 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:10:13.822
    May  9 16:10:13.822: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename services 05/09/23 16:10:13.822
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:10:13.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:10:13.847
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-562 05/09/23 16:10:13.851
    STEP: creating replication controller nodeport-test in namespace services-562 05/09/23 16:10:13.872
    I0509 16:10:13.880546      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-562, replica count: 2
    I0509 16:10:16.931878      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 16:10:16.931: INFO: Creating new exec pod
    May  9 16:10:16.971: INFO: Waiting up to 5m0s for pod "execpodh2lrk" in namespace "services-562" to be "running"
    May  9 16:10:16.984: INFO: Pod "execpodh2lrk": Phase="Pending", Reason="", readiness=false. Elapsed: 12.899938ms
    May  9 16:10:18.990: INFO: Pod "execpodh2lrk": Phase="Running", Reason="", readiness=true. Elapsed: 2.018633297s
    May  9 16:10:18.990: INFO: Pod "execpodh2lrk" satisfied condition "running"
    May  9 16:10:19.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-562 exec execpodh2lrk -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    May  9 16:10:20.219: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    May  9 16:10:20.219: INFO: stdout: ""
    May  9 16:10:20.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-562 exec execpodh2lrk -- /bin/sh -x -c nc -v -z -w 2 10.3.238.132 80'
    May  9 16:10:20.426: INFO: stderr: "+ nc -v -z -w 2 10.3.238.132 80\nConnection to 10.3.238.132 80 port [tcp/http] succeeded!\n"
    May  9 16:10:20.426: INFO: stdout: ""
    May  9 16:10:20.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-562 exec execpodh2lrk -- /bin/sh -x -c nc -v -z -w 2 51.68.93.170 30797'
    May  9 16:10:20.611: INFO: stderr: "+ nc -v -z -w 2 51.68.93.170 30797\nConnection to 51.68.93.170 30797 port [tcp/*] succeeded!\n"
    May  9 16:10:20.611: INFO: stdout: ""
    May  9 16:10:20.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-562 exec execpodh2lrk -- /bin/sh -x -c nc -v -z -w 2 51.68.94.118 30797'
    May  9 16:10:20.854: INFO: stderr: "+ nc -v -z -w 2 51.68.94.118 30797\nConnection to 51.68.94.118 30797 port [tcp/*] succeeded!\n"
    May  9 16:10:20.855: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  9 16:10:20.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-562" for this suite. 05/09/23 16:10:20.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:10:20.898
May  9 16:10:20.898: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 16:10:20.899
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:10:20.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:10:20.983
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 05/09/23 16:10:21.01
May  9 16:10:21.029: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c2530fa-e49e-4576-bdb3-a0f0abb7b8f4" in namespace "projected-4942" to be "Succeeded or Failed"
May  9 16:10:21.036: INFO: Pod "downwardapi-volume-2c2530fa-e49e-4576-bdb3-a0f0abb7b8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.025367ms
May  9 16:10:23.044: INFO: Pod "downwardapi-volume-2c2530fa-e49e-4576-bdb3-a0f0abb7b8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0146327s
May  9 16:10:25.043: INFO: Pod "downwardapi-volume-2c2530fa-e49e-4576-bdb3-a0f0abb7b8f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013691638s
STEP: Saw pod success 05/09/23 16:10:25.043
May  9 16:10:25.043: INFO: Pod "downwardapi-volume-2c2530fa-e49e-4576-bdb3-a0f0abb7b8f4" satisfied condition "Succeeded or Failed"
May  9 16:10:25.050: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-2c2530fa-e49e-4576-bdb3-a0f0abb7b8f4 container client-container: <nil>
STEP: delete the pod 05/09/23 16:10:25.06
May  9 16:10:25.076: INFO: Waiting for pod downwardapi-volume-2c2530fa-e49e-4576-bdb3-a0f0abb7b8f4 to disappear
May  9 16:10:25.081: INFO: Pod downwardapi-volume-2c2530fa-e49e-4576-bdb3-a0f0abb7b8f4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  9 16:10:25.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4942" for this suite. 05/09/23 16:10:25.087
------------------------------
â€¢ [4.198 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:10:20.898
    May  9 16:10:20.898: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 16:10:20.899
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:10:20.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:10:20.983
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 05/09/23 16:10:21.01
    May  9 16:10:21.029: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c2530fa-e49e-4576-bdb3-a0f0abb7b8f4" in namespace "projected-4942" to be "Succeeded or Failed"
    May  9 16:10:21.036: INFO: Pod "downwardapi-volume-2c2530fa-e49e-4576-bdb3-a0f0abb7b8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.025367ms
    May  9 16:10:23.044: INFO: Pod "downwardapi-volume-2c2530fa-e49e-4576-bdb3-a0f0abb7b8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0146327s
    May  9 16:10:25.043: INFO: Pod "downwardapi-volume-2c2530fa-e49e-4576-bdb3-a0f0abb7b8f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013691638s
    STEP: Saw pod success 05/09/23 16:10:25.043
    May  9 16:10:25.043: INFO: Pod "downwardapi-volume-2c2530fa-e49e-4576-bdb3-a0f0abb7b8f4" satisfied condition "Succeeded or Failed"
    May  9 16:10:25.050: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-2c2530fa-e49e-4576-bdb3-a0f0abb7b8f4 container client-container: <nil>
    STEP: delete the pod 05/09/23 16:10:25.06
    May  9 16:10:25.076: INFO: Waiting for pod downwardapi-volume-2c2530fa-e49e-4576-bdb3-a0f0abb7b8f4 to disappear
    May  9 16:10:25.081: INFO: Pod downwardapi-volume-2c2530fa-e49e-4576-bdb3-a0f0abb7b8f4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  9 16:10:25.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4942" for this suite. 05/09/23 16:10:25.087
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:10:25.096
May  9 16:10:25.096: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename proxy 05/09/23 16:10:25.097
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:10:25.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:10:25.116
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
May  9 16:10:25.120: INFO: Creating pod...
May  9 16:10:25.133: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4806" to be "running"
May  9 16:10:25.149: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 16.041352ms
May  9 16:10:27.154: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.021114417s
May  9 16:10:27.154: INFO: Pod "agnhost" satisfied condition "running"
May  9 16:10:27.154: INFO: Creating service...
May  9 16:10:27.168: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/pods/agnhost/proxy/some/path/with/DELETE
May  9 16:10:27.180: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  9 16:10:27.180: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/pods/agnhost/proxy/some/path/with/GET
May  9 16:10:27.190: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May  9 16:10:27.190: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/pods/agnhost/proxy/some/path/with/HEAD
May  9 16:10:27.199: INFO: http.Client request:HEAD | StatusCode:200
May  9 16:10:27.199: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/pods/agnhost/proxy/some/path/with/OPTIONS
May  9 16:10:27.205: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  9 16:10:27.205: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/pods/agnhost/proxy/some/path/with/PATCH
May  9 16:10:27.214: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  9 16:10:27.214: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/pods/agnhost/proxy/some/path/with/POST
May  9 16:10:27.220: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  9 16:10:27.220: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/pods/agnhost/proxy/some/path/with/PUT
May  9 16:10:27.229: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May  9 16:10:27.229: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/services/test-service/proxy/some/path/with/DELETE
May  9 16:10:27.239: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  9 16:10:27.239: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/services/test-service/proxy/some/path/with/GET
May  9 16:10:27.249: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May  9 16:10:27.249: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/services/test-service/proxy/some/path/with/HEAD
May  9 16:10:27.260: INFO: http.Client request:HEAD | StatusCode:200
May  9 16:10:27.260: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/services/test-service/proxy/some/path/with/OPTIONS
May  9 16:10:27.269: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  9 16:10:27.269: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/services/test-service/proxy/some/path/with/PATCH
May  9 16:10:27.280: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  9 16:10:27.280: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/services/test-service/proxy/some/path/with/POST
May  9 16:10:27.288: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  9 16:10:27.288: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/services/test-service/proxy/some/path/with/PUT
May  9 16:10:27.330: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
May  9 16:10:27.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-4806" for this suite. 05/09/23 16:10:27.342
------------------------------
â€¢ [2.265 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:10:25.096
    May  9 16:10:25.096: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename proxy 05/09/23 16:10:25.097
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:10:25.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:10:25.116
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    May  9 16:10:25.120: INFO: Creating pod...
    May  9 16:10:25.133: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4806" to be "running"
    May  9 16:10:25.149: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 16.041352ms
    May  9 16:10:27.154: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.021114417s
    May  9 16:10:27.154: INFO: Pod "agnhost" satisfied condition "running"
    May  9 16:10:27.154: INFO: Creating service...
    May  9 16:10:27.168: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/pods/agnhost/proxy/some/path/with/DELETE
    May  9 16:10:27.180: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  9 16:10:27.180: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/pods/agnhost/proxy/some/path/with/GET
    May  9 16:10:27.190: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    May  9 16:10:27.190: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/pods/agnhost/proxy/some/path/with/HEAD
    May  9 16:10:27.199: INFO: http.Client request:HEAD | StatusCode:200
    May  9 16:10:27.199: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/pods/agnhost/proxy/some/path/with/OPTIONS
    May  9 16:10:27.205: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  9 16:10:27.205: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/pods/agnhost/proxy/some/path/with/PATCH
    May  9 16:10:27.214: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  9 16:10:27.214: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/pods/agnhost/proxy/some/path/with/POST
    May  9 16:10:27.220: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  9 16:10:27.220: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/pods/agnhost/proxy/some/path/with/PUT
    May  9 16:10:27.229: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May  9 16:10:27.229: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/services/test-service/proxy/some/path/with/DELETE
    May  9 16:10:27.239: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  9 16:10:27.239: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/services/test-service/proxy/some/path/with/GET
    May  9 16:10:27.249: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    May  9 16:10:27.249: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/services/test-service/proxy/some/path/with/HEAD
    May  9 16:10:27.260: INFO: http.Client request:HEAD | StatusCode:200
    May  9 16:10:27.260: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/services/test-service/proxy/some/path/with/OPTIONS
    May  9 16:10:27.269: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  9 16:10:27.269: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/services/test-service/proxy/some/path/with/PATCH
    May  9 16:10:27.280: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  9 16:10:27.280: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/services/test-service/proxy/some/path/with/POST
    May  9 16:10:27.288: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  9 16:10:27.288: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-4806/services/test-service/proxy/some/path/with/PUT
    May  9 16:10:27.330: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    May  9 16:10:27.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-4806" for this suite. 05/09/23 16:10:27.342
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:10:27.361
May  9 16:10:27.361: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename webhook 05/09/23 16:10:27.362
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:10:27.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:10:27.381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/09/23 16:10:27.397
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:10:27.704
STEP: Deploying the webhook pod 05/09/23 16:10:27.712
STEP: Wait for the deployment to be ready 05/09/23 16:10:27.724
May  9 16:10:27.734: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 16:10:29.783
STEP: Verifying the service has paired with the endpoint 05/09/23 16:10:29.797
May  9 16:10:30.797: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 05/09/23 16:10:30.802
STEP: create a configmap that should be updated by the webhook 05/09/23 16:10:30.824
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:10:30.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3413" for this suite. 05/09/23 16:10:30.91
STEP: Destroying namespace "webhook-3413-markers" for this suite. 05/09/23 16:10:30.923
------------------------------
â€¢ [3.570 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:10:27.361
    May  9 16:10:27.361: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename webhook 05/09/23 16:10:27.362
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:10:27.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:10:27.381
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/09/23 16:10:27.397
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:10:27.704
    STEP: Deploying the webhook pod 05/09/23 16:10:27.712
    STEP: Wait for the deployment to be ready 05/09/23 16:10:27.724
    May  9 16:10:27.734: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 16:10:29.783
    STEP: Verifying the service has paired with the endpoint 05/09/23 16:10:29.797
    May  9 16:10:30.797: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 05/09/23 16:10:30.802
    STEP: create a configmap that should be updated by the webhook 05/09/23 16:10:30.824
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:10:30.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3413" for this suite. 05/09/23 16:10:30.91
    STEP: Destroying namespace "webhook-3413-markers" for this suite. 05/09/23 16:10:30.923
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:10:30.931
May  9 16:10:30.932: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename subpath 05/09/23 16:10:30.932
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:10:30.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:10:30.95
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/09/23 16:10:30.954
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-67lm 05/09/23 16:10:30.965
STEP: Creating a pod to test atomic-volume-subpath 05/09/23 16:10:30.965
May  9 16:10:30.974: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-67lm" in namespace "subpath-2414" to be "Succeeded or Failed"
May  9 16:10:30.979: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Pending", Reason="", readiness=false. Elapsed: 5.053962ms
May  9 16:10:32.986: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 2.011628882s
May  9 16:10:34.985: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 4.011023844s
May  9 16:10:36.995: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 6.020886661s
May  9 16:10:38.986: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 8.012221878s
May  9 16:10:40.987: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 10.013323399s
May  9 16:10:42.984: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 12.010406708s
May  9 16:10:44.987: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 14.012581956s
May  9 16:10:46.986: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 16.011651663s
May  9 16:10:48.985: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 18.011248133s
May  9 16:10:50.987: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 20.013383974s
May  9 16:10:52.986: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 22.012289049s
May  9 16:10:54.985: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=false. Elapsed: 24.011362752s
May  9 16:10:56.987: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.013185554s
STEP: Saw pod success 05/09/23 16:10:56.987
May  9 16:10:56.988: INFO: Pod "pod-subpath-test-downwardapi-67lm" satisfied condition "Succeeded or Failed"
May  9 16:10:56.992: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-subpath-test-downwardapi-67lm container test-container-subpath-downwardapi-67lm: <nil>
STEP: delete the pod 05/09/23 16:10:57.011
May  9 16:10:57.025: INFO: Waiting for pod pod-subpath-test-downwardapi-67lm to disappear
May  9 16:10:57.031: INFO: Pod pod-subpath-test-downwardapi-67lm no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-67lm 05/09/23 16:10:57.031
May  9 16:10:57.031: INFO: Deleting pod "pod-subpath-test-downwardapi-67lm" in namespace "subpath-2414"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May  9 16:10:57.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2414" for this suite. 05/09/23 16:10:57.043
------------------------------
â€¢ [SLOW TEST] [26.125 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:10:30.931
    May  9 16:10:30.932: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename subpath 05/09/23 16:10:30.932
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:10:30.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:10:30.95
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/09/23 16:10:30.954
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-67lm 05/09/23 16:10:30.965
    STEP: Creating a pod to test atomic-volume-subpath 05/09/23 16:10:30.965
    May  9 16:10:30.974: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-67lm" in namespace "subpath-2414" to be "Succeeded or Failed"
    May  9 16:10:30.979: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Pending", Reason="", readiness=false. Elapsed: 5.053962ms
    May  9 16:10:32.986: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 2.011628882s
    May  9 16:10:34.985: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 4.011023844s
    May  9 16:10:36.995: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 6.020886661s
    May  9 16:10:38.986: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 8.012221878s
    May  9 16:10:40.987: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 10.013323399s
    May  9 16:10:42.984: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 12.010406708s
    May  9 16:10:44.987: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 14.012581956s
    May  9 16:10:46.986: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 16.011651663s
    May  9 16:10:48.985: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 18.011248133s
    May  9 16:10:50.987: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 20.013383974s
    May  9 16:10:52.986: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=true. Elapsed: 22.012289049s
    May  9 16:10:54.985: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Running", Reason="", readiness=false. Elapsed: 24.011362752s
    May  9 16:10:56.987: INFO: Pod "pod-subpath-test-downwardapi-67lm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.013185554s
    STEP: Saw pod success 05/09/23 16:10:56.987
    May  9 16:10:56.988: INFO: Pod "pod-subpath-test-downwardapi-67lm" satisfied condition "Succeeded or Failed"
    May  9 16:10:56.992: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-subpath-test-downwardapi-67lm container test-container-subpath-downwardapi-67lm: <nil>
    STEP: delete the pod 05/09/23 16:10:57.011
    May  9 16:10:57.025: INFO: Waiting for pod pod-subpath-test-downwardapi-67lm to disappear
    May  9 16:10:57.031: INFO: Pod pod-subpath-test-downwardapi-67lm no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-67lm 05/09/23 16:10:57.031
    May  9 16:10:57.031: INFO: Deleting pod "pod-subpath-test-downwardapi-67lm" in namespace "subpath-2414"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May  9 16:10:57.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2414" for this suite. 05/09/23 16:10:57.043
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:10:57.057
May  9 16:10:57.057: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename csistoragecapacity 05/09/23 16:10:57.058
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:10:57.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:10:57.087
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 05/09/23 16:10:57.092
STEP: getting /apis/storage.k8s.io 05/09/23 16:10:57.096
STEP: getting /apis/storage.k8s.io/v1 05/09/23 16:10:57.097
STEP: creating 05/09/23 16:10:57.099
STEP: watching 05/09/23 16:10:57.12
May  9 16:10:57.120: INFO: starting watch
STEP: getting 05/09/23 16:10:57.129
STEP: listing in namespace 05/09/23 16:10:57.133
STEP: listing across namespaces 05/09/23 16:10:57.14
STEP: patching 05/09/23 16:10:57.145
STEP: updating 05/09/23 16:10:57.151
May  9 16:10:57.157: INFO: waiting for watch events with expected annotations in namespace
May  9 16:10:57.157: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 05/09/23 16:10:57.158
STEP: deleting a collection 05/09/23 16:10:57.179
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
May  9 16:10:57.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-517" for this suite. 05/09/23 16:10:57.205
------------------------------
â€¢ [0.157 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:10:57.057
    May  9 16:10:57.057: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename csistoragecapacity 05/09/23 16:10:57.058
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:10:57.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:10:57.087
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 05/09/23 16:10:57.092
    STEP: getting /apis/storage.k8s.io 05/09/23 16:10:57.096
    STEP: getting /apis/storage.k8s.io/v1 05/09/23 16:10:57.097
    STEP: creating 05/09/23 16:10:57.099
    STEP: watching 05/09/23 16:10:57.12
    May  9 16:10:57.120: INFO: starting watch
    STEP: getting 05/09/23 16:10:57.129
    STEP: listing in namespace 05/09/23 16:10:57.133
    STEP: listing across namespaces 05/09/23 16:10:57.14
    STEP: patching 05/09/23 16:10:57.145
    STEP: updating 05/09/23 16:10:57.151
    May  9 16:10:57.157: INFO: waiting for watch events with expected annotations in namespace
    May  9 16:10:57.157: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 05/09/23 16:10:57.158
    STEP: deleting a collection 05/09/23 16:10:57.179
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    May  9 16:10:57.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-517" for this suite. 05/09/23 16:10:57.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:10:57.216
May  9 16:10:57.216: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename webhook 05/09/23 16:10:57.217
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:10:57.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:10:57.236
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/09/23 16:10:57.257
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:10:57.735
STEP: Deploying the webhook pod 05/09/23 16:10:57.746
STEP: Wait for the deployment to be ready 05/09/23 16:10:57.76
May  9 16:10:57.770: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 16:10:59.787
STEP: Verifying the service has paired with the endpoint 05/09/23 16:10:59.802
May  9 16:11:00.802: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 05/09/23 16:11:00.809
STEP: create a namespace for the webhook 05/09/23 16:11:00.85
STEP: create a configmap should be unconditionally rejected by the webhook 05/09/23 16:11:00.863
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:11:00.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3295" for this suite. 05/09/23 16:11:01.017
STEP: Destroying namespace "webhook-3295-markers" for this suite. 05/09/23 16:11:01.046
------------------------------
â€¢ [3.841 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:10:57.216
    May  9 16:10:57.216: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename webhook 05/09/23 16:10:57.217
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:10:57.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:10:57.236
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/09/23 16:10:57.257
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:10:57.735
    STEP: Deploying the webhook pod 05/09/23 16:10:57.746
    STEP: Wait for the deployment to be ready 05/09/23 16:10:57.76
    May  9 16:10:57.770: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 16:10:59.787
    STEP: Verifying the service has paired with the endpoint 05/09/23 16:10:59.802
    May  9 16:11:00.802: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 05/09/23 16:11:00.809
    STEP: create a namespace for the webhook 05/09/23 16:11:00.85
    STEP: create a configmap should be unconditionally rejected by the webhook 05/09/23 16:11:00.863
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:11:00.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3295" for this suite. 05/09/23 16:11:01.017
    STEP: Destroying namespace "webhook-3295-markers" for this suite. 05/09/23 16:11:01.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:11:01.058
May  9 16:11:01.058: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename hostport 05/09/23 16:11:01.059
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:11:01.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:11:01.084
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 05/09/23 16:11:01.094
May  9 16:11:01.106: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-7850" to be "running and ready"
May  9 16:11:01.114: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.128723ms
May  9 16:11:01.114: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:11:03.122: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015353632s
May  9 16:11:03.122: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:11:05.119: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.01226566s
May  9 16:11:05.119: INFO: The phase of Pod pod1 is Running (Ready = true)
May  9 16:11:05.119: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 51.68.91.222 on the node which pod1 resides and expect scheduled 05/09/23 16:11:05.119
May  9 16:11:05.130: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-7850" to be "running and ready"
May  9 16:11:05.137: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.123042ms
May  9 16:11:05.137: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:11:07.142: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.012363556s
May  9 16:11:07.143: INFO: The phase of Pod pod2 is Running (Ready = true)
May  9 16:11:07.143: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 51.68.91.222 but use UDP protocol on the node which pod2 resides 05/09/23 16:11:07.143
May  9 16:11:07.150: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-7850" to be "running and ready"
May  9 16:11:07.155: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.192042ms
May  9 16:11:07.155: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:11:09.164: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.013021827s
May  9 16:11:09.164: INFO: The phase of Pod pod3 is Running (Ready = true)
May  9 16:11:09.164: INFO: Pod "pod3" satisfied condition "running and ready"
May  9 16:11:09.170: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-7850" to be "running and ready"
May  9 16:11:09.179: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 9.069693ms
May  9 16:11:09.179: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
May  9 16:11:11.198: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.027929376s
May  9 16:11:11.198: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
May  9 16:11:11.198: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 05/09/23 16:11:11.204
May  9 16:11:11.204: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 51.68.91.222 http://127.0.0.1:54323/hostname] Namespace:hostport-7850 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:11:11.204: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:11:11.205: INFO: ExecWithOptions: Clientset creation
May  9 16:11:11.205: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/hostport-7850/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+51.68.91.222+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 51.68.91.222, port: 54323 05/09/23 16:11:11.344
May  9 16:11:11.344: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://51.68.91.222:54323/hostname] Namespace:hostport-7850 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:11:11.344: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:11:11.345: INFO: ExecWithOptions: Clientset creation
May  9 16:11:11.345: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/hostport-7850/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F51.68.91.222%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 51.68.91.222, port: 54323 UDP 05/09/23 16:11:11.486
May  9 16:11:11.486: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 51.68.91.222 54323] Namespace:hostport-7850 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:11:11.486: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:11:11.487: INFO: ExecWithOptions: Clientset creation
May  9 16:11:11.487: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/hostport-7850/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+51.68.91.222+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
May  9 16:11:16.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-7850" for this suite. 05/09/23 16:11:16.637
------------------------------
â€¢ [SLOW TEST] [15.587 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:11:01.058
    May  9 16:11:01.058: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename hostport 05/09/23 16:11:01.059
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:11:01.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:11:01.084
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 05/09/23 16:11:01.094
    May  9 16:11:01.106: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-7850" to be "running and ready"
    May  9 16:11:01.114: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.128723ms
    May  9 16:11:01.114: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:11:03.122: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015353632s
    May  9 16:11:03.122: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:11:05.119: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.01226566s
    May  9 16:11:05.119: INFO: The phase of Pod pod1 is Running (Ready = true)
    May  9 16:11:05.119: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 51.68.91.222 on the node which pod1 resides and expect scheduled 05/09/23 16:11:05.119
    May  9 16:11:05.130: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-7850" to be "running and ready"
    May  9 16:11:05.137: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.123042ms
    May  9 16:11:05.137: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:11:07.142: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.012363556s
    May  9 16:11:07.143: INFO: The phase of Pod pod2 is Running (Ready = true)
    May  9 16:11:07.143: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 51.68.91.222 but use UDP protocol on the node which pod2 resides 05/09/23 16:11:07.143
    May  9 16:11:07.150: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-7850" to be "running and ready"
    May  9 16:11:07.155: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.192042ms
    May  9 16:11:07.155: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:11:09.164: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.013021827s
    May  9 16:11:09.164: INFO: The phase of Pod pod3 is Running (Ready = true)
    May  9 16:11:09.164: INFO: Pod "pod3" satisfied condition "running and ready"
    May  9 16:11:09.170: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-7850" to be "running and ready"
    May  9 16:11:09.179: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 9.069693ms
    May  9 16:11:09.179: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:11:11.198: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.027929376s
    May  9 16:11:11.198: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    May  9 16:11:11.198: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 05/09/23 16:11:11.204
    May  9 16:11:11.204: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 51.68.91.222 http://127.0.0.1:54323/hostname] Namespace:hostport-7850 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:11:11.204: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:11:11.205: INFO: ExecWithOptions: Clientset creation
    May  9 16:11:11.205: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/hostport-7850/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+51.68.91.222+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 51.68.91.222, port: 54323 05/09/23 16:11:11.344
    May  9 16:11:11.344: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://51.68.91.222:54323/hostname] Namespace:hostport-7850 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:11:11.344: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:11:11.345: INFO: ExecWithOptions: Clientset creation
    May  9 16:11:11.345: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/hostport-7850/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F51.68.91.222%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 51.68.91.222, port: 54323 UDP 05/09/23 16:11:11.486
    May  9 16:11:11.486: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 51.68.91.222 54323] Namespace:hostport-7850 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:11:11.486: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:11:11.487: INFO: ExecWithOptions: Clientset creation
    May  9 16:11:11.487: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/hostport-7850/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+51.68.91.222+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    May  9 16:11:16.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-7850" for this suite. 05/09/23 16:11:16.637
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:11:16.645
May  9 16:11:16.645: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename webhook 05/09/23 16:11:16.646
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:11:16.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:11:16.669
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/09/23 16:11:16.69
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:11:17.266
STEP: Deploying the webhook pod 05/09/23 16:11:17.274
STEP: Wait for the deployment to be ready 05/09/23 16:11:17.288
May  9 16:11:17.298: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 16:11:19.316
STEP: Verifying the service has paired with the endpoint 05/09/23 16:11:19.329
May  9 16:11:20.330: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 05/09/23 16:11:20.335
STEP: Updating a mutating webhook configuration's rules to not include the create operation 05/09/23 16:11:20.362
STEP: Creating a configMap that should not be mutated 05/09/23 16:11:20.37
STEP: Patching a mutating webhook configuration's rules to include the create operation 05/09/23 16:11:20.386
STEP: Creating a configMap that should be mutated 05/09/23 16:11:20.395
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:11:20.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9933" for this suite. 05/09/23 16:11:20.484
STEP: Destroying namespace "webhook-9933-markers" for this suite. 05/09/23 16:11:20.492
------------------------------
â€¢ [3.857 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:11:16.645
    May  9 16:11:16.645: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename webhook 05/09/23 16:11:16.646
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:11:16.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:11:16.669
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/09/23 16:11:16.69
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:11:17.266
    STEP: Deploying the webhook pod 05/09/23 16:11:17.274
    STEP: Wait for the deployment to be ready 05/09/23 16:11:17.288
    May  9 16:11:17.298: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 16:11:19.316
    STEP: Verifying the service has paired with the endpoint 05/09/23 16:11:19.329
    May  9 16:11:20.330: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 05/09/23 16:11:20.335
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 05/09/23 16:11:20.362
    STEP: Creating a configMap that should not be mutated 05/09/23 16:11:20.37
    STEP: Patching a mutating webhook configuration's rules to include the create operation 05/09/23 16:11:20.386
    STEP: Creating a configMap that should be mutated 05/09/23 16:11:20.395
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:11:20.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9933" for this suite. 05/09/23 16:11:20.484
    STEP: Destroying namespace "webhook-9933-markers" for this suite. 05/09/23 16:11:20.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:11:20.503
May  9 16:11:20.503: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename aggregator 05/09/23 16:11:20.504
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:11:20.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:11:20.527
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
May  9 16:11:20.531: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 05/09/23 16:11:20.531
May  9 16:11:20.999: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May  9 16:11:23.051: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:11:25.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:11:27.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:11:29.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:11:31.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:11:33.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:11:35.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:11:37.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:11:39.061: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:11:41.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:11:43.057: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:11:45.210: INFO: Waited 139.877845ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 05/09/23 16:11:45.304
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 05/09/23 16:11:45.308
STEP: List APIServices 05/09/23 16:11:45.318
May  9 16:11:45.326: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
May  9 16:11:45.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-2525" for this suite. 05/09/23 16:11:45.582
------------------------------
â€¢ [SLOW TEST] [25.144 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:11:20.503
    May  9 16:11:20.503: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename aggregator 05/09/23 16:11:20.504
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:11:20.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:11:20.527
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    May  9 16:11:20.531: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 05/09/23 16:11:20.531
    May  9 16:11:20.999: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    May  9 16:11:23.051: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:11:25.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:11:27.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:11:29.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:11:31.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:11:33.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:11:35.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:11:37.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:11:39.061: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:11:41.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:11:43.057: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 11, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 11, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:11:45.210: INFO: Waited 139.877845ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 05/09/23 16:11:45.304
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 05/09/23 16:11:45.308
    STEP: List APIServices 05/09/23 16:11:45.318
    May  9 16:11:45.326: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    May  9 16:11:45.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-2525" for this suite. 05/09/23 16:11:45.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:11:45.65
May  9 16:11:45.650: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename cronjob 05/09/23 16:11:45.651
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:11:45.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:11:45.672
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 05/09/23 16:11:45.676
STEP: Ensuring a job is scheduled 05/09/23 16:11:45.682
STEP: Ensuring exactly one is scheduled 05/09/23 16:12:01.762
STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/09/23 16:12:01.768
STEP: Ensuring no more jobs are scheduled 05/09/23 16:12:01.774
STEP: Removing cronjob 05/09/23 16:17:01.785
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May  9 16:17:01.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9386" for this suite. 05/09/23 16:17:01.802
------------------------------
â€¢ [SLOW TEST] [316.161 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:11:45.65
    May  9 16:11:45.650: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename cronjob 05/09/23 16:11:45.651
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:11:45.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:11:45.672
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 05/09/23 16:11:45.676
    STEP: Ensuring a job is scheduled 05/09/23 16:11:45.682
    STEP: Ensuring exactly one is scheduled 05/09/23 16:12:01.762
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/09/23 16:12:01.768
    STEP: Ensuring no more jobs are scheduled 05/09/23 16:12:01.774
    STEP: Removing cronjob 05/09/23 16:17:01.785
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May  9 16:17:01.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9386" for this suite. 05/09/23 16:17:01.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:17:01.813
May  9 16:17:01.813: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename container-probe 05/09/23 16:17:01.814
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:17:01.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:17:01.885
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-205cc93b-52d0-4b1a-aba4-0c7c56a1892e in namespace container-probe-5920 05/09/23 16:17:01.891
May  9 16:17:01.902: INFO: Waiting up to 5m0s for pod "busybox-205cc93b-52d0-4b1a-aba4-0c7c56a1892e" in namespace "container-probe-5920" to be "not pending"
May  9 16:17:01.906: INFO: Pod "busybox-205cc93b-52d0-4b1a-aba4-0c7c56a1892e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.829137ms
May  9 16:17:03.912: INFO: Pod "busybox-205cc93b-52d0-4b1a-aba4-0c7c56a1892e": Phase="Running", Reason="", readiness=true. Elapsed: 2.009139135s
May  9 16:17:03.912: INFO: Pod "busybox-205cc93b-52d0-4b1a-aba4-0c7c56a1892e" satisfied condition "not pending"
May  9 16:17:03.912: INFO: Started pod busybox-205cc93b-52d0-4b1a-aba4-0c7c56a1892e in namespace container-probe-5920
STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 16:17:03.912
May  9 16:17:03.919: INFO: Initial restart count of pod busybox-205cc93b-52d0-4b1a-aba4-0c7c56a1892e is 0
May  9 16:17:54.126: INFO: Restart count of pod container-probe-5920/busybox-205cc93b-52d0-4b1a-aba4-0c7c56a1892e is now 1 (50.207254033s elapsed)
STEP: deleting the pod 05/09/23 16:17:54.126
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May  9 16:17:54.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5920" for this suite. 05/09/23 16:17:54.151
------------------------------
â€¢ [SLOW TEST] [52.346 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:17:01.813
    May  9 16:17:01.813: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename container-probe 05/09/23 16:17:01.814
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:17:01.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:17:01.885
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-205cc93b-52d0-4b1a-aba4-0c7c56a1892e in namespace container-probe-5920 05/09/23 16:17:01.891
    May  9 16:17:01.902: INFO: Waiting up to 5m0s for pod "busybox-205cc93b-52d0-4b1a-aba4-0c7c56a1892e" in namespace "container-probe-5920" to be "not pending"
    May  9 16:17:01.906: INFO: Pod "busybox-205cc93b-52d0-4b1a-aba4-0c7c56a1892e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.829137ms
    May  9 16:17:03.912: INFO: Pod "busybox-205cc93b-52d0-4b1a-aba4-0c7c56a1892e": Phase="Running", Reason="", readiness=true. Elapsed: 2.009139135s
    May  9 16:17:03.912: INFO: Pod "busybox-205cc93b-52d0-4b1a-aba4-0c7c56a1892e" satisfied condition "not pending"
    May  9 16:17:03.912: INFO: Started pod busybox-205cc93b-52d0-4b1a-aba4-0c7c56a1892e in namespace container-probe-5920
    STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 16:17:03.912
    May  9 16:17:03.919: INFO: Initial restart count of pod busybox-205cc93b-52d0-4b1a-aba4-0c7c56a1892e is 0
    May  9 16:17:54.126: INFO: Restart count of pod container-probe-5920/busybox-205cc93b-52d0-4b1a-aba4-0c7c56a1892e is now 1 (50.207254033s elapsed)
    STEP: deleting the pod 05/09/23 16:17:54.126
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May  9 16:17:54.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5920" for this suite. 05/09/23 16:17:54.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:17:54.159
May  9 16:17:54.159: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename webhook 05/09/23 16:17:54.16
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:17:54.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:17:54.182
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/09/23 16:17:54.207
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:17:55.421
STEP: Deploying the webhook pod 05/09/23 16:17:55.433
STEP: Wait for the deployment to be ready 05/09/23 16:17:55.447
May  9 16:17:55.456: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/09/23 16:17:57.471
STEP: Verifying the service has paired with the endpoint 05/09/23 16:17:57.485
May  9 16:17:58.486: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/09/23 16:17:58.49
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/09/23 16:17:58.516
STEP: Creating a dummy validating-webhook-configuration object 05/09/23 16:17:58.536
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 05/09/23 16:17:58.551
STEP: Creating a dummy mutating-webhook-configuration object 05/09/23 16:17:58.559
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 05/09/23 16:17:58.57
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:17:58.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-119" for this suite. 05/09/23 16:17:58.646
STEP: Destroying namespace "webhook-119-markers" for this suite. 05/09/23 16:17:58.655
------------------------------
â€¢ [4.504 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:17:54.159
    May  9 16:17:54.159: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename webhook 05/09/23 16:17:54.16
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:17:54.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:17:54.182
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/09/23 16:17:54.207
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:17:55.421
    STEP: Deploying the webhook pod 05/09/23 16:17:55.433
    STEP: Wait for the deployment to be ready 05/09/23 16:17:55.447
    May  9 16:17:55.456: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/09/23 16:17:57.471
    STEP: Verifying the service has paired with the endpoint 05/09/23 16:17:57.485
    May  9 16:17:58.486: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/09/23 16:17:58.49
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/09/23 16:17:58.516
    STEP: Creating a dummy validating-webhook-configuration object 05/09/23 16:17:58.536
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 05/09/23 16:17:58.551
    STEP: Creating a dummy mutating-webhook-configuration object 05/09/23 16:17:58.559
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 05/09/23 16:17:58.57
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:17:58.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-119" for this suite. 05/09/23 16:17:58.646
    STEP: Destroying namespace "webhook-119-markers" for this suite. 05/09/23 16:17:58.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:17:58.664
May  9 16:17:58.665: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename downward-api 05/09/23 16:17:58.665
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:17:58.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:17:58.687
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 05/09/23 16:17:58.692
May  9 16:17:58.702: INFO: Waiting up to 5m0s for pod "downward-api-43cf681d-5c55-42ce-b426-124b3a7d3d8e" in namespace "downward-api-3628" to be "Succeeded or Failed"
May  9 16:17:58.712: INFO: Pod "downward-api-43cf681d-5c55-42ce-b426-124b3a7d3d8e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.65461ms
May  9 16:18:00.720: INFO: Pod "downward-api-43cf681d-5c55-42ce-b426-124b3a7d3d8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017605244s
May  9 16:18:02.719: INFO: Pod "downward-api-43cf681d-5c55-42ce-b426-124b3a7d3d8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016992255s
STEP: Saw pod success 05/09/23 16:18:02.719
May  9 16:18:02.719: INFO: Pod "downward-api-43cf681d-5c55-42ce-b426-124b3a7d3d8e" satisfied condition "Succeeded or Failed"
May  9 16:18:02.725: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downward-api-43cf681d-5c55-42ce-b426-124b3a7d3d8e container dapi-container: <nil>
STEP: delete the pod 05/09/23 16:18:02.79
May  9 16:18:02.806: INFO: Waiting for pod downward-api-43cf681d-5c55-42ce-b426-124b3a7d3d8e to disappear
May  9 16:18:02.810: INFO: Pod downward-api-43cf681d-5c55-42ce-b426-124b3a7d3d8e no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May  9 16:18:02.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3628" for this suite. 05/09/23 16:18:02.817
------------------------------
â€¢ [4.162 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:17:58.664
    May  9 16:17:58.665: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename downward-api 05/09/23 16:17:58.665
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:17:58.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:17:58.687
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 05/09/23 16:17:58.692
    May  9 16:17:58.702: INFO: Waiting up to 5m0s for pod "downward-api-43cf681d-5c55-42ce-b426-124b3a7d3d8e" in namespace "downward-api-3628" to be "Succeeded or Failed"
    May  9 16:17:58.712: INFO: Pod "downward-api-43cf681d-5c55-42ce-b426-124b3a7d3d8e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.65461ms
    May  9 16:18:00.720: INFO: Pod "downward-api-43cf681d-5c55-42ce-b426-124b3a7d3d8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017605244s
    May  9 16:18:02.719: INFO: Pod "downward-api-43cf681d-5c55-42ce-b426-124b3a7d3d8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016992255s
    STEP: Saw pod success 05/09/23 16:18:02.719
    May  9 16:18:02.719: INFO: Pod "downward-api-43cf681d-5c55-42ce-b426-124b3a7d3d8e" satisfied condition "Succeeded or Failed"
    May  9 16:18:02.725: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downward-api-43cf681d-5c55-42ce-b426-124b3a7d3d8e container dapi-container: <nil>
    STEP: delete the pod 05/09/23 16:18:02.79
    May  9 16:18:02.806: INFO: Waiting for pod downward-api-43cf681d-5c55-42ce-b426-124b3a7d3d8e to disappear
    May  9 16:18:02.810: INFO: Pod downward-api-43cf681d-5c55-42ce-b426-124b3a7d3d8e no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May  9 16:18:02.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3628" for this suite. 05/09/23 16:18:02.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:18:02.828
May  9 16:18:02.828: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 16:18:02.829
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:18:02.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:18:02.853
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-6719db36-cc92-40f8-881c-52feb6d26046 05/09/23 16:18:02.861
STEP: Creating a pod to test consume configMaps 05/09/23 16:18:02.871
May  9 16:18:02.885: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2aa5b169-fdbc-48ed-b27b-f015bb82b13c" in namespace "projected-984" to be "Succeeded or Failed"
May  9 16:18:02.892: INFO: Pod "pod-projected-configmaps-2aa5b169-fdbc-48ed-b27b-f015bb82b13c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.148072ms
May  9 16:18:04.899: INFO: Pod "pod-projected-configmaps-2aa5b169-fdbc-48ed-b27b-f015bb82b13c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013997138s
May  9 16:18:06.899: INFO: Pod "pod-projected-configmaps-2aa5b169-fdbc-48ed-b27b-f015bb82b13c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014150255s
STEP: Saw pod success 05/09/23 16:18:06.899
May  9 16:18:06.899: INFO: Pod "pod-projected-configmaps-2aa5b169-fdbc-48ed-b27b-f015bb82b13c" satisfied condition "Succeeded or Failed"
May  9 16:18:06.905: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-configmaps-2aa5b169-fdbc-48ed-b27b-f015bb82b13c container agnhost-container: <nil>
STEP: delete the pod 05/09/23 16:18:06.916
May  9 16:18:06.930: INFO: Waiting for pod pod-projected-configmaps-2aa5b169-fdbc-48ed-b27b-f015bb82b13c to disappear
May  9 16:18:06.934: INFO: Pod pod-projected-configmaps-2aa5b169-fdbc-48ed-b27b-f015bb82b13c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May  9 16:18:06.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-984" for this suite. 05/09/23 16:18:06.941
------------------------------
â€¢ [4.121 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:18:02.828
    May  9 16:18:02.828: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 16:18:02.829
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:18:02.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:18:02.853
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-6719db36-cc92-40f8-881c-52feb6d26046 05/09/23 16:18:02.861
    STEP: Creating a pod to test consume configMaps 05/09/23 16:18:02.871
    May  9 16:18:02.885: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2aa5b169-fdbc-48ed-b27b-f015bb82b13c" in namespace "projected-984" to be "Succeeded or Failed"
    May  9 16:18:02.892: INFO: Pod "pod-projected-configmaps-2aa5b169-fdbc-48ed-b27b-f015bb82b13c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.148072ms
    May  9 16:18:04.899: INFO: Pod "pod-projected-configmaps-2aa5b169-fdbc-48ed-b27b-f015bb82b13c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013997138s
    May  9 16:18:06.899: INFO: Pod "pod-projected-configmaps-2aa5b169-fdbc-48ed-b27b-f015bb82b13c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014150255s
    STEP: Saw pod success 05/09/23 16:18:06.899
    May  9 16:18:06.899: INFO: Pod "pod-projected-configmaps-2aa5b169-fdbc-48ed-b27b-f015bb82b13c" satisfied condition "Succeeded or Failed"
    May  9 16:18:06.905: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-configmaps-2aa5b169-fdbc-48ed-b27b-f015bb82b13c container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 16:18:06.916
    May  9 16:18:06.930: INFO: Waiting for pod pod-projected-configmaps-2aa5b169-fdbc-48ed-b27b-f015bb82b13c to disappear
    May  9 16:18:06.934: INFO: Pod pod-projected-configmaps-2aa5b169-fdbc-48ed-b27b-f015bb82b13c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May  9 16:18:06.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-984" for this suite. 05/09/23 16:18:06.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:18:06.952
May  9 16:18:06.953: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubectl 05/09/23 16:18:06.953
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:18:06.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:18:06.974
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 05/09/23 16:18:06.979
May  9 16:18:06.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-659 create -f -'
May  9 16:18:08.509: INFO: stderr: ""
May  9 16:18:08.509: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/09/23 16:18:08.509
May  9 16:18:09.516: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 16:18:09.516: INFO: Found 0 / 1
May  9 16:18:10.517: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 16:18:10.517: INFO: Found 1 / 1
May  9 16:18:10.517: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 05/09/23 16:18:10.517
May  9 16:18:10.523: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 16:18:10.523: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  9 16:18:10.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-659 patch pod agnhost-primary-hwgj7 -p {"metadata":{"annotations":{"x":"y"}}}'
May  9 16:18:10.620: INFO: stderr: ""
May  9 16:18:10.620: INFO: stdout: "pod/agnhost-primary-hwgj7 patched\n"
STEP: checking annotations 05/09/23 16:18:10.62
May  9 16:18:10.626: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 16:18:10.626: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  9 16:18:10.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-659" for this suite. 05/09/23 16:18:10.641
------------------------------
â€¢ [3.701 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:18:06.952
    May  9 16:18:06.953: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubectl 05/09/23 16:18:06.953
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:18:06.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:18:06.974
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 05/09/23 16:18:06.979
    May  9 16:18:06.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-659 create -f -'
    May  9 16:18:08.509: INFO: stderr: ""
    May  9 16:18:08.509: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/09/23 16:18:08.509
    May  9 16:18:09.516: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 16:18:09.516: INFO: Found 0 / 1
    May  9 16:18:10.517: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 16:18:10.517: INFO: Found 1 / 1
    May  9 16:18:10.517: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 05/09/23 16:18:10.517
    May  9 16:18:10.523: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 16:18:10.523: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May  9 16:18:10.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-659 patch pod agnhost-primary-hwgj7 -p {"metadata":{"annotations":{"x":"y"}}}'
    May  9 16:18:10.620: INFO: stderr: ""
    May  9 16:18:10.620: INFO: stdout: "pod/agnhost-primary-hwgj7 patched\n"
    STEP: checking annotations 05/09/23 16:18:10.62
    May  9 16:18:10.626: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 16:18:10.626: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  9 16:18:10.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-659" for this suite. 05/09/23 16:18:10.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:18:10.654
May  9 16:18:10.654: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename statefulset 05/09/23 16:18:10.655
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:18:10.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:18:10.673
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8699 05/09/23 16:18:10.677
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
May  9 16:18:10.698: INFO: Found 0 stateful pods, waiting for 1
May  9 16:18:20.703: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 05/09/23 16:18:20.714
W0509 16:18:20.723380      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May  9 16:18:20.734: INFO: Found 1 stateful pods, waiting for 2
May  9 16:18:30.740: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  9 16:18:30.740: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 05/09/23 16:18:30.752
STEP: Delete all of the StatefulSets 05/09/23 16:18:30.757
STEP: Verify that StatefulSets have been deleted 05/09/23 16:18:30.768
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May  9 16:18:30.773: INFO: Deleting all statefulset in ns statefulset-8699
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May  9 16:18:30.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8699" for this suite. 05/09/23 16:18:30.801
------------------------------
â€¢ [SLOW TEST] [20.159 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:18:10.654
    May  9 16:18:10.654: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename statefulset 05/09/23 16:18:10.655
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:18:10.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:18:10.673
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8699 05/09/23 16:18:10.677
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    May  9 16:18:10.698: INFO: Found 0 stateful pods, waiting for 1
    May  9 16:18:20.703: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 05/09/23 16:18:20.714
    W0509 16:18:20.723380      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May  9 16:18:20.734: INFO: Found 1 stateful pods, waiting for 2
    May  9 16:18:30.740: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May  9 16:18:30.740: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 05/09/23 16:18:30.752
    STEP: Delete all of the StatefulSets 05/09/23 16:18:30.757
    STEP: Verify that StatefulSets have been deleted 05/09/23 16:18:30.768
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May  9 16:18:30.773: INFO: Deleting all statefulset in ns statefulset-8699
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May  9 16:18:30.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8699" for this suite. 05/09/23 16:18:30.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:18:30.817
May  9 16:18:30.817: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename csiinlinevolumes 05/09/23 16:18:30.818
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:18:30.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:18:30.869
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 05/09/23 16:18:30.874
STEP: getting 05/09/23 16:18:30.893
STEP: listing in namespace 05/09/23 16:18:30.898
STEP: patching 05/09/23 16:18:30.911
STEP: deleting 05/09/23 16:18:30.923
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
May  9 16:18:30.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-4259" for this suite. 05/09/23 16:18:30.948
------------------------------
â€¢ [0.138 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:18:30.817
    May  9 16:18:30.817: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename csiinlinevolumes 05/09/23 16:18:30.818
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:18:30.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:18:30.869
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 05/09/23 16:18:30.874
    STEP: getting 05/09/23 16:18:30.893
    STEP: listing in namespace 05/09/23 16:18:30.898
    STEP: patching 05/09/23 16:18:30.911
    STEP: deleting 05/09/23 16:18:30.923
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    May  9 16:18:30.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-4259" for this suite. 05/09/23 16:18:30.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:18:30.957
May  9 16:18:30.957: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename emptydir 05/09/23 16:18:30.958
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:18:30.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:18:30.982
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 05/09/23 16:18:30.987
May  9 16:18:30.997: INFO: Waiting up to 5m0s for pod "pod-69de2e21-41b3-4081-b178-f2d8fd706067" in namespace "emptydir-6641" to be "Succeeded or Failed"
May  9 16:18:31.003: INFO: Pod "pod-69de2e21-41b3-4081-b178-f2d8fd706067": Phase="Pending", Reason="", readiness=false. Elapsed: 6.575535ms
May  9 16:18:33.011: INFO: Pod "pod-69de2e21-41b3-4081-b178-f2d8fd706067": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013810333s
May  9 16:18:35.065: INFO: Pod "pod-69de2e21-41b3-4081-b178-f2d8fd706067": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068322603s
STEP: Saw pod success 05/09/23 16:18:35.065
May  9 16:18:35.065: INFO: Pod "pod-69de2e21-41b3-4081-b178-f2d8fd706067" satisfied condition "Succeeded or Failed"
May  9 16:18:35.070: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-69de2e21-41b3-4081-b178-f2d8fd706067 container test-container: <nil>
STEP: delete the pod 05/09/23 16:18:35.081
May  9 16:18:35.098: INFO: Waiting for pod pod-69de2e21-41b3-4081-b178-f2d8fd706067 to disappear
May  9 16:18:35.103: INFO: Pod pod-69de2e21-41b3-4081-b178-f2d8fd706067 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  9 16:18:35.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6641" for this suite. 05/09/23 16:18:35.109
------------------------------
â€¢ [4.204 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:18:30.957
    May  9 16:18:30.957: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename emptydir 05/09/23 16:18:30.958
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:18:30.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:18:30.982
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 05/09/23 16:18:30.987
    May  9 16:18:30.997: INFO: Waiting up to 5m0s for pod "pod-69de2e21-41b3-4081-b178-f2d8fd706067" in namespace "emptydir-6641" to be "Succeeded or Failed"
    May  9 16:18:31.003: INFO: Pod "pod-69de2e21-41b3-4081-b178-f2d8fd706067": Phase="Pending", Reason="", readiness=false. Elapsed: 6.575535ms
    May  9 16:18:33.011: INFO: Pod "pod-69de2e21-41b3-4081-b178-f2d8fd706067": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013810333s
    May  9 16:18:35.065: INFO: Pod "pod-69de2e21-41b3-4081-b178-f2d8fd706067": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068322603s
    STEP: Saw pod success 05/09/23 16:18:35.065
    May  9 16:18:35.065: INFO: Pod "pod-69de2e21-41b3-4081-b178-f2d8fd706067" satisfied condition "Succeeded or Failed"
    May  9 16:18:35.070: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-69de2e21-41b3-4081-b178-f2d8fd706067 container test-container: <nil>
    STEP: delete the pod 05/09/23 16:18:35.081
    May  9 16:18:35.098: INFO: Waiting for pod pod-69de2e21-41b3-4081-b178-f2d8fd706067 to disappear
    May  9 16:18:35.103: INFO: Pod pod-69de2e21-41b3-4081-b178-f2d8fd706067 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  9 16:18:35.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6641" for this suite. 05/09/23 16:18:35.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:18:35.161
May  9 16:18:35.161: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename deployment 05/09/23 16:18:35.163
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:18:35.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:18:35.198
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
May  9 16:18:35.203: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May  9 16:18:35.216: INFO: Pod name sample-pod: Found 0 pods out of 1
May  9 16:18:40.223: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/09/23 16:18:40.223
May  9 16:18:40.223: INFO: Creating deployment "test-rolling-update-deployment"
May  9 16:18:40.230: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May  9 16:18:40.244: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May  9 16:18:42.255: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May  9 16:18:42.260: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  9 16:18:42.274: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6423  3b73d738-462d-4a18-9b5c-d4db176228e7 317970865 1 2023-05-09 16:18:40 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-09 16:18:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:18:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004890e78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-09 16:18:40 +0000 UTC,LastTransitionTime:2023-05-09 16:18:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-05-09 16:18:41 +0000 UTC,LastTransitionTime:2023-05-09 16:18:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  9 16:18:42.278: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-6423  ca66605e-c35d-4247-b8fb-32118f1a2537 317970851 1 2023-05-09 16:18:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3b73d738-462d-4a18-9b5c-d4db176228e7 0xc004891367 0xc004891368}] [] [{kube-controller-manager Update apps/v1 2023-05-09 16:18:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b73d738-462d-4a18-9b5c-d4db176228e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:18:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004891418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  9 16:18:42.278: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May  9 16:18:42.278: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6423  9c403b6c-12fc-48a2-82cb-7a0a4e7a86ff 317970860 2 2023-05-09 16:18:35 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3b73d738-462d-4a18-9b5c-d4db176228e7 0xc004891237 0xc004891238}] [] [{e2e.test Update apps/v1 2023-05-09 16:18:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:18:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b73d738-462d-4a18-9b5c-d4db176228e7\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:18:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0048912f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  9 16:18:42.283: INFO: Pod "test-rolling-update-deployment-7549d9f46d-txfxs" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-txfxs test-rolling-update-deployment-7549d9f46d- deployment-6423  e84a8da1-301d-4d55-bf2e-823dbf72fc0c 317970850 0 2023-05-09 16:18:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:cda676ef1ec2fa6375a50ebc72aff4bdf86b222dc97ad4fa716b550ac2b85ca9 cni.projectcalico.org/podIP:10.2.1.206/32 cni.projectcalico.org/podIPs:10.2.1.206/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d ca66605e-c35d-4247-b8fb-32118f1a2537 0xc004891897 0xc004891898}] [] [{calico Update v1 2023-05-09 16:18:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 16:18:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca66605e-c35d-4247-b8fb-32118f1a2537\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 16:18:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6m7g9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6m7g9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:18:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:18:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:18:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:18:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:10.2.1.206,StartTime:2023-05-09 16:18:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 16:18:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://55306c4085c7a2a5692a77ba38fc0c3b28006bb4140a8dd0c9ad09cc0e67a9ae,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May  9 16:18:42.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6423" for this suite. 05/09/23 16:18:42.292
------------------------------
â€¢ [SLOW TEST] [7.139 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:18:35.161
    May  9 16:18:35.161: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename deployment 05/09/23 16:18:35.163
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:18:35.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:18:35.198
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    May  9 16:18:35.203: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    May  9 16:18:35.216: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  9 16:18:40.223: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/09/23 16:18:40.223
    May  9 16:18:40.223: INFO: Creating deployment "test-rolling-update-deployment"
    May  9 16:18:40.230: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    May  9 16:18:40.244: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    May  9 16:18:42.255: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    May  9 16:18:42.260: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  9 16:18:42.274: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6423  3b73d738-462d-4a18-9b5c-d4db176228e7 317970865 1 2023-05-09 16:18:40 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-09 16:18:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:18:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004890e78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-09 16:18:40 +0000 UTC,LastTransitionTime:2023-05-09 16:18:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-05-09 16:18:41 +0000 UTC,LastTransitionTime:2023-05-09 16:18:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  9 16:18:42.278: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-6423  ca66605e-c35d-4247-b8fb-32118f1a2537 317970851 1 2023-05-09 16:18:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3b73d738-462d-4a18-9b5c-d4db176228e7 0xc004891367 0xc004891368}] [] [{kube-controller-manager Update apps/v1 2023-05-09 16:18:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b73d738-462d-4a18-9b5c-d4db176228e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:18:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004891418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  9 16:18:42.278: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    May  9 16:18:42.278: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6423  9c403b6c-12fc-48a2-82cb-7a0a4e7a86ff 317970860 2 2023-05-09 16:18:35 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3b73d738-462d-4a18-9b5c-d4db176228e7 0xc004891237 0xc004891238}] [] [{e2e.test Update apps/v1 2023-05-09 16:18:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:18:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b73d738-462d-4a18-9b5c-d4db176228e7\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:18:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0048912f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  9 16:18:42.283: INFO: Pod "test-rolling-update-deployment-7549d9f46d-txfxs" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-txfxs test-rolling-update-deployment-7549d9f46d- deployment-6423  e84a8da1-301d-4d55-bf2e-823dbf72fc0c 317970850 0 2023-05-09 16:18:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:cda676ef1ec2fa6375a50ebc72aff4bdf86b222dc97ad4fa716b550ac2b85ca9 cni.projectcalico.org/podIP:10.2.1.206/32 cni.projectcalico.org/podIPs:10.2.1.206/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d ca66605e-c35d-4247-b8fb-32118f1a2537 0xc004891897 0xc004891898}] [] [{calico Update v1 2023-05-09 16:18:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 16:18:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca66605e-c35d-4247-b8fb-32118f1a2537\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 16:18:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6m7g9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6m7g9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:18:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:18:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:18:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:18:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:10.2.1.206,StartTime:2023-05-09 16:18:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 16:18:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://55306c4085c7a2a5692a77ba38fc0c3b28006bb4140a8dd0c9ad09cc0e67a9ae,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May  9 16:18:42.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6423" for this suite. 05/09/23 16:18:42.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:18:42.302
May  9 16:18:42.302: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename secrets 05/09/23 16:18:42.302
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:18:42.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:18:42.324
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-a73a582e-f9be-4baf-ba22-aa1be2dcd7bd 05/09/23 16:18:42.339
STEP: Creating secret with name s-test-opt-upd-cb452400-8681-4b75-a19f-16eb8d3713b2 05/09/23 16:18:42.344
STEP: Creating the pod 05/09/23 16:18:42.352
May  9 16:18:42.363: INFO: Waiting up to 5m0s for pod "pod-secrets-b25b3254-dd81-4d9c-8adb-ce03e0436efa" in namespace "secrets-1861" to be "running and ready"
May  9 16:18:42.368: INFO: Pod "pod-secrets-b25b3254-dd81-4d9c-8adb-ce03e0436efa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.190461ms
May  9 16:18:42.368: INFO: The phase of Pod pod-secrets-b25b3254-dd81-4d9c-8adb-ce03e0436efa is Pending, waiting for it to be Running (with Ready = true)
May  9 16:18:44.374: INFO: Pod "pod-secrets-b25b3254-dd81-4d9c-8adb-ce03e0436efa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01138993s
May  9 16:18:44.374: INFO: The phase of Pod pod-secrets-b25b3254-dd81-4d9c-8adb-ce03e0436efa is Pending, waiting for it to be Running (with Ready = true)
May  9 16:18:46.375: INFO: Pod "pod-secrets-b25b3254-dd81-4d9c-8adb-ce03e0436efa": Phase="Running", Reason="", readiness=true. Elapsed: 4.012670954s
May  9 16:18:46.376: INFO: The phase of Pod pod-secrets-b25b3254-dd81-4d9c-8adb-ce03e0436efa is Running (Ready = true)
May  9 16:18:46.376: INFO: Pod "pod-secrets-b25b3254-dd81-4d9c-8adb-ce03e0436efa" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-a73a582e-f9be-4baf-ba22-aa1be2dcd7bd 05/09/23 16:18:46.416
STEP: Updating secret s-test-opt-upd-cb452400-8681-4b75-a19f-16eb8d3713b2 05/09/23 16:18:46.424
STEP: Creating secret with name s-test-opt-create-71fc7620-6e7d-40b2-8e29-13093a99c021 05/09/23 16:18:46.431
STEP: waiting to observe update in volume 05/09/23 16:18:46.437
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May  9 16:19:48.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1861" for this suite. 05/09/23 16:19:48.979
------------------------------
â€¢ [SLOW TEST] [66.686 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:18:42.302
    May  9 16:18:42.302: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename secrets 05/09/23 16:18:42.302
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:18:42.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:18:42.324
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-a73a582e-f9be-4baf-ba22-aa1be2dcd7bd 05/09/23 16:18:42.339
    STEP: Creating secret with name s-test-opt-upd-cb452400-8681-4b75-a19f-16eb8d3713b2 05/09/23 16:18:42.344
    STEP: Creating the pod 05/09/23 16:18:42.352
    May  9 16:18:42.363: INFO: Waiting up to 5m0s for pod "pod-secrets-b25b3254-dd81-4d9c-8adb-ce03e0436efa" in namespace "secrets-1861" to be "running and ready"
    May  9 16:18:42.368: INFO: Pod "pod-secrets-b25b3254-dd81-4d9c-8adb-ce03e0436efa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.190461ms
    May  9 16:18:42.368: INFO: The phase of Pod pod-secrets-b25b3254-dd81-4d9c-8adb-ce03e0436efa is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:18:44.374: INFO: Pod "pod-secrets-b25b3254-dd81-4d9c-8adb-ce03e0436efa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01138993s
    May  9 16:18:44.374: INFO: The phase of Pod pod-secrets-b25b3254-dd81-4d9c-8adb-ce03e0436efa is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:18:46.375: INFO: Pod "pod-secrets-b25b3254-dd81-4d9c-8adb-ce03e0436efa": Phase="Running", Reason="", readiness=true. Elapsed: 4.012670954s
    May  9 16:18:46.376: INFO: The phase of Pod pod-secrets-b25b3254-dd81-4d9c-8adb-ce03e0436efa is Running (Ready = true)
    May  9 16:18:46.376: INFO: Pod "pod-secrets-b25b3254-dd81-4d9c-8adb-ce03e0436efa" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-a73a582e-f9be-4baf-ba22-aa1be2dcd7bd 05/09/23 16:18:46.416
    STEP: Updating secret s-test-opt-upd-cb452400-8681-4b75-a19f-16eb8d3713b2 05/09/23 16:18:46.424
    STEP: Creating secret with name s-test-opt-create-71fc7620-6e7d-40b2-8e29-13093a99c021 05/09/23 16:18:46.431
    STEP: waiting to observe update in volume 05/09/23 16:18:46.437
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May  9 16:19:48.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1861" for this suite. 05/09/23 16:19:48.979
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:19:48.987
May  9 16:19:48.988: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename security-context-test 05/09/23 16:19:48.989
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:19:49.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:19:49.013
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
May  9 16:19:49.030: INFO: Waiting up to 5m0s for pod "busybox-user-65534-40f00bfb-afc4-4632-aa01-135a6c1ad7ae" in namespace "security-context-test-4836" to be "Succeeded or Failed"
May  9 16:19:49.038: INFO: Pod "busybox-user-65534-40f00bfb-afc4-4632-aa01-135a6c1ad7ae": Phase="Pending", Reason="", readiness=false. Elapsed: 8.546505ms
May  9 16:19:51.053: INFO: Pod "busybox-user-65534-40f00bfb-afc4-4632-aa01-135a6c1ad7ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023293903s
May  9 16:19:53.047: INFO: Pod "busybox-user-65534-40f00bfb-afc4-4632-aa01-135a6c1ad7ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017487891s
May  9 16:19:53.047: INFO: Pod "busybox-user-65534-40f00bfb-afc4-4632-aa01-135a6c1ad7ae" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May  9 16:19:53.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4836" for this suite. 05/09/23 16:19:53.053
------------------------------
â€¢ [4.076 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:19:48.987
    May  9 16:19:48.988: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename security-context-test 05/09/23 16:19:48.989
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:19:49.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:19:49.013
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    May  9 16:19:49.030: INFO: Waiting up to 5m0s for pod "busybox-user-65534-40f00bfb-afc4-4632-aa01-135a6c1ad7ae" in namespace "security-context-test-4836" to be "Succeeded or Failed"
    May  9 16:19:49.038: INFO: Pod "busybox-user-65534-40f00bfb-afc4-4632-aa01-135a6c1ad7ae": Phase="Pending", Reason="", readiness=false. Elapsed: 8.546505ms
    May  9 16:19:51.053: INFO: Pod "busybox-user-65534-40f00bfb-afc4-4632-aa01-135a6c1ad7ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023293903s
    May  9 16:19:53.047: INFO: Pod "busybox-user-65534-40f00bfb-afc4-4632-aa01-135a6c1ad7ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017487891s
    May  9 16:19:53.047: INFO: Pod "busybox-user-65534-40f00bfb-afc4-4632-aa01-135a6c1ad7ae" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May  9 16:19:53.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4836" for this suite. 05/09/23 16:19:53.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:19:53.065
May  9 16:19:53.065: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename events 05/09/23 16:19:53.066
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:19:53.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:19:53.086
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 05/09/23 16:19:53.09
STEP: listing events in all namespaces 05/09/23 16:19:53.095
STEP: listing events in test namespace 05/09/23 16:19:53.111
STEP: listing events with field selection filtering on source 05/09/23 16:19:53.115
STEP: listing events with field selection filtering on reportingController 05/09/23 16:19:53.124
STEP: getting the test event 05/09/23 16:19:53.13
STEP: patching the test event 05/09/23 16:19:53.135
STEP: getting the test event 05/09/23 16:19:53.144
STEP: updating the test event 05/09/23 16:19:53.148
STEP: getting the test event 05/09/23 16:19:53.159
STEP: deleting the test event 05/09/23 16:19:53.164
STEP: listing events in all namespaces 05/09/23 16:19:53.172
STEP: listing events in test namespace 05/09/23 16:19:53.178
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
May  9 16:19:53.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4514" for this suite. 05/09/23 16:19:53.189
------------------------------
â€¢ [0.133 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:19:53.065
    May  9 16:19:53.065: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename events 05/09/23 16:19:53.066
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:19:53.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:19:53.086
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 05/09/23 16:19:53.09
    STEP: listing events in all namespaces 05/09/23 16:19:53.095
    STEP: listing events in test namespace 05/09/23 16:19:53.111
    STEP: listing events with field selection filtering on source 05/09/23 16:19:53.115
    STEP: listing events with field selection filtering on reportingController 05/09/23 16:19:53.124
    STEP: getting the test event 05/09/23 16:19:53.13
    STEP: patching the test event 05/09/23 16:19:53.135
    STEP: getting the test event 05/09/23 16:19:53.144
    STEP: updating the test event 05/09/23 16:19:53.148
    STEP: getting the test event 05/09/23 16:19:53.159
    STEP: deleting the test event 05/09/23 16:19:53.164
    STEP: listing events in all namespaces 05/09/23 16:19:53.172
    STEP: listing events in test namespace 05/09/23 16:19:53.178
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    May  9 16:19:53.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4514" for this suite. 05/09/23 16:19:53.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:19:53.198
May  9 16:19:53.198: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename services 05/09/23 16:19:53.199
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:19:53.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:19:53.219
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 05/09/23 16:19:53.229
STEP: waiting for available Endpoint 05/09/23 16:19:53.235
STEP: listing all Endpoints 05/09/23 16:19:53.237
STEP: updating the Endpoint 05/09/23 16:19:53.244
STEP: fetching the Endpoint 05/09/23 16:19:53.253
STEP: patching the Endpoint 05/09/23 16:19:53.257
STEP: fetching the Endpoint 05/09/23 16:19:53.265
STEP: deleting the Endpoint by Collection 05/09/23 16:19:53.269
STEP: waiting for Endpoint deletion 05/09/23 16:19:53.279
STEP: fetching the Endpoint 05/09/23 16:19:53.282
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  9 16:19:53.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5488" for this suite. 05/09/23 16:19:53.304
------------------------------
â€¢ [0.114 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:19:53.198
    May  9 16:19:53.198: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename services 05/09/23 16:19:53.199
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:19:53.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:19:53.219
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 05/09/23 16:19:53.229
    STEP: waiting for available Endpoint 05/09/23 16:19:53.235
    STEP: listing all Endpoints 05/09/23 16:19:53.237
    STEP: updating the Endpoint 05/09/23 16:19:53.244
    STEP: fetching the Endpoint 05/09/23 16:19:53.253
    STEP: patching the Endpoint 05/09/23 16:19:53.257
    STEP: fetching the Endpoint 05/09/23 16:19:53.265
    STEP: deleting the Endpoint by Collection 05/09/23 16:19:53.269
    STEP: waiting for Endpoint deletion 05/09/23 16:19:53.279
    STEP: fetching the Endpoint 05/09/23 16:19:53.282
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  9 16:19:53.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5488" for this suite. 05/09/23 16:19:53.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:19:53.313
May  9 16:19:53.313: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename services 05/09/23 16:19:53.313
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:19:53.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:19:53.345
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-3628 05/09/23 16:19:53.349
STEP: creating service affinity-clusterip in namespace services-3628 05/09/23 16:19:53.349
STEP: creating replication controller affinity-clusterip in namespace services-3628 05/09/23 16:19:53.365
I0509 16:19:53.373913      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3628, replica count: 3
I0509 16:19:56.425293      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 16:19:56.435: INFO: Creating new exec pod
May  9 16:19:56.442: INFO: Waiting up to 5m0s for pod "execpod-affinity2sdlv" in namespace "services-3628" to be "running"
May  9 16:19:56.451: INFO: Pod "execpod-affinity2sdlv": Phase="Pending", Reason="", readiness=false. Elapsed: 9.016997ms
May  9 16:19:58.455: INFO: Pod "execpod-affinity2sdlv": Phase="Running", Reason="", readiness=true. Elapsed: 2.013817396s
May  9 16:19:58.455: INFO: Pod "execpod-affinity2sdlv" satisfied condition "running"
May  9 16:19:59.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-3628 exec execpod-affinity2sdlv -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
May  9 16:19:59.702: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May  9 16:19:59.702: INFO: stdout: ""
May  9 16:19:59.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-3628 exec execpod-affinity2sdlv -- /bin/sh -x -c nc -v -z -w 2 10.3.106.17 80'
May  9 16:19:59.922: INFO: stderr: "+ nc -v -z -w 2 10.3.106.17 80\nConnection to 10.3.106.17 80 port [tcp/http] succeeded!\n"
May  9 16:19:59.922: INFO: stdout: ""
May  9 16:19:59.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-3628 exec execpod-affinity2sdlv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.106.17:80/ ; done'
May  9 16:20:00.246: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n"
May  9 16:20:00.246: INFO: stdout: "\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z"
May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
May  9 16:20:00.246: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3628, will wait for the garbage collector to delete the pods 05/09/23 16:20:00.269
May  9 16:20:00.356: INFO: Deleting ReplicationController affinity-clusterip took: 27.920363ms
May  9 16:20:00.456: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.878562ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  9 16:20:02.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3628" for this suite. 05/09/23 16:20:02.683
------------------------------
â€¢ [SLOW TEST] [9.377 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:19:53.313
    May  9 16:19:53.313: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename services 05/09/23 16:19:53.313
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:19:53.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:19:53.345
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-3628 05/09/23 16:19:53.349
    STEP: creating service affinity-clusterip in namespace services-3628 05/09/23 16:19:53.349
    STEP: creating replication controller affinity-clusterip in namespace services-3628 05/09/23 16:19:53.365
    I0509 16:19:53.373913      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3628, replica count: 3
    I0509 16:19:56.425293      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 16:19:56.435: INFO: Creating new exec pod
    May  9 16:19:56.442: INFO: Waiting up to 5m0s for pod "execpod-affinity2sdlv" in namespace "services-3628" to be "running"
    May  9 16:19:56.451: INFO: Pod "execpod-affinity2sdlv": Phase="Pending", Reason="", readiness=false. Elapsed: 9.016997ms
    May  9 16:19:58.455: INFO: Pod "execpod-affinity2sdlv": Phase="Running", Reason="", readiness=true. Elapsed: 2.013817396s
    May  9 16:19:58.455: INFO: Pod "execpod-affinity2sdlv" satisfied condition "running"
    May  9 16:19:59.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-3628 exec execpod-affinity2sdlv -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    May  9 16:19:59.702: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    May  9 16:19:59.702: INFO: stdout: ""
    May  9 16:19:59.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-3628 exec execpod-affinity2sdlv -- /bin/sh -x -c nc -v -z -w 2 10.3.106.17 80'
    May  9 16:19:59.922: INFO: stderr: "+ nc -v -z -w 2 10.3.106.17 80\nConnection to 10.3.106.17 80 port [tcp/http] succeeded!\n"
    May  9 16:19:59.922: INFO: stdout: ""
    May  9 16:19:59.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-3628 exec execpod-affinity2sdlv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.106.17:80/ ; done'
    May  9 16:20:00.246: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.106.17:80/\n"
    May  9 16:20:00.246: INFO: stdout: "\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z\naffinity-clusterip-kwd8z"
    May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
    May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
    May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
    May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
    May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
    May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
    May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
    May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
    May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
    May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
    May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
    May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
    May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
    May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
    May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
    May  9 16:20:00.246: INFO: Received response from host: affinity-clusterip-kwd8z
    May  9 16:20:00.246: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-3628, will wait for the garbage collector to delete the pods 05/09/23 16:20:00.269
    May  9 16:20:00.356: INFO: Deleting ReplicationController affinity-clusterip took: 27.920363ms
    May  9 16:20:00.456: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.878562ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  9 16:20:02.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3628" for this suite. 05/09/23 16:20:02.683
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:20:02.692
May  9 16:20:02.692: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename statefulset 05/09/23 16:20:02.692
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:20:02.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:20:02.71
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4148 05/09/23 16:20:02.715
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 05/09/23 16:20:02.727
STEP: Creating pod with conflicting port in namespace statefulset-4148 05/09/23 16:20:02.732
STEP: Waiting until pod test-pod will start running in namespace statefulset-4148 05/09/23 16:20:02.741
May  9 16:20:02.741: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-4148" to be "running"
May  9 16:20:02.747: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.087952ms
May  9 16:20:04.756: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014109581s
May  9 16:20:04.756: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-4148 05/09/23 16:20:04.756
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4148 05/09/23 16:20:04.764
May  9 16:20:04.781: INFO: Observed stateful pod in namespace: statefulset-4148, name: ss-0, uid: 44142d3a-4355-47bb-ab8c-563927ab1a6f, status phase: Pending. Waiting for statefulset controller to delete.
May  9 16:20:04.800: INFO: Observed stateful pod in namespace: statefulset-4148, name: ss-0, uid: 44142d3a-4355-47bb-ab8c-563927ab1a6f, status phase: Failed. Waiting for statefulset controller to delete.
May  9 16:20:04.813: INFO: Observed stateful pod in namespace: statefulset-4148, name: ss-0, uid: 44142d3a-4355-47bb-ab8c-563927ab1a6f, status phase: Failed. Waiting for statefulset controller to delete.
May  9 16:20:04.816: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4148
STEP: Removing pod with conflicting port in namespace statefulset-4148 05/09/23 16:20:04.816
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4148 and will be in running state 05/09/23 16:20:04.834
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May  9 16:20:06.859: INFO: Deleting all statefulset in ns statefulset-4148
May  9 16:20:06.963: INFO: Scaling statefulset ss to 0
May  9 16:20:17.078: INFO: Waiting for statefulset status.replicas updated to 0
May  9 16:20:17.082: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May  9 16:20:17.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4148" for this suite. 05/09/23 16:20:17.107
------------------------------
â€¢ [SLOW TEST] [14.424 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:20:02.692
    May  9 16:20:02.692: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename statefulset 05/09/23 16:20:02.692
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:20:02.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:20:02.71
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4148 05/09/23 16:20:02.715
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 05/09/23 16:20:02.727
    STEP: Creating pod with conflicting port in namespace statefulset-4148 05/09/23 16:20:02.732
    STEP: Waiting until pod test-pod will start running in namespace statefulset-4148 05/09/23 16:20:02.741
    May  9 16:20:02.741: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-4148" to be "running"
    May  9 16:20:02.747: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.087952ms
    May  9 16:20:04.756: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014109581s
    May  9 16:20:04.756: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-4148 05/09/23 16:20:04.756
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4148 05/09/23 16:20:04.764
    May  9 16:20:04.781: INFO: Observed stateful pod in namespace: statefulset-4148, name: ss-0, uid: 44142d3a-4355-47bb-ab8c-563927ab1a6f, status phase: Pending. Waiting for statefulset controller to delete.
    May  9 16:20:04.800: INFO: Observed stateful pod in namespace: statefulset-4148, name: ss-0, uid: 44142d3a-4355-47bb-ab8c-563927ab1a6f, status phase: Failed. Waiting for statefulset controller to delete.
    May  9 16:20:04.813: INFO: Observed stateful pod in namespace: statefulset-4148, name: ss-0, uid: 44142d3a-4355-47bb-ab8c-563927ab1a6f, status phase: Failed. Waiting for statefulset controller to delete.
    May  9 16:20:04.816: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4148
    STEP: Removing pod with conflicting port in namespace statefulset-4148 05/09/23 16:20:04.816
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4148 and will be in running state 05/09/23 16:20:04.834
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May  9 16:20:06.859: INFO: Deleting all statefulset in ns statefulset-4148
    May  9 16:20:06.963: INFO: Scaling statefulset ss to 0
    May  9 16:20:17.078: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 16:20:17.082: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May  9 16:20:17.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4148" for this suite. 05/09/23 16:20:17.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:20:17.116
May  9 16:20:17.116: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename pod-network-test 05/09/23 16:20:17.117
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:20:17.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:20:17.134
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-5765 05/09/23 16:20:17.138
STEP: creating a selector 05/09/23 16:20:17.138
STEP: Creating the service pods in kubernetes 05/09/23 16:20:17.138
May  9 16:20:17.138: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  9 16:20:17.169: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5765" to be "running and ready"
May  9 16:20:17.174: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.235391ms
May  9 16:20:17.174: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:20:19.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013480904s
May  9 16:20:19.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:20:21.183: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013869688s
May  9 16:20:21.183: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:20:23.180: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011015392s
May  9 16:20:23.180: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:20:25.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012655299s
May  9 16:20:25.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:20:27.181: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.012456838s
May  9 16:20:27.181: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:20:29.183: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014434074s
May  9 16:20:29.183: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:20:31.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.012848105s
May  9 16:20:31.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:20:33.181: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012155527s
May  9 16:20:33.181: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:20:35.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012653855s
May  9 16:20:35.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:20:37.181: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.01249396s
May  9 16:20:37.181: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:20:39.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.013548532s
May  9 16:20:39.183: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  9 16:20:39.183: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  9 16:20:39.189: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5765" to be "running and ready"
May  9 16:20:39.196: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.531331ms
May  9 16:20:39.196: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  9 16:20:39.196: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  9 16:20:39.203: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5765" to be "running and ready"
May  9 16:20:39.209: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.681122ms
May  9 16:20:39.209: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  9 16:20:39.209: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/09/23 16:20:39.214
May  9 16:20:39.221: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5765" to be "running"
May  9 16:20:39.239: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.789363ms
May  9 16:20:41.245: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.023901634s
May  9 16:20:41.245: INFO: Pod "test-container-pod" satisfied condition "running"
May  9 16:20:41.250: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May  9 16:20:41.250: INFO: Breadth first check of 10.2.1.212 on host 51.68.91.222...
May  9 16:20:41.255: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.1.213:9080/dial?request=hostname&protocol=udp&host=10.2.1.212&port=8081&tries=1'] Namespace:pod-network-test-5765 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:20:41.255: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:20:41.255: INFO: ExecWithOptions: Clientset creation
May  9 16:20:41.256: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-5765/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.1.213%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.2.1.212%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  9 16:20:41.450: INFO: Waiting for responses: map[]
May  9 16:20:41.450: INFO: reached 10.2.1.212 after 0/1 tries
May  9 16:20:41.450: INFO: Breadth first check of 10.2.0.150 on host 51.68.93.170...
May  9 16:20:41.457: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.1.213:9080/dial?request=hostname&protocol=udp&host=10.2.0.150&port=8081&tries=1'] Namespace:pod-network-test-5765 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:20:41.457: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:20:41.457: INFO: ExecWithOptions: Clientset creation
May  9 16:20:41.457: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-5765/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.1.213%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.2.0.150%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  9 16:20:41.600: INFO: Waiting for responses: map[]
May  9 16:20:41.600: INFO: reached 10.2.0.150 after 0/1 tries
May  9 16:20:41.600: INFO: Breadth first check of 10.2.2.147 on host 51.68.94.118...
May  9 16:20:41.606: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.1.213:9080/dial?request=hostname&protocol=udp&host=10.2.2.147&port=8081&tries=1'] Namespace:pod-network-test-5765 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:20:41.606: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:20:41.607: INFO: ExecWithOptions: Clientset creation
May  9 16:20:41.607: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-5765/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.1.213%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.2.2.147%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  9 16:20:41.733: INFO: Waiting for responses: map[]
May  9 16:20:41.733: INFO: reached 10.2.2.147 after 0/1 tries
May  9 16:20:41.733: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
May  9 16:20:41.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-5765" for this suite. 05/09/23 16:20:41.74
------------------------------
â€¢ [SLOW TEST] [24.631 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:20:17.116
    May  9 16:20:17.116: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename pod-network-test 05/09/23 16:20:17.117
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:20:17.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:20:17.134
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-5765 05/09/23 16:20:17.138
    STEP: creating a selector 05/09/23 16:20:17.138
    STEP: Creating the service pods in kubernetes 05/09/23 16:20:17.138
    May  9 16:20:17.138: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  9 16:20:17.169: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5765" to be "running and ready"
    May  9 16:20:17.174: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.235391ms
    May  9 16:20:17.174: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:20:19.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013480904s
    May  9 16:20:19.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:20:21.183: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013869688s
    May  9 16:20:21.183: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:20:23.180: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011015392s
    May  9 16:20:23.180: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:20:25.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012655299s
    May  9 16:20:25.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:20:27.181: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.012456838s
    May  9 16:20:27.181: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:20:29.183: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014434074s
    May  9 16:20:29.183: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:20:31.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.012848105s
    May  9 16:20:31.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:20:33.181: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012155527s
    May  9 16:20:33.181: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:20:35.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012653855s
    May  9 16:20:35.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:20:37.181: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.01249396s
    May  9 16:20:37.181: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:20:39.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.013548532s
    May  9 16:20:39.183: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  9 16:20:39.183: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  9 16:20:39.189: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5765" to be "running and ready"
    May  9 16:20:39.196: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.531331ms
    May  9 16:20:39.196: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  9 16:20:39.196: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  9 16:20:39.203: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5765" to be "running and ready"
    May  9 16:20:39.209: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.681122ms
    May  9 16:20:39.209: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  9 16:20:39.209: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/09/23 16:20:39.214
    May  9 16:20:39.221: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5765" to be "running"
    May  9 16:20:39.239: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.789363ms
    May  9 16:20:41.245: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.023901634s
    May  9 16:20:41.245: INFO: Pod "test-container-pod" satisfied condition "running"
    May  9 16:20:41.250: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May  9 16:20:41.250: INFO: Breadth first check of 10.2.1.212 on host 51.68.91.222...
    May  9 16:20:41.255: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.1.213:9080/dial?request=hostname&protocol=udp&host=10.2.1.212&port=8081&tries=1'] Namespace:pod-network-test-5765 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:20:41.255: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:20:41.255: INFO: ExecWithOptions: Clientset creation
    May  9 16:20:41.256: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-5765/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.1.213%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.2.1.212%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  9 16:20:41.450: INFO: Waiting for responses: map[]
    May  9 16:20:41.450: INFO: reached 10.2.1.212 after 0/1 tries
    May  9 16:20:41.450: INFO: Breadth first check of 10.2.0.150 on host 51.68.93.170...
    May  9 16:20:41.457: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.1.213:9080/dial?request=hostname&protocol=udp&host=10.2.0.150&port=8081&tries=1'] Namespace:pod-network-test-5765 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:20:41.457: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:20:41.457: INFO: ExecWithOptions: Clientset creation
    May  9 16:20:41.457: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-5765/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.1.213%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.2.0.150%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  9 16:20:41.600: INFO: Waiting for responses: map[]
    May  9 16:20:41.600: INFO: reached 10.2.0.150 after 0/1 tries
    May  9 16:20:41.600: INFO: Breadth first check of 10.2.2.147 on host 51.68.94.118...
    May  9 16:20:41.606: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.1.213:9080/dial?request=hostname&protocol=udp&host=10.2.2.147&port=8081&tries=1'] Namespace:pod-network-test-5765 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:20:41.606: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:20:41.607: INFO: ExecWithOptions: Clientset creation
    May  9 16:20:41.607: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-5765/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.1.213%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.2.2.147%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  9 16:20:41.733: INFO: Waiting for responses: map[]
    May  9 16:20:41.733: INFO: reached 10.2.2.147 after 0/1 tries
    May  9 16:20:41.733: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    May  9 16:20:41.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-5765" for this suite. 05/09/23 16:20:41.74
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:20:41.749
May  9 16:20:41.749: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename svcaccounts 05/09/23 16:20:41.75
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:20:41.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:20:41.774
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
May  9 16:20:41.787: INFO: Got root ca configmap in namespace "svcaccounts-1084"
May  9 16:20:41.799: INFO: Deleted root ca configmap in namespace "svcaccounts-1084"
STEP: waiting for a new root ca configmap created 05/09/23 16:20:42.3
May  9 16:20:42.306: INFO: Recreated root ca configmap in namespace "svcaccounts-1084"
May  9 16:20:42.312: INFO: Updated root ca configmap in namespace "svcaccounts-1084"
STEP: waiting for the root ca configmap reconciled 05/09/23 16:20:42.813
May  9 16:20:42.819: INFO: Reconciled root ca configmap in namespace "svcaccounts-1084"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May  9 16:20:42.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1084" for this suite. 05/09/23 16:20:42.827
------------------------------
â€¢ [1.088 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:20:41.749
    May  9 16:20:41.749: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename svcaccounts 05/09/23 16:20:41.75
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:20:41.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:20:41.774
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    May  9 16:20:41.787: INFO: Got root ca configmap in namespace "svcaccounts-1084"
    May  9 16:20:41.799: INFO: Deleted root ca configmap in namespace "svcaccounts-1084"
    STEP: waiting for a new root ca configmap created 05/09/23 16:20:42.3
    May  9 16:20:42.306: INFO: Recreated root ca configmap in namespace "svcaccounts-1084"
    May  9 16:20:42.312: INFO: Updated root ca configmap in namespace "svcaccounts-1084"
    STEP: waiting for the root ca configmap reconciled 05/09/23 16:20:42.813
    May  9 16:20:42.819: INFO: Reconciled root ca configmap in namespace "svcaccounts-1084"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May  9 16:20:42.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1084" for this suite. 05/09/23 16:20:42.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:20:42.839
May  9 16:20:42.839: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename sched-preemption 05/09/23 16:20:42.84
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:20:42.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:20:42.866
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
May  9 16:20:42.888: INFO: Waiting up to 1m0s for all nodes to be ready
May  9 16:21:42.938: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 05/09/23 16:21:42.942
May  9 16:21:42.969: INFO: Created pod: pod0-0-sched-preemption-low-priority
May  9 16:21:42.976: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May  9 16:21:43.009: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May  9 16:21:43.036: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May  9 16:21:43.066: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May  9 16:21:43.075: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 05/09/23 16:21:43.076
May  9 16:21:43.076: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5688" to be "running"
May  9 16:21:43.080: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.503512ms
May  9 16:21:45.086: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.010152463s
May  9 16:21:45.086: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
May  9 16:21:45.086: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5688" to be "running"
May  9 16:21:45.092: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.574027ms
May  9 16:21:45.092: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
May  9 16:21:45.092: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5688" to be "running"
May  9 16:21:45.096: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.358004ms
May  9 16:21:45.096: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
May  9 16:21:45.096: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5688" to be "running"
May  9 16:21:45.101: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.28785ms
May  9 16:21:45.101: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
May  9 16:21:45.101: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5688" to be "running"
May  9 16:21:45.109: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.874179ms
May  9 16:21:47.118: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.016865946s
May  9 16:21:47.118: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
May  9 16:21:47.118: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5688" to be "running"
May  9 16:21:47.124: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.233912ms
May  9 16:21:47.124: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 05/09/23 16:21:47.124
May  9 16:21:47.134: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5688" to be "running"
May  9 16:21:47.143: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.199714ms
May  9 16:21:49.153: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018934144s
May  9 16:21:51.148: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.01459654s
May  9 16:21:51.148: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 16:21:51.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5688" for this suite. 05/09/23 16:21:51.254
------------------------------
â€¢ [SLOW TEST] [68.423 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:20:42.839
    May  9 16:20:42.839: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename sched-preemption 05/09/23 16:20:42.84
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:20:42.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:20:42.866
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    May  9 16:20:42.888: INFO: Waiting up to 1m0s for all nodes to be ready
    May  9 16:21:42.938: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 05/09/23 16:21:42.942
    May  9 16:21:42.969: INFO: Created pod: pod0-0-sched-preemption-low-priority
    May  9 16:21:42.976: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    May  9 16:21:43.009: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    May  9 16:21:43.036: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    May  9 16:21:43.066: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    May  9 16:21:43.075: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 05/09/23 16:21:43.076
    May  9 16:21:43.076: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5688" to be "running"
    May  9 16:21:43.080: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.503512ms
    May  9 16:21:45.086: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.010152463s
    May  9 16:21:45.086: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    May  9 16:21:45.086: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5688" to be "running"
    May  9 16:21:45.092: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.574027ms
    May  9 16:21:45.092: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    May  9 16:21:45.092: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5688" to be "running"
    May  9 16:21:45.096: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.358004ms
    May  9 16:21:45.096: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    May  9 16:21:45.096: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5688" to be "running"
    May  9 16:21:45.101: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.28785ms
    May  9 16:21:45.101: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    May  9 16:21:45.101: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5688" to be "running"
    May  9 16:21:45.109: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.874179ms
    May  9 16:21:47.118: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.016865946s
    May  9 16:21:47.118: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    May  9 16:21:47.118: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5688" to be "running"
    May  9 16:21:47.124: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.233912ms
    May  9 16:21:47.124: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 05/09/23 16:21:47.124
    May  9 16:21:47.134: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5688" to be "running"
    May  9 16:21:47.143: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.199714ms
    May  9 16:21:49.153: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018934144s
    May  9 16:21:51.148: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.01459654s
    May  9 16:21:51.148: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 16:21:51.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5688" for this suite. 05/09/23 16:21:51.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:21:51.264
May  9 16:21:51.264: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename services 05/09/23 16:21:51.265
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:21:51.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:21:51.286
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6297 05/09/23 16:21:51.292
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/09/23 16:21:51.308
STEP: creating service externalsvc in namespace services-6297 05/09/23 16:21:51.308
STEP: creating replication controller externalsvc in namespace services-6297 05/09/23 16:21:51.322
I0509 16:21:51.328869      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6297, replica count: 2
I0509 16:21:54.379804      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 05/09/23 16:21:54.385
May  9 16:21:54.407: INFO: Creating new exec pod
May  9 16:21:54.412: INFO: Waiting up to 5m0s for pod "execpodrbf9k" in namespace "services-6297" to be "running"
May  9 16:21:54.418: INFO: Pod "execpodrbf9k": Phase="Pending", Reason="", readiness=false. Elapsed: 5.055256ms
May  9 16:21:56.427: INFO: Pod "execpodrbf9k": Phase="Running", Reason="", readiness=true. Elapsed: 2.014734843s
May  9 16:21:56.427: INFO: Pod "execpodrbf9k" satisfied condition "running"
May  9 16:21:56.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-6297 exec execpodrbf9k -- /bin/sh -x -c nslookup clusterip-service.services-6297.svc.cluster.local'
May  9 16:21:56.778: INFO: stderr: "+ nslookup clusterip-service.services-6297.svc.cluster.local\n"
May  9 16:21:56.778: INFO: stdout: "Server:\t\t10.3.0.10\nAddress:\t10.3.0.10#53\n\nclusterip-service.services-6297.svc.cluster.local\tcanonical name = externalsvc.services-6297.svc.cluster.local.\nName:\texternalsvc.services-6297.svc.cluster.local\nAddress: 10.3.13.141\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6297, will wait for the garbage collector to delete the pods 05/09/23 16:21:56.778
May  9 16:21:56.843: INFO: Deleting ReplicationController externalsvc took: 7.586049ms
May  9 16:21:56.943: INFO: Terminating ReplicationController externalsvc pods took: 100.525924ms
May  9 16:21:59.067: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  9 16:21:59.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6297" for this suite. 05/09/23 16:21:59.09
------------------------------
â€¢ [SLOW TEST] [7.836 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:21:51.264
    May  9 16:21:51.264: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename services 05/09/23 16:21:51.265
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:21:51.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:21:51.286
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6297 05/09/23 16:21:51.292
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/09/23 16:21:51.308
    STEP: creating service externalsvc in namespace services-6297 05/09/23 16:21:51.308
    STEP: creating replication controller externalsvc in namespace services-6297 05/09/23 16:21:51.322
    I0509 16:21:51.328869      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6297, replica count: 2
    I0509 16:21:54.379804      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 05/09/23 16:21:54.385
    May  9 16:21:54.407: INFO: Creating new exec pod
    May  9 16:21:54.412: INFO: Waiting up to 5m0s for pod "execpodrbf9k" in namespace "services-6297" to be "running"
    May  9 16:21:54.418: INFO: Pod "execpodrbf9k": Phase="Pending", Reason="", readiness=false. Elapsed: 5.055256ms
    May  9 16:21:56.427: INFO: Pod "execpodrbf9k": Phase="Running", Reason="", readiness=true. Elapsed: 2.014734843s
    May  9 16:21:56.427: INFO: Pod "execpodrbf9k" satisfied condition "running"
    May  9 16:21:56.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-6297 exec execpodrbf9k -- /bin/sh -x -c nslookup clusterip-service.services-6297.svc.cluster.local'
    May  9 16:21:56.778: INFO: stderr: "+ nslookup clusterip-service.services-6297.svc.cluster.local\n"
    May  9 16:21:56.778: INFO: stdout: "Server:\t\t10.3.0.10\nAddress:\t10.3.0.10#53\n\nclusterip-service.services-6297.svc.cluster.local\tcanonical name = externalsvc.services-6297.svc.cluster.local.\nName:\texternalsvc.services-6297.svc.cluster.local\nAddress: 10.3.13.141\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-6297, will wait for the garbage collector to delete the pods 05/09/23 16:21:56.778
    May  9 16:21:56.843: INFO: Deleting ReplicationController externalsvc took: 7.586049ms
    May  9 16:21:56.943: INFO: Terminating ReplicationController externalsvc pods took: 100.525924ms
    May  9 16:21:59.067: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  9 16:21:59.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6297" for this suite. 05/09/23 16:21:59.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:21:59.101
May  9 16:21:59.101: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename svcaccounts 05/09/23 16:21:59.102
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:21:59.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:21:59.121
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
May  9 16:21:59.141: INFO: Waiting up to 5m0s for pod "pod-service-account-6a072696-00e0-412c-8c5a-a9126f8ac6b8" in namespace "svcaccounts-7033" to be "running"
May  9 16:21:59.146: INFO: Pod "pod-service-account-6a072696-00e0-412c-8c5a-a9126f8ac6b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.637546ms
May  9 16:22:01.153: INFO: Pod "pod-service-account-6a072696-00e0-412c-8c5a-a9126f8ac6b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.012597328s
May  9 16:22:01.154: INFO: Pod "pod-service-account-6a072696-00e0-412c-8c5a-a9126f8ac6b8" satisfied condition "running"
STEP: reading a file in the container 05/09/23 16:22:01.154
May  9 16:22:01.154: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7033 pod-service-account-6a072696-00e0-412c-8c5a-a9126f8ac6b8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 05/09/23 16:22:01.397
May  9 16:22:01.398: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7033 pod-service-account-6a072696-00e0-412c-8c5a-a9126f8ac6b8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 05/09/23 16:22:01.636
May  9 16:22:01.636: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7033 pod-service-account-6a072696-00e0-412c-8c5a-a9126f8ac6b8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
May  9 16:22:01.878: INFO: Got root ca configmap in namespace "svcaccounts-7033"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May  9 16:22:01.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7033" for this suite. 05/09/23 16:22:01.888
------------------------------
â€¢ [2.795 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:21:59.101
    May  9 16:21:59.101: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename svcaccounts 05/09/23 16:21:59.102
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:21:59.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:21:59.121
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    May  9 16:21:59.141: INFO: Waiting up to 5m0s for pod "pod-service-account-6a072696-00e0-412c-8c5a-a9126f8ac6b8" in namespace "svcaccounts-7033" to be "running"
    May  9 16:21:59.146: INFO: Pod "pod-service-account-6a072696-00e0-412c-8c5a-a9126f8ac6b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.637546ms
    May  9 16:22:01.153: INFO: Pod "pod-service-account-6a072696-00e0-412c-8c5a-a9126f8ac6b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.012597328s
    May  9 16:22:01.154: INFO: Pod "pod-service-account-6a072696-00e0-412c-8c5a-a9126f8ac6b8" satisfied condition "running"
    STEP: reading a file in the container 05/09/23 16:22:01.154
    May  9 16:22:01.154: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7033 pod-service-account-6a072696-00e0-412c-8c5a-a9126f8ac6b8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 05/09/23 16:22:01.397
    May  9 16:22:01.398: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7033 pod-service-account-6a072696-00e0-412c-8c5a-a9126f8ac6b8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 05/09/23 16:22:01.636
    May  9 16:22:01.636: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7033 pod-service-account-6a072696-00e0-412c-8c5a-a9126f8ac6b8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    May  9 16:22:01.878: INFO: Got root ca configmap in namespace "svcaccounts-7033"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May  9 16:22:01.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7033" for this suite. 05/09/23 16:22:01.888
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:22:01.896
May  9 16:22:01.896: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename dns 05/09/23 16:22:01.897
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:22:01.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:22:01.918
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 05/09/23 16:22:01.923
May  9 16:22:01.933: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8010  24c5afd0-6c28-46bb-919a-1576336e79eb 317991650 0 2023-05-09 16:22:01 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-09 16:22:01 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rp8t6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rp8t6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 16:22:01.934: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8010" to be "running and ready"
May  9 16:22:01.938: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.48798ms
May  9 16:22:01.938: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May  9 16:22:03.951: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.017612713s
May  9 16:22:03.951: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
May  9 16:22:03.951: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 05/09/23 16:22:03.951
May  9 16:22:03.951: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8010 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:22:03.951: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:22:03.952: INFO: ExecWithOptions: Clientset creation
May  9 16:22:03.952: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/dns-8010/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 05/09/23 16:22:04.132
May  9 16:22:04.132: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8010 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:22:04.132: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:22:04.133: INFO: ExecWithOptions: Clientset creation
May  9 16:22:04.133: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/dns-8010/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  9 16:22:04.331: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May  9 16:22:04.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8010" for this suite. 05/09/23 16:22:04.353
------------------------------
â€¢ [2.467 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:22:01.896
    May  9 16:22:01.896: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename dns 05/09/23 16:22:01.897
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:22:01.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:22:01.918
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 05/09/23 16:22:01.923
    May  9 16:22:01.933: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8010  24c5afd0-6c28-46bb-919a-1576336e79eb 317991650 0 2023-05-09 16:22:01 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-09 16:22:01 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rp8t6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rp8t6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 16:22:01.934: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8010" to be "running and ready"
    May  9 16:22:01.938: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.48798ms
    May  9 16:22:01.938: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:22:03.951: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.017612713s
    May  9 16:22:03.951: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    May  9 16:22:03.951: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 05/09/23 16:22:03.951
    May  9 16:22:03.951: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8010 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:22:03.951: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:22:03.952: INFO: ExecWithOptions: Clientset creation
    May  9 16:22:03.952: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/dns-8010/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 05/09/23 16:22:04.132
    May  9 16:22:04.132: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8010 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:22:04.132: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:22:04.133: INFO: ExecWithOptions: Clientset creation
    May  9 16:22:04.133: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/dns-8010/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  9 16:22:04.331: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May  9 16:22:04.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8010" for this suite. 05/09/23 16:22:04.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:22:04.366
May  9 16:22:04.366: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename pods 05/09/23 16:22:04.366
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:22:04.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:22:04.405
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 05/09/23 16:22:04.411
STEP: submitting the pod to kubernetes 05/09/23 16:22:04.411
May  9 16:22:04.427: INFO: Waiting up to 5m0s for pod "pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d" in namespace "pods-4987" to be "running and ready"
May  9 16:22:04.434: INFO: Pod "pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.155629ms
May  9 16:22:04.434: INFO: The phase of Pod pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d is Pending, waiting for it to be Running (with Ready = true)
May  9 16:22:06.445: INFO: Pod "pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.018028996s
May  9 16:22:06.445: INFO: The phase of Pod pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d is Running (Ready = true)
May  9 16:22:06.445: INFO: Pod "pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 05/09/23 16:22:06.451
STEP: updating the pod 05/09/23 16:22:06.457
May  9 16:22:06.969: INFO: Successfully updated pod "pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d"
May  9 16:22:06.969: INFO: Waiting up to 5m0s for pod "pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d" in namespace "pods-4987" to be "running"
May  9 16:22:06.977: INFO: Pod "pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d": Phase="Running", Reason="", readiness=true. Elapsed: 7.712893ms
May  9 16:22:06.977: INFO: Pod "pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 05/09/23 16:22:06.977
May  9 16:22:06.993: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  9 16:22:06.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4987" for this suite. 05/09/23 16:22:07.004
------------------------------
â€¢ [2.649 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:22:04.366
    May  9 16:22:04.366: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename pods 05/09/23 16:22:04.366
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:22:04.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:22:04.405
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 05/09/23 16:22:04.411
    STEP: submitting the pod to kubernetes 05/09/23 16:22:04.411
    May  9 16:22:04.427: INFO: Waiting up to 5m0s for pod "pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d" in namespace "pods-4987" to be "running and ready"
    May  9 16:22:04.434: INFO: Pod "pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.155629ms
    May  9 16:22:04.434: INFO: The phase of Pod pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:22:06.445: INFO: Pod "pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.018028996s
    May  9 16:22:06.445: INFO: The phase of Pod pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d is Running (Ready = true)
    May  9 16:22:06.445: INFO: Pod "pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 05/09/23 16:22:06.451
    STEP: updating the pod 05/09/23 16:22:06.457
    May  9 16:22:06.969: INFO: Successfully updated pod "pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d"
    May  9 16:22:06.969: INFO: Waiting up to 5m0s for pod "pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d" in namespace "pods-4987" to be "running"
    May  9 16:22:06.977: INFO: Pod "pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d": Phase="Running", Reason="", readiness=true. Elapsed: 7.712893ms
    May  9 16:22:06.977: INFO: Pod "pod-update-89f883cf-ec3b-4747-90f7-829f1af17a9d" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 05/09/23 16:22:06.977
    May  9 16:22:06.993: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  9 16:22:06.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4987" for this suite. 05/09/23 16:22:07.004
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:22:07.017
May  9 16:22:07.017: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename configmap 05/09/23 16:22:07.018
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:22:07.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:22:07.038
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-cb4a0f25-5adb-4776-af49-302cc9f48456 05/09/23 16:22:07.042
STEP: Creating a pod to test consume configMaps 05/09/23 16:22:07.047
May  9 16:22:07.060: INFO: Waiting up to 5m0s for pod "pod-configmaps-302c451b-f2c4-4c75-9d03-2d3a9fbfc48e" in namespace "configmap-2274" to be "Succeeded or Failed"
May  9 16:22:07.066: INFO: Pod "pod-configmaps-302c451b-f2c4-4c75-9d03-2d3a9fbfc48e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.990598ms
May  9 16:22:09.080: INFO: Pod "pod-configmaps-302c451b-f2c4-4c75-9d03-2d3a9fbfc48e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019917574s
May  9 16:22:11.072: INFO: Pod "pod-configmaps-302c451b-f2c4-4c75-9d03-2d3a9fbfc48e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012590218s
STEP: Saw pod success 05/09/23 16:22:11.072
May  9 16:22:11.072: INFO: Pod "pod-configmaps-302c451b-f2c4-4c75-9d03-2d3a9fbfc48e" satisfied condition "Succeeded or Failed"
May  9 16:22:11.079: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 pod pod-configmaps-302c451b-f2c4-4c75-9d03-2d3a9fbfc48e container agnhost-container: <nil>
STEP: delete the pod 05/09/23 16:22:11.132
May  9 16:22:11.147: INFO: Waiting for pod pod-configmaps-302c451b-f2c4-4c75-9d03-2d3a9fbfc48e to disappear
May  9 16:22:11.152: INFO: Pod pod-configmaps-302c451b-f2c4-4c75-9d03-2d3a9fbfc48e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  9 16:22:11.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2274" for this suite. 05/09/23 16:22:11.165
------------------------------
â€¢ [4.158 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:22:07.017
    May  9 16:22:07.017: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename configmap 05/09/23 16:22:07.018
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:22:07.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:22:07.038
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-cb4a0f25-5adb-4776-af49-302cc9f48456 05/09/23 16:22:07.042
    STEP: Creating a pod to test consume configMaps 05/09/23 16:22:07.047
    May  9 16:22:07.060: INFO: Waiting up to 5m0s for pod "pod-configmaps-302c451b-f2c4-4c75-9d03-2d3a9fbfc48e" in namespace "configmap-2274" to be "Succeeded or Failed"
    May  9 16:22:07.066: INFO: Pod "pod-configmaps-302c451b-f2c4-4c75-9d03-2d3a9fbfc48e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.990598ms
    May  9 16:22:09.080: INFO: Pod "pod-configmaps-302c451b-f2c4-4c75-9d03-2d3a9fbfc48e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019917574s
    May  9 16:22:11.072: INFO: Pod "pod-configmaps-302c451b-f2c4-4c75-9d03-2d3a9fbfc48e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012590218s
    STEP: Saw pod success 05/09/23 16:22:11.072
    May  9 16:22:11.072: INFO: Pod "pod-configmaps-302c451b-f2c4-4c75-9d03-2d3a9fbfc48e" satisfied condition "Succeeded or Failed"
    May  9 16:22:11.079: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 pod pod-configmaps-302c451b-f2c4-4c75-9d03-2d3a9fbfc48e container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 16:22:11.132
    May  9 16:22:11.147: INFO: Waiting for pod pod-configmaps-302c451b-f2c4-4c75-9d03-2d3a9fbfc48e to disappear
    May  9 16:22:11.152: INFO: Pod pod-configmaps-302c451b-f2c4-4c75-9d03-2d3a9fbfc48e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  9 16:22:11.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2274" for this suite. 05/09/23 16:22:11.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:22:11.185
May  9 16:22:11.185: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename resourcequota 05/09/23 16:22:11.186
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:22:11.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:22:11.213
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 05/09/23 16:22:11.217
STEP: Ensuring ResourceQuota status is calculated 05/09/23 16:22:11.224
STEP: Creating a ResourceQuota with not best effort scope 05/09/23 16:22:13.23
STEP: Ensuring ResourceQuota status is calculated 05/09/23 16:22:13.237
STEP: Creating a best-effort pod 05/09/23 16:22:15.243
STEP: Ensuring resource quota with best effort scope captures the pod usage 05/09/23 16:22:15.259
STEP: Ensuring resource quota with not best effort ignored the pod usage 05/09/23 16:22:17.266
STEP: Deleting the pod 05/09/23 16:22:19.272
STEP: Ensuring resource quota status released the pod usage 05/09/23 16:22:19.289
STEP: Creating a not best-effort pod 05/09/23 16:22:21.297
STEP: Ensuring resource quota with not best effort scope captures the pod usage 05/09/23 16:22:21.311
STEP: Ensuring resource quota with best effort scope ignored the pod usage 05/09/23 16:22:23.317
STEP: Deleting the pod 05/09/23 16:22:25.324
STEP: Ensuring resource quota status released the pod usage 05/09/23 16:22:25.336
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  9 16:22:27.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3530" for this suite. 05/09/23 16:22:27.349
------------------------------
â€¢ [SLOW TEST] [16.174 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:22:11.185
    May  9 16:22:11.185: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename resourcequota 05/09/23 16:22:11.186
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:22:11.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:22:11.213
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 05/09/23 16:22:11.217
    STEP: Ensuring ResourceQuota status is calculated 05/09/23 16:22:11.224
    STEP: Creating a ResourceQuota with not best effort scope 05/09/23 16:22:13.23
    STEP: Ensuring ResourceQuota status is calculated 05/09/23 16:22:13.237
    STEP: Creating a best-effort pod 05/09/23 16:22:15.243
    STEP: Ensuring resource quota with best effort scope captures the pod usage 05/09/23 16:22:15.259
    STEP: Ensuring resource quota with not best effort ignored the pod usage 05/09/23 16:22:17.266
    STEP: Deleting the pod 05/09/23 16:22:19.272
    STEP: Ensuring resource quota status released the pod usage 05/09/23 16:22:19.289
    STEP: Creating a not best-effort pod 05/09/23 16:22:21.297
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 05/09/23 16:22:21.311
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 05/09/23 16:22:23.317
    STEP: Deleting the pod 05/09/23 16:22:25.324
    STEP: Ensuring resource quota status released the pod usage 05/09/23 16:22:25.336
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  9 16:22:27.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3530" for this suite. 05/09/23 16:22:27.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:22:27.361
May  9 16:22:27.361: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename configmap 05/09/23 16:22:27.362
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:22:27.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:22:27.384
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-7bb32025-f4b5-4c9c-a290-4593edb4b04e 05/09/23 16:22:27.389
STEP: Creating a pod to test consume configMaps 05/09/23 16:22:27.396
May  9 16:22:27.406: INFO: Waiting up to 5m0s for pod "pod-configmaps-d99a8d77-13f6-4153-b7a5-8b1d5e4e4f46" in namespace "configmap-9046" to be "Succeeded or Failed"
May  9 16:22:27.411: INFO: Pod "pod-configmaps-d99a8d77-13f6-4153-b7a5-8b1d5e4e4f46": Phase="Pending", Reason="", readiness=false. Elapsed: 5.378782ms
May  9 16:22:29.420: INFO: Pod "pod-configmaps-d99a8d77-13f6-4153-b7a5-8b1d5e4e4f46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01402134s
May  9 16:22:31.438: INFO: Pod "pod-configmaps-d99a8d77-13f6-4153-b7a5-8b1d5e4e4f46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032094448s
STEP: Saw pod success 05/09/23 16:22:31.438
May  9 16:22:31.438: INFO: Pod "pod-configmaps-d99a8d77-13f6-4153-b7a5-8b1d5e4e4f46" satisfied condition "Succeeded or Failed"
May  9 16:22:31.445: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-configmaps-d99a8d77-13f6-4153-b7a5-8b1d5e4e4f46 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 16:22:31.495
May  9 16:22:31.513: INFO: Waiting for pod pod-configmaps-d99a8d77-13f6-4153-b7a5-8b1d5e4e4f46 to disappear
May  9 16:22:31.517: INFO: Pod pod-configmaps-d99a8d77-13f6-4153-b7a5-8b1d5e4e4f46 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  9 16:22:31.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9046" for this suite. 05/09/23 16:22:31.533
------------------------------
â€¢ [4.193 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:22:27.361
    May  9 16:22:27.361: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename configmap 05/09/23 16:22:27.362
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:22:27.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:22:27.384
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-7bb32025-f4b5-4c9c-a290-4593edb4b04e 05/09/23 16:22:27.389
    STEP: Creating a pod to test consume configMaps 05/09/23 16:22:27.396
    May  9 16:22:27.406: INFO: Waiting up to 5m0s for pod "pod-configmaps-d99a8d77-13f6-4153-b7a5-8b1d5e4e4f46" in namespace "configmap-9046" to be "Succeeded or Failed"
    May  9 16:22:27.411: INFO: Pod "pod-configmaps-d99a8d77-13f6-4153-b7a5-8b1d5e4e4f46": Phase="Pending", Reason="", readiness=false. Elapsed: 5.378782ms
    May  9 16:22:29.420: INFO: Pod "pod-configmaps-d99a8d77-13f6-4153-b7a5-8b1d5e4e4f46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01402134s
    May  9 16:22:31.438: INFO: Pod "pod-configmaps-d99a8d77-13f6-4153-b7a5-8b1d5e4e4f46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032094448s
    STEP: Saw pod success 05/09/23 16:22:31.438
    May  9 16:22:31.438: INFO: Pod "pod-configmaps-d99a8d77-13f6-4153-b7a5-8b1d5e4e4f46" satisfied condition "Succeeded or Failed"
    May  9 16:22:31.445: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-configmaps-d99a8d77-13f6-4153-b7a5-8b1d5e4e4f46 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 16:22:31.495
    May  9 16:22:31.513: INFO: Waiting for pod pod-configmaps-d99a8d77-13f6-4153-b7a5-8b1d5e4e4f46 to disappear
    May  9 16:22:31.517: INFO: Pod pod-configmaps-d99a8d77-13f6-4153-b7a5-8b1d5e4e4f46 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  9 16:22:31.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9046" for this suite. 05/09/23 16:22:31.533
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:22:31.554
May  9 16:22:31.554: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename sched-preemption 05/09/23 16:22:31.555
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:22:31.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:22:31.579
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
May  9 16:22:31.603: INFO: Waiting up to 1m0s for all nodes to be ready
May  9 16:23:31.655: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:23:31.663
May  9 16:23:31.663: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename sched-preemption-path 05/09/23 16:23:31.664
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:23:31.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:23:31.688
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
May  9 16:23:31.710: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
May  9 16:23:31.714: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
May  9 16:23:31.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 16:23:31.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-1783" for this suite. 05/09/23 16:23:31.841
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1854" for this suite. 05/09/23 16:23:31.849
------------------------------
â€¢ [SLOW TEST] [60.303 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:22:31.554
    May  9 16:22:31.554: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename sched-preemption 05/09/23 16:22:31.555
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:22:31.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:22:31.579
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    May  9 16:22:31.603: INFO: Waiting up to 1m0s for all nodes to be ready
    May  9 16:23:31.655: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:23:31.663
    May  9 16:23:31.663: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename sched-preemption-path 05/09/23 16:23:31.664
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:23:31.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:23:31.688
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    May  9 16:23:31.710: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    May  9 16:23:31.714: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    May  9 16:23:31.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 16:23:31.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-1783" for this suite. 05/09/23 16:23:31.841
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1854" for this suite. 05/09/23 16:23:31.849
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:23:31.858
May  9 16:23:31.858: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 16:23:31.858
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:23:31.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:23:31.886
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 05/09/23 16:23:31.894
May  9 16:23:31.904: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a47c09f2-1864-4fbe-980e-6bf85ea07206" in namespace "projected-2420" to be "Succeeded or Failed"
May  9 16:23:31.910: INFO: Pod "downwardapi-volume-a47c09f2-1864-4fbe-980e-6bf85ea07206": Phase="Pending", Reason="", readiness=false. Elapsed: 6.158293ms
May  9 16:23:33.918: INFO: Pod "downwardapi-volume-a47c09f2-1864-4fbe-980e-6bf85ea07206": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013746107s
May  9 16:23:35.919: INFO: Pod "downwardapi-volume-a47c09f2-1864-4fbe-980e-6bf85ea07206": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014372067s
STEP: Saw pod success 05/09/23 16:23:35.919
May  9 16:23:35.919: INFO: Pod "downwardapi-volume-a47c09f2-1864-4fbe-980e-6bf85ea07206" satisfied condition "Succeeded or Failed"
May  9 16:23:35.923: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-a47c09f2-1864-4fbe-980e-6bf85ea07206 container client-container: <nil>
STEP: delete the pod 05/09/23 16:23:35.941
May  9 16:23:35.958: INFO: Waiting for pod downwardapi-volume-a47c09f2-1864-4fbe-980e-6bf85ea07206 to disappear
May  9 16:23:35.962: INFO: Pod downwardapi-volume-a47c09f2-1864-4fbe-980e-6bf85ea07206 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  9 16:23:35.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2420" for this suite. 05/09/23 16:23:35.971
------------------------------
â€¢ [4.123 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:23:31.858
    May  9 16:23:31.858: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 16:23:31.858
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:23:31.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:23:31.886
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 05/09/23 16:23:31.894
    May  9 16:23:31.904: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a47c09f2-1864-4fbe-980e-6bf85ea07206" in namespace "projected-2420" to be "Succeeded or Failed"
    May  9 16:23:31.910: INFO: Pod "downwardapi-volume-a47c09f2-1864-4fbe-980e-6bf85ea07206": Phase="Pending", Reason="", readiness=false. Elapsed: 6.158293ms
    May  9 16:23:33.918: INFO: Pod "downwardapi-volume-a47c09f2-1864-4fbe-980e-6bf85ea07206": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013746107s
    May  9 16:23:35.919: INFO: Pod "downwardapi-volume-a47c09f2-1864-4fbe-980e-6bf85ea07206": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014372067s
    STEP: Saw pod success 05/09/23 16:23:35.919
    May  9 16:23:35.919: INFO: Pod "downwardapi-volume-a47c09f2-1864-4fbe-980e-6bf85ea07206" satisfied condition "Succeeded or Failed"
    May  9 16:23:35.923: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-a47c09f2-1864-4fbe-980e-6bf85ea07206 container client-container: <nil>
    STEP: delete the pod 05/09/23 16:23:35.941
    May  9 16:23:35.958: INFO: Waiting for pod downwardapi-volume-a47c09f2-1864-4fbe-980e-6bf85ea07206 to disappear
    May  9 16:23:35.962: INFO: Pod downwardapi-volume-a47c09f2-1864-4fbe-980e-6bf85ea07206 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  9 16:23:35.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2420" for this suite. 05/09/23 16:23:35.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:23:35.986
May  9 16:23:35.986: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename certificates 05/09/23 16:23:35.987
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:23:36.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:23:36.008
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 05/09/23 16:23:37.079
STEP: getting /apis/certificates.k8s.io 05/09/23 16:23:37.083
STEP: getting /apis/certificates.k8s.io/v1 05/09/23 16:23:37.085
STEP: creating 05/09/23 16:23:37.087
STEP: getting 05/09/23 16:23:37.106
STEP: listing 05/09/23 16:23:37.112
STEP: watching 05/09/23 16:23:37.123
May  9 16:23:37.123: INFO: starting watch
STEP: patching 05/09/23 16:23:37.125
STEP: updating 05/09/23 16:23:37.132
May  9 16:23:37.142: INFO: waiting for watch events with expected annotations
May  9 16:23:37.142: INFO: saw patched and updated annotations
STEP: getting /approval 05/09/23 16:23:37.142
STEP: patching /approval 05/09/23 16:23:37.152
STEP: updating /approval 05/09/23 16:23:37.172
STEP: getting /status 05/09/23 16:23:37.178
STEP: patching /status 05/09/23 16:23:37.183
STEP: updating /status 05/09/23 16:23:37.191
STEP: deleting 05/09/23 16:23:37.2
STEP: deleting a collection 05/09/23 16:23:37.218
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:23:37.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-18" for this suite. 05/09/23 16:23:37.247
------------------------------
â€¢ [1.269 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:23:35.986
    May  9 16:23:35.986: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename certificates 05/09/23 16:23:35.987
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:23:36.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:23:36.008
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 05/09/23 16:23:37.079
    STEP: getting /apis/certificates.k8s.io 05/09/23 16:23:37.083
    STEP: getting /apis/certificates.k8s.io/v1 05/09/23 16:23:37.085
    STEP: creating 05/09/23 16:23:37.087
    STEP: getting 05/09/23 16:23:37.106
    STEP: listing 05/09/23 16:23:37.112
    STEP: watching 05/09/23 16:23:37.123
    May  9 16:23:37.123: INFO: starting watch
    STEP: patching 05/09/23 16:23:37.125
    STEP: updating 05/09/23 16:23:37.132
    May  9 16:23:37.142: INFO: waiting for watch events with expected annotations
    May  9 16:23:37.142: INFO: saw patched and updated annotations
    STEP: getting /approval 05/09/23 16:23:37.142
    STEP: patching /approval 05/09/23 16:23:37.152
    STEP: updating /approval 05/09/23 16:23:37.172
    STEP: getting /status 05/09/23 16:23:37.178
    STEP: patching /status 05/09/23 16:23:37.183
    STEP: updating /status 05/09/23 16:23:37.191
    STEP: deleting 05/09/23 16:23:37.2
    STEP: deleting a collection 05/09/23 16:23:37.218
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:23:37.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-18" for this suite. 05/09/23 16:23:37.247
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:23:37.255
May  9 16:23:37.255: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 16:23:37.256
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:23:37.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:23:37.294
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-c5c6b8ac-f5e1-46c7-9590-669e0ac169eb 05/09/23 16:23:37.299
STEP: Creating a pod to test consume secrets 05/09/23 16:23:37.306
May  9 16:23:37.318: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0719f445-4b85-4da8-8c93-22f65f0c68b9" in namespace "projected-6474" to be "Succeeded or Failed"
May  9 16:23:37.322: INFO: Pod "pod-projected-secrets-0719f445-4b85-4da8-8c93-22f65f0c68b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.361871ms
May  9 16:23:39.328: INFO: Pod "pod-projected-secrets-0719f445-4b85-4da8-8c93-22f65f0c68b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010562547s
May  9 16:23:41.330: INFO: Pod "pod-projected-secrets-0719f445-4b85-4da8-8c93-22f65f0c68b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012573709s
STEP: Saw pod success 05/09/23 16:23:41.33
May  9 16:23:41.330: INFO: Pod "pod-projected-secrets-0719f445-4b85-4da8-8c93-22f65f0c68b9" satisfied condition "Succeeded or Failed"
May  9 16:23:41.335: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-secrets-0719f445-4b85-4da8-8c93-22f65f0c68b9 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/09/23 16:23:41.346
May  9 16:23:41.360: INFO: Waiting for pod pod-projected-secrets-0719f445-4b85-4da8-8c93-22f65f0c68b9 to disappear
May  9 16:23:41.363: INFO: Pod pod-projected-secrets-0719f445-4b85-4da8-8c93-22f65f0c68b9 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May  9 16:23:41.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6474" for this suite. 05/09/23 16:23:41.37
------------------------------
â€¢ [4.123 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:23:37.255
    May  9 16:23:37.255: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 16:23:37.256
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:23:37.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:23:37.294
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-c5c6b8ac-f5e1-46c7-9590-669e0ac169eb 05/09/23 16:23:37.299
    STEP: Creating a pod to test consume secrets 05/09/23 16:23:37.306
    May  9 16:23:37.318: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0719f445-4b85-4da8-8c93-22f65f0c68b9" in namespace "projected-6474" to be "Succeeded or Failed"
    May  9 16:23:37.322: INFO: Pod "pod-projected-secrets-0719f445-4b85-4da8-8c93-22f65f0c68b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.361871ms
    May  9 16:23:39.328: INFO: Pod "pod-projected-secrets-0719f445-4b85-4da8-8c93-22f65f0c68b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010562547s
    May  9 16:23:41.330: INFO: Pod "pod-projected-secrets-0719f445-4b85-4da8-8c93-22f65f0c68b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012573709s
    STEP: Saw pod success 05/09/23 16:23:41.33
    May  9 16:23:41.330: INFO: Pod "pod-projected-secrets-0719f445-4b85-4da8-8c93-22f65f0c68b9" satisfied condition "Succeeded or Failed"
    May  9 16:23:41.335: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-secrets-0719f445-4b85-4da8-8c93-22f65f0c68b9 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 16:23:41.346
    May  9 16:23:41.360: INFO: Waiting for pod pod-projected-secrets-0719f445-4b85-4da8-8c93-22f65f0c68b9 to disappear
    May  9 16:23:41.363: INFO: Pod pod-projected-secrets-0719f445-4b85-4da8-8c93-22f65f0c68b9 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May  9 16:23:41.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6474" for this suite. 05/09/23 16:23:41.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:23:41.379
May  9 16:23:41.379: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename endpointslice 05/09/23 16:23:41.379
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:23:41.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:23:41.4
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
May  9 16:23:41.416: INFO: Endpoints addresses: [10.3.0.1] , ports: [6443]
May  9 16:23:41.417: INFO: EndpointSlices addresses: [10.3.0.1] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
May  9 16:23:41.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-8670" for this suite. 05/09/23 16:23:41.422
------------------------------
â€¢ [0.052 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:23:41.379
    May  9 16:23:41.379: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename endpointslice 05/09/23 16:23:41.379
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:23:41.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:23:41.4
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    May  9 16:23:41.416: INFO: Endpoints addresses: [10.3.0.1] , ports: [6443]
    May  9 16:23:41.417: INFO: EndpointSlices addresses: [10.3.0.1] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    May  9 16:23:41.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-8670" for this suite. 05/09/23 16:23:41.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:23:41.431
May  9 16:23:41.431: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename configmap 05/09/23 16:23:41.432
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:23:41.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:23:41.451
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 05/09/23 16:23:41.455
STEP: fetching the ConfigMap 05/09/23 16:23:41.461
STEP: patching the ConfigMap 05/09/23 16:23:41.468
STEP: listing all ConfigMaps in all namespaces with a label selector 05/09/23 16:23:41.474
STEP: deleting the ConfigMap by collection with a label selector 05/09/23 16:23:41.48
STEP: listing all ConfigMaps in test namespace 05/09/23 16:23:41.49
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  9 16:23:41.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4411" for this suite. 05/09/23 16:23:41.5
------------------------------
â€¢ [0.078 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:23:41.431
    May  9 16:23:41.431: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename configmap 05/09/23 16:23:41.432
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:23:41.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:23:41.451
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 05/09/23 16:23:41.455
    STEP: fetching the ConfigMap 05/09/23 16:23:41.461
    STEP: patching the ConfigMap 05/09/23 16:23:41.468
    STEP: listing all ConfigMaps in all namespaces with a label selector 05/09/23 16:23:41.474
    STEP: deleting the ConfigMap by collection with a label selector 05/09/23 16:23:41.48
    STEP: listing all ConfigMaps in test namespace 05/09/23 16:23:41.49
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  9 16:23:41.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4411" for this suite. 05/09/23 16:23:41.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:23:41.509
May  9 16:23:41.509: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename secrets 05/09/23 16:23:41.51
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:23:41.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:23:41.579
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-6987b04c-7ec1-49c4-9d94-594905b7919e 05/09/23 16:23:41.583
STEP: Creating a pod to test consume secrets 05/09/23 16:23:41.6
May  9 16:23:41.661: INFO: Waiting up to 5m0s for pod "pod-secrets-5ac2f44c-9f0e-4c0c-8ed9-fbd336939b81" in namespace "secrets-7164" to be "Succeeded or Failed"
May  9 16:23:41.675: INFO: Pod "pod-secrets-5ac2f44c-9f0e-4c0c-8ed9-fbd336939b81": Phase="Pending", Reason="", readiness=false. Elapsed: 13.902936ms
May  9 16:23:43.682: INFO: Pod "pod-secrets-5ac2f44c-9f0e-4c0c-8ed9-fbd336939b81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021122124s
May  9 16:23:45.682: INFO: Pod "pod-secrets-5ac2f44c-9f0e-4c0c-8ed9-fbd336939b81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020714338s
STEP: Saw pod success 05/09/23 16:23:45.682
May  9 16:23:45.682: INFO: Pod "pod-secrets-5ac2f44c-9f0e-4c0c-8ed9-fbd336939b81" satisfied condition "Succeeded or Failed"
May  9 16:23:45.687: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-secrets-5ac2f44c-9f0e-4c0c-8ed9-fbd336939b81 container secret-volume-test: <nil>
STEP: delete the pod 05/09/23 16:23:45.7
May  9 16:23:45.728: INFO: Waiting for pod pod-secrets-5ac2f44c-9f0e-4c0c-8ed9-fbd336939b81 to disappear
May  9 16:23:45.750: INFO: Pod pod-secrets-5ac2f44c-9f0e-4c0c-8ed9-fbd336939b81 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May  9 16:23:45.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7164" for this suite. 05/09/23 16:23:45.758
------------------------------
â€¢ [4.263 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:23:41.509
    May  9 16:23:41.509: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename secrets 05/09/23 16:23:41.51
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:23:41.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:23:41.579
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-6987b04c-7ec1-49c4-9d94-594905b7919e 05/09/23 16:23:41.583
    STEP: Creating a pod to test consume secrets 05/09/23 16:23:41.6
    May  9 16:23:41.661: INFO: Waiting up to 5m0s for pod "pod-secrets-5ac2f44c-9f0e-4c0c-8ed9-fbd336939b81" in namespace "secrets-7164" to be "Succeeded or Failed"
    May  9 16:23:41.675: INFO: Pod "pod-secrets-5ac2f44c-9f0e-4c0c-8ed9-fbd336939b81": Phase="Pending", Reason="", readiness=false. Elapsed: 13.902936ms
    May  9 16:23:43.682: INFO: Pod "pod-secrets-5ac2f44c-9f0e-4c0c-8ed9-fbd336939b81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021122124s
    May  9 16:23:45.682: INFO: Pod "pod-secrets-5ac2f44c-9f0e-4c0c-8ed9-fbd336939b81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020714338s
    STEP: Saw pod success 05/09/23 16:23:45.682
    May  9 16:23:45.682: INFO: Pod "pod-secrets-5ac2f44c-9f0e-4c0c-8ed9-fbd336939b81" satisfied condition "Succeeded or Failed"
    May  9 16:23:45.687: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-secrets-5ac2f44c-9f0e-4c0c-8ed9-fbd336939b81 container secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 16:23:45.7
    May  9 16:23:45.728: INFO: Waiting for pod pod-secrets-5ac2f44c-9f0e-4c0c-8ed9-fbd336939b81 to disappear
    May  9 16:23:45.750: INFO: Pod pod-secrets-5ac2f44c-9f0e-4c0c-8ed9-fbd336939b81 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May  9 16:23:45.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7164" for this suite. 05/09/23 16:23:45.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:23:45.773
May  9 16:23:45.774: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 16:23:45.774
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:23:45.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:23:45.801
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-72d83e4a-ad6d-459e-9cec-d9fe96e807a9 05/09/23 16:23:45.805
STEP: Creating a pod to test consume secrets 05/09/23 16:23:45.811
May  9 16:23:45.841: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9123fa7a-e284-499c-bbac-9ef7badf6ddc" in namespace "projected-5302" to be "Succeeded or Failed"
May  9 16:23:45.856: INFO: Pod "pod-projected-secrets-9123fa7a-e284-499c-bbac-9ef7badf6ddc": Phase="Pending", Reason="", readiness=false. Elapsed: 15.758732ms
May  9 16:23:47.863: INFO: Pod "pod-projected-secrets-9123fa7a-e284-499c-bbac-9ef7badf6ddc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022265651s
May  9 16:23:49.865: INFO: Pod "pod-projected-secrets-9123fa7a-e284-499c-bbac-9ef7badf6ddc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024110327s
STEP: Saw pod success 05/09/23 16:23:49.865
May  9 16:23:49.865: INFO: Pod "pod-projected-secrets-9123fa7a-e284-499c-bbac-9ef7badf6ddc" satisfied condition "Succeeded or Failed"
May  9 16:23:49.870: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-secrets-9123fa7a-e284-499c-bbac-9ef7badf6ddc container projected-secret-volume-test: <nil>
STEP: delete the pod 05/09/23 16:23:49.882
May  9 16:23:49.899: INFO: Waiting for pod pod-projected-secrets-9123fa7a-e284-499c-bbac-9ef7badf6ddc to disappear
May  9 16:23:49.904: INFO: Pod pod-projected-secrets-9123fa7a-e284-499c-bbac-9ef7badf6ddc no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May  9 16:23:49.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5302" for this suite. 05/09/23 16:23:49.909
------------------------------
â€¢ [4.147 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:23:45.773
    May  9 16:23:45.774: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 16:23:45.774
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:23:45.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:23:45.801
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-72d83e4a-ad6d-459e-9cec-d9fe96e807a9 05/09/23 16:23:45.805
    STEP: Creating a pod to test consume secrets 05/09/23 16:23:45.811
    May  9 16:23:45.841: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9123fa7a-e284-499c-bbac-9ef7badf6ddc" in namespace "projected-5302" to be "Succeeded or Failed"
    May  9 16:23:45.856: INFO: Pod "pod-projected-secrets-9123fa7a-e284-499c-bbac-9ef7badf6ddc": Phase="Pending", Reason="", readiness=false. Elapsed: 15.758732ms
    May  9 16:23:47.863: INFO: Pod "pod-projected-secrets-9123fa7a-e284-499c-bbac-9ef7badf6ddc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022265651s
    May  9 16:23:49.865: INFO: Pod "pod-projected-secrets-9123fa7a-e284-499c-bbac-9ef7badf6ddc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024110327s
    STEP: Saw pod success 05/09/23 16:23:49.865
    May  9 16:23:49.865: INFO: Pod "pod-projected-secrets-9123fa7a-e284-499c-bbac-9ef7badf6ddc" satisfied condition "Succeeded or Failed"
    May  9 16:23:49.870: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-secrets-9123fa7a-e284-499c-bbac-9ef7badf6ddc container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 16:23:49.882
    May  9 16:23:49.899: INFO: Waiting for pod pod-projected-secrets-9123fa7a-e284-499c-bbac-9ef7badf6ddc to disappear
    May  9 16:23:49.904: INFO: Pod pod-projected-secrets-9123fa7a-e284-499c-bbac-9ef7badf6ddc no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May  9 16:23:49.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5302" for this suite. 05/09/23 16:23:49.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:23:49.922
May  9 16:23:49.922: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename sched-preemption 05/09/23 16:23:49.923
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:23:49.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:23:49.943
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
May  9 16:23:49.965: INFO: Waiting up to 1m0s for all nodes to be ready
May  9 16:24:50.011: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:24:50.016
May  9 16:24:50.017: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename sched-preemption-path 05/09/23 16:24:50.017
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:24:50.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:24:50.041
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 05/09/23 16:24:50.045
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/09/23 16:24:50.045
May  9 16:24:50.057: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-948" to be "running"
May  9 16:24:50.063: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.190966ms
May  9 16:24:52.070: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.012696573s
May  9 16:24:52.070: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/09/23 16:24:52.075
May  9 16:24:52.094: INFO: found a healthy node: nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
May  9 16:25:00.194: INFO: pods created so far: [1 1 1]
May  9 16:25:00.194: INFO: length of pods created so far: 3
May  9 16:25:02.215: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
May  9 16:25:09.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 16:25:09.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-948" for this suite. 05/09/23 16:25:09.347
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-4890" for this suite. 05/09/23 16:25:09.358
------------------------------
â€¢ [SLOW TEST] [79.447 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:23:49.922
    May  9 16:23:49.922: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename sched-preemption 05/09/23 16:23:49.923
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:23:49.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:23:49.943
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    May  9 16:23:49.965: INFO: Waiting up to 1m0s for all nodes to be ready
    May  9 16:24:50.011: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:24:50.016
    May  9 16:24:50.017: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename sched-preemption-path 05/09/23 16:24:50.017
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:24:50.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:24:50.041
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 05/09/23 16:24:50.045
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/09/23 16:24:50.045
    May  9 16:24:50.057: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-948" to be "running"
    May  9 16:24:50.063: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.190966ms
    May  9 16:24:52.070: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.012696573s
    May  9 16:24:52.070: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/09/23 16:24:52.075
    May  9 16:24:52.094: INFO: found a healthy node: nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    May  9 16:25:00.194: INFO: pods created so far: [1 1 1]
    May  9 16:25:00.194: INFO: length of pods created so far: 3
    May  9 16:25:02.215: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    May  9 16:25:09.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 16:25:09.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-948" for this suite. 05/09/23 16:25:09.347
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-4890" for this suite. 05/09/23 16:25:09.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:25:09.37
May  9 16:25:09.370: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename security-context-test 05/09/23 16:25:09.372
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:25:09.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:25:09.399
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
May  9 16:25:09.422: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-699ecd36-d6d9-456b-bd55-f3a8122c6983" in namespace "security-context-test-3068" to be "Succeeded or Failed"
May  9 16:25:09.431: INFO: Pod "busybox-privileged-false-699ecd36-d6d9-456b-bd55-f3a8122c6983": Phase="Pending", Reason="", readiness=false. Elapsed: 9.205618ms
May  9 16:25:11.437: INFO: Pod "busybox-privileged-false-699ecd36-d6d9-456b-bd55-f3a8122c6983": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014779509s
May  9 16:25:13.442: INFO: Pod "busybox-privileged-false-699ecd36-d6d9-456b-bd55-f3a8122c6983": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020089386s
May  9 16:25:13.442: INFO: Pod "busybox-privileged-false-699ecd36-d6d9-456b-bd55-f3a8122c6983" satisfied condition "Succeeded or Failed"
May  9 16:25:13.501: INFO: Got logs for pod "busybox-privileged-false-699ecd36-d6d9-456b-bd55-f3a8122c6983": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May  9 16:25:13.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3068" for this suite. 05/09/23 16:25:13.509
------------------------------
â€¢ [4.148 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:25:09.37
    May  9 16:25:09.370: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename security-context-test 05/09/23 16:25:09.372
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:25:09.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:25:09.399
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    May  9 16:25:09.422: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-699ecd36-d6d9-456b-bd55-f3a8122c6983" in namespace "security-context-test-3068" to be "Succeeded or Failed"
    May  9 16:25:09.431: INFO: Pod "busybox-privileged-false-699ecd36-d6d9-456b-bd55-f3a8122c6983": Phase="Pending", Reason="", readiness=false. Elapsed: 9.205618ms
    May  9 16:25:11.437: INFO: Pod "busybox-privileged-false-699ecd36-d6d9-456b-bd55-f3a8122c6983": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014779509s
    May  9 16:25:13.442: INFO: Pod "busybox-privileged-false-699ecd36-d6d9-456b-bd55-f3a8122c6983": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020089386s
    May  9 16:25:13.442: INFO: Pod "busybox-privileged-false-699ecd36-d6d9-456b-bd55-f3a8122c6983" satisfied condition "Succeeded or Failed"
    May  9 16:25:13.501: INFO: Got logs for pod "busybox-privileged-false-699ecd36-d6d9-456b-bd55-f3a8122c6983": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May  9 16:25:13.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3068" for this suite. 05/09/23 16:25:13.509
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:25:13.519
May  9 16:25:13.519: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename subpath 05/09/23 16:25:13.52
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:25:13.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:25:13.541
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/09/23 16:25:13.547
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-xklv 05/09/23 16:25:13.559
STEP: Creating a pod to test atomic-volume-subpath 05/09/23 16:25:13.559
May  9 16:25:13.571: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xklv" in namespace "subpath-7434" to be "Succeeded or Failed"
May  9 16:25:13.579: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Pending", Reason="", readiness=false. Elapsed: 7.82696ms
May  9 16:25:15.584: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 2.01340178s
May  9 16:25:17.587: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 4.016525799s
May  9 16:25:19.586: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 6.015329236s
May  9 16:25:21.586: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 8.014842077s
May  9 16:25:23.586: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 10.015532682s
May  9 16:25:25.590: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 12.01897098s
May  9 16:25:27.585: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 14.014442701s
May  9 16:25:29.590: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 16.018986386s
May  9 16:25:31.595: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 18.024166197s
May  9 16:25:33.587: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 20.016534808s
May  9 16:25:35.587: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=false. Elapsed: 22.016096494s
May  9 16:25:37.585: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013788877s
STEP: Saw pod success 05/09/23 16:25:37.585
May  9 16:25:37.585: INFO: Pod "pod-subpath-test-configmap-xklv" satisfied condition "Succeeded or Failed"
May  9 16:25:37.589: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 pod pod-subpath-test-configmap-xklv container test-container-subpath-configmap-xklv: <nil>
STEP: delete the pod 05/09/23 16:25:37.601
May  9 16:25:37.618: INFO: Waiting for pod pod-subpath-test-configmap-xklv to disappear
May  9 16:25:37.623: INFO: Pod pod-subpath-test-configmap-xklv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xklv 05/09/23 16:25:37.623
May  9 16:25:37.623: INFO: Deleting pod "pod-subpath-test-configmap-xklv" in namespace "subpath-7434"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May  9 16:25:37.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7434" for this suite. 05/09/23 16:25:37.635
------------------------------
â€¢ [SLOW TEST] [24.126 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:25:13.519
    May  9 16:25:13.519: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename subpath 05/09/23 16:25:13.52
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:25:13.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:25:13.541
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/09/23 16:25:13.547
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-xklv 05/09/23 16:25:13.559
    STEP: Creating a pod to test atomic-volume-subpath 05/09/23 16:25:13.559
    May  9 16:25:13.571: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xklv" in namespace "subpath-7434" to be "Succeeded or Failed"
    May  9 16:25:13.579: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Pending", Reason="", readiness=false. Elapsed: 7.82696ms
    May  9 16:25:15.584: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 2.01340178s
    May  9 16:25:17.587: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 4.016525799s
    May  9 16:25:19.586: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 6.015329236s
    May  9 16:25:21.586: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 8.014842077s
    May  9 16:25:23.586: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 10.015532682s
    May  9 16:25:25.590: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 12.01897098s
    May  9 16:25:27.585: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 14.014442701s
    May  9 16:25:29.590: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 16.018986386s
    May  9 16:25:31.595: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 18.024166197s
    May  9 16:25:33.587: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=true. Elapsed: 20.016534808s
    May  9 16:25:35.587: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Running", Reason="", readiness=false. Elapsed: 22.016096494s
    May  9 16:25:37.585: INFO: Pod "pod-subpath-test-configmap-xklv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013788877s
    STEP: Saw pod success 05/09/23 16:25:37.585
    May  9 16:25:37.585: INFO: Pod "pod-subpath-test-configmap-xklv" satisfied condition "Succeeded or Failed"
    May  9 16:25:37.589: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 pod pod-subpath-test-configmap-xklv container test-container-subpath-configmap-xklv: <nil>
    STEP: delete the pod 05/09/23 16:25:37.601
    May  9 16:25:37.618: INFO: Waiting for pod pod-subpath-test-configmap-xklv to disappear
    May  9 16:25:37.623: INFO: Pod pod-subpath-test-configmap-xklv no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-xklv 05/09/23 16:25:37.623
    May  9 16:25:37.623: INFO: Deleting pod "pod-subpath-test-configmap-xklv" in namespace "subpath-7434"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May  9 16:25:37.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7434" for this suite. 05/09/23 16:25:37.635
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:25:37.647
May  9 16:25:37.647: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 16:25:37.648
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:25:37.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:25:37.668
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-28ab10e0-4c00-4bcb-be37-c5a22a2d7068 05/09/23 16:25:37.673
STEP: Creating a pod to test consume configMaps 05/09/23 16:25:37.679
May  9 16:25:37.689: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bda2581e-2003-41bc-a3cf-2a513b11ca87" in namespace "projected-8430" to be "Succeeded or Failed"
May  9 16:25:37.695: INFO: Pod "pod-projected-configmaps-bda2581e-2003-41bc-a3cf-2a513b11ca87": Phase="Pending", Reason="", readiness=false. Elapsed: 6.662373ms
May  9 16:25:39.711: INFO: Pod "pod-projected-configmaps-bda2581e-2003-41bc-a3cf-2a513b11ca87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022107599s
May  9 16:25:41.702: INFO: Pod "pod-projected-configmaps-bda2581e-2003-41bc-a3cf-2a513b11ca87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01358896s
STEP: Saw pod success 05/09/23 16:25:41.702
May  9 16:25:41.702: INFO: Pod "pod-projected-configmaps-bda2581e-2003-41bc-a3cf-2a513b11ca87" satisfied condition "Succeeded or Failed"
May  9 16:25:41.709: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-configmaps-bda2581e-2003-41bc-a3cf-2a513b11ca87 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 16:25:41.761
May  9 16:25:41.933: INFO: Waiting for pod pod-projected-configmaps-bda2581e-2003-41bc-a3cf-2a513b11ca87 to disappear
May  9 16:25:41.939: INFO: Pod pod-projected-configmaps-bda2581e-2003-41bc-a3cf-2a513b11ca87 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May  9 16:25:41.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8430" for this suite. 05/09/23 16:25:41.946
------------------------------
â€¢ [4.310 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:25:37.647
    May  9 16:25:37.647: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 16:25:37.648
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:25:37.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:25:37.668
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-28ab10e0-4c00-4bcb-be37-c5a22a2d7068 05/09/23 16:25:37.673
    STEP: Creating a pod to test consume configMaps 05/09/23 16:25:37.679
    May  9 16:25:37.689: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bda2581e-2003-41bc-a3cf-2a513b11ca87" in namespace "projected-8430" to be "Succeeded or Failed"
    May  9 16:25:37.695: INFO: Pod "pod-projected-configmaps-bda2581e-2003-41bc-a3cf-2a513b11ca87": Phase="Pending", Reason="", readiness=false. Elapsed: 6.662373ms
    May  9 16:25:39.711: INFO: Pod "pod-projected-configmaps-bda2581e-2003-41bc-a3cf-2a513b11ca87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022107599s
    May  9 16:25:41.702: INFO: Pod "pod-projected-configmaps-bda2581e-2003-41bc-a3cf-2a513b11ca87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01358896s
    STEP: Saw pod success 05/09/23 16:25:41.702
    May  9 16:25:41.702: INFO: Pod "pod-projected-configmaps-bda2581e-2003-41bc-a3cf-2a513b11ca87" satisfied condition "Succeeded or Failed"
    May  9 16:25:41.709: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-configmaps-bda2581e-2003-41bc-a3cf-2a513b11ca87 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 16:25:41.761
    May  9 16:25:41.933: INFO: Waiting for pod pod-projected-configmaps-bda2581e-2003-41bc-a3cf-2a513b11ca87 to disappear
    May  9 16:25:41.939: INFO: Pod pod-projected-configmaps-bda2581e-2003-41bc-a3cf-2a513b11ca87 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May  9 16:25:41.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8430" for this suite. 05/09/23 16:25:41.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:25:41.958
May  9 16:25:41.959: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename deployment 05/09/23 16:25:41.959
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:25:41.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:25:41.983
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
May  9 16:25:42.000: INFO: Pod name rollover-pod: Found 0 pods out of 1
May  9 16:25:47.011: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/09/23 16:25:47.011
May  9 16:25:47.011: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May  9 16:25:49.019: INFO: Creating deployment "test-rollover-deployment"
May  9 16:25:49.030: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May  9 16:25:51.042: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May  9 16:25:51.052: INFO: Ensure that both replica sets have 1 created replica
May  9 16:25:51.062: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May  9 16:25:51.084: INFO: Updating deployment test-rollover-deployment
May  9 16:25:51.084: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May  9 16:25:53.096: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May  9 16:25:53.108: INFO: Make sure deployment "test-rollover-deployment" is complete
May  9 16:25:53.116: INFO: all replica sets need to contain the pod-template-hash label
May  9 16:25:53.116: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:25:55.269: INFO: all replica sets need to contain the pod-template-hash label
May  9 16:25:55.269: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:25:57.129: INFO: all replica sets need to contain the pod-template-hash label
May  9 16:25:57.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:25:59.130: INFO: all replica sets need to contain the pod-template-hash label
May  9 16:25:59.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:26:01.229: INFO: all replica sets need to contain the pod-template-hash label
May  9 16:26:01.229: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:26:03.129: INFO: 
May  9 16:26:03.129: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  9 16:26:03.150: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9398  debd8c41-b31d-4fcb-8a98-23cccd32b036 318015060 2 2023-05-09 16:25:49 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-09 16:25:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:26:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b2d848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-09 16:25:49 +0000 UTC,LastTransitionTime:2023-05-09 16:25:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-05-09 16:26:02 +0000 UTC,LastTransitionTime:2023-05-09 16:25:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  9 16:26:03.155: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-9398  469dab12-9699-4e94-a30e-4c642cea139b 318015043 2 2023-05-09 16:25:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment debd8c41-b31d-4fcb-8a98-23cccd32b036 0xc004b2dd17 0xc004b2dd18}] [] [{kube-controller-manager Update apps/v1 2023-05-09 16:25:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"debd8c41-b31d-4fcb-8a98-23cccd32b036\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:26:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b2ddc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  9 16:26:03.155: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May  9 16:26:03.155: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9398  930fb7f4-2800-420c-9298-34891d9904b1 318015059 2 2023-05-09 16:25:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment debd8c41-b31d-4fcb-8a98-23cccd32b036 0xc004b2dbd7 0xc004b2dbd8}] [] [{e2e.test Update apps/v1 2023-05-09 16:25:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:26:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"debd8c41-b31d-4fcb-8a98-23cccd32b036\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:26:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004b2dca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  9 16:26:03.155: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-9398  2ef14a2a-dcb4-4ebf-8867-489cf55bc300 318013858 2 2023-05-09 16:25:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment debd8c41-b31d-4fcb-8a98-23cccd32b036 0xc004b2de37 0xc004b2de38}] [] [{kube-controller-manager Update apps/v1 2023-05-09 16:25:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"debd8c41-b31d-4fcb-8a98-23cccd32b036\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:25:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b2dee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  9 16:26:03.160: INFO: Pod "test-rollover-deployment-6c6df9974f-642dz" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-642dz test-rollover-deployment-6c6df9974f- deployment-9398  ad4d79c9-a296-4cd7-8108-d41b6fd5d1a1 318013982 0 2023-05-09 16:25:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:85242b668f411eac3f47063db2393a4e2a7867f685475995856c2bad586be47f cni.projectcalico.org/podIP:10.2.1.235/32 cni.projectcalico.org/podIPs:10.2.1.235/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 469dab12-9699-4e94-a30e-4c642cea139b 0xc005829c77 0xc005829c78}] [] [{calico Update v1 2023-05-09 16:25:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 16:25:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"469dab12-9699-4e94-a30e-4c642cea139b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 16:25:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9nfq4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9nfq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:25:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:25:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:25:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:25:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:10.2.1.235,StartTime:2023-05-09 16:25:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 16:25:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://56834add7c0a5107cf4ca7a0e6546ff40a498fcc3e6d8da8d656e328f8fa739b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May  9 16:26:03.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9398" for this suite. 05/09/23 16:26:03.168
------------------------------
â€¢ [SLOW TEST] [21.220 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:25:41.958
    May  9 16:25:41.959: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename deployment 05/09/23 16:25:41.959
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:25:41.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:25:41.983
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    May  9 16:25:42.000: INFO: Pod name rollover-pod: Found 0 pods out of 1
    May  9 16:25:47.011: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/09/23 16:25:47.011
    May  9 16:25:47.011: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    May  9 16:25:49.019: INFO: Creating deployment "test-rollover-deployment"
    May  9 16:25:49.030: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    May  9 16:25:51.042: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    May  9 16:25:51.052: INFO: Ensure that both replica sets have 1 created replica
    May  9 16:25:51.062: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    May  9 16:25:51.084: INFO: Updating deployment test-rollover-deployment
    May  9 16:25:51.084: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    May  9 16:25:53.096: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    May  9 16:25:53.108: INFO: Make sure deployment "test-rollover-deployment" is complete
    May  9 16:25:53.116: INFO: all replica sets need to contain the pod-template-hash label
    May  9 16:25:53.116: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:25:55.269: INFO: all replica sets need to contain the pod-template-hash label
    May  9 16:25:55.269: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:25:57.129: INFO: all replica sets need to contain the pod-template-hash label
    May  9 16:25:57.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:25:59.130: INFO: all replica sets need to contain the pod-template-hash label
    May  9 16:25:59.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:26:01.229: INFO: all replica sets need to contain the pod-template-hash label
    May  9 16:26:01.229: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 25, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 25, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:26:03.129: INFO: 
    May  9 16:26:03.129: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  9 16:26:03.150: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-9398  debd8c41-b31d-4fcb-8a98-23cccd32b036 318015060 2 2023-05-09 16:25:49 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-09 16:25:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:26:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b2d848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-09 16:25:49 +0000 UTC,LastTransitionTime:2023-05-09 16:25:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-05-09 16:26:02 +0000 UTC,LastTransitionTime:2023-05-09 16:25:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  9 16:26:03.155: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-9398  469dab12-9699-4e94-a30e-4c642cea139b 318015043 2 2023-05-09 16:25:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment debd8c41-b31d-4fcb-8a98-23cccd32b036 0xc004b2dd17 0xc004b2dd18}] [] [{kube-controller-manager Update apps/v1 2023-05-09 16:25:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"debd8c41-b31d-4fcb-8a98-23cccd32b036\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:26:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b2ddc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  9 16:26:03.155: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    May  9 16:26:03.155: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9398  930fb7f4-2800-420c-9298-34891d9904b1 318015059 2 2023-05-09 16:25:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment debd8c41-b31d-4fcb-8a98-23cccd32b036 0xc004b2dbd7 0xc004b2dbd8}] [] [{e2e.test Update apps/v1 2023-05-09 16:25:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:26:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"debd8c41-b31d-4fcb-8a98-23cccd32b036\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:26:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004b2dca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  9 16:26:03.155: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-9398  2ef14a2a-dcb4-4ebf-8867-489cf55bc300 318013858 2 2023-05-09 16:25:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment debd8c41-b31d-4fcb-8a98-23cccd32b036 0xc004b2de37 0xc004b2de38}] [] [{kube-controller-manager Update apps/v1 2023-05-09 16:25:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"debd8c41-b31d-4fcb-8a98-23cccd32b036\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:25:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b2dee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  9 16:26:03.160: INFO: Pod "test-rollover-deployment-6c6df9974f-642dz" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-642dz test-rollover-deployment-6c6df9974f- deployment-9398  ad4d79c9-a296-4cd7-8108-d41b6fd5d1a1 318013982 0 2023-05-09 16:25:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:85242b668f411eac3f47063db2393a4e2a7867f685475995856c2bad586be47f cni.projectcalico.org/podIP:10.2.1.235/32 cni.projectcalico.org/podIPs:10.2.1.235/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 469dab12-9699-4e94-a30e-4c642cea139b 0xc005829c77 0xc005829c78}] [] [{calico Update v1 2023-05-09 16:25:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 16:25:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"469dab12-9699-4e94-a30e-4c642cea139b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 16:25:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9nfq4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9nfq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:25:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:25:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:25:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:25:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:10.2.1.235,StartTime:2023-05-09 16:25:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 16:25:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://56834add7c0a5107cf4ca7a0e6546ff40a498fcc3e6d8da8d656e328f8fa739b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May  9 16:26:03.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9398" for this suite. 05/09/23 16:26:03.168
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:26:03.179
May  9 16:26:03.179: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename downward-api 05/09/23 16:26:03.18
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:26:03.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:26:03.201
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 05/09/23 16:26:03.205
May  9 16:26:03.215: INFO: Waiting up to 5m0s for pod "downward-api-e3983ec1-2cfb-4826-be82-66a8d63ee084" in namespace "downward-api-6346" to be "Succeeded or Failed"
May  9 16:26:03.220: INFO: Pod "downward-api-e3983ec1-2cfb-4826-be82-66a8d63ee084": Phase="Pending", Reason="", readiness=false. Elapsed: 4.773909ms
May  9 16:26:05.227: INFO: Pod "downward-api-e3983ec1-2cfb-4826-be82-66a8d63ee084": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011537206s
May  9 16:26:07.226: INFO: Pod "downward-api-e3983ec1-2cfb-4826-be82-66a8d63ee084": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011142497s
May  9 16:26:09.227: INFO: Pod "downward-api-e3983ec1-2cfb-4826-be82-66a8d63ee084": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011355615s
STEP: Saw pod success 05/09/23 16:26:09.227
May  9 16:26:09.227: INFO: Pod "downward-api-e3983ec1-2cfb-4826-be82-66a8d63ee084" satisfied condition "Succeeded or Failed"
May  9 16:26:09.233: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downward-api-e3983ec1-2cfb-4826-be82-66a8d63ee084 container dapi-container: <nil>
STEP: delete the pod 05/09/23 16:26:09.246
May  9 16:26:09.266: INFO: Waiting for pod downward-api-e3983ec1-2cfb-4826-be82-66a8d63ee084 to disappear
May  9 16:26:09.270: INFO: Pod downward-api-e3983ec1-2cfb-4826-be82-66a8d63ee084 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May  9 16:26:09.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6346" for this suite. 05/09/23 16:26:09.278
------------------------------
â€¢ [SLOW TEST] [6.109 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:26:03.179
    May  9 16:26:03.179: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename downward-api 05/09/23 16:26:03.18
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:26:03.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:26:03.201
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 05/09/23 16:26:03.205
    May  9 16:26:03.215: INFO: Waiting up to 5m0s for pod "downward-api-e3983ec1-2cfb-4826-be82-66a8d63ee084" in namespace "downward-api-6346" to be "Succeeded or Failed"
    May  9 16:26:03.220: INFO: Pod "downward-api-e3983ec1-2cfb-4826-be82-66a8d63ee084": Phase="Pending", Reason="", readiness=false. Elapsed: 4.773909ms
    May  9 16:26:05.227: INFO: Pod "downward-api-e3983ec1-2cfb-4826-be82-66a8d63ee084": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011537206s
    May  9 16:26:07.226: INFO: Pod "downward-api-e3983ec1-2cfb-4826-be82-66a8d63ee084": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011142497s
    May  9 16:26:09.227: INFO: Pod "downward-api-e3983ec1-2cfb-4826-be82-66a8d63ee084": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011355615s
    STEP: Saw pod success 05/09/23 16:26:09.227
    May  9 16:26:09.227: INFO: Pod "downward-api-e3983ec1-2cfb-4826-be82-66a8d63ee084" satisfied condition "Succeeded or Failed"
    May  9 16:26:09.233: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downward-api-e3983ec1-2cfb-4826-be82-66a8d63ee084 container dapi-container: <nil>
    STEP: delete the pod 05/09/23 16:26:09.246
    May  9 16:26:09.266: INFO: Waiting for pod downward-api-e3983ec1-2cfb-4826-be82-66a8d63ee084 to disappear
    May  9 16:26:09.270: INFO: Pod downward-api-e3983ec1-2cfb-4826-be82-66a8d63ee084 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May  9 16:26:09.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6346" for this suite. 05/09/23 16:26:09.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:26:09.289
May  9 16:26:09.289: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename replication-controller 05/09/23 16:26:09.29
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:26:09.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:26:09.31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 05/09/23 16:26:09.319
STEP: waiting for RC to be added 05/09/23 16:26:09.327
STEP: waiting for available Replicas 05/09/23 16:26:09.328
STEP: patching ReplicationController 05/09/23 16:26:10.563
STEP: waiting for RC to be modified 05/09/23 16:26:10.571
STEP: patching ReplicationController status 05/09/23 16:26:10.571
STEP: waiting for RC to be modified 05/09/23 16:26:10.578
STEP: waiting for available Replicas 05/09/23 16:26:10.579
STEP: fetching ReplicationController status 05/09/23 16:26:10.588
STEP: patching ReplicationController scale 05/09/23 16:26:10.595
STEP: waiting for RC to be modified 05/09/23 16:26:10.602
STEP: waiting for ReplicationController's scale to be the max amount 05/09/23 16:26:10.602
STEP: fetching ReplicationController; ensuring that it's patched 05/09/23 16:26:11.994
STEP: updating ReplicationController status 05/09/23 16:26:12.003
STEP: waiting for RC to be modified 05/09/23 16:26:12.011
STEP: listing all ReplicationControllers 05/09/23 16:26:12.011
STEP: checking that ReplicationController has expected values 05/09/23 16:26:12.016
STEP: deleting ReplicationControllers by collection 05/09/23 16:26:12.016
STEP: waiting for ReplicationController to have a DELETED watchEvent 05/09/23 16:26:12.028
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May  9 16:26:12.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1245" for this suite. 05/09/23 16:26:12.073
------------------------------
â€¢ [2.795 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:26:09.289
    May  9 16:26:09.289: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename replication-controller 05/09/23 16:26:09.29
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:26:09.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:26:09.31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 05/09/23 16:26:09.319
    STEP: waiting for RC to be added 05/09/23 16:26:09.327
    STEP: waiting for available Replicas 05/09/23 16:26:09.328
    STEP: patching ReplicationController 05/09/23 16:26:10.563
    STEP: waiting for RC to be modified 05/09/23 16:26:10.571
    STEP: patching ReplicationController status 05/09/23 16:26:10.571
    STEP: waiting for RC to be modified 05/09/23 16:26:10.578
    STEP: waiting for available Replicas 05/09/23 16:26:10.579
    STEP: fetching ReplicationController status 05/09/23 16:26:10.588
    STEP: patching ReplicationController scale 05/09/23 16:26:10.595
    STEP: waiting for RC to be modified 05/09/23 16:26:10.602
    STEP: waiting for ReplicationController's scale to be the max amount 05/09/23 16:26:10.602
    STEP: fetching ReplicationController; ensuring that it's patched 05/09/23 16:26:11.994
    STEP: updating ReplicationController status 05/09/23 16:26:12.003
    STEP: waiting for RC to be modified 05/09/23 16:26:12.011
    STEP: listing all ReplicationControllers 05/09/23 16:26:12.011
    STEP: checking that ReplicationController has expected values 05/09/23 16:26:12.016
    STEP: deleting ReplicationControllers by collection 05/09/23 16:26:12.016
    STEP: waiting for ReplicationController to have a DELETED watchEvent 05/09/23 16:26:12.028
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May  9 16:26:12.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1245" for this suite. 05/09/23 16:26:12.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:26:12.086
May  9 16:26:12.086: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 16:26:12.087
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:26:12.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:26:12.107
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 05/09/23 16:26:12.112
May  9 16:26:12.129: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a" in namespace "projected-4413" to be "Succeeded or Failed"
May  9 16:26:12.136: INFO: Pod "downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.519739ms
May  9 16:26:14.145: INFO: Pod "downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015053669s
May  9 16:26:16.193: INFO: Pod "downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063402185s
May  9 16:26:18.142: INFO: Pod "downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012845501s
May  9 16:26:20.143: INFO: Pod "downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013904349s
May  9 16:26:22.145: INFO: Pod "downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.015146652s
STEP: Saw pod success 05/09/23 16:26:22.145
May  9 16:26:22.145: INFO: Pod "downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a" satisfied condition "Succeeded or Failed"
May  9 16:26:22.150: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a container client-container: <nil>
STEP: delete the pod 05/09/23 16:26:22.161
May  9 16:26:22.177: INFO: Waiting for pod downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a to disappear
May  9 16:26:22.185: INFO: Pod downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  9 16:26:22.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4413" for this suite. 05/09/23 16:26:22.192
------------------------------
â€¢ [SLOW TEST] [10.116 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:26:12.086
    May  9 16:26:12.086: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 16:26:12.087
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:26:12.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:26:12.107
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 05/09/23 16:26:12.112
    May  9 16:26:12.129: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a" in namespace "projected-4413" to be "Succeeded or Failed"
    May  9 16:26:12.136: INFO: Pod "downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.519739ms
    May  9 16:26:14.145: INFO: Pod "downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015053669s
    May  9 16:26:16.193: INFO: Pod "downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063402185s
    May  9 16:26:18.142: INFO: Pod "downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012845501s
    May  9 16:26:20.143: INFO: Pod "downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013904349s
    May  9 16:26:22.145: INFO: Pod "downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.015146652s
    STEP: Saw pod success 05/09/23 16:26:22.145
    May  9 16:26:22.145: INFO: Pod "downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a" satisfied condition "Succeeded or Failed"
    May  9 16:26:22.150: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a container client-container: <nil>
    STEP: delete the pod 05/09/23 16:26:22.161
    May  9 16:26:22.177: INFO: Waiting for pod downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a to disappear
    May  9 16:26:22.185: INFO: Pod downwardapi-volume-5c291ee3-f116-4f6f-b279-66378746f15a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  9 16:26:22.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4413" for this suite. 05/09/23 16:26:22.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:26:22.204
May  9 16:26:22.204: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename gc 05/09/23 16:26:22.206
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:26:22.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:26:22.232
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 05/09/23 16:26:22.241
STEP: create the rc2 05/09/23 16:26:22.249
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 05/09/23 16:26:27.381
STEP: delete the rc simpletest-rc-to-be-deleted 05/09/23 16:26:29.632
STEP: wait for the rc to be deleted 05/09/23 16:26:29.641
May  9 16:26:34.704: INFO: 68 pods remaining
May  9 16:26:34.704: INFO: 68 pods has nil DeletionTimestamp
May  9 16:26:34.704: INFO: 
STEP: Gathering metrics 05/09/23 16:26:39.663
W0509 16:26:39.682232      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  9 16:26:39.682: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May  9 16:26:39.682: INFO: Deleting pod "simpletest-rc-to-be-deleted-22kx6" in namespace "gc-5845"
May  9 16:26:39.696: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lj7d" in namespace "gc-5845"
May  9 16:26:39.719: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mqqs" in namespace "gc-5845"
May  9 16:26:39.910: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mt54" in namespace "gc-5845"
May  9 16:26:39.937: INFO: Deleting pod "simpletest-rc-to-be-deleted-2txwq" in namespace "gc-5845"
May  9 16:26:39.958: INFO: Deleting pod "simpletest-rc-to-be-deleted-45qqk" in namespace "gc-5845"
May  9 16:26:39.974: INFO: Deleting pod "simpletest-rc-to-be-deleted-4n47z" in namespace "gc-5845"
May  9 16:26:39.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-4sc4q" in namespace "gc-5845"
May  9 16:26:40.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vkcq" in namespace "gc-5845"
May  9 16:26:40.026: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wgpb" in namespace "gc-5845"
May  9 16:26:40.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-4z2rn" in namespace "gc-5845"
May  9 16:26:40.079: INFO: Deleting pod "simpletest-rc-to-be-deleted-52l4g" in namespace "gc-5845"
May  9 16:26:40.094: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vwv7" in namespace "gc-5845"
May  9 16:26:40.131: INFO: Deleting pod "simpletest-rc-to-be-deleted-76k8x" in namespace "gc-5845"
May  9 16:26:40.151: INFO: Deleting pod "simpletest-rc-to-be-deleted-7p8hk" in namespace "gc-5845"
May  9 16:26:40.163: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rxzv" in namespace "gc-5845"
May  9 16:26:40.182: INFO: Deleting pod "simpletest-rc-to-be-deleted-7spj9" in namespace "gc-5845"
May  9 16:26:40.205: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xs4w" in namespace "gc-5845"
May  9 16:26:40.222: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f9tq" in namespace "gc-5845"
May  9 16:26:40.245: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gmm6" in namespace "gc-5845"
May  9 16:26:40.270: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gnbj" in namespace "gc-5845"
May  9 16:26:40.298: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mw9k" in namespace "gc-5845"
May  9 16:26:40.313: INFO: Deleting pod "simpletest-rc-to-be-deleted-8z9hh" in namespace "gc-5845"
May  9 16:26:40.330: INFO: Deleting pod "simpletest-rc-to-be-deleted-b59vm" in namespace "gc-5845"
May  9 16:26:40.358: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5c2h" in namespace "gc-5845"
May  9 16:26:40.374: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmkkb" in namespace "gc-5845"
May  9 16:26:40.386: INFO: Deleting pod "simpletest-rc-to-be-deleted-bppl2" in namespace "gc-5845"
May  9 16:26:40.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4bkw" in namespace "gc-5845"
May  9 16:26:40.417: INFO: Deleting pod "simpletest-rc-to-be-deleted-cglzn" in namespace "gc-5845"
May  9 16:26:40.433: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjpw2" in namespace "gc-5845"
May  9 16:26:40.467: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnd7w" in namespace "gc-5845"
May  9 16:26:40.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnnth" in namespace "gc-5845"
May  9 16:26:40.506: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwxk6" in namespace "gc-5845"
May  9 16:26:40.519: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7b69" in namespace "gc-5845"
May  9 16:26:40.538: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddfrm" in namespace "gc-5845"
May  9 16:26:40.574: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtr4j" in namespace "gc-5845"
May  9 16:26:40.591: INFO: Deleting pod "simpletest-rc-to-be-deleted-dzsgb" in namespace "gc-5845"
May  9 16:26:40.607: INFO: Deleting pod "simpletest-rc-to-be-deleted-flx2w" in namespace "gc-5845"
May  9 16:26:40.621: INFO: Deleting pod "simpletest-rc-to-be-deleted-g9rx6" in namespace "gc-5845"
May  9 16:26:40.636: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggfll" in namespace "gc-5845"
May  9 16:26:40.675: INFO: Deleting pod "simpletest-rc-to-be-deleted-gknqj" in namespace "gc-5845"
May  9 16:26:40.694: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvjrc" in namespace "gc-5845"
May  9 16:26:40.710: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwqnq" in namespace "gc-5845"
May  9 16:26:40.726: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4q22" in namespace "gc-5845"
May  9 16:26:40.746: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9snx" in namespace "gc-5845"
May  9 16:26:40.772: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdh28" in namespace "gc-5845"
May  9 16:26:40.798: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmbtg" in namespace "gc-5845"
May  9 16:26:40.866: INFO: Deleting pod "simpletest-rc-to-be-deleted-hnmlp" in namespace "gc-5845"
May  9 16:26:40.904: INFO: Deleting pod "simpletest-rc-to-be-deleted-jvx6m" in namespace "gc-5845"
May  9 16:26:40.938: INFO: Deleting pod "simpletest-rc-to-be-deleted-jzmn5" in namespace "gc-5845"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May  9 16:26:40.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5845" for this suite. 05/09/23 16:26:40.995
------------------------------
â€¢ [SLOW TEST] [18.878 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:26:22.204
    May  9 16:26:22.204: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename gc 05/09/23 16:26:22.206
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:26:22.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:26:22.232
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 05/09/23 16:26:22.241
    STEP: create the rc2 05/09/23 16:26:22.249
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 05/09/23 16:26:27.381
    STEP: delete the rc simpletest-rc-to-be-deleted 05/09/23 16:26:29.632
    STEP: wait for the rc to be deleted 05/09/23 16:26:29.641
    May  9 16:26:34.704: INFO: 68 pods remaining
    May  9 16:26:34.704: INFO: 68 pods has nil DeletionTimestamp
    May  9 16:26:34.704: INFO: 
    STEP: Gathering metrics 05/09/23 16:26:39.663
    W0509 16:26:39.682232      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May  9 16:26:39.682: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    May  9 16:26:39.682: INFO: Deleting pod "simpletest-rc-to-be-deleted-22kx6" in namespace "gc-5845"
    May  9 16:26:39.696: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lj7d" in namespace "gc-5845"
    May  9 16:26:39.719: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mqqs" in namespace "gc-5845"
    May  9 16:26:39.910: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mt54" in namespace "gc-5845"
    May  9 16:26:39.937: INFO: Deleting pod "simpletest-rc-to-be-deleted-2txwq" in namespace "gc-5845"
    May  9 16:26:39.958: INFO: Deleting pod "simpletest-rc-to-be-deleted-45qqk" in namespace "gc-5845"
    May  9 16:26:39.974: INFO: Deleting pod "simpletest-rc-to-be-deleted-4n47z" in namespace "gc-5845"
    May  9 16:26:39.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-4sc4q" in namespace "gc-5845"
    May  9 16:26:40.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vkcq" in namespace "gc-5845"
    May  9 16:26:40.026: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wgpb" in namespace "gc-5845"
    May  9 16:26:40.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-4z2rn" in namespace "gc-5845"
    May  9 16:26:40.079: INFO: Deleting pod "simpletest-rc-to-be-deleted-52l4g" in namespace "gc-5845"
    May  9 16:26:40.094: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vwv7" in namespace "gc-5845"
    May  9 16:26:40.131: INFO: Deleting pod "simpletest-rc-to-be-deleted-76k8x" in namespace "gc-5845"
    May  9 16:26:40.151: INFO: Deleting pod "simpletest-rc-to-be-deleted-7p8hk" in namespace "gc-5845"
    May  9 16:26:40.163: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rxzv" in namespace "gc-5845"
    May  9 16:26:40.182: INFO: Deleting pod "simpletest-rc-to-be-deleted-7spj9" in namespace "gc-5845"
    May  9 16:26:40.205: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xs4w" in namespace "gc-5845"
    May  9 16:26:40.222: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f9tq" in namespace "gc-5845"
    May  9 16:26:40.245: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gmm6" in namespace "gc-5845"
    May  9 16:26:40.270: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gnbj" in namespace "gc-5845"
    May  9 16:26:40.298: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mw9k" in namespace "gc-5845"
    May  9 16:26:40.313: INFO: Deleting pod "simpletest-rc-to-be-deleted-8z9hh" in namespace "gc-5845"
    May  9 16:26:40.330: INFO: Deleting pod "simpletest-rc-to-be-deleted-b59vm" in namespace "gc-5845"
    May  9 16:26:40.358: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5c2h" in namespace "gc-5845"
    May  9 16:26:40.374: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmkkb" in namespace "gc-5845"
    May  9 16:26:40.386: INFO: Deleting pod "simpletest-rc-to-be-deleted-bppl2" in namespace "gc-5845"
    May  9 16:26:40.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4bkw" in namespace "gc-5845"
    May  9 16:26:40.417: INFO: Deleting pod "simpletest-rc-to-be-deleted-cglzn" in namespace "gc-5845"
    May  9 16:26:40.433: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjpw2" in namespace "gc-5845"
    May  9 16:26:40.467: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnd7w" in namespace "gc-5845"
    May  9 16:26:40.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnnth" in namespace "gc-5845"
    May  9 16:26:40.506: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwxk6" in namespace "gc-5845"
    May  9 16:26:40.519: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7b69" in namespace "gc-5845"
    May  9 16:26:40.538: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddfrm" in namespace "gc-5845"
    May  9 16:26:40.574: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtr4j" in namespace "gc-5845"
    May  9 16:26:40.591: INFO: Deleting pod "simpletest-rc-to-be-deleted-dzsgb" in namespace "gc-5845"
    May  9 16:26:40.607: INFO: Deleting pod "simpletest-rc-to-be-deleted-flx2w" in namespace "gc-5845"
    May  9 16:26:40.621: INFO: Deleting pod "simpletest-rc-to-be-deleted-g9rx6" in namespace "gc-5845"
    May  9 16:26:40.636: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggfll" in namespace "gc-5845"
    May  9 16:26:40.675: INFO: Deleting pod "simpletest-rc-to-be-deleted-gknqj" in namespace "gc-5845"
    May  9 16:26:40.694: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvjrc" in namespace "gc-5845"
    May  9 16:26:40.710: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwqnq" in namespace "gc-5845"
    May  9 16:26:40.726: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4q22" in namespace "gc-5845"
    May  9 16:26:40.746: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9snx" in namespace "gc-5845"
    May  9 16:26:40.772: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdh28" in namespace "gc-5845"
    May  9 16:26:40.798: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmbtg" in namespace "gc-5845"
    May  9 16:26:40.866: INFO: Deleting pod "simpletest-rc-to-be-deleted-hnmlp" in namespace "gc-5845"
    May  9 16:26:40.904: INFO: Deleting pod "simpletest-rc-to-be-deleted-jvx6m" in namespace "gc-5845"
    May  9 16:26:40.938: INFO: Deleting pod "simpletest-rc-to-be-deleted-jzmn5" in namespace "gc-5845"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May  9 16:26:40.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5845" for this suite. 05/09/23 16:26:40.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:26:41.082
May  9 16:26:41.082: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename crd-webhook 05/09/23 16:26:41.083
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:26:41.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:26:41.891
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 05/09/23 16:26:41.897
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/09/23 16:26:42.241
STEP: Deploying the custom resource conversion webhook pod 05/09/23 16:26:42.252
STEP: Wait for the deployment to be ready 05/09/23 16:26:42.268
May  9 16:26:42.280: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May  9 16:26:44.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 26, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 26, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 26, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 26, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:26:46.320: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 26, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 26, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 26, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 26, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/09/23 16:26:48.301
STEP: Verifying the service has paired with the endpoint 05/09/23 16:26:48.316
May  9 16:26:49.317: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
May  9 16:26:49.322: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Creating a v1 custom resource 05/09/23 16:26:52.441
STEP: Create a v2 custom resource 05/09/23 16:26:52.47
STEP: List CRs in v1 05/09/23 16:26:52.621
STEP: List CRs in v2 05/09/23 16:26:52.633
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:26:53.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-4435" for this suite. 05/09/23 16:26:53.213
------------------------------
â€¢ [SLOW TEST] [12.141 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:26:41.082
    May  9 16:26:41.082: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename crd-webhook 05/09/23 16:26:41.083
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:26:41.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:26:41.891
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 05/09/23 16:26:41.897
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/09/23 16:26:42.241
    STEP: Deploying the custom resource conversion webhook pod 05/09/23 16:26:42.252
    STEP: Wait for the deployment to be ready 05/09/23 16:26:42.268
    May  9 16:26:42.280: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    May  9 16:26:44.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 26, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 26, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 26, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 26, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:26:46.320: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 26, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 26, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 26, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 26, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/09/23 16:26:48.301
    STEP: Verifying the service has paired with the endpoint 05/09/23 16:26:48.316
    May  9 16:26:49.317: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    May  9 16:26:49.322: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Creating a v1 custom resource 05/09/23 16:26:52.441
    STEP: Create a v2 custom resource 05/09/23 16:26:52.47
    STEP: List CRs in v1 05/09/23 16:26:52.621
    STEP: List CRs in v2 05/09/23 16:26:52.633
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:26:53.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-4435" for this suite. 05/09/23 16:26:53.213
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:26:53.224
May  9 16:26:53.224: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename security-context-test 05/09/23 16:26:53.225
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:26:53.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:26:53.252
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
May  9 16:26:53.268: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-5ad3fd43-6e79-4fae-8f76-bc9dcc4f47c3" in namespace "security-context-test-9800" to be "Succeeded or Failed"
May  9 16:26:53.274: INFO: Pod "busybox-readonly-false-5ad3fd43-6e79-4fae-8f76-bc9dcc4f47c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.572817ms
May  9 16:26:55.281: INFO: Pod "busybox-readonly-false-5ad3fd43-6e79-4fae-8f76-bc9dcc4f47c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013194638s
May  9 16:26:57.280: INFO: Pod "busybox-readonly-false-5ad3fd43-6e79-4fae-8f76-bc9dcc4f47c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01149222s
May  9 16:26:57.280: INFO: Pod "busybox-readonly-false-5ad3fd43-6e79-4fae-8f76-bc9dcc4f47c3" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May  9 16:26:57.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-9800" for this suite. 05/09/23 16:26:57.286
------------------------------
â€¢ [4.070 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:26:53.224
    May  9 16:26:53.224: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename security-context-test 05/09/23 16:26:53.225
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:26:53.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:26:53.252
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    May  9 16:26:53.268: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-5ad3fd43-6e79-4fae-8f76-bc9dcc4f47c3" in namespace "security-context-test-9800" to be "Succeeded or Failed"
    May  9 16:26:53.274: INFO: Pod "busybox-readonly-false-5ad3fd43-6e79-4fae-8f76-bc9dcc4f47c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.572817ms
    May  9 16:26:55.281: INFO: Pod "busybox-readonly-false-5ad3fd43-6e79-4fae-8f76-bc9dcc4f47c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013194638s
    May  9 16:26:57.280: INFO: Pod "busybox-readonly-false-5ad3fd43-6e79-4fae-8f76-bc9dcc4f47c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01149222s
    May  9 16:26:57.280: INFO: Pod "busybox-readonly-false-5ad3fd43-6e79-4fae-8f76-bc9dcc4f47c3" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May  9 16:26:57.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-9800" for this suite. 05/09/23 16:26:57.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:26:57.298
May  9 16:26:57.298: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename configmap 05/09/23 16:26:57.299
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:26:57.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:26:57.319
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-32001fee-b79f-4a22-9641-7d2e66b583b8 05/09/23 16:26:57.33
STEP: Creating the pod 05/09/23 16:26:57.336
May  9 16:26:57.347: INFO: Waiting up to 5m0s for pod "pod-configmaps-d6b6c8a2-65bf-403b-9d5f-aac0271ee187" in namespace "configmap-497" to be "running and ready"
May  9 16:26:57.369: INFO: Pod "pod-configmaps-d6b6c8a2-65bf-403b-9d5f-aac0271ee187": Phase="Pending", Reason="", readiness=false. Elapsed: 22.379141ms
May  9 16:26:57.369: INFO: The phase of Pod pod-configmaps-d6b6c8a2-65bf-403b-9d5f-aac0271ee187 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:26:59.376: INFO: Pod "pod-configmaps-d6b6c8a2-65bf-403b-9d5f-aac0271ee187": Phase="Running", Reason="", readiness=true. Elapsed: 2.028884739s
May  9 16:26:59.376: INFO: The phase of Pod pod-configmaps-d6b6c8a2-65bf-403b-9d5f-aac0271ee187 is Running (Ready = true)
May  9 16:26:59.376: INFO: Pod "pod-configmaps-d6b6c8a2-65bf-403b-9d5f-aac0271ee187" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-32001fee-b79f-4a22-9641-7d2e66b583b8 05/09/23 16:26:59.395
STEP: waiting to observe update in volume 05/09/23 16:26:59.403
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  9 16:27:01.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-497" for this suite. 05/09/23 16:27:01.453
------------------------------
â€¢ [4.166 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:26:57.298
    May  9 16:26:57.298: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename configmap 05/09/23 16:26:57.299
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:26:57.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:26:57.319
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-32001fee-b79f-4a22-9641-7d2e66b583b8 05/09/23 16:26:57.33
    STEP: Creating the pod 05/09/23 16:26:57.336
    May  9 16:26:57.347: INFO: Waiting up to 5m0s for pod "pod-configmaps-d6b6c8a2-65bf-403b-9d5f-aac0271ee187" in namespace "configmap-497" to be "running and ready"
    May  9 16:26:57.369: INFO: Pod "pod-configmaps-d6b6c8a2-65bf-403b-9d5f-aac0271ee187": Phase="Pending", Reason="", readiness=false. Elapsed: 22.379141ms
    May  9 16:26:57.369: INFO: The phase of Pod pod-configmaps-d6b6c8a2-65bf-403b-9d5f-aac0271ee187 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:26:59.376: INFO: Pod "pod-configmaps-d6b6c8a2-65bf-403b-9d5f-aac0271ee187": Phase="Running", Reason="", readiness=true. Elapsed: 2.028884739s
    May  9 16:26:59.376: INFO: The phase of Pod pod-configmaps-d6b6c8a2-65bf-403b-9d5f-aac0271ee187 is Running (Ready = true)
    May  9 16:26:59.376: INFO: Pod "pod-configmaps-d6b6c8a2-65bf-403b-9d5f-aac0271ee187" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-32001fee-b79f-4a22-9641-7d2e66b583b8 05/09/23 16:26:59.395
    STEP: waiting to observe update in volume 05/09/23 16:26:59.403
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  9 16:27:01.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-497" for this suite. 05/09/23 16:27:01.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:27:01.464
May  9 16:27:01.464: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 16:27:01.465
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:27:01.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:27:01.488
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
May  9 16:27:01.494: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/09/23 16:27:03.342
May  9 16:27:03.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-9412 --namespace=crd-publish-openapi-9412 create -f -'
May  9 16:27:04.536: INFO: stderr: ""
May  9 16:27:04.536: INFO: stdout: "e2e-test-crd-publish-openapi-4799-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May  9 16:27:04.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-9412 --namespace=crd-publish-openapi-9412 delete e2e-test-crd-publish-openapi-4799-crds test-cr'
May  9 16:27:04.628: INFO: stderr: ""
May  9 16:27:04.628: INFO: stdout: "e2e-test-crd-publish-openapi-4799-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May  9 16:27:04.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-9412 --namespace=crd-publish-openapi-9412 apply -f -'
May  9 16:27:04.973: INFO: stderr: ""
May  9 16:27:04.973: INFO: stdout: "e2e-test-crd-publish-openapi-4799-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May  9 16:27:04.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-9412 --namespace=crd-publish-openapi-9412 delete e2e-test-crd-publish-openapi-4799-crds test-cr'
May  9 16:27:05.092: INFO: stderr: ""
May  9 16:27:05.092: INFO: stdout: "e2e-test-crd-publish-openapi-4799-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 05/09/23 16:27:05.092
May  9 16:27:05.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-9412 explain e2e-test-crd-publish-openapi-4799-crds'
May  9 16:27:05.314: INFO: stderr: ""
May  9 16:27:05.314: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4799-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:27:07.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9412" for this suite. 05/09/23 16:27:07.598
------------------------------
â€¢ [SLOW TEST] [6.143 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:27:01.464
    May  9 16:27:01.464: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 16:27:01.465
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:27:01.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:27:01.488
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    May  9 16:27:01.494: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/09/23 16:27:03.342
    May  9 16:27:03.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-9412 --namespace=crd-publish-openapi-9412 create -f -'
    May  9 16:27:04.536: INFO: stderr: ""
    May  9 16:27:04.536: INFO: stdout: "e2e-test-crd-publish-openapi-4799-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    May  9 16:27:04.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-9412 --namespace=crd-publish-openapi-9412 delete e2e-test-crd-publish-openapi-4799-crds test-cr'
    May  9 16:27:04.628: INFO: stderr: ""
    May  9 16:27:04.628: INFO: stdout: "e2e-test-crd-publish-openapi-4799-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    May  9 16:27:04.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-9412 --namespace=crd-publish-openapi-9412 apply -f -'
    May  9 16:27:04.973: INFO: stderr: ""
    May  9 16:27:04.973: INFO: stdout: "e2e-test-crd-publish-openapi-4799-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    May  9 16:27:04.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-9412 --namespace=crd-publish-openapi-9412 delete e2e-test-crd-publish-openapi-4799-crds test-cr'
    May  9 16:27:05.092: INFO: stderr: ""
    May  9 16:27:05.092: INFO: stdout: "e2e-test-crd-publish-openapi-4799-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 05/09/23 16:27:05.092
    May  9 16:27:05.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-9412 explain e2e-test-crd-publish-openapi-4799-crds'
    May  9 16:27:05.314: INFO: stderr: ""
    May  9 16:27:05.314: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4799-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:27:07.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9412" for this suite. 05/09/23 16:27:07.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:27:07.608
May  9 16:27:07.608: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename configmap 05/09/23 16:27:07.608
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:27:07.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:27:07.629
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-44042127-a92d-4dbc-aa77-79fc8ffb6aec 05/09/23 16:27:07.635
STEP: Creating a pod to test consume configMaps 05/09/23 16:27:07.642
May  9 16:27:07.654: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd89eb71-f12a-4acb-9e9c-c9c962c0ad02" in namespace "configmap-5358" to be "Succeeded or Failed"
May  9 16:27:07.658: INFO: Pod "pod-configmaps-cd89eb71-f12a-4acb-9e9c-c9c962c0ad02": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058124ms
May  9 16:27:09.665: INFO: Pod "pod-configmaps-cd89eb71-f12a-4acb-9e9c-c9c962c0ad02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011697981s
May  9 16:27:11.667: INFO: Pod "pod-configmaps-cd89eb71-f12a-4acb-9e9c-c9c962c0ad02": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013648436s
May  9 16:27:13.665: INFO: Pod "pod-configmaps-cd89eb71-f12a-4acb-9e9c-c9c962c0ad02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011117603s
STEP: Saw pod success 05/09/23 16:27:13.665
May  9 16:27:13.665: INFO: Pod "pod-configmaps-cd89eb71-f12a-4acb-9e9c-c9c962c0ad02" satisfied condition "Succeeded or Failed"
May  9 16:27:13.670: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-configmaps-cd89eb71-f12a-4acb-9e9c-c9c962c0ad02 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 16:27:13.681
May  9 16:27:13.698: INFO: Waiting for pod pod-configmaps-cd89eb71-f12a-4acb-9e9c-c9c962c0ad02 to disappear
May  9 16:27:13.702: INFO: Pod pod-configmaps-cd89eb71-f12a-4acb-9e9c-c9c962c0ad02 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  9 16:27:13.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5358" for this suite. 05/09/23 16:27:13.708
------------------------------
â€¢ [SLOW TEST] [6.111 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:27:07.608
    May  9 16:27:07.608: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename configmap 05/09/23 16:27:07.608
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:27:07.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:27:07.629
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-44042127-a92d-4dbc-aa77-79fc8ffb6aec 05/09/23 16:27:07.635
    STEP: Creating a pod to test consume configMaps 05/09/23 16:27:07.642
    May  9 16:27:07.654: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd89eb71-f12a-4acb-9e9c-c9c962c0ad02" in namespace "configmap-5358" to be "Succeeded or Failed"
    May  9 16:27:07.658: INFO: Pod "pod-configmaps-cd89eb71-f12a-4acb-9e9c-c9c962c0ad02": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058124ms
    May  9 16:27:09.665: INFO: Pod "pod-configmaps-cd89eb71-f12a-4acb-9e9c-c9c962c0ad02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011697981s
    May  9 16:27:11.667: INFO: Pod "pod-configmaps-cd89eb71-f12a-4acb-9e9c-c9c962c0ad02": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013648436s
    May  9 16:27:13.665: INFO: Pod "pod-configmaps-cd89eb71-f12a-4acb-9e9c-c9c962c0ad02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011117603s
    STEP: Saw pod success 05/09/23 16:27:13.665
    May  9 16:27:13.665: INFO: Pod "pod-configmaps-cd89eb71-f12a-4acb-9e9c-c9c962c0ad02" satisfied condition "Succeeded or Failed"
    May  9 16:27:13.670: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-configmaps-cd89eb71-f12a-4acb-9e9c-c9c962c0ad02 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 16:27:13.681
    May  9 16:27:13.698: INFO: Waiting for pod pod-configmaps-cd89eb71-f12a-4acb-9e9c-c9c962c0ad02 to disappear
    May  9 16:27:13.702: INFO: Pod pod-configmaps-cd89eb71-f12a-4acb-9e9c-c9c962c0ad02 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  9 16:27:13.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5358" for this suite. 05/09/23 16:27:13.708
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:27:13.72
May  9 16:27:13.721: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename webhook 05/09/23 16:27:13.721
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:27:13.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:27:13.74
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/09/23 16:27:13.775
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:27:14.562
STEP: Deploying the webhook pod 05/09/23 16:27:14.575
STEP: Wait for the deployment to be ready 05/09/23 16:27:14.589
May  9 16:27:14.608: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  9 16:27:16.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:27:18.635: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:27:20.634: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/09/23 16:27:22.633
STEP: Verifying the service has paired with the endpoint 05/09/23 16:27:22.648
May  9 16:27:23.648: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
May  9 16:27:23.654: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4297-crds.webhook.example.com via the AdmissionRegistration API 05/09/23 16:27:24.177
STEP: Creating a custom resource while v1 is storage version 05/09/23 16:27:24.2
STEP: Patching Custom Resource Definition to set v2 as storage 05/09/23 16:27:26.39
STEP: Patching the custom resource while v2 is storage version 05/09/23 16:27:26.399
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:27:27.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3486" for this suite. 05/09/23 16:27:27.13
STEP: Destroying namespace "webhook-3486-markers" for this suite. 05/09/23 16:27:27.143
------------------------------
â€¢ [SLOW TEST] [13.432 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:27:13.72
    May  9 16:27:13.721: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename webhook 05/09/23 16:27:13.721
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:27:13.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:27:13.74
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/09/23 16:27:13.775
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:27:14.562
    STEP: Deploying the webhook pod 05/09/23 16:27:14.575
    STEP: Wait for the deployment to be ready 05/09/23 16:27:14.589
    May  9 16:27:14.608: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May  9 16:27:16.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:27:18.635: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:27:20.634: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 27, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/09/23 16:27:22.633
    STEP: Verifying the service has paired with the endpoint 05/09/23 16:27:22.648
    May  9 16:27:23.648: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    May  9 16:27:23.654: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4297-crds.webhook.example.com via the AdmissionRegistration API 05/09/23 16:27:24.177
    STEP: Creating a custom resource while v1 is storage version 05/09/23 16:27:24.2
    STEP: Patching Custom Resource Definition to set v2 as storage 05/09/23 16:27:26.39
    STEP: Patching the custom resource while v2 is storage version 05/09/23 16:27:26.399
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:27:27.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3486" for this suite. 05/09/23 16:27:27.13
    STEP: Destroying namespace "webhook-3486-markers" for this suite. 05/09/23 16:27:27.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:27:27.154
May  9 16:27:27.154: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename namespaces 05/09/23 16:27:27.155
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:27:27.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:27:27.178
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 05/09/23 16:27:27.184
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:27:27.201
STEP: Creating a pod in the namespace 05/09/23 16:27:27.206
STEP: Waiting for the pod to have running status 05/09/23 16:27:27.216
May  9 16:27:27.216: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8816" to be "running"
May  9 16:27:27.219: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.53856ms
May  9 16:27:29.228: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011817798s
May  9 16:27:29.228: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 05/09/23 16:27:29.228
STEP: Waiting for the namespace to be removed. 05/09/23 16:27:29.244
STEP: Recreating the namespace 05/09/23 16:27:41.277
STEP: Verifying there are no pods in the namespace 05/09/23 16:27:41.296
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 16:27:41.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1191" for this suite. 05/09/23 16:27:41.311
STEP: Destroying namespace "nsdeletetest-8816" for this suite. 05/09/23 16:27:41.32
May  9 16:27:41.327: INFO: Namespace nsdeletetest-8816 was already deleted
STEP: Destroying namespace "nsdeletetest-1714" for this suite. 05/09/23 16:27:41.327
------------------------------
â€¢ [SLOW TEST] [14.182 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:27:27.154
    May  9 16:27:27.154: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename namespaces 05/09/23 16:27:27.155
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:27:27.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:27:27.178
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 05/09/23 16:27:27.184
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:27:27.201
    STEP: Creating a pod in the namespace 05/09/23 16:27:27.206
    STEP: Waiting for the pod to have running status 05/09/23 16:27:27.216
    May  9 16:27:27.216: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8816" to be "running"
    May  9 16:27:27.219: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.53856ms
    May  9 16:27:29.228: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011817798s
    May  9 16:27:29.228: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 05/09/23 16:27:29.228
    STEP: Waiting for the namespace to be removed. 05/09/23 16:27:29.244
    STEP: Recreating the namespace 05/09/23 16:27:41.277
    STEP: Verifying there are no pods in the namespace 05/09/23 16:27:41.296
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 16:27:41.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1191" for this suite. 05/09/23 16:27:41.311
    STEP: Destroying namespace "nsdeletetest-8816" for this suite. 05/09/23 16:27:41.32
    May  9 16:27:41.327: INFO: Namespace nsdeletetest-8816 was already deleted
    STEP: Destroying namespace "nsdeletetest-1714" for this suite. 05/09/23 16:27:41.327
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:27:41.336
May  9 16:27:41.337: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename resourcequota 05/09/23 16:27:41.338
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:27:41.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:27:41.366
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 05/09/23 16:27:41.371
STEP: Creating a ResourceQuota 05/09/23 16:27:46.379
STEP: Ensuring resource quota status is calculated 05/09/23 16:27:46.387
STEP: Creating a ReplicationController 05/09/23 16:27:48.396
STEP: Ensuring resource quota status captures replication controller creation 05/09/23 16:27:48.419
STEP: Deleting a ReplicationController 05/09/23 16:27:50.425
STEP: Ensuring resource quota status released usage 05/09/23 16:27:50.435
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  9 16:27:52.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9787" for this suite. 05/09/23 16:27:52.45
------------------------------
â€¢ [SLOW TEST] [11.123 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:27:41.336
    May  9 16:27:41.337: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename resourcequota 05/09/23 16:27:41.338
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:27:41.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:27:41.366
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 05/09/23 16:27:41.371
    STEP: Creating a ResourceQuota 05/09/23 16:27:46.379
    STEP: Ensuring resource quota status is calculated 05/09/23 16:27:46.387
    STEP: Creating a ReplicationController 05/09/23 16:27:48.396
    STEP: Ensuring resource quota status captures replication controller creation 05/09/23 16:27:48.419
    STEP: Deleting a ReplicationController 05/09/23 16:27:50.425
    STEP: Ensuring resource quota status released usage 05/09/23 16:27:50.435
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  9 16:27:52.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9787" for this suite. 05/09/23 16:27:52.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:27:52.46
May  9 16:27:52.460: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename emptydir 05/09/23 16:27:52.461
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:27:52.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:27:52.484
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 05/09/23 16:27:52.489
May  9 16:27:52.499: INFO: Waiting up to 5m0s for pod "pod-9fdb09de-cfa3-4f50-a5f2-84702f870aaa" in namespace "emptydir-7460" to be "Succeeded or Failed"
May  9 16:27:52.504: INFO: Pod "pod-9fdb09de-cfa3-4f50-a5f2-84702f870aaa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.828434ms
May  9 16:27:54.511: INFO: Pod "pod-9fdb09de-cfa3-4f50-a5f2-84702f870aaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012371705s
May  9 16:27:56.511: INFO: Pod "pod-9fdb09de-cfa3-4f50-a5f2-84702f870aaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012739689s
STEP: Saw pod success 05/09/23 16:27:56.511
May  9 16:27:56.512: INFO: Pod "pod-9fdb09de-cfa3-4f50-a5f2-84702f870aaa" satisfied condition "Succeeded or Failed"
May  9 16:27:56.517: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-9fdb09de-cfa3-4f50-a5f2-84702f870aaa container test-container: <nil>
STEP: delete the pod 05/09/23 16:27:56.535
May  9 16:27:56.553: INFO: Waiting for pod pod-9fdb09de-cfa3-4f50-a5f2-84702f870aaa to disappear
May  9 16:27:56.557: INFO: Pod pod-9fdb09de-cfa3-4f50-a5f2-84702f870aaa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  9 16:27:56.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7460" for this suite. 05/09/23 16:27:56.563
------------------------------
â€¢ [4.113 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:27:52.46
    May  9 16:27:52.460: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename emptydir 05/09/23 16:27:52.461
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:27:52.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:27:52.484
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 05/09/23 16:27:52.489
    May  9 16:27:52.499: INFO: Waiting up to 5m0s for pod "pod-9fdb09de-cfa3-4f50-a5f2-84702f870aaa" in namespace "emptydir-7460" to be "Succeeded or Failed"
    May  9 16:27:52.504: INFO: Pod "pod-9fdb09de-cfa3-4f50-a5f2-84702f870aaa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.828434ms
    May  9 16:27:54.511: INFO: Pod "pod-9fdb09de-cfa3-4f50-a5f2-84702f870aaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012371705s
    May  9 16:27:56.511: INFO: Pod "pod-9fdb09de-cfa3-4f50-a5f2-84702f870aaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012739689s
    STEP: Saw pod success 05/09/23 16:27:56.511
    May  9 16:27:56.512: INFO: Pod "pod-9fdb09de-cfa3-4f50-a5f2-84702f870aaa" satisfied condition "Succeeded or Failed"
    May  9 16:27:56.517: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-9fdb09de-cfa3-4f50-a5f2-84702f870aaa container test-container: <nil>
    STEP: delete the pod 05/09/23 16:27:56.535
    May  9 16:27:56.553: INFO: Waiting for pod pod-9fdb09de-cfa3-4f50-a5f2-84702f870aaa to disappear
    May  9 16:27:56.557: INFO: Pod pod-9fdb09de-cfa3-4f50-a5f2-84702f870aaa no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  9 16:27:56.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7460" for this suite. 05/09/23 16:27:56.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:27:56.574
May  9 16:27:56.574: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 16:27:56.575
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:27:56.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:27:56.595
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 05/09/23 16:27:56.6
May  9 16:27:56.601: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: mark a version not serverd 05/09/23 16:28:03.151
STEP: check the unserved version gets removed 05/09/23 16:28:03.199
STEP: check the other version is not changed 05/09/23 16:28:04.505
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:28:08.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4089" for this suite. 05/09/23 16:28:08.278
------------------------------
â€¢ [SLOW TEST] [11.716 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:27:56.574
    May  9 16:27:56.574: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 16:27:56.575
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:27:56.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:27:56.595
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 05/09/23 16:27:56.6
    May  9 16:27:56.601: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: mark a version not serverd 05/09/23 16:28:03.151
    STEP: check the unserved version gets removed 05/09/23 16:28:03.199
    STEP: check the other version is not changed 05/09/23 16:28:04.505
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:28:08.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4089" for this suite. 05/09/23 16:28:08.278
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:28:08.29
May  9 16:28:08.290: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename namespaces 05/09/23 16:28:08.292
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:28:08.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:28:08.32
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 05/09/23 16:28:08.327
May  9 16:28:08.332: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 05/09/23 16:28:08.332
May  9 16:28:08.339: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 05/09/23 16:28:08.339
May  9 16:28:08.352: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 16:28:08.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6683" for this suite. 05/09/23 16:28:08.36
------------------------------
â€¢ [0.079 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:28:08.29
    May  9 16:28:08.290: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename namespaces 05/09/23 16:28:08.292
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:28:08.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:28:08.32
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 05/09/23 16:28:08.327
    May  9 16:28:08.332: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 05/09/23 16:28:08.332
    May  9 16:28:08.339: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 05/09/23 16:28:08.339
    May  9 16:28:08.352: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 16:28:08.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6683" for this suite. 05/09/23 16:28:08.36
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:28:08.37
May  9 16:28:08.371: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename container-probe 05/09/23 16:28:08.371
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:28:08.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:28:08.398
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-9275eebd-822c-44db-9375-1a2506e02979 in namespace container-probe-850 05/09/23 16:28:08.403
May  9 16:28:08.422: INFO: Waiting up to 5m0s for pod "liveness-9275eebd-822c-44db-9375-1a2506e02979" in namespace "container-probe-850" to be "not pending"
May  9 16:28:08.426: INFO: Pod "liveness-9275eebd-822c-44db-9375-1a2506e02979": Phase="Pending", Reason="", readiness=false. Elapsed: 4.359598ms
May  9 16:28:10.433: INFO: Pod "liveness-9275eebd-822c-44db-9375-1a2506e02979": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011057906s
May  9 16:28:12.433: INFO: Pod "liveness-9275eebd-822c-44db-9375-1a2506e02979": Phase="Running", Reason="", readiness=true. Elapsed: 4.011540012s
May  9 16:28:12.433: INFO: Pod "liveness-9275eebd-822c-44db-9375-1a2506e02979" satisfied condition "not pending"
May  9 16:28:12.433: INFO: Started pod liveness-9275eebd-822c-44db-9375-1a2506e02979 in namespace container-probe-850
STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 16:28:12.433
May  9 16:28:12.438: INFO: Initial restart count of pod liveness-9275eebd-822c-44db-9375-1a2506e02979 is 0
May  9 16:28:30.512: INFO: Restart count of pod container-probe-850/liveness-9275eebd-822c-44db-9375-1a2506e02979 is now 1 (18.074033591s elapsed)
STEP: deleting the pod 05/09/23 16:28:30.512
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May  9 16:28:30.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-850" for this suite. 05/09/23 16:28:30.536
------------------------------
â€¢ [SLOW TEST] [22.176 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:28:08.37
    May  9 16:28:08.371: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename container-probe 05/09/23 16:28:08.371
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:28:08.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:28:08.398
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-9275eebd-822c-44db-9375-1a2506e02979 in namespace container-probe-850 05/09/23 16:28:08.403
    May  9 16:28:08.422: INFO: Waiting up to 5m0s for pod "liveness-9275eebd-822c-44db-9375-1a2506e02979" in namespace "container-probe-850" to be "not pending"
    May  9 16:28:08.426: INFO: Pod "liveness-9275eebd-822c-44db-9375-1a2506e02979": Phase="Pending", Reason="", readiness=false. Elapsed: 4.359598ms
    May  9 16:28:10.433: INFO: Pod "liveness-9275eebd-822c-44db-9375-1a2506e02979": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011057906s
    May  9 16:28:12.433: INFO: Pod "liveness-9275eebd-822c-44db-9375-1a2506e02979": Phase="Running", Reason="", readiness=true. Elapsed: 4.011540012s
    May  9 16:28:12.433: INFO: Pod "liveness-9275eebd-822c-44db-9375-1a2506e02979" satisfied condition "not pending"
    May  9 16:28:12.433: INFO: Started pod liveness-9275eebd-822c-44db-9375-1a2506e02979 in namespace container-probe-850
    STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 16:28:12.433
    May  9 16:28:12.438: INFO: Initial restart count of pod liveness-9275eebd-822c-44db-9375-1a2506e02979 is 0
    May  9 16:28:30.512: INFO: Restart count of pod container-probe-850/liveness-9275eebd-822c-44db-9375-1a2506e02979 is now 1 (18.074033591s elapsed)
    STEP: deleting the pod 05/09/23 16:28:30.512
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May  9 16:28:30.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-850" for this suite. 05/09/23 16:28:30.536
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:28:30.547
May  9 16:28:30.547: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename container-runtime 05/09/23 16:28:30.548
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:28:30.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:28:30.584
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 05/09/23 16:28:30.59
STEP: wait for the container to reach Succeeded 05/09/23 16:28:30.6
STEP: get the container status 05/09/23 16:28:35.64
STEP: the container should be terminated 05/09/23 16:28:35.646
STEP: the termination message should be set 05/09/23 16:28:35.646
May  9 16:28:35.646: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 05/09/23 16:28:35.646
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May  9 16:28:35.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3925" for this suite. 05/09/23 16:28:35.669
------------------------------
â€¢ [SLOW TEST] [5.129 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:28:30.547
    May  9 16:28:30.547: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename container-runtime 05/09/23 16:28:30.548
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:28:30.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:28:30.584
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 05/09/23 16:28:30.59
    STEP: wait for the container to reach Succeeded 05/09/23 16:28:30.6
    STEP: get the container status 05/09/23 16:28:35.64
    STEP: the container should be terminated 05/09/23 16:28:35.646
    STEP: the termination message should be set 05/09/23 16:28:35.646
    May  9 16:28:35.646: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 05/09/23 16:28:35.646
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May  9 16:28:35.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3925" for this suite. 05/09/23 16:28:35.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:28:35.677
May  9 16:28:35.677: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename emptydir 05/09/23 16:28:35.678
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:28:35.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:28:35.698
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 05/09/23 16:28:35.703
May  9 16:28:35.714: INFO: Waiting up to 5m0s for pod "pod-5cd35404-6241-4e0a-b711-ad5445dc7767" in namespace "emptydir-5547" to be "Succeeded or Failed"
May  9 16:28:35.727: INFO: Pod "pod-5cd35404-6241-4e0a-b711-ad5445dc7767": Phase="Pending", Reason="", readiness=false. Elapsed: 12.879609ms
May  9 16:28:37.735: INFO: Pod "pod-5cd35404-6241-4e0a-b711-ad5445dc7767": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020563065s
May  9 16:28:39.748: INFO: Pod "pod-5cd35404-6241-4e0a-b711-ad5445dc7767": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033183725s
May  9 16:28:41.734: INFO: Pod "pod-5cd35404-6241-4e0a-b711-ad5445dc7767": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019471525s
STEP: Saw pod success 05/09/23 16:28:41.734
May  9 16:28:41.734: INFO: Pod "pod-5cd35404-6241-4e0a-b711-ad5445dc7767" satisfied condition "Succeeded or Failed"
May  9 16:28:41.740: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-5cd35404-6241-4e0a-b711-ad5445dc7767 container test-container: <nil>
STEP: delete the pod 05/09/23 16:28:41.752
May  9 16:28:41.769: INFO: Waiting for pod pod-5cd35404-6241-4e0a-b711-ad5445dc7767 to disappear
May  9 16:28:41.772: INFO: Pod pod-5cd35404-6241-4e0a-b711-ad5445dc7767 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  9 16:28:41.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5547" for this suite. 05/09/23 16:28:41.779
------------------------------
â€¢ [SLOW TEST] [6.109 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:28:35.677
    May  9 16:28:35.677: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename emptydir 05/09/23 16:28:35.678
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:28:35.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:28:35.698
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 05/09/23 16:28:35.703
    May  9 16:28:35.714: INFO: Waiting up to 5m0s for pod "pod-5cd35404-6241-4e0a-b711-ad5445dc7767" in namespace "emptydir-5547" to be "Succeeded or Failed"
    May  9 16:28:35.727: INFO: Pod "pod-5cd35404-6241-4e0a-b711-ad5445dc7767": Phase="Pending", Reason="", readiness=false. Elapsed: 12.879609ms
    May  9 16:28:37.735: INFO: Pod "pod-5cd35404-6241-4e0a-b711-ad5445dc7767": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020563065s
    May  9 16:28:39.748: INFO: Pod "pod-5cd35404-6241-4e0a-b711-ad5445dc7767": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033183725s
    May  9 16:28:41.734: INFO: Pod "pod-5cd35404-6241-4e0a-b711-ad5445dc7767": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019471525s
    STEP: Saw pod success 05/09/23 16:28:41.734
    May  9 16:28:41.734: INFO: Pod "pod-5cd35404-6241-4e0a-b711-ad5445dc7767" satisfied condition "Succeeded or Failed"
    May  9 16:28:41.740: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-5cd35404-6241-4e0a-b711-ad5445dc7767 container test-container: <nil>
    STEP: delete the pod 05/09/23 16:28:41.752
    May  9 16:28:41.769: INFO: Waiting for pod pod-5cd35404-6241-4e0a-b711-ad5445dc7767 to disappear
    May  9 16:28:41.772: INFO: Pod pod-5cd35404-6241-4e0a-b711-ad5445dc7767 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  9 16:28:41.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5547" for this suite. 05/09/23 16:28:41.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:28:41.787
May  9 16:28:41.787: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename secrets 05/09/23 16:28:41.787
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:28:41.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:28:41.807
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-7587f3c4-7996-462f-aa69-103226c89e74 05/09/23 16:28:41.812
STEP: Creating a pod to test consume secrets 05/09/23 16:28:41.818
May  9 16:28:41.828: INFO: Waiting up to 5m0s for pod "pod-secrets-c212b34d-a90a-4b90-8783-35180da64dcd" in namespace "secrets-7762" to be "Succeeded or Failed"
May  9 16:28:41.835: INFO: Pod "pod-secrets-c212b34d-a90a-4b90-8783-35180da64dcd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.335758ms
May  9 16:28:43.841: INFO: Pod "pod-secrets-c212b34d-a90a-4b90-8783-35180da64dcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01227807s
May  9 16:28:45.842: INFO: Pod "pod-secrets-c212b34d-a90a-4b90-8783-35180da64dcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013701916s
STEP: Saw pod success 05/09/23 16:28:45.842
May  9 16:28:45.842: INFO: Pod "pod-secrets-c212b34d-a90a-4b90-8783-35180da64dcd" satisfied condition "Succeeded or Failed"
May  9 16:28:45.848: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-secrets-c212b34d-a90a-4b90-8783-35180da64dcd container secret-volume-test: <nil>
STEP: delete the pod 05/09/23 16:28:45.86
May  9 16:28:45.876: INFO: Waiting for pod pod-secrets-c212b34d-a90a-4b90-8783-35180da64dcd to disappear
May  9 16:28:45.880: INFO: Pod pod-secrets-c212b34d-a90a-4b90-8783-35180da64dcd no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May  9 16:28:45.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7762" for this suite. 05/09/23 16:28:45.886
------------------------------
â€¢ [4.109 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:28:41.787
    May  9 16:28:41.787: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename secrets 05/09/23 16:28:41.787
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:28:41.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:28:41.807
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-7587f3c4-7996-462f-aa69-103226c89e74 05/09/23 16:28:41.812
    STEP: Creating a pod to test consume secrets 05/09/23 16:28:41.818
    May  9 16:28:41.828: INFO: Waiting up to 5m0s for pod "pod-secrets-c212b34d-a90a-4b90-8783-35180da64dcd" in namespace "secrets-7762" to be "Succeeded or Failed"
    May  9 16:28:41.835: INFO: Pod "pod-secrets-c212b34d-a90a-4b90-8783-35180da64dcd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.335758ms
    May  9 16:28:43.841: INFO: Pod "pod-secrets-c212b34d-a90a-4b90-8783-35180da64dcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01227807s
    May  9 16:28:45.842: INFO: Pod "pod-secrets-c212b34d-a90a-4b90-8783-35180da64dcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013701916s
    STEP: Saw pod success 05/09/23 16:28:45.842
    May  9 16:28:45.842: INFO: Pod "pod-secrets-c212b34d-a90a-4b90-8783-35180da64dcd" satisfied condition "Succeeded or Failed"
    May  9 16:28:45.848: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-secrets-c212b34d-a90a-4b90-8783-35180da64dcd container secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 16:28:45.86
    May  9 16:28:45.876: INFO: Waiting for pod pod-secrets-c212b34d-a90a-4b90-8783-35180da64dcd to disappear
    May  9 16:28:45.880: INFO: Pod pod-secrets-c212b34d-a90a-4b90-8783-35180da64dcd no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May  9 16:28:45.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7762" for this suite. 05/09/23 16:28:45.886
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:28:45.897
May  9 16:28:45.897: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename statefulset 05/09/23 16:28:45.897
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:28:45.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:28:45.919
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4256 05/09/23 16:28:45.924
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-4256 05/09/23 16:28:45.934
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4256 05/09/23 16:28:45.94
May  9 16:28:45.948: INFO: Found 0 stateful pods, waiting for 1
May  9 16:28:55.957: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 05/09/23 16:28:55.957
May  9 16:28:55.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-4256 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 16:28:56.198: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 16:28:56.198: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 16:28:56.198: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  9 16:28:56.254: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  9 16:29:06.283: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  9 16:29:06.283: INFO: Waiting for statefulset status.replicas updated to 0
May  9 16:29:06.313: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
May  9 16:29:06.313: INFO: ss-0  nodepool-8cc7f47e-9b0c-4801-88-node-7ad816  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:28:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:28:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:28:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:28:45 +0000 UTC  }]
May  9 16:29:06.313: INFO: 
May  9 16:29:06.313: INFO: StatefulSet ss has not reached scale 3, at 1
May  9 16:29:07.320: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993989778s
May  9 16:29:08.327: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987519541s
May  9 16:29:09.335: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980086655s
May  9 16:29:10.343: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972609484s
May  9 16:29:11.350: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.963955237s
May  9 16:29:12.358: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.956959234s
May  9 16:29:13.388: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.949323866s
May  9 16:29:14.396: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.918822483s
May  9 16:29:15.403: INFO: Verifying statefulset ss doesn't scale past 3 for another 910.779328ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4256 05/09/23 16:29:16.403
May  9 16:29:16.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-4256 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  9 16:29:16.669: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  9 16:29:16.669: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  9 16:29:16.669: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  9 16:29:16.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-4256 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  9 16:29:16.911: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May  9 16:29:16.911: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  9 16:29:16.911: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  9 16:29:16.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-4256 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  9 16:29:17.164: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May  9 16:29:17.164: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  9 16:29:17.164: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  9 16:29:17.170: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
May  9 16:29:27.183: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  9 16:29:27.183: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  9 16:29:27.183: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 05/09/23 16:29:27.183
May  9 16:29:27.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-4256 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 16:29:27.437: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 16:29:27.437: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 16:29:27.437: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  9 16:29:27.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-4256 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 16:29:27.695: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 16:29:27.696: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 16:29:27.696: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  9 16:29:27.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-4256 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 16:29:27.916: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 16:29:27.916: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 16:29:27.916: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  9 16:29:27.916: INFO: Waiting for statefulset status.replicas updated to 0
May  9 16:29:27.922: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May  9 16:29:37.935: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  9 16:29:37.935: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  9 16:29:37.935: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  9 16:29:37.952: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
May  9 16:29:37.952: INFO: ss-0  nodepool-8cc7f47e-9b0c-4801-88-node-7ad816  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:28:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:28:45 +0000 UTC  }]
May  9 16:29:37.952: INFO: ss-1  nodepool-8cc7f47e-9b0c-4801-88-node-f76f62  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:06 +0000 UTC  }]
May  9 16:29:37.953: INFO: ss-2  nodepool-8cc7f47e-9b0c-4801-88-node-bbade7  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:06 +0000 UTC  }]
May  9 16:29:37.953: INFO: 
May  9 16:29:37.953: INFO: StatefulSet ss has not reached scale 0, at 3
May  9 16:29:38.959: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
May  9 16:29:38.959: INFO: ss-0  nodepool-8cc7f47e-9b0c-4801-88-node-7ad816  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:28:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:28:45 +0000 UTC  }]
May  9 16:29:38.959: INFO: ss-2  nodepool-8cc7f47e-9b0c-4801-88-node-bbade7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:06 +0000 UTC  }]
May  9 16:29:38.959: INFO: 
May  9 16:29:38.959: INFO: StatefulSet ss has not reached scale 0, at 2
May  9 16:29:39.967: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.987300641s
May  9 16:29:40.974: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.979790365s
May  9 16:29:41.981: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.972782586s
May  9 16:29:42.987: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.965479207s
May  9 16:29:43.992: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.960020245s
May  9 16:29:44.998: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.954265651s
May  9 16:29:46.005: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.948250919s
May  9 16:29:47.011: INFO: Verifying statefulset ss doesn't scale past 0 for another 942.021431ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4256 05/09/23 16:29:48.011
May  9 16:29:48.017: INFO: Scaling statefulset ss to 0
May  9 16:29:48.038: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May  9 16:29:48.044: INFO: Deleting all statefulset in ns statefulset-4256
May  9 16:29:48.048: INFO: Scaling statefulset ss to 0
May  9 16:29:48.062: INFO: Waiting for statefulset status.replicas updated to 0
May  9 16:29:48.071: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May  9 16:29:48.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4256" for this suite. 05/09/23 16:29:48.107
------------------------------
â€¢ [SLOW TEST] [62.223 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:28:45.897
    May  9 16:28:45.897: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename statefulset 05/09/23 16:28:45.897
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:28:45.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:28:45.919
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4256 05/09/23 16:28:45.924
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-4256 05/09/23 16:28:45.934
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4256 05/09/23 16:28:45.94
    May  9 16:28:45.948: INFO: Found 0 stateful pods, waiting for 1
    May  9 16:28:55.957: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 05/09/23 16:28:55.957
    May  9 16:28:55.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-4256 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 16:28:56.198: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 16:28:56.198: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 16:28:56.198: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  9 16:28:56.254: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    May  9 16:29:06.283: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  9 16:29:06.283: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 16:29:06.313: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
    May  9 16:29:06.313: INFO: ss-0  nodepool-8cc7f47e-9b0c-4801-88-node-7ad816  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:28:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:28:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:28:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:28:45 +0000 UTC  }]
    May  9 16:29:06.313: INFO: 
    May  9 16:29:06.313: INFO: StatefulSet ss has not reached scale 3, at 1
    May  9 16:29:07.320: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993989778s
    May  9 16:29:08.327: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987519541s
    May  9 16:29:09.335: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980086655s
    May  9 16:29:10.343: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972609484s
    May  9 16:29:11.350: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.963955237s
    May  9 16:29:12.358: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.956959234s
    May  9 16:29:13.388: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.949323866s
    May  9 16:29:14.396: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.918822483s
    May  9 16:29:15.403: INFO: Verifying statefulset ss doesn't scale past 3 for another 910.779328ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4256 05/09/23 16:29:16.403
    May  9 16:29:16.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-4256 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  9 16:29:16.669: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  9 16:29:16.669: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  9 16:29:16.669: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  9 16:29:16.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-4256 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  9 16:29:16.911: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    May  9 16:29:16.911: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  9 16:29:16.911: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  9 16:29:16.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-4256 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  9 16:29:17.164: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    May  9 16:29:17.164: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  9 16:29:17.164: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  9 16:29:17.170: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    May  9 16:29:27.183: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May  9 16:29:27.183: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    May  9 16:29:27.183: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 05/09/23 16:29:27.183
    May  9 16:29:27.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-4256 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 16:29:27.437: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 16:29:27.437: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 16:29:27.437: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  9 16:29:27.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-4256 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 16:29:27.695: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 16:29:27.696: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 16:29:27.696: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  9 16:29:27.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-4256 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 16:29:27.916: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 16:29:27.916: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 16:29:27.916: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  9 16:29:27.916: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 16:29:27.922: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    May  9 16:29:37.935: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  9 16:29:37.935: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    May  9 16:29:37.935: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    May  9 16:29:37.952: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
    May  9 16:29:37.952: INFO: ss-0  nodepool-8cc7f47e-9b0c-4801-88-node-7ad816  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:28:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:28:45 +0000 UTC  }]
    May  9 16:29:37.952: INFO: ss-1  nodepool-8cc7f47e-9b0c-4801-88-node-f76f62  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:06 +0000 UTC  }]
    May  9 16:29:37.953: INFO: ss-2  nodepool-8cc7f47e-9b0c-4801-88-node-bbade7  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:06 +0000 UTC  }]
    May  9 16:29:37.953: INFO: 
    May  9 16:29:37.953: INFO: StatefulSet ss has not reached scale 0, at 3
    May  9 16:29:38.959: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
    May  9 16:29:38.959: INFO: ss-0  nodepool-8cc7f47e-9b0c-4801-88-node-7ad816  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:28:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:28:45 +0000 UTC  }]
    May  9 16:29:38.959: INFO: ss-2  nodepool-8cc7f47e-9b0c-4801-88-node-bbade7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:29:06 +0000 UTC  }]
    May  9 16:29:38.959: INFO: 
    May  9 16:29:38.959: INFO: StatefulSet ss has not reached scale 0, at 2
    May  9 16:29:39.967: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.987300641s
    May  9 16:29:40.974: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.979790365s
    May  9 16:29:41.981: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.972782586s
    May  9 16:29:42.987: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.965479207s
    May  9 16:29:43.992: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.960020245s
    May  9 16:29:44.998: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.954265651s
    May  9 16:29:46.005: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.948250919s
    May  9 16:29:47.011: INFO: Verifying statefulset ss doesn't scale past 0 for another 942.021431ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4256 05/09/23 16:29:48.011
    May  9 16:29:48.017: INFO: Scaling statefulset ss to 0
    May  9 16:29:48.038: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May  9 16:29:48.044: INFO: Deleting all statefulset in ns statefulset-4256
    May  9 16:29:48.048: INFO: Scaling statefulset ss to 0
    May  9 16:29:48.062: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 16:29:48.071: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May  9 16:29:48.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4256" for this suite. 05/09/23 16:29:48.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:29:48.121
May  9 16:29:48.121: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename job 05/09/23 16:29:48.122
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:29:48.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:29:48.147
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 05/09/23 16:29:48.158
STEP: Patching the Job 05/09/23 16:29:48.167
STEP: Watching for Job to be patched 05/09/23 16:29:48.175
May  9 16:29:48.178: INFO: Event ADDED observed for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5] and annotations: map[batch.kubernetes.io/job-tracking:]
May  9 16:29:48.178: INFO: Event MODIFIED found for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 05/09/23 16:29:48.178
STEP: Watching for Job to be updated 05/09/23 16:29:48.215
May  9 16:29:48.217: INFO: Event MODIFIED found for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 16:29:48.217: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 05/09/23 16:29:48.217
May  9 16:29:48.222: INFO: Job: e2e-jszb5 as labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched]
STEP: Waiting for job to complete 05/09/23 16:29:48.222
STEP: Delete a job collection with a labelselector 05/09/23 16:29:58.227
STEP: Watching for Job to be deleted 05/09/23 16:29:58.242
May  9 16:29:58.246: INFO: Event MODIFIED observed for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 16:29:58.246: INFO: Event MODIFIED observed for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 16:29:58.246: INFO: Event MODIFIED observed for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 16:29:58.246: INFO: Event MODIFIED observed for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 16:29:58.246: INFO: Event MODIFIED observed for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 16:29:58.246: INFO: Event MODIFIED observed for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 16:29:58.247: INFO: Event MODIFIED observed for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  9 16:29:58.247: INFO: Event DELETED found for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 05/09/23 16:29:58.247
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May  9 16:29:58.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2432" for this suite. 05/09/23 16:29:58.266
------------------------------
â€¢ [SLOW TEST] [10.156 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:29:48.121
    May  9 16:29:48.121: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename job 05/09/23 16:29:48.122
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:29:48.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:29:48.147
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 05/09/23 16:29:48.158
    STEP: Patching the Job 05/09/23 16:29:48.167
    STEP: Watching for Job to be patched 05/09/23 16:29:48.175
    May  9 16:29:48.178: INFO: Event ADDED observed for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5] and annotations: map[batch.kubernetes.io/job-tracking:]
    May  9 16:29:48.178: INFO: Event MODIFIED found for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 05/09/23 16:29:48.178
    STEP: Watching for Job to be updated 05/09/23 16:29:48.215
    May  9 16:29:48.217: INFO: Event MODIFIED found for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 16:29:48.217: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 05/09/23 16:29:48.217
    May  9 16:29:48.222: INFO: Job: e2e-jszb5 as labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched]
    STEP: Waiting for job to complete 05/09/23 16:29:48.222
    STEP: Delete a job collection with a labelselector 05/09/23 16:29:58.227
    STEP: Watching for Job to be deleted 05/09/23 16:29:58.242
    May  9 16:29:58.246: INFO: Event MODIFIED observed for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 16:29:58.246: INFO: Event MODIFIED observed for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 16:29:58.246: INFO: Event MODIFIED observed for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 16:29:58.246: INFO: Event MODIFIED observed for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 16:29:58.246: INFO: Event MODIFIED observed for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 16:29:58.246: INFO: Event MODIFIED observed for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 16:29:58.247: INFO: Event MODIFIED observed for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  9 16:29:58.247: INFO: Event DELETED found for Job e2e-jszb5 in namespace job-2432 with labels: map[e2e-job-label:e2e-jszb5 e2e-jszb5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 05/09/23 16:29:58.247
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May  9 16:29:58.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2432" for this suite. 05/09/23 16:29:58.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:29:58.278
May  9 16:29:58.278: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename webhook 05/09/23 16:29:58.279
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:29:58.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:29:58.373
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/09/23 16:29:58.479
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:29:58.712
STEP: Deploying the webhook pod 05/09/23 16:29:58.724
STEP: Wait for the deployment to be ready 05/09/23 16:29:58.741
May  9 16:29:58.750: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 16:30:00.776
STEP: Verifying the service has paired with the endpoint 05/09/23 16:30:00.79
May  9 16:30:01.790: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 05/09/23 16:30:01.796
STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 16:30:01.82
STEP: Updating a validating webhook configuration's rules to not include the create operation 05/09/23 16:30:01.833
STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 16:30:01.847
STEP: Patching a validating webhook configuration's rules to include the create operation 05/09/23 16:30:01.867
STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 16:30:01.877
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:30:01.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4191" for this suite. 05/09/23 16:30:01.953
STEP: Destroying namespace "webhook-4191-markers" for this suite. 05/09/23 16:30:01.973
------------------------------
â€¢ [3.705 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:29:58.278
    May  9 16:29:58.278: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename webhook 05/09/23 16:29:58.279
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:29:58.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:29:58.373
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/09/23 16:29:58.479
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:29:58.712
    STEP: Deploying the webhook pod 05/09/23 16:29:58.724
    STEP: Wait for the deployment to be ready 05/09/23 16:29:58.741
    May  9 16:29:58.750: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 16:30:00.776
    STEP: Verifying the service has paired with the endpoint 05/09/23 16:30:00.79
    May  9 16:30:01.790: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 05/09/23 16:30:01.796
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 16:30:01.82
    STEP: Updating a validating webhook configuration's rules to not include the create operation 05/09/23 16:30:01.833
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 16:30:01.847
    STEP: Patching a validating webhook configuration's rules to include the create operation 05/09/23 16:30:01.867
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 16:30:01.877
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:30:01.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4191" for this suite. 05/09/23 16:30:01.953
    STEP: Destroying namespace "webhook-4191-markers" for this suite. 05/09/23 16:30:01.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:30:01.985
May  9 16:30:01.985: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename gc 05/09/23 16:30:01.987
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:30:02.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:30:02.012
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 05/09/23 16:30:02.017
STEP: Wait for the Deployment to create new ReplicaSet 05/09/23 16:30:02.029
STEP: delete the deployment 05/09/23 16:30:02.057
STEP: wait for all rs to be garbage collected 05/09/23 16:30:02.086
STEP: expected 0 pods, got 1 pods 05/09/23 16:30:02.121
STEP: Gathering metrics 05/09/23 16:30:02.641
W0509 16:30:02.655326      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  9 16:30:02.655: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May  9 16:30:02.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6689" for this suite. 05/09/23 16:30:02.661
------------------------------
â€¢ [0.686 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:30:01.985
    May  9 16:30:01.985: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename gc 05/09/23 16:30:01.987
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:30:02.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:30:02.012
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 05/09/23 16:30:02.017
    STEP: Wait for the Deployment to create new ReplicaSet 05/09/23 16:30:02.029
    STEP: delete the deployment 05/09/23 16:30:02.057
    STEP: wait for all rs to be garbage collected 05/09/23 16:30:02.086
    STEP: expected 0 pods, got 1 pods 05/09/23 16:30:02.121
    STEP: Gathering metrics 05/09/23 16:30:02.641
    W0509 16:30:02.655326      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May  9 16:30:02.655: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May  9 16:30:02.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6689" for this suite. 05/09/23 16:30:02.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:30:02.671
May  9 16:30:02.672: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename container-probe 05/09/23 16:30:02.672
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:30:02.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:30:02.696
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-336058f1-2dfa-495c-8cbb-7e7691bf2228 in namespace container-probe-4194 05/09/23 16:30:02.701
May  9 16:30:02.711: INFO: Waiting up to 5m0s for pod "test-webserver-336058f1-2dfa-495c-8cbb-7e7691bf2228" in namespace "container-probe-4194" to be "not pending"
May  9 16:30:02.716: INFO: Pod "test-webserver-336058f1-2dfa-495c-8cbb-7e7691bf2228": Phase="Pending", Reason="", readiness=false. Elapsed: 4.598878ms
May  9 16:30:04.731: INFO: Pod "test-webserver-336058f1-2dfa-495c-8cbb-7e7691bf2228": Phase="Running", Reason="", readiness=true. Elapsed: 2.02022535s
May  9 16:30:04.731: INFO: Pod "test-webserver-336058f1-2dfa-495c-8cbb-7e7691bf2228" satisfied condition "not pending"
May  9 16:30:04.731: INFO: Started pod test-webserver-336058f1-2dfa-495c-8cbb-7e7691bf2228 in namespace container-probe-4194
STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 16:30:04.731
May  9 16:30:04.742: INFO: Initial restart count of pod test-webserver-336058f1-2dfa-495c-8cbb-7e7691bf2228 is 0
STEP: deleting the pod 05/09/23 16:34:05.964
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May  9 16:34:05.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4194" for this suite. 05/09/23 16:34:05.987
------------------------------
â€¢ [SLOW TEST] [243.326 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:30:02.671
    May  9 16:30:02.672: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename container-probe 05/09/23 16:30:02.672
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:30:02.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:30:02.696
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-336058f1-2dfa-495c-8cbb-7e7691bf2228 in namespace container-probe-4194 05/09/23 16:30:02.701
    May  9 16:30:02.711: INFO: Waiting up to 5m0s for pod "test-webserver-336058f1-2dfa-495c-8cbb-7e7691bf2228" in namespace "container-probe-4194" to be "not pending"
    May  9 16:30:02.716: INFO: Pod "test-webserver-336058f1-2dfa-495c-8cbb-7e7691bf2228": Phase="Pending", Reason="", readiness=false. Elapsed: 4.598878ms
    May  9 16:30:04.731: INFO: Pod "test-webserver-336058f1-2dfa-495c-8cbb-7e7691bf2228": Phase="Running", Reason="", readiness=true. Elapsed: 2.02022535s
    May  9 16:30:04.731: INFO: Pod "test-webserver-336058f1-2dfa-495c-8cbb-7e7691bf2228" satisfied condition "not pending"
    May  9 16:30:04.731: INFO: Started pod test-webserver-336058f1-2dfa-495c-8cbb-7e7691bf2228 in namespace container-probe-4194
    STEP: checking the pod's current state and verifying that restartCount is present 05/09/23 16:30:04.731
    May  9 16:30:04.742: INFO: Initial restart count of pod test-webserver-336058f1-2dfa-495c-8cbb-7e7691bf2228 is 0
    STEP: deleting the pod 05/09/23 16:34:05.964
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May  9 16:34:05.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4194" for this suite. 05/09/23 16:34:05.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:34:06
May  9 16:34:06.000: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 16:34:06.001
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:34:06.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:34:06.034
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
May  9 16:34:06.042: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:34:06.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-624" for this suite. 05/09/23 16:34:06.6
------------------------------
â€¢ [0.608 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:34:06
    May  9 16:34:06.000: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 16:34:06.001
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:34:06.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:34:06.034
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    May  9 16:34:06.042: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:34:06.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-624" for this suite. 05/09/23 16:34:06.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:34:06.609
May  9 16:34:06.609: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename job 05/09/23 16:34:06.61
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:34:06.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:34:06.632
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 05/09/23 16:34:06.636
STEP: Ensuring job reaches completions 05/09/23 16:34:06.652
STEP: Ensuring pods with index for job exist 05/09/23 16:34:16.658
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May  9 16:34:16.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7982" for this suite. 05/09/23 16:34:16.673
------------------------------
â€¢ [SLOW TEST] [10.071 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:34:06.609
    May  9 16:34:06.609: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename job 05/09/23 16:34:06.61
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:34:06.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:34:06.632
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 05/09/23 16:34:06.636
    STEP: Ensuring job reaches completions 05/09/23 16:34:06.652
    STEP: Ensuring pods with index for job exist 05/09/23 16:34:16.658
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May  9 16:34:16.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7982" for this suite. 05/09/23 16:34:16.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:34:16.682
May  9 16:34:16.682: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename statefulset 05/09/23 16:34:16.682
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:34:16.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:34:16.709
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5804 05/09/23 16:34:16.713
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-5804 05/09/23 16:34:16.734
May  9 16:34:16.745: INFO: Found 0 stateful pods, waiting for 1
May  9 16:34:26.751: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 05/09/23 16:34:26.765
STEP: Getting /status 05/09/23 16:34:26.773
May  9 16:34:26.780: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 05/09/23 16:34:26.78
May  9 16:34:26.792: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 05/09/23 16:34:26.792
May  9 16:34:26.796: INFO: Observed &StatefulSet event: ADDED
May  9 16:34:26.796: INFO: Found Statefulset ss in namespace statefulset-5804 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  9 16:34:26.796: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 05/09/23 16:34:26.796
May  9 16:34:26.796: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May  9 16:34:26.805: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 05/09/23 16:34:26.805
May  9 16:34:26.808: INFO: Observed &StatefulSet event: ADDED
May  9 16:34:26.808: INFO: Observed Statefulset ss in namespace statefulset-5804 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  9 16:34:26.808: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May  9 16:34:26.808: INFO: Deleting all statefulset in ns statefulset-5804
May  9 16:34:26.812: INFO: Scaling statefulset ss to 0
May  9 16:34:36.838: INFO: Waiting for statefulset status.replicas updated to 0
May  9 16:34:36.844: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May  9 16:34:36.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5804" for this suite. 05/09/23 16:34:36.88
------------------------------
â€¢ [SLOW TEST] [20.218 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:34:16.682
    May  9 16:34:16.682: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename statefulset 05/09/23 16:34:16.682
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:34:16.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:34:16.709
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5804 05/09/23 16:34:16.713
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-5804 05/09/23 16:34:16.734
    May  9 16:34:16.745: INFO: Found 0 stateful pods, waiting for 1
    May  9 16:34:26.751: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 05/09/23 16:34:26.765
    STEP: Getting /status 05/09/23 16:34:26.773
    May  9 16:34:26.780: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 05/09/23 16:34:26.78
    May  9 16:34:26.792: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 05/09/23 16:34:26.792
    May  9 16:34:26.796: INFO: Observed &StatefulSet event: ADDED
    May  9 16:34:26.796: INFO: Found Statefulset ss in namespace statefulset-5804 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  9 16:34:26.796: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 05/09/23 16:34:26.796
    May  9 16:34:26.796: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May  9 16:34:26.805: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 05/09/23 16:34:26.805
    May  9 16:34:26.808: INFO: Observed &StatefulSet event: ADDED
    May  9 16:34:26.808: INFO: Observed Statefulset ss in namespace statefulset-5804 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  9 16:34:26.808: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May  9 16:34:26.808: INFO: Deleting all statefulset in ns statefulset-5804
    May  9 16:34:26.812: INFO: Scaling statefulset ss to 0
    May  9 16:34:36.838: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 16:34:36.844: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May  9 16:34:36.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5804" for this suite. 05/09/23 16:34:36.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:34:36.9
May  9 16:34:36.900: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename replicaset 05/09/23 16:34:36.901
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:34:36.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:34:36.919
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 05/09/23 16:34:36.923
May  9 16:34:36.935: INFO: Pod name sample-pod: Found 0 pods out of 1
May  9 16:34:41.943: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/09/23 16:34:41.943
STEP: getting scale subresource 05/09/23 16:34:41.943
STEP: updating a scale subresource 05/09/23 16:34:41.949
STEP: verifying the replicaset Spec.Replicas was modified 05/09/23 16:34:41.957
STEP: Patch a scale subresource 05/09/23 16:34:41.966
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May  9 16:34:41.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6315" for this suite. 05/09/23 16:34:41.993
------------------------------
â€¢ [SLOW TEST] [5.104 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:34:36.9
    May  9 16:34:36.900: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename replicaset 05/09/23 16:34:36.901
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:34:36.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:34:36.919
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 05/09/23 16:34:36.923
    May  9 16:34:36.935: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  9 16:34:41.943: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/09/23 16:34:41.943
    STEP: getting scale subresource 05/09/23 16:34:41.943
    STEP: updating a scale subresource 05/09/23 16:34:41.949
    STEP: verifying the replicaset Spec.Replicas was modified 05/09/23 16:34:41.957
    STEP: Patch a scale subresource 05/09/23 16:34:41.966
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May  9 16:34:41.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6315" for this suite. 05/09/23 16:34:41.993
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:34:42.005
May  9 16:34:42.005: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 16:34:42.006
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:34:42.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:34:42.067
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 05/09/23 16:34:42.074
May  9 16:34:42.074: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:34:44.127: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:34:51.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7905" for this suite. 05/09/23 16:34:51.919
------------------------------
â€¢ [SLOW TEST] [9.926 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:34:42.005
    May  9 16:34:42.005: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 16:34:42.006
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:34:42.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:34:42.067
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 05/09/23 16:34:42.074
    May  9 16:34:42.074: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:34:44.127: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:34:51.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7905" for this suite. 05/09/23 16:34:51.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:34:51.931
May  9 16:34:51.931: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename daemonsets 05/09/23 16:34:51.932
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:34:51.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:34:51.956
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
May  9 16:34:51.988: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 05/09/23 16:34:51.994
May  9 16:34:51.999: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:34:51.999: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 05/09/23 16:34:51.999
May  9 16:34:52.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:34:52.023: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 is running 0 daemon pod, expected 1
May  9 16:34:53.031: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:34:53.031: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 is running 0 daemon pod, expected 1
May  9 16:34:54.042: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  9 16:34:54.042: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 05/09/23 16:34:54.047
May  9 16:34:54.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  9 16:34:54.079: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
May  9 16:34:55.086: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:34:55.086: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 05/09/23 16:34:55.086
May  9 16:34:55.098: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:34:55.098: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 is running 0 daemon pod, expected 1
May  9 16:34:56.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:34:56.109: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 is running 0 daemon pod, expected 1
May  9 16:34:57.130: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:34:57.130: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 is running 0 daemon pod, expected 1
May  9 16:34:58.107: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  9 16:34:58.107: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/09/23 16:34:58.12
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4423, will wait for the garbage collector to delete the pods 05/09/23 16:34:58.12
May  9 16:34:58.186: INFO: Deleting DaemonSet.extensions daemon-set took: 10.132188ms
May  9 16:34:58.287: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.09475ms
May  9 16:35:00.993: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:35:00.993: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  9 16:35:00.999: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"318069543"},"items":null}

May  9 16:35:01.004: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"318069543"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 16:35:01.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4423" for this suite. 05/09/23 16:35:01.047
------------------------------
â€¢ [SLOW TEST] [9.125 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:34:51.931
    May  9 16:34:51.931: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename daemonsets 05/09/23 16:34:51.932
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:34:51.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:34:51.956
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    May  9 16:34:51.988: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 05/09/23 16:34:51.994
    May  9 16:34:51.999: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:34:51.999: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 05/09/23 16:34:51.999
    May  9 16:34:52.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:34:52.023: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 is running 0 daemon pod, expected 1
    May  9 16:34:53.031: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:34:53.031: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 is running 0 daemon pod, expected 1
    May  9 16:34:54.042: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  9 16:34:54.042: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 05/09/23 16:34:54.047
    May  9 16:34:54.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  9 16:34:54.079: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    May  9 16:34:55.086: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:34:55.086: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 05/09/23 16:34:55.086
    May  9 16:34:55.098: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:34:55.098: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 is running 0 daemon pod, expected 1
    May  9 16:34:56.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:34:56.109: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 is running 0 daemon pod, expected 1
    May  9 16:34:57.130: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:34:57.130: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 is running 0 daemon pod, expected 1
    May  9 16:34:58.107: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  9 16:34:58.107: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/09/23 16:34:58.12
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4423, will wait for the garbage collector to delete the pods 05/09/23 16:34:58.12
    May  9 16:34:58.186: INFO: Deleting DaemonSet.extensions daemon-set took: 10.132188ms
    May  9 16:34:58.287: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.09475ms
    May  9 16:35:00.993: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:35:00.993: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  9 16:35:00.999: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"318069543"},"items":null}

    May  9 16:35:01.004: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"318069543"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 16:35:01.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4423" for this suite. 05/09/23 16:35:01.047
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:35:01.056
May  9 16:35:01.056: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename replication-controller 05/09/23 16:35:01.057
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:35:01.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:35:01.128
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 05/09/23 16:35:01.135
May  9 16:35:01.146: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-1796" to be "running and ready"
May  9 16:35:01.164: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 17.515034ms
May  9 16:35:01.164: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
May  9 16:35:03.173: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.026268926s
May  9 16:35:03.173: INFO: The phase of Pod pod-adoption is Running (Ready = true)
May  9 16:35:03.173: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 05/09/23 16:35:03.177
STEP: Then the orphan pod is adopted 05/09/23 16:35:03.184
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May  9 16:35:04.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1796" for this suite. 05/09/23 16:35:04.205
------------------------------
â€¢ [3.162 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:35:01.056
    May  9 16:35:01.056: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename replication-controller 05/09/23 16:35:01.057
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:35:01.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:35:01.128
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 05/09/23 16:35:01.135
    May  9 16:35:01.146: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-1796" to be "running and ready"
    May  9 16:35:01.164: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 17.515034ms
    May  9 16:35:01.164: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:35:03.173: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.026268926s
    May  9 16:35:03.173: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    May  9 16:35:03.173: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 05/09/23 16:35:03.177
    STEP: Then the orphan pod is adopted 05/09/23 16:35:03.184
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May  9 16:35:04.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1796" for this suite. 05/09/23 16:35:04.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:35:04.22
May  9 16:35:04.220: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename services 05/09/23 16:35:04.221
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:35:04.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:35:04.254
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-9079 05/09/23 16:35:04.258
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9079 to expose endpoints map[] 05/09/23 16:35:04.274
May  9 16:35:04.287: INFO: successfully validated that service multi-endpoint-test in namespace services-9079 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9079 05/09/23 16:35:04.287
May  9 16:35:04.299: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9079" to be "running and ready"
May  9 16:35:04.307: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.702575ms
May  9 16:35:04.307: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:35:06.317: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.017205089s
May  9 16:35:06.317: INFO: The phase of Pod pod1 is Running (Ready = true)
May  9 16:35:06.317: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9079 to expose endpoints map[pod1:[100]] 05/09/23 16:35:06.322
May  9 16:35:06.342: INFO: successfully validated that service multi-endpoint-test in namespace services-9079 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9079 05/09/23 16:35:06.342
May  9 16:35:06.350: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9079" to be "running and ready"
May  9 16:35:06.356: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.925618ms
May  9 16:35:06.356: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:35:08.363: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013118959s
May  9 16:35:08.363: INFO: The phase of Pod pod2 is Running (Ready = true)
May  9 16:35:08.363: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9079 to expose endpoints map[pod1:[100] pod2:[101]] 05/09/23 16:35:08.368
May  9 16:35:08.390: INFO: successfully validated that service multi-endpoint-test in namespace services-9079 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 05/09/23 16:35:08.39
May  9 16:35:08.390: INFO: Creating new exec pod
May  9 16:35:08.399: INFO: Waiting up to 5m0s for pod "execpodxjnbc" in namespace "services-9079" to be "running"
May  9 16:35:08.404: INFO: Pod "execpodxjnbc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.390763ms
May  9 16:35:10.411: INFO: Pod "execpodxjnbc": Phase="Running", Reason="", readiness=true. Elapsed: 2.011989691s
May  9 16:35:10.411: INFO: Pod "execpodxjnbc" satisfied condition "running"
May  9 16:35:11.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-9079 exec execpodxjnbc -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
May  9 16:35:11.635: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
May  9 16:35:11.635: INFO: stdout: ""
May  9 16:35:11.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-9079 exec execpodxjnbc -- /bin/sh -x -c nc -v -z -w 2 10.3.230.240 80'
May  9 16:35:11.863: INFO: stderr: "+ nc -v -z -w 2 10.3.230.240 80\nConnection to 10.3.230.240 80 port [tcp/http] succeeded!\n"
May  9 16:35:11.863: INFO: stdout: ""
May  9 16:35:11.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-9079 exec execpodxjnbc -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
May  9 16:35:12.086: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
May  9 16:35:12.086: INFO: stdout: ""
May  9 16:35:12.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-9079 exec execpodxjnbc -- /bin/sh -x -c nc -v -z -w 2 10.3.230.240 81'
May  9 16:35:12.333: INFO: stderr: "+ nc -v -z -w 2 10.3.230.240 81\nConnection to 10.3.230.240 81 port [tcp/*] succeeded!\n"
May  9 16:35:12.333: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-9079 05/09/23 16:35:12.333
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9079 to expose endpoints map[pod2:[101]] 05/09/23 16:35:12.349
May  9 16:35:12.367: INFO: successfully validated that service multi-endpoint-test in namespace services-9079 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9079 05/09/23 16:35:12.367
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9079 to expose endpoints map[] 05/09/23 16:35:12.382
May  9 16:35:12.395: INFO: successfully validated that service multi-endpoint-test in namespace services-9079 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  9 16:35:12.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9079" for this suite. 05/09/23 16:35:12.422
------------------------------
â€¢ [SLOW TEST] [8.211 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:35:04.22
    May  9 16:35:04.220: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename services 05/09/23 16:35:04.221
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:35:04.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:35:04.254
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-9079 05/09/23 16:35:04.258
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9079 to expose endpoints map[] 05/09/23 16:35:04.274
    May  9 16:35:04.287: INFO: successfully validated that service multi-endpoint-test in namespace services-9079 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9079 05/09/23 16:35:04.287
    May  9 16:35:04.299: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9079" to be "running and ready"
    May  9 16:35:04.307: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.702575ms
    May  9 16:35:04.307: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:35:06.317: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.017205089s
    May  9 16:35:06.317: INFO: The phase of Pod pod1 is Running (Ready = true)
    May  9 16:35:06.317: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9079 to expose endpoints map[pod1:[100]] 05/09/23 16:35:06.322
    May  9 16:35:06.342: INFO: successfully validated that service multi-endpoint-test in namespace services-9079 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-9079 05/09/23 16:35:06.342
    May  9 16:35:06.350: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9079" to be "running and ready"
    May  9 16:35:06.356: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.925618ms
    May  9 16:35:06.356: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:35:08.363: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013118959s
    May  9 16:35:08.363: INFO: The phase of Pod pod2 is Running (Ready = true)
    May  9 16:35:08.363: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9079 to expose endpoints map[pod1:[100] pod2:[101]] 05/09/23 16:35:08.368
    May  9 16:35:08.390: INFO: successfully validated that service multi-endpoint-test in namespace services-9079 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 05/09/23 16:35:08.39
    May  9 16:35:08.390: INFO: Creating new exec pod
    May  9 16:35:08.399: INFO: Waiting up to 5m0s for pod "execpodxjnbc" in namespace "services-9079" to be "running"
    May  9 16:35:08.404: INFO: Pod "execpodxjnbc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.390763ms
    May  9 16:35:10.411: INFO: Pod "execpodxjnbc": Phase="Running", Reason="", readiness=true. Elapsed: 2.011989691s
    May  9 16:35:10.411: INFO: Pod "execpodxjnbc" satisfied condition "running"
    May  9 16:35:11.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-9079 exec execpodxjnbc -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    May  9 16:35:11.635: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    May  9 16:35:11.635: INFO: stdout: ""
    May  9 16:35:11.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-9079 exec execpodxjnbc -- /bin/sh -x -c nc -v -z -w 2 10.3.230.240 80'
    May  9 16:35:11.863: INFO: stderr: "+ nc -v -z -w 2 10.3.230.240 80\nConnection to 10.3.230.240 80 port [tcp/http] succeeded!\n"
    May  9 16:35:11.863: INFO: stdout: ""
    May  9 16:35:11.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-9079 exec execpodxjnbc -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    May  9 16:35:12.086: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    May  9 16:35:12.086: INFO: stdout: ""
    May  9 16:35:12.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-9079 exec execpodxjnbc -- /bin/sh -x -c nc -v -z -w 2 10.3.230.240 81'
    May  9 16:35:12.333: INFO: stderr: "+ nc -v -z -w 2 10.3.230.240 81\nConnection to 10.3.230.240 81 port [tcp/*] succeeded!\n"
    May  9 16:35:12.333: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-9079 05/09/23 16:35:12.333
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9079 to expose endpoints map[pod2:[101]] 05/09/23 16:35:12.349
    May  9 16:35:12.367: INFO: successfully validated that service multi-endpoint-test in namespace services-9079 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-9079 05/09/23 16:35:12.367
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9079 to expose endpoints map[] 05/09/23 16:35:12.382
    May  9 16:35:12.395: INFO: successfully validated that service multi-endpoint-test in namespace services-9079 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  9 16:35:12.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9079" for this suite. 05/09/23 16:35:12.422
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:35:12.431
May  9 16:35:12.431: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename sched-preemption 05/09/23 16:35:12.432
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:35:12.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:35:12.456
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
May  9 16:35:12.476: INFO: Waiting up to 1m0s for all nodes to be ready
May  9 16:36:12.526: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 05/09/23 16:36:12.531
May  9 16:36:12.556: INFO: Created pod: pod0-0-sched-preemption-low-priority
May  9 16:36:12.565: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May  9 16:36:12.584: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May  9 16:36:12.592: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May  9 16:36:12.608: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May  9 16:36:12.616: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 05/09/23 16:36:12.616
May  9 16:36:12.616: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9171" to be "running"
May  9 16:36:12.624: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.858486ms
May  9 16:36:14.632: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.015598528s
May  9 16:36:14.632: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
May  9 16:36:14.632: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9171" to be "running"
May  9 16:36:14.636: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.232278ms
May  9 16:36:14.636: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
May  9 16:36:14.636: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9171" to be "running"
May  9 16:36:14.640: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.348742ms
May  9 16:36:14.640: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
May  9 16:36:14.640: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9171" to be "running"
May  9 16:36:14.646: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.864797ms
May  9 16:36:14.646: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
May  9 16:36:14.646: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9171" to be "running"
May  9 16:36:14.652: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.165472ms
May  9 16:36:16.660: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.014131508s
May  9 16:36:16.661: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
May  9 16:36:16.661: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9171" to be "running"
May  9 16:36:16.666: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.855903ms
May  9 16:36:16.667: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 05/09/23 16:36:16.667
May  9 16:36:16.682: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
May  9 16:36:16.690: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.343134ms
May  9 16:36:18.698: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015885067s
May  9 16:36:20.700: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.017629231s
May  9 16:36:20.700: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 16:36:20.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9171" for this suite. 05/09/23 16:36:20.818
------------------------------
â€¢ [SLOW TEST] [68.395 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:35:12.431
    May  9 16:35:12.431: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename sched-preemption 05/09/23 16:35:12.432
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:35:12.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:35:12.456
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    May  9 16:35:12.476: INFO: Waiting up to 1m0s for all nodes to be ready
    May  9 16:36:12.526: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 05/09/23 16:36:12.531
    May  9 16:36:12.556: INFO: Created pod: pod0-0-sched-preemption-low-priority
    May  9 16:36:12.565: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    May  9 16:36:12.584: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    May  9 16:36:12.592: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    May  9 16:36:12.608: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    May  9 16:36:12.616: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 05/09/23 16:36:12.616
    May  9 16:36:12.616: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9171" to be "running"
    May  9 16:36:12.624: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.858486ms
    May  9 16:36:14.632: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.015598528s
    May  9 16:36:14.632: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    May  9 16:36:14.632: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9171" to be "running"
    May  9 16:36:14.636: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.232278ms
    May  9 16:36:14.636: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    May  9 16:36:14.636: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9171" to be "running"
    May  9 16:36:14.640: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.348742ms
    May  9 16:36:14.640: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    May  9 16:36:14.640: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9171" to be "running"
    May  9 16:36:14.646: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.864797ms
    May  9 16:36:14.646: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    May  9 16:36:14.646: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9171" to be "running"
    May  9 16:36:14.652: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.165472ms
    May  9 16:36:16.660: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.014131508s
    May  9 16:36:16.661: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    May  9 16:36:16.661: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9171" to be "running"
    May  9 16:36:16.666: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.855903ms
    May  9 16:36:16.667: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 05/09/23 16:36:16.667
    May  9 16:36:16.682: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    May  9 16:36:16.690: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.343134ms
    May  9 16:36:18.698: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015885067s
    May  9 16:36:20.700: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.017629231s
    May  9 16:36:20.700: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 16:36:20.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9171" for this suite. 05/09/23 16:36:20.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:36:20.826
May  9 16:36:20.826: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename services 05/09/23 16:36:20.827
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:36:20.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:36:20.852
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2056 05/09/23 16:36:20.857
STEP: changing the ExternalName service to type=ClusterIP 05/09/23 16:36:20.866
STEP: creating replication controller externalname-service in namespace services-2056 05/09/23 16:36:20.883
I0509 16:36:20.890333      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2056, replica count: 2
I0509 16:36:23.942506      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 16:36:23.942: INFO: Creating new exec pod
May  9 16:36:23.950: INFO: Waiting up to 5m0s for pod "execpodcmxhc" in namespace "services-2056" to be "running"
May  9 16:36:23.959: INFO: Pod "execpodcmxhc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.796268ms
May  9 16:36:25.965: INFO: Pod "execpodcmxhc": Phase="Running", Reason="", readiness=true. Elapsed: 2.014856271s
May  9 16:36:25.965: INFO: Pod "execpodcmxhc" satisfied condition "running"
May  9 16:36:26.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-2056 exec execpodcmxhc -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
May  9 16:36:27.234: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  9 16:36:27.234: INFO: stdout: ""
May  9 16:36:27.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-2056 exec execpodcmxhc -- /bin/sh -x -c nc -v -z -w 2 10.3.218.161 80'
May  9 16:36:27.726: INFO: stderr: "+ nc -v -z -w 2 10.3.218.161 80\nConnection to 10.3.218.161 80 port [tcp/http] succeeded!\n"
May  9 16:36:27.726: INFO: stdout: ""
May  9 16:36:27.726: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  9 16:36:27.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2056" for this suite. 05/09/23 16:36:27.77
------------------------------
â€¢ [SLOW TEST] [6.952 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:36:20.826
    May  9 16:36:20.826: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename services 05/09/23 16:36:20.827
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:36:20.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:36:20.852
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2056 05/09/23 16:36:20.857
    STEP: changing the ExternalName service to type=ClusterIP 05/09/23 16:36:20.866
    STEP: creating replication controller externalname-service in namespace services-2056 05/09/23 16:36:20.883
    I0509 16:36:20.890333      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2056, replica count: 2
    I0509 16:36:23.942506      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 16:36:23.942: INFO: Creating new exec pod
    May  9 16:36:23.950: INFO: Waiting up to 5m0s for pod "execpodcmxhc" in namespace "services-2056" to be "running"
    May  9 16:36:23.959: INFO: Pod "execpodcmxhc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.796268ms
    May  9 16:36:25.965: INFO: Pod "execpodcmxhc": Phase="Running", Reason="", readiness=true. Elapsed: 2.014856271s
    May  9 16:36:25.965: INFO: Pod "execpodcmxhc" satisfied condition "running"
    May  9 16:36:26.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-2056 exec execpodcmxhc -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    May  9 16:36:27.234: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    May  9 16:36:27.234: INFO: stdout: ""
    May  9 16:36:27.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-2056 exec execpodcmxhc -- /bin/sh -x -c nc -v -z -w 2 10.3.218.161 80'
    May  9 16:36:27.726: INFO: stderr: "+ nc -v -z -w 2 10.3.218.161 80\nConnection to 10.3.218.161 80 port [tcp/http] succeeded!\n"
    May  9 16:36:27.726: INFO: stdout: ""
    May  9 16:36:27.726: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  9 16:36:27.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2056" for this suite. 05/09/23 16:36:27.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:36:27.778
May  9 16:36:27.778: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename services 05/09/23 16:36:27.779
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:36:27.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:36:27.798
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 05/09/23 16:36:27.802
May  9 16:36:27.803: INFO: Creating e2e-svc-a-xdvpk
May  9 16:36:27.816: INFO: Creating e2e-svc-b-dmhd2
May  9 16:36:27.835: INFO: Creating e2e-svc-c-9tmvz
STEP: deleting service collection 05/09/23 16:36:27.855
May  9 16:36:27.895: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  9 16:36:27.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6971" for this suite. 05/09/23 16:36:27.9
------------------------------
â€¢ [0.131 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:36:27.778
    May  9 16:36:27.778: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename services 05/09/23 16:36:27.779
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:36:27.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:36:27.798
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 05/09/23 16:36:27.802
    May  9 16:36:27.803: INFO: Creating e2e-svc-a-xdvpk
    May  9 16:36:27.816: INFO: Creating e2e-svc-b-dmhd2
    May  9 16:36:27.835: INFO: Creating e2e-svc-c-9tmvz
    STEP: deleting service collection 05/09/23 16:36:27.855
    May  9 16:36:27.895: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  9 16:36:27.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6971" for this suite. 05/09/23 16:36:27.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:36:27.909
May  9 16:36:27.909: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubelet-test 05/09/23 16:36:27.91
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:36:27.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:36:27.964
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 05/09/23 16:36:27.982
May  9 16:36:27.982: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases9776b609-0882-45aa-b5e3-107b099ab837" in namespace "kubelet-test-8986" to be "completed"
May  9 16:36:27.989: INFO: Pod "agnhost-host-aliases9776b609-0882-45aa-b5e3-107b099ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 6.156057ms
May  9 16:36:29.997: INFO: Pod "agnhost-host-aliases9776b609-0882-45aa-b5e3-107b099ab837": Phase="Running", Reason="", readiness=true. Elapsed: 2.014521949s
May  9 16:36:31.996: INFO: Pod "agnhost-host-aliases9776b609-0882-45aa-b5e3-107b099ab837": Phase="Running", Reason="", readiness=false. Elapsed: 4.013514783s
May  9 16:36:33.997: INFO: Pod "agnhost-host-aliases9776b609-0882-45aa-b5e3-107b099ab837": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014288418s
May  9 16:36:33.997: INFO: Pod "agnhost-host-aliases9776b609-0882-45aa-b5e3-107b099ab837" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May  9 16:36:34.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8986" for this suite. 05/09/23 16:36:34.061
------------------------------
â€¢ [SLOW TEST] [6.159 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:36:27.909
    May  9 16:36:27.909: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubelet-test 05/09/23 16:36:27.91
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:36:27.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:36:27.964
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 05/09/23 16:36:27.982
    May  9 16:36:27.982: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases9776b609-0882-45aa-b5e3-107b099ab837" in namespace "kubelet-test-8986" to be "completed"
    May  9 16:36:27.989: INFO: Pod "agnhost-host-aliases9776b609-0882-45aa-b5e3-107b099ab837": Phase="Pending", Reason="", readiness=false. Elapsed: 6.156057ms
    May  9 16:36:29.997: INFO: Pod "agnhost-host-aliases9776b609-0882-45aa-b5e3-107b099ab837": Phase="Running", Reason="", readiness=true. Elapsed: 2.014521949s
    May  9 16:36:31.996: INFO: Pod "agnhost-host-aliases9776b609-0882-45aa-b5e3-107b099ab837": Phase="Running", Reason="", readiness=false. Elapsed: 4.013514783s
    May  9 16:36:33.997: INFO: Pod "agnhost-host-aliases9776b609-0882-45aa-b5e3-107b099ab837": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014288418s
    May  9 16:36:33.997: INFO: Pod "agnhost-host-aliases9776b609-0882-45aa-b5e3-107b099ab837" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May  9 16:36:34.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8986" for this suite. 05/09/23 16:36:34.061
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:36:34.07
May  9 16:36:34.070: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename namespaces 05/09/23 16:36:34.072
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:36:34.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:36:34.101
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-3237" 05/09/23 16:36:34.106
May  9 16:36:34.117: INFO: Namespace "namespaces-3237" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"f3f539e6-417f-4a68-8004-5f74a75285d0", "kubernetes.io/metadata.name":"namespaces-3237", "namespaces-3237":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 16:36:34.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3237" for this suite. 05/09/23 16:36:34.122
------------------------------
â€¢ [0.100 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:36:34.07
    May  9 16:36:34.070: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename namespaces 05/09/23 16:36:34.072
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:36:34.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:36:34.101
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-3237" 05/09/23 16:36:34.106
    May  9 16:36:34.117: INFO: Namespace "namespaces-3237" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"f3f539e6-417f-4a68-8004-5f74a75285d0", "kubernetes.io/metadata.name":"namespaces-3237", "namespaces-3237":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 16:36:34.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3237" for this suite. 05/09/23 16:36:34.122
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:36:34.171
May  9 16:36:34.171: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename emptydir 05/09/23 16:36:34.172
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:36:34.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:36:34.197
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 05/09/23 16:36:34.201
May  9 16:36:34.214: INFO: Waiting up to 5m0s for pod "pod-5b5807d5-5482-46ae-9df5-3dbc24b61bd1" in namespace "emptydir-4878" to be "Succeeded or Failed"
May  9 16:36:34.220: INFO: Pod "pod-5b5807d5-5482-46ae-9df5-3dbc24b61bd1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.908061ms
May  9 16:36:36.228: INFO: Pod "pod-5b5807d5-5482-46ae-9df5-3dbc24b61bd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013365086s
May  9 16:36:38.225: INFO: Pod "pod-5b5807d5-5482-46ae-9df5-3dbc24b61bd1": Phase="Running", Reason="", readiness=false. Elapsed: 4.010914245s
May  9 16:36:40.226: INFO: Pod "pod-5b5807d5-5482-46ae-9df5-3dbc24b61bd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012027979s
STEP: Saw pod success 05/09/23 16:36:40.226
May  9 16:36:40.226: INFO: Pod "pod-5b5807d5-5482-46ae-9df5-3dbc24b61bd1" satisfied condition "Succeeded or Failed"
May  9 16:36:40.232: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-5b5807d5-5482-46ae-9df5-3dbc24b61bd1 container test-container: <nil>
STEP: delete the pod 05/09/23 16:36:40.286
May  9 16:36:40.303: INFO: Waiting for pod pod-5b5807d5-5482-46ae-9df5-3dbc24b61bd1 to disappear
May  9 16:36:40.308: INFO: Pod pod-5b5807d5-5482-46ae-9df5-3dbc24b61bd1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  9 16:36:40.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4878" for this suite. 05/09/23 16:36:40.313
------------------------------
â€¢ [SLOW TEST] [6.153 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:36:34.171
    May  9 16:36:34.171: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename emptydir 05/09/23 16:36:34.172
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:36:34.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:36:34.197
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 05/09/23 16:36:34.201
    May  9 16:36:34.214: INFO: Waiting up to 5m0s for pod "pod-5b5807d5-5482-46ae-9df5-3dbc24b61bd1" in namespace "emptydir-4878" to be "Succeeded or Failed"
    May  9 16:36:34.220: INFO: Pod "pod-5b5807d5-5482-46ae-9df5-3dbc24b61bd1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.908061ms
    May  9 16:36:36.228: INFO: Pod "pod-5b5807d5-5482-46ae-9df5-3dbc24b61bd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013365086s
    May  9 16:36:38.225: INFO: Pod "pod-5b5807d5-5482-46ae-9df5-3dbc24b61bd1": Phase="Running", Reason="", readiness=false. Elapsed: 4.010914245s
    May  9 16:36:40.226: INFO: Pod "pod-5b5807d5-5482-46ae-9df5-3dbc24b61bd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012027979s
    STEP: Saw pod success 05/09/23 16:36:40.226
    May  9 16:36:40.226: INFO: Pod "pod-5b5807d5-5482-46ae-9df5-3dbc24b61bd1" satisfied condition "Succeeded or Failed"
    May  9 16:36:40.232: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-5b5807d5-5482-46ae-9df5-3dbc24b61bd1 container test-container: <nil>
    STEP: delete the pod 05/09/23 16:36:40.286
    May  9 16:36:40.303: INFO: Waiting for pod pod-5b5807d5-5482-46ae-9df5-3dbc24b61bd1 to disappear
    May  9 16:36:40.308: INFO: Pod pod-5b5807d5-5482-46ae-9df5-3dbc24b61bd1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  9 16:36:40.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4878" for this suite. 05/09/23 16:36:40.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:36:40.324
May  9 16:36:40.324: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename init-container 05/09/23 16:36:40.325
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:36:40.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:36:40.397
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 05/09/23 16:36:40.401
May  9 16:36:40.401: INFO: PodSpec: initContainers in spec.initContainers
May  9 16:37:24.345: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b49970b0-9cf6-476c-8afd-f23d492d9843", GenerateName:"", Namespace:"init-container-723", SelfLink:"", UID:"2c14f07a-7df3-4d05-83ae-b108a28ae9f3", ResourceVersion:"318083482", Generation:0, CreationTimestamp:time.Date(2023, time.May, 9, 16, 36, 40, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"401291687"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"4541fa6b3bec85aee63be1c27ca04b5ed8cdd33ffc2786b5659b3e4d19a4cb76", "cni.projectcalico.org/podIP":"10.2.1.51/32", "cni.projectcalico.org/podIPs":"10.2.1.51/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 9, 16, 36, 40, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000f5acf0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 9, 16, 36, 40, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000f5ad98), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 9, 16, 37, 24, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000f5ae40), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-l9wv4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00115be60), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-l9wv4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-l9wv4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-l9wv4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0040fae18), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"nodepool-8cc7f47e-9b0c-4801-88-node-7ad816", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004457e30), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0040fae90)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0040faeb0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0040faeb8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0040faebc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001030bc0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 9, 16, 36, 40, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 9, 16, 36, 40, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 9, 16, 36, 40, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 9, 16, 36, 40, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"51.68.91.222", PodIP:"10.2.1.51", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.2.1.51"}}, StartTime:time.Date(2023, time.May, 9, 16, 36, 40, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004457f10)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004457f80)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://88c03b8ba2fdfc73d1008c619318c410fda21542b408a07f074a39c54c66f217", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00115bf00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00115bec0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0040faf3f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May  9 16:37:24.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-723" for this suite. 05/09/23 16:37:24.353
------------------------------
â€¢ [SLOW TEST] [44.037 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:36:40.324
    May  9 16:36:40.324: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename init-container 05/09/23 16:36:40.325
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:36:40.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:36:40.397
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 05/09/23 16:36:40.401
    May  9 16:36:40.401: INFO: PodSpec: initContainers in spec.initContainers
    May  9 16:37:24.345: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b49970b0-9cf6-476c-8afd-f23d492d9843", GenerateName:"", Namespace:"init-container-723", SelfLink:"", UID:"2c14f07a-7df3-4d05-83ae-b108a28ae9f3", ResourceVersion:"318083482", Generation:0, CreationTimestamp:time.Date(2023, time.May, 9, 16, 36, 40, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"401291687"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"4541fa6b3bec85aee63be1c27ca04b5ed8cdd33ffc2786b5659b3e4d19a4cb76", "cni.projectcalico.org/podIP":"10.2.1.51/32", "cni.projectcalico.org/podIPs":"10.2.1.51/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 9, 16, 36, 40, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000f5acf0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 9, 16, 36, 40, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000f5ad98), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 9, 16, 37, 24, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000f5ae40), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-l9wv4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00115be60), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-l9wv4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-l9wv4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-l9wv4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0040fae18), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"nodepool-8cc7f47e-9b0c-4801-88-node-7ad816", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004457e30), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0040fae90)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0040faeb0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0040faeb8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0040faebc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001030bc0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 9, 16, 36, 40, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 9, 16, 36, 40, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 9, 16, 36, 40, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 9, 16, 36, 40, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"51.68.91.222", PodIP:"10.2.1.51", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.2.1.51"}}, StartTime:time.Date(2023, time.May, 9, 16, 36, 40, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004457f10)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004457f80)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://88c03b8ba2fdfc73d1008c619318c410fda21542b408a07f074a39c54c66f217", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00115bf00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00115bec0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0040faf3f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May  9 16:37:24.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-723" for this suite. 05/09/23 16:37:24.353
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:37:24.361
May  9 16:37:24.361: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename var-expansion 05/09/23 16:37:24.362
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:37:24.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:37:24.384
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
May  9 16:37:24.400: INFO: Waiting up to 2m0s for pod "var-expansion-976adcf6-44fa-42bd-bd38-40f11f45d3ee" in namespace "var-expansion-7106" to be "container 0 failed with reason CreateContainerConfigError"
May  9 16:37:24.405: INFO: Pod "var-expansion-976adcf6-44fa-42bd-bd38-40f11f45d3ee": Phase="Pending", Reason="", readiness=false. Elapsed: 5.241571ms
May  9 16:37:26.415: INFO: Pod "var-expansion-976adcf6-44fa-42bd-bd38-40f11f45d3ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01483774s
May  9 16:37:26.415: INFO: Pod "var-expansion-976adcf6-44fa-42bd-bd38-40f11f45d3ee" satisfied condition "container 0 failed with reason CreateContainerConfigError"
May  9 16:37:26.415: INFO: Deleting pod "var-expansion-976adcf6-44fa-42bd-bd38-40f11f45d3ee" in namespace "var-expansion-7106"
May  9 16:37:26.429: INFO: Wait up to 5m0s for pod "var-expansion-976adcf6-44fa-42bd-bd38-40f11f45d3ee" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May  9 16:37:28.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7106" for this suite. 05/09/23 16:37:28.449
------------------------------
â€¢ [4.096 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:37:24.361
    May  9 16:37:24.361: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename var-expansion 05/09/23 16:37:24.362
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:37:24.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:37:24.384
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    May  9 16:37:24.400: INFO: Waiting up to 2m0s for pod "var-expansion-976adcf6-44fa-42bd-bd38-40f11f45d3ee" in namespace "var-expansion-7106" to be "container 0 failed with reason CreateContainerConfigError"
    May  9 16:37:24.405: INFO: Pod "var-expansion-976adcf6-44fa-42bd-bd38-40f11f45d3ee": Phase="Pending", Reason="", readiness=false. Elapsed: 5.241571ms
    May  9 16:37:26.415: INFO: Pod "var-expansion-976adcf6-44fa-42bd-bd38-40f11f45d3ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01483774s
    May  9 16:37:26.415: INFO: Pod "var-expansion-976adcf6-44fa-42bd-bd38-40f11f45d3ee" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    May  9 16:37:26.415: INFO: Deleting pod "var-expansion-976adcf6-44fa-42bd-bd38-40f11f45d3ee" in namespace "var-expansion-7106"
    May  9 16:37:26.429: INFO: Wait up to 5m0s for pod "var-expansion-976adcf6-44fa-42bd-bd38-40f11f45d3ee" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May  9 16:37:28.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7106" for this suite. 05/09/23 16:37:28.449
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:37:28.458
May  9 16:37:28.458: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 16:37:28.459
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:37:28.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:37:28.483
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
May  9 16:37:28.574: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:37:35.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2364" for this suite. 05/09/23 16:37:35.506
------------------------------
â€¢ [SLOW TEST] [7.063 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:37:28.458
    May  9 16:37:28.458: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 16:37:28.459
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:37:28.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:37:28.483
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    May  9 16:37:28.574: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:37:35.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2364" for this suite. 05/09/23 16:37:35.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:37:35.521
May  9 16:37:35.521: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubectl 05/09/23 16:37:35.522
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:37:35.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:37:35.542
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 05/09/23 16:37:35.546
May  9 16:37:35.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6550 create -f -'
May  9 16:37:37.102: INFO: stderr: ""
May  9 16:37:37.102: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 05/09/23 16:37:37.102
May  9 16:37:37.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6550 diff -f -'
May  9 16:37:37.314: INFO: rc: 1
May  9 16:37:37.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6550 delete -f -'
May  9 16:37:37.398: INFO: stderr: ""
May  9 16:37:37.398: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  9 16:37:37.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6550" for this suite. 05/09/23 16:37:37.405
------------------------------
â€¢ [1.892 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:37:35.521
    May  9 16:37:35.521: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubectl 05/09/23 16:37:35.522
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:37:35.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:37:35.542
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 05/09/23 16:37:35.546
    May  9 16:37:35.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6550 create -f -'
    May  9 16:37:37.102: INFO: stderr: ""
    May  9 16:37:37.102: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 05/09/23 16:37:37.102
    May  9 16:37:37.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6550 diff -f -'
    May  9 16:37:37.314: INFO: rc: 1
    May  9 16:37:37.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6550 delete -f -'
    May  9 16:37:37.398: INFO: stderr: ""
    May  9 16:37:37.398: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  9 16:37:37.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6550" for this suite. 05/09/23 16:37:37.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:37:37.422
May  9 16:37:37.422: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename emptydir 05/09/23 16:37:37.423
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:37:37.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:37:37.452
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 05/09/23 16:37:37.456
May  9 16:37:37.465: INFO: Waiting up to 5m0s for pod "pod-728fdf60-2a7b-4ac6-9a82-e9d8cdcbf501" in namespace "emptydir-3707" to be "Succeeded or Failed"
May  9 16:37:37.471: INFO: Pod "pod-728fdf60-2a7b-4ac6-9a82-e9d8cdcbf501": Phase="Pending", Reason="", readiness=false. Elapsed: 5.977825ms
May  9 16:37:39.477: INFO: Pod "pod-728fdf60-2a7b-4ac6-9a82-e9d8cdcbf501": Phase="Running", Reason="", readiness=false. Elapsed: 2.011927506s
May  9 16:37:41.479: INFO: Pod "pod-728fdf60-2a7b-4ac6-9a82-e9d8cdcbf501": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013859529s
STEP: Saw pod success 05/09/23 16:37:41.479
May  9 16:37:41.480: INFO: Pod "pod-728fdf60-2a7b-4ac6-9a82-e9d8cdcbf501" satisfied condition "Succeeded or Failed"
May  9 16:37:41.485: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-728fdf60-2a7b-4ac6-9a82-e9d8cdcbf501 container test-container: <nil>
STEP: delete the pod 05/09/23 16:37:41.496
May  9 16:37:41.513: INFO: Waiting for pod pod-728fdf60-2a7b-4ac6-9a82-e9d8cdcbf501 to disappear
May  9 16:37:41.517: INFO: Pod pod-728fdf60-2a7b-4ac6-9a82-e9d8cdcbf501 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  9 16:37:41.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3707" for this suite. 05/09/23 16:37:41.524
------------------------------
â€¢ [4.112 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:37:37.422
    May  9 16:37:37.422: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename emptydir 05/09/23 16:37:37.423
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:37:37.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:37:37.452
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 05/09/23 16:37:37.456
    May  9 16:37:37.465: INFO: Waiting up to 5m0s for pod "pod-728fdf60-2a7b-4ac6-9a82-e9d8cdcbf501" in namespace "emptydir-3707" to be "Succeeded or Failed"
    May  9 16:37:37.471: INFO: Pod "pod-728fdf60-2a7b-4ac6-9a82-e9d8cdcbf501": Phase="Pending", Reason="", readiness=false. Elapsed: 5.977825ms
    May  9 16:37:39.477: INFO: Pod "pod-728fdf60-2a7b-4ac6-9a82-e9d8cdcbf501": Phase="Running", Reason="", readiness=false. Elapsed: 2.011927506s
    May  9 16:37:41.479: INFO: Pod "pod-728fdf60-2a7b-4ac6-9a82-e9d8cdcbf501": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013859529s
    STEP: Saw pod success 05/09/23 16:37:41.479
    May  9 16:37:41.480: INFO: Pod "pod-728fdf60-2a7b-4ac6-9a82-e9d8cdcbf501" satisfied condition "Succeeded or Failed"
    May  9 16:37:41.485: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-728fdf60-2a7b-4ac6-9a82-e9d8cdcbf501 container test-container: <nil>
    STEP: delete the pod 05/09/23 16:37:41.496
    May  9 16:37:41.513: INFO: Waiting for pod pod-728fdf60-2a7b-4ac6-9a82-e9d8cdcbf501 to disappear
    May  9 16:37:41.517: INFO: Pod pod-728fdf60-2a7b-4ac6-9a82-e9d8cdcbf501 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  9 16:37:41.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3707" for this suite. 05/09/23 16:37:41.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:37:41.536
May  9 16:37:41.536: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename secrets 05/09/23 16:37:41.537
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:37:41.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:37:41.563
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-6a3fa460-f724-46ec-a11f-6c163216fd0b 05/09/23 16:37:41.568
STEP: Creating a pod to test consume secrets 05/09/23 16:37:41.574
May  9 16:37:41.584: INFO: Waiting up to 5m0s for pod "pod-secrets-b4d51cdf-3fce-4a02-9637-f3ecf7aae477" in namespace "secrets-9745" to be "Succeeded or Failed"
May  9 16:37:41.592: INFO: Pod "pod-secrets-b4d51cdf-3fce-4a02-9637-f3ecf7aae477": Phase="Pending", Reason="", readiness=false. Elapsed: 8.806248ms
May  9 16:37:43.599: INFO: Pod "pod-secrets-b4d51cdf-3fce-4a02-9637-f3ecf7aae477": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015360098s
May  9 16:37:45.602: INFO: Pod "pod-secrets-b4d51cdf-3fce-4a02-9637-f3ecf7aae477": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018302759s
STEP: Saw pod success 05/09/23 16:37:45.602
May  9 16:37:45.602: INFO: Pod "pod-secrets-b4d51cdf-3fce-4a02-9637-f3ecf7aae477" satisfied condition "Succeeded or Failed"
May  9 16:37:45.607: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-secrets-b4d51cdf-3fce-4a02-9637-f3ecf7aae477 container secret-volume-test: <nil>
STEP: delete the pod 05/09/23 16:37:45.621
May  9 16:37:45.639: INFO: Waiting for pod pod-secrets-b4d51cdf-3fce-4a02-9637-f3ecf7aae477 to disappear
May  9 16:37:45.644: INFO: Pod pod-secrets-b4d51cdf-3fce-4a02-9637-f3ecf7aae477 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May  9 16:37:45.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9745" for this suite. 05/09/23 16:37:45.649
------------------------------
â€¢ [4.123 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:37:41.536
    May  9 16:37:41.536: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename secrets 05/09/23 16:37:41.537
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:37:41.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:37:41.563
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-6a3fa460-f724-46ec-a11f-6c163216fd0b 05/09/23 16:37:41.568
    STEP: Creating a pod to test consume secrets 05/09/23 16:37:41.574
    May  9 16:37:41.584: INFO: Waiting up to 5m0s for pod "pod-secrets-b4d51cdf-3fce-4a02-9637-f3ecf7aae477" in namespace "secrets-9745" to be "Succeeded or Failed"
    May  9 16:37:41.592: INFO: Pod "pod-secrets-b4d51cdf-3fce-4a02-9637-f3ecf7aae477": Phase="Pending", Reason="", readiness=false. Elapsed: 8.806248ms
    May  9 16:37:43.599: INFO: Pod "pod-secrets-b4d51cdf-3fce-4a02-9637-f3ecf7aae477": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015360098s
    May  9 16:37:45.602: INFO: Pod "pod-secrets-b4d51cdf-3fce-4a02-9637-f3ecf7aae477": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018302759s
    STEP: Saw pod success 05/09/23 16:37:45.602
    May  9 16:37:45.602: INFO: Pod "pod-secrets-b4d51cdf-3fce-4a02-9637-f3ecf7aae477" satisfied condition "Succeeded or Failed"
    May  9 16:37:45.607: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-secrets-b4d51cdf-3fce-4a02-9637-f3ecf7aae477 container secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 16:37:45.621
    May  9 16:37:45.639: INFO: Waiting for pod pod-secrets-b4d51cdf-3fce-4a02-9637-f3ecf7aae477 to disappear
    May  9 16:37:45.644: INFO: Pod pod-secrets-b4d51cdf-3fce-4a02-9637-f3ecf7aae477 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May  9 16:37:45.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9745" for this suite. 05/09/23 16:37:45.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:37:45.66
May  9 16:37:45.660: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename pods 05/09/23 16:37:45.661
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:37:45.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:37:45.684
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 05/09/23 16:37:45.688
STEP: setting up watch 05/09/23 16:37:45.688
STEP: submitting the pod to kubernetes 05/09/23 16:37:45.794
STEP: verifying the pod is in kubernetes 05/09/23 16:37:45.806
STEP: verifying pod creation was observed 05/09/23 16:37:45.812
May  9 16:37:45.812: INFO: Waiting up to 5m0s for pod "pod-submit-remove-dcf6b479-e4a5-4aca-890a-197557f321da" in namespace "pods-373" to be "running"
May  9 16:37:45.817: INFO: Pod "pod-submit-remove-dcf6b479-e4a5-4aca-890a-197557f321da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.453317ms
May  9 16:37:47.823: INFO: Pod "pod-submit-remove-dcf6b479-e4a5-4aca-890a-197557f321da": Phase="Running", Reason="", readiness=true. Elapsed: 2.010943418s
May  9 16:37:47.823: INFO: Pod "pod-submit-remove-dcf6b479-e4a5-4aca-890a-197557f321da" satisfied condition "running"
STEP: deleting the pod gracefully 05/09/23 16:37:47.829
STEP: verifying pod deletion was observed 05/09/23 16:37:47.839
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  9 16:37:50.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-373" for this suite. 05/09/23 16:37:50.453
------------------------------
â€¢ [4.803 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:37:45.66
    May  9 16:37:45.660: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename pods 05/09/23 16:37:45.661
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:37:45.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:37:45.684
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 05/09/23 16:37:45.688
    STEP: setting up watch 05/09/23 16:37:45.688
    STEP: submitting the pod to kubernetes 05/09/23 16:37:45.794
    STEP: verifying the pod is in kubernetes 05/09/23 16:37:45.806
    STEP: verifying pod creation was observed 05/09/23 16:37:45.812
    May  9 16:37:45.812: INFO: Waiting up to 5m0s for pod "pod-submit-remove-dcf6b479-e4a5-4aca-890a-197557f321da" in namespace "pods-373" to be "running"
    May  9 16:37:45.817: INFO: Pod "pod-submit-remove-dcf6b479-e4a5-4aca-890a-197557f321da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.453317ms
    May  9 16:37:47.823: INFO: Pod "pod-submit-remove-dcf6b479-e4a5-4aca-890a-197557f321da": Phase="Running", Reason="", readiness=true. Elapsed: 2.010943418s
    May  9 16:37:47.823: INFO: Pod "pod-submit-remove-dcf6b479-e4a5-4aca-890a-197557f321da" satisfied condition "running"
    STEP: deleting the pod gracefully 05/09/23 16:37:47.829
    STEP: verifying pod deletion was observed 05/09/23 16:37:47.839
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  9 16:37:50.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-373" for this suite. 05/09/23 16:37:50.453
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:37:50.463
May  9 16:37:50.463: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename conformance-tests 05/09/23 16:37:50.464
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:37:50.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:37:50.481
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 05/09/23 16:37:50.485
May  9 16:37:50.485: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
May  9 16:37:50.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-9853" for this suite. 05/09/23 16:37:50.499
------------------------------
â€¢ [0.045 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:37:50.463
    May  9 16:37:50.463: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename conformance-tests 05/09/23 16:37:50.464
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:37:50.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:37:50.481
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 05/09/23 16:37:50.485
    May  9 16:37:50.485: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    May  9 16:37:50.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-9853" for this suite. 05/09/23 16:37:50.499
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:37:50.509
May  9 16:37:50.509: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename emptydir 05/09/23 16:37:50.51
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:37:50.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:37:50.531
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 05/09/23 16:37:50.535
May  9 16:37:50.546: INFO: Waiting up to 5m0s for pod "pod-b62259fa-75ef-4b26-991b-df32306ef416" in namespace "emptydir-4823" to be "Succeeded or Failed"
May  9 16:37:50.552: INFO: Pod "pod-b62259fa-75ef-4b26-991b-df32306ef416": Phase="Pending", Reason="", readiness=false. Elapsed: 5.606164ms
May  9 16:37:52.557: INFO: Pod "pod-b62259fa-75ef-4b26-991b-df32306ef416": Phase="Running", Reason="", readiness=false. Elapsed: 2.011271246s
May  9 16:37:54.560: INFO: Pod "pod-b62259fa-75ef-4b26-991b-df32306ef416": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013527922s
STEP: Saw pod success 05/09/23 16:37:54.56
May  9 16:37:54.560: INFO: Pod "pod-b62259fa-75ef-4b26-991b-df32306ef416" satisfied condition "Succeeded or Failed"
May  9 16:37:54.565: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-b62259fa-75ef-4b26-991b-df32306ef416 container test-container: <nil>
STEP: delete the pod 05/09/23 16:37:54.576
May  9 16:37:54.593: INFO: Waiting for pod pod-b62259fa-75ef-4b26-991b-df32306ef416 to disappear
May  9 16:37:54.598: INFO: Pod pod-b62259fa-75ef-4b26-991b-df32306ef416 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  9 16:37:54.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4823" for this suite. 05/09/23 16:37:54.605
------------------------------
â€¢ [4.106 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:37:50.509
    May  9 16:37:50.509: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename emptydir 05/09/23 16:37:50.51
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:37:50.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:37:50.531
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 05/09/23 16:37:50.535
    May  9 16:37:50.546: INFO: Waiting up to 5m0s for pod "pod-b62259fa-75ef-4b26-991b-df32306ef416" in namespace "emptydir-4823" to be "Succeeded or Failed"
    May  9 16:37:50.552: INFO: Pod "pod-b62259fa-75ef-4b26-991b-df32306ef416": Phase="Pending", Reason="", readiness=false. Elapsed: 5.606164ms
    May  9 16:37:52.557: INFO: Pod "pod-b62259fa-75ef-4b26-991b-df32306ef416": Phase="Running", Reason="", readiness=false. Elapsed: 2.011271246s
    May  9 16:37:54.560: INFO: Pod "pod-b62259fa-75ef-4b26-991b-df32306ef416": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013527922s
    STEP: Saw pod success 05/09/23 16:37:54.56
    May  9 16:37:54.560: INFO: Pod "pod-b62259fa-75ef-4b26-991b-df32306ef416" satisfied condition "Succeeded or Failed"
    May  9 16:37:54.565: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-b62259fa-75ef-4b26-991b-df32306ef416 container test-container: <nil>
    STEP: delete the pod 05/09/23 16:37:54.576
    May  9 16:37:54.593: INFO: Waiting for pod pod-b62259fa-75ef-4b26-991b-df32306ef416 to disappear
    May  9 16:37:54.598: INFO: Pod pod-b62259fa-75ef-4b26-991b-df32306ef416 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  9 16:37:54.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4823" for this suite. 05/09/23 16:37:54.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:37:54.615
May  9 16:37:54.615: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename disruption 05/09/23 16:37:54.616
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:37:54.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:37:54.64
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 05/09/23 16:37:54.644
STEP: Waiting for the pdb to be processed 05/09/23 16:37:54.661
STEP: First trying to evict a pod which shouldn't be evictable 05/09/23 16:37:56.679
STEP: Waiting for all pods to be running 05/09/23 16:37:56.68
May  9 16:37:56.686: INFO: pods: 0 < 3
STEP: locating a running pod 05/09/23 16:37:58.693
STEP: Updating the pdb to allow a pod to be evicted 05/09/23 16:37:58.707
STEP: Waiting for the pdb to be processed 05/09/23 16:37:58.722
STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/09/23 16:37:58.726
STEP: Waiting for all pods to be running 05/09/23 16:37:58.726
STEP: Waiting for the pdb to observed all healthy pods 05/09/23 16:37:58.734
STEP: Patching the pdb to disallow a pod to be evicted 05/09/23 16:37:58.763
STEP: Waiting for the pdb to be processed 05/09/23 16:37:58.782
STEP: Waiting for all pods to be running 05/09/23 16:38:00.793
STEP: locating a running pod 05/09/23 16:38:00.802
STEP: Deleting the pdb to allow a pod to be evicted 05/09/23 16:38:00.816
STEP: Waiting for the pdb to be deleted 05/09/23 16:38:00.826
STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/09/23 16:38:00.83
STEP: Waiting for all pods to be running 05/09/23 16:38:00.83
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May  9 16:38:00.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-3357" for this suite. 05/09/23 16:38:00.885
------------------------------
â€¢ [SLOW TEST] [6.280 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:37:54.615
    May  9 16:37:54.615: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename disruption 05/09/23 16:37:54.616
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:37:54.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:37:54.64
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 05/09/23 16:37:54.644
    STEP: Waiting for the pdb to be processed 05/09/23 16:37:54.661
    STEP: First trying to evict a pod which shouldn't be evictable 05/09/23 16:37:56.679
    STEP: Waiting for all pods to be running 05/09/23 16:37:56.68
    May  9 16:37:56.686: INFO: pods: 0 < 3
    STEP: locating a running pod 05/09/23 16:37:58.693
    STEP: Updating the pdb to allow a pod to be evicted 05/09/23 16:37:58.707
    STEP: Waiting for the pdb to be processed 05/09/23 16:37:58.722
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/09/23 16:37:58.726
    STEP: Waiting for all pods to be running 05/09/23 16:37:58.726
    STEP: Waiting for the pdb to observed all healthy pods 05/09/23 16:37:58.734
    STEP: Patching the pdb to disallow a pod to be evicted 05/09/23 16:37:58.763
    STEP: Waiting for the pdb to be processed 05/09/23 16:37:58.782
    STEP: Waiting for all pods to be running 05/09/23 16:38:00.793
    STEP: locating a running pod 05/09/23 16:38:00.802
    STEP: Deleting the pdb to allow a pod to be evicted 05/09/23 16:38:00.816
    STEP: Waiting for the pdb to be deleted 05/09/23 16:38:00.826
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/09/23 16:38:00.83
    STEP: Waiting for all pods to be running 05/09/23 16:38:00.83
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May  9 16:38:00.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-3357" for this suite. 05/09/23 16:38:00.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:38:00.895
May  9 16:38:00.895: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename secrets 05/09/23 16:38:00.897
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:00.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:00.919
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-d3141b07-7c65-42a4-bdbe-763a093224f2 05/09/23 16:38:00.924
STEP: Creating a pod to test consume secrets 05/09/23 16:38:00.93
May  9 16:38:00.943: INFO: Waiting up to 5m0s for pod "pod-secrets-3f56d799-3d18-43c8-bfa5-a01c4a76e649" in namespace "secrets-6035" to be "Succeeded or Failed"
May  9 16:38:00.953: INFO: Pod "pod-secrets-3f56d799-3d18-43c8-bfa5-a01c4a76e649": Phase="Pending", Reason="", readiness=false. Elapsed: 9.617989ms
May  9 16:38:02.959: INFO: Pod "pod-secrets-3f56d799-3d18-43c8-bfa5-a01c4a76e649": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016251877s
May  9 16:38:04.960: INFO: Pod "pod-secrets-3f56d799-3d18-43c8-bfa5-a01c4a76e649": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01657689s
STEP: Saw pod success 05/09/23 16:38:04.96
May  9 16:38:04.960: INFO: Pod "pod-secrets-3f56d799-3d18-43c8-bfa5-a01c4a76e649" satisfied condition "Succeeded or Failed"
May  9 16:38:04.965: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-secrets-3f56d799-3d18-43c8-bfa5-a01c4a76e649 container secret-volume-test: <nil>
STEP: delete the pod 05/09/23 16:38:04.976
May  9 16:38:04.996: INFO: Waiting for pod pod-secrets-3f56d799-3d18-43c8-bfa5-a01c4a76e649 to disappear
May  9 16:38:05.001: INFO: Pod pod-secrets-3f56d799-3d18-43c8-bfa5-a01c4a76e649 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May  9 16:38:05.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6035" for this suite. 05/09/23 16:38:05.008
------------------------------
â€¢ [4.122 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:38:00.895
    May  9 16:38:00.895: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename secrets 05/09/23 16:38:00.897
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:00.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:00.919
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-d3141b07-7c65-42a4-bdbe-763a093224f2 05/09/23 16:38:00.924
    STEP: Creating a pod to test consume secrets 05/09/23 16:38:00.93
    May  9 16:38:00.943: INFO: Waiting up to 5m0s for pod "pod-secrets-3f56d799-3d18-43c8-bfa5-a01c4a76e649" in namespace "secrets-6035" to be "Succeeded or Failed"
    May  9 16:38:00.953: INFO: Pod "pod-secrets-3f56d799-3d18-43c8-bfa5-a01c4a76e649": Phase="Pending", Reason="", readiness=false. Elapsed: 9.617989ms
    May  9 16:38:02.959: INFO: Pod "pod-secrets-3f56d799-3d18-43c8-bfa5-a01c4a76e649": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016251877s
    May  9 16:38:04.960: INFO: Pod "pod-secrets-3f56d799-3d18-43c8-bfa5-a01c4a76e649": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01657689s
    STEP: Saw pod success 05/09/23 16:38:04.96
    May  9 16:38:04.960: INFO: Pod "pod-secrets-3f56d799-3d18-43c8-bfa5-a01c4a76e649" satisfied condition "Succeeded or Failed"
    May  9 16:38:04.965: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-secrets-3f56d799-3d18-43c8-bfa5-a01c4a76e649 container secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 16:38:04.976
    May  9 16:38:04.996: INFO: Waiting for pod pod-secrets-3f56d799-3d18-43c8-bfa5-a01c4a76e649 to disappear
    May  9 16:38:05.001: INFO: Pod pod-secrets-3f56d799-3d18-43c8-bfa5-a01c4a76e649 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May  9 16:38:05.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6035" for this suite. 05/09/23 16:38:05.008
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:38:05.018
May  9 16:38:05.018: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename dns 05/09/23 16:38:05.019
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:05.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:05.053
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5577.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5577.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 05/09/23 16:38:05.097
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5577.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5577.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 05/09/23 16:38:05.097
STEP: creating a pod to probe /etc/hosts 05/09/23 16:38:05.097
STEP: submitting the pod to kubernetes 05/09/23 16:38:05.097
May  9 16:38:05.110: INFO: Waiting up to 15m0s for pod "dns-test-b165900c-9b5f-4785-bca9-ad6e95514d5d" in namespace "dns-5577" to be "running"
May  9 16:38:05.116: INFO: Pod "dns-test-b165900c-9b5f-4785-bca9-ad6e95514d5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.390499ms
May  9 16:38:07.123: INFO: Pod "dns-test-b165900c-9b5f-4785-bca9-ad6e95514d5d": Phase="Running", Reason="", readiness=true. Elapsed: 2.01313456s
May  9 16:38:07.123: INFO: Pod "dns-test-b165900c-9b5f-4785-bca9-ad6e95514d5d" satisfied condition "running"
STEP: retrieving the pod 05/09/23 16:38:07.123
STEP: looking for the results for each expected name from probers 05/09/23 16:38:07.128
May  9 16:38:07.161: INFO: DNS probes using dns-5577/dns-test-b165900c-9b5f-4785-bca9-ad6e95514d5d succeeded

STEP: deleting the pod 05/09/23 16:38:07.161
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May  9 16:38:07.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5577" for this suite. 05/09/23 16:38:07.186
------------------------------
â€¢ [2.178 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:38:05.018
    May  9 16:38:05.018: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename dns 05/09/23 16:38:05.019
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:05.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:05.053
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5577.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5577.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     05/09/23 16:38:05.097
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5577.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5577.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     05/09/23 16:38:05.097
    STEP: creating a pod to probe /etc/hosts 05/09/23 16:38:05.097
    STEP: submitting the pod to kubernetes 05/09/23 16:38:05.097
    May  9 16:38:05.110: INFO: Waiting up to 15m0s for pod "dns-test-b165900c-9b5f-4785-bca9-ad6e95514d5d" in namespace "dns-5577" to be "running"
    May  9 16:38:05.116: INFO: Pod "dns-test-b165900c-9b5f-4785-bca9-ad6e95514d5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.390499ms
    May  9 16:38:07.123: INFO: Pod "dns-test-b165900c-9b5f-4785-bca9-ad6e95514d5d": Phase="Running", Reason="", readiness=true. Elapsed: 2.01313456s
    May  9 16:38:07.123: INFO: Pod "dns-test-b165900c-9b5f-4785-bca9-ad6e95514d5d" satisfied condition "running"
    STEP: retrieving the pod 05/09/23 16:38:07.123
    STEP: looking for the results for each expected name from probers 05/09/23 16:38:07.128
    May  9 16:38:07.161: INFO: DNS probes using dns-5577/dns-test-b165900c-9b5f-4785-bca9-ad6e95514d5d succeeded

    STEP: deleting the pod 05/09/23 16:38:07.161
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May  9 16:38:07.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5577" for this suite. 05/09/23 16:38:07.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:38:07.196
May  9 16:38:07.197: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename downward-api 05/09/23 16:38:07.197
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:07.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:07.218
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 05/09/23 16:38:07.222
May  9 16:38:07.233: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1a58aa6-800f-48b8-9bc9-f1050bb85dca" in namespace "downward-api-3942" to be "Succeeded or Failed"
May  9 16:38:07.239: INFO: Pod "downwardapi-volume-d1a58aa6-800f-48b8-9bc9-f1050bb85dca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.453781ms
May  9 16:38:09.246: INFO: Pod "downwardapi-volume-d1a58aa6-800f-48b8-9bc9-f1050bb85dca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012732964s
May  9 16:38:11.249: INFO: Pod "downwardapi-volume-d1a58aa6-800f-48b8-9bc9-f1050bb85dca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015700992s
STEP: Saw pod success 05/09/23 16:38:11.249
May  9 16:38:11.249: INFO: Pod "downwardapi-volume-d1a58aa6-800f-48b8-9bc9-f1050bb85dca" satisfied condition "Succeeded or Failed"
May  9 16:38:11.255: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-d1a58aa6-800f-48b8-9bc9-f1050bb85dca container client-container: <nil>
STEP: delete the pod 05/09/23 16:38:11.264
May  9 16:38:11.281: INFO: Waiting for pod downwardapi-volume-d1a58aa6-800f-48b8-9bc9-f1050bb85dca to disappear
May  9 16:38:11.289: INFO: Pod downwardapi-volume-d1a58aa6-800f-48b8-9bc9-f1050bb85dca no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  9 16:38:11.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3942" for this suite. 05/09/23 16:38:11.295
------------------------------
â€¢ [4.107 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:38:07.196
    May  9 16:38:07.197: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename downward-api 05/09/23 16:38:07.197
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:07.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:07.218
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 05/09/23 16:38:07.222
    May  9 16:38:07.233: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1a58aa6-800f-48b8-9bc9-f1050bb85dca" in namespace "downward-api-3942" to be "Succeeded or Failed"
    May  9 16:38:07.239: INFO: Pod "downwardapi-volume-d1a58aa6-800f-48b8-9bc9-f1050bb85dca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.453781ms
    May  9 16:38:09.246: INFO: Pod "downwardapi-volume-d1a58aa6-800f-48b8-9bc9-f1050bb85dca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012732964s
    May  9 16:38:11.249: INFO: Pod "downwardapi-volume-d1a58aa6-800f-48b8-9bc9-f1050bb85dca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015700992s
    STEP: Saw pod success 05/09/23 16:38:11.249
    May  9 16:38:11.249: INFO: Pod "downwardapi-volume-d1a58aa6-800f-48b8-9bc9-f1050bb85dca" satisfied condition "Succeeded or Failed"
    May  9 16:38:11.255: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-d1a58aa6-800f-48b8-9bc9-f1050bb85dca container client-container: <nil>
    STEP: delete the pod 05/09/23 16:38:11.264
    May  9 16:38:11.281: INFO: Waiting for pod downwardapi-volume-d1a58aa6-800f-48b8-9bc9-f1050bb85dca to disappear
    May  9 16:38:11.289: INFO: Pod downwardapi-volume-d1a58aa6-800f-48b8-9bc9-f1050bb85dca no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  9 16:38:11.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3942" for this suite. 05/09/23 16:38:11.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:38:11.306
May  9 16:38:11.306: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename services 05/09/23 16:38:11.307
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:11.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:11.328
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-7277 05/09/23 16:38:11.332
STEP: creating service affinity-nodeport-transition in namespace services-7277 05/09/23 16:38:11.332
STEP: creating replication controller affinity-nodeport-transition in namespace services-7277 05/09/23 16:38:11.353
I0509 16:38:11.362624      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-7277, replica count: 3
I0509 16:38:14.413775      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 16:38:14.430: INFO: Creating new exec pod
May  9 16:38:14.437: INFO: Waiting up to 5m0s for pod "execpod-affinitykrzg6" in namespace "services-7277" to be "running"
May  9 16:38:14.441: INFO: Pod "execpod-affinitykrzg6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.351668ms
May  9 16:38:16.447: INFO: Pod "execpod-affinitykrzg6": Phase="Running", Reason="", readiness=true. Elapsed: 2.010489905s
May  9 16:38:16.447: INFO: Pod "execpod-affinitykrzg6" satisfied condition "running"
May  9 16:38:17.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-7277 exec execpod-affinitykrzg6 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
May  9 16:38:17.678: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May  9 16:38:17.678: INFO: stdout: ""
May  9 16:38:17.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-7277 exec execpod-affinitykrzg6 -- /bin/sh -x -c nc -v -z -w 2 10.3.247.147 80'
May  9 16:38:17.897: INFO: stderr: "+ nc -v -z -w 2 10.3.247.147 80\nConnection to 10.3.247.147 80 port [tcp/http] succeeded!\n"
May  9 16:38:17.897: INFO: stdout: ""
May  9 16:38:17.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-7277 exec execpod-affinitykrzg6 -- /bin/sh -x -c nc -v -z -w 2 51.68.91.222 30213'
May  9 16:38:18.123: INFO: stderr: "+ nc -v -z -w 2 51.68.91.222 30213\nConnection to 51.68.91.222 30213 port [tcp/*] succeeded!\n"
May  9 16:38:18.123: INFO: stdout: ""
May  9 16:38:18.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-7277 exec execpod-affinitykrzg6 -- /bin/sh -x -c nc -v -z -w 2 51.68.94.118 30213'
May  9 16:38:18.348: INFO: stderr: "+ nc -v -z -w 2 51.68.94.118 30213\nConnection to 51.68.94.118 30213 port [tcp/*] succeeded!\n"
May  9 16:38:18.348: INFO: stdout: ""
May  9 16:38:18.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-7277 exec execpod-affinitykrzg6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://51.68.91.222:30213/ ; done'
May  9 16:38:18.673: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n"
May  9 16:38:18.673: INFO: stdout: "\naffinity-nodeport-transition-q2lln\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-2r797\naffinity-nodeport-transition-2r797\naffinity-nodeport-transition-2r797\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-q2lln\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-q2lln\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-2r797\naffinity-nodeport-transition-2r797\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-q2lln"
May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-q2lln
May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-2r797
May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-2r797
May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-2r797
May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-q2lln
May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-q2lln
May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-2r797
May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-2r797
May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-q2lln
May  9 16:38:18.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-7277 exec execpod-affinitykrzg6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://51.68.91.222:30213/ ; done'
May  9 16:38:18.986: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n"
May  9 16:38:18.987: INFO: stdout: "\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk"
May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
May  9 16:38:18.987: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7277, will wait for the garbage collector to delete the pods 05/09/23 16:38:19.002
May  9 16:38:19.068: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.440204ms
May  9 16:38:19.169: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.668773ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  9 16:38:21.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7277" for this suite. 05/09/23 16:38:21.401
------------------------------
â€¢ [SLOW TEST] [10.104 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:38:11.306
    May  9 16:38:11.306: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename services 05/09/23 16:38:11.307
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:11.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:11.328
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-7277 05/09/23 16:38:11.332
    STEP: creating service affinity-nodeport-transition in namespace services-7277 05/09/23 16:38:11.332
    STEP: creating replication controller affinity-nodeport-transition in namespace services-7277 05/09/23 16:38:11.353
    I0509 16:38:11.362624      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-7277, replica count: 3
    I0509 16:38:14.413775      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 16:38:14.430: INFO: Creating new exec pod
    May  9 16:38:14.437: INFO: Waiting up to 5m0s for pod "execpod-affinitykrzg6" in namespace "services-7277" to be "running"
    May  9 16:38:14.441: INFO: Pod "execpod-affinitykrzg6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.351668ms
    May  9 16:38:16.447: INFO: Pod "execpod-affinitykrzg6": Phase="Running", Reason="", readiness=true. Elapsed: 2.010489905s
    May  9 16:38:16.447: INFO: Pod "execpod-affinitykrzg6" satisfied condition "running"
    May  9 16:38:17.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-7277 exec execpod-affinitykrzg6 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    May  9 16:38:17.678: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    May  9 16:38:17.678: INFO: stdout: ""
    May  9 16:38:17.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-7277 exec execpod-affinitykrzg6 -- /bin/sh -x -c nc -v -z -w 2 10.3.247.147 80'
    May  9 16:38:17.897: INFO: stderr: "+ nc -v -z -w 2 10.3.247.147 80\nConnection to 10.3.247.147 80 port [tcp/http] succeeded!\n"
    May  9 16:38:17.897: INFO: stdout: ""
    May  9 16:38:17.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-7277 exec execpod-affinitykrzg6 -- /bin/sh -x -c nc -v -z -w 2 51.68.91.222 30213'
    May  9 16:38:18.123: INFO: stderr: "+ nc -v -z -w 2 51.68.91.222 30213\nConnection to 51.68.91.222 30213 port [tcp/*] succeeded!\n"
    May  9 16:38:18.123: INFO: stdout: ""
    May  9 16:38:18.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-7277 exec execpod-affinitykrzg6 -- /bin/sh -x -c nc -v -z -w 2 51.68.94.118 30213'
    May  9 16:38:18.348: INFO: stderr: "+ nc -v -z -w 2 51.68.94.118 30213\nConnection to 51.68.94.118 30213 port [tcp/*] succeeded!\n"
    May  9 16:38:18.348: INFO: stdout: ""
    May  9 16:38:18.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-7277 exec execpod-affinitykrzg6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://51.68.91.222:30213/ ; done'
    May  9 16:38:18.673: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n"
    May  9 16:38:18.673: INFO: stdout: "\naffinity-nodeport-transition-q2lln\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-2r797\naffinity-nodeport-transition-2r797\naffinity-nodeport-transition-2r797\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-q2lln\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-q2lln\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-2r797\naffinity-nodeport-transition-2r797\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-q2lln"
    May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-q2lln
    May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-2r797
    May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-2r797
    May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-2r797
    May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-q2lln
    May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-q2lln
    May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-2r797
    May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-2r797
    May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.673: INFO: Received response from host: affinity-nodeport-transition-q2lln
    May  9 16:38:18.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-7277 exec execpod-affinitykrzg6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://51.68.91.222:30213/ ; done'
    May  9 16:38:18.986: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30213/\n"
    May  9 16:38:18.987: INFO: stdout: "\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk\naffinity-nodeport-transition-8wjbk"
    May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.987: INFO: Received response from host: affinity-nodeport-transition-8wjbk
    May  9 16:38:18.987: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7277, will wait for the garbage collector to delete the pods 05/09/23 16:38:19.002
    May  9 16:38:19.068: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.440204ms
    May  9 16:38:19.169: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.668773ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  9 16:38:21.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7277" for this suite. 05/09/23 16:38:21.401
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:38:21.41
May  9 16:38:21.410: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename server-version 05/09/23 16:38:21.411
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:21.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:21.434
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 05/09/23 16:38:21.438
STEP: Confirm major version 05/09/23 16:38:21.439
May  9 16:38:21.439: INFO: Major version: 1
STEP: Confirm minor version 05/09/23 16:38:21.439
May  9 16:38:21.440: INFO: cleanMinorVersion: 26
May  9 16:38:21.440: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
May  9 16:38:21.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-990" for this suite. 05/09/23 16:38:21.445
------------------------------
â€¢ [0.042 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:38:21.41
    May  9 16:38:21.410: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename server-version 05/09/23 16:38:21.411
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:21.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:21.434
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 05/09/23 16:38:21.438
    STEP: Confirm major version 05/09/23 16:38:21.439
    May  9 16:38:21.439: INFO: Major version: 1
    STEP: Confirm minor version 05/09/23 16:38:21.439
    May  9 16:38:21.440: INFO: cleanMinorVersion: 26
    May  9 16:38:21.440: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    May  9 16:38:21.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-990" for this suite. 05/09/23 16:38:21.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:38:21.453
May  9 16:38:21.453: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename container-lifecycle-hook 05/09/23 16:38:21.454
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:21.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:21.472
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 05/09/23 16:38:21.484
May  9 16:38:21.493: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1610" to be "running and ready"
May  9 16:38:21.497: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.385447ms
May  9 16:38:21.497: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  9 16:38:23.503: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010663643s
May  9 16:38:23.503: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  9 16:38:25.511: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.018503559s
May  9 16:38:25.511: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  9 16:38:25.511: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 05/09/23 16:38:25.517
May  9 16:38:25.524: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-1610" to be "running and ready"
May  9 16:38:25.529: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.310567ms
May  9 16:38:25.529: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May  9 16:38:27.536: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011806182s
May  9 16:38:27.536: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
May  9 16:38:27.536: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 05/09/23 16:38:27.541
May  9 16:38:27.550: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  9 16:38:27.557: INFO: Pod pod-with-prestop-exec-hook still exists
May  9 16:38:29.558: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  9 16:38:29.567: INFO: Pod pod-with-prestop-exec-hook still exists
May  9 16:38:31.558: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  9 16:38:31.565: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 05/09/23 16:38:31.565
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
May  9 16:38:31.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1610" for this suite. 05/09/23 16:38:31.624
------------------------------
â€¢ [SLOW TEST] [10.180 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:38:21.453
    May  9 16:38:21.453: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/09/23 16:38:21.454
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:21.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:21.472
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 05/09/23 16:38:21.484
    May  9 16:38:21.493: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1610" to be "running and ready"
    May  9 16:38:21.497: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.385447ms
    May  9 16:38:21.497: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:38:23.503: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010663643s
    May  9 16:38:23.503: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:38:25.511: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.018503559s
    May  9 16:38:25.511: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  9 16:38:25.511: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 05/09/23 16:38:25.517
    May  9 16:38:25.524: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-1610" to be "running and ready"
    May  9 16:38:25.529: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.310567ms
    May  9 16:38:25.529: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:38:27.536: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011806182s
    May  9 16:38:27.536: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    May  9 16:38:27.536: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 05/09/23 16:38:27.541
    May  9 16:38:27.550: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May  9 16:38:27.557: INFO: Pod pod-with-prestop-exec-hook still exists
    May  9 16:38:29.558: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May  9 16:38:29.567: INFO: Pod pod-with-prestop-exec-hook still exists
    May  9 16:38:31.558: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May  9 16:38:31.565: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 05/09/23 16:38:31.565
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    May  9 16:38:31.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1610" for this suite. 05/09/23 16:38:31.624
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:38:31.633
May  9 16:38:31.633: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename disruption 05/09/23 16:38:31.634
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:31.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:31.656
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 05/09/23 16:38:31.666
STEP: Waiting for all pods to be running 05/09/23 16:38:31.74
May  9 16:38:31.748: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May  9 16:38:33.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2270" for this suite. 05/09/23 16:38:33.872
------------------------------
â€¢ [2.247 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:38:31.633
    May  9 16:38:31.633: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename disruption 05/09/23 16:38:31.634
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:31.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:31.656
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 05/09/23 16:38:31.666
    STEP: Waiting for all pods to be running 05/09/23 16:38:31.74
    May  9 16:38:31.748: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May  9 16:38:33.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2270" for this suite. 05/09/23 16:38:33.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:38:33.882
May  9 16:38:33.882: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename pods 05/09/23 16:38:33.883
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:33.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:33.904
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 05/09/23 16:38:33.908
May  9 16:38:33.967: INFO: Waiting up to 5m0s for pod "pod-66qps" in namespace "pods-456" to be "running"
May  9 16:38:33.974: INFO: Pod "pod-66qps": Phase="Pending", Reason="", readiness=false. Elapsed: 6.862647ms
May  9 16:38:35.979: INFO: Pod "pod-66qps": Phase="Running", Reason="", readiness=true. Elapsed: 2.012553475s
May  9 16:38:35.979: INFO: Pod "pod-66qps" satisfied condition "running"
STEP: patching /status 05/09/23 16:38:35.979
May  9 16:38:35.990: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  9 16:38:35.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-456" for this suite. 05/09/23 16:38:35.997
------------------------------
â€¢ [2.124 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:38:33.882
    May  9 16:38:33.882: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename pods 05/09/23 16:38:33.883
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:33.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:33.904
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 05/09/23 16:38:33.908
    May  9 16:38:33.967: INFO: Waiting up to 5m0s for pod "pod-66qps" in namespace "pods-456" to be "running"
    May  9 16:38:33.974: INFO: Pod "pod-66qps": Phase="Pending", Reason="", readiness=false. Elapsed: 6.862647ms
    May  9 16:38:35.979: INFO: Pod "pod-66qps": Phase="Running", Reason="", readiness=true. Elapsed: 2.012553475s
    May  9 16:38:35.979: INFO: Pod "pod-66qps" satisfied condition "running"
    STEP: patching /status 05/09/23 16:38:35.979
    May  9 16:38:35.990: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  9 16:38:35.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-456" for this suite. 05/09/23 16:38:35.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:38:36.011
May  9 16:38:36.011: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename downward-api 05/09/23 16:38:36.012
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:36.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:36.034
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 05/09/23 16:38:36.04
May  9 16:38:36.053: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92f04700-30d9-4e57-a02e-a04529ff843e" in namespace "downward-api-7835" to be "Succeeded or Failed"
May  9 16:38:36.059: INFO: Pod "downwardapi-volume-92f04700-30d9-4e57-a02e-a04529ff843e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.512546ms
May  9 16:38:38.066: INFO: Pod "downwardapi-volume-92f04700-30d9-4e57-a02e-a04529ff843e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012958135s
May  9 16:38:40.067: INFO: Pod "downwardapi-volume-92f04700-30d9-4e57-a02e-a04529ff843e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014509852s
STEP: Saw pod success 05/09/23 16:38:40.067
May  9 16:38:40.067: INFO: Pod "downwardapi-volume-92f04700-30d9-4e57-a02e-a04529ff843e" satisfied condition "Succeeded or Failed"
May  9 16:38:40.072: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod downwardapi-volume-92f04700-30d9-4e57-a02e-a04529ff843e container client-container: <nil>
STEP: delete the pod 05/09/23 16:38:40.124
May  9 16:38:40.143: INFO: Waiting for pod downwardapi-volume-92f04700-30d9-4e57-a02e-a04529ff843e to disappear
May  9 16:38:40.148: INFO: Pod downwardapi-volume-92f04700-30d9-4e57-a02e-a04529ff843e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  9 16:38:40.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7835" for this suite. 05/09/23 16:38:40.154
------------------------------
â€¢ [4.151 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:38:36.011
    May  9 16:38:36.011: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename downward-api 05/09/23 16:38:36.012
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:36.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:36.034
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 05/09/23 16:38:36.04
    May  9 16:38:36.053: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92f04700-30d9-4e57-a02e-a04529ff843e" in namespace "downward-api-7835" to be "Succeeded or Failed"
    May  9 16:38:36.059: INFO: Pod "downwardapi-volume-92f04700-30d9-4e57-a02e-a04529ff843e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.512546ms
    May  9 16:38:38.066: INFO: Pod "downwardapi-volume-92f04700-30d9-4e57-a02e-a04529ff843e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012958135s
    May  9 16:38:40.067: INFO: Pod "downwardapi-volume-92f04700-30d9-4e57-a02e-a04529ff843e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014509852s
    STEP: Saw pod success 05/09/23 16:38:40.067
    May  9 16:38:40.067: INFO: Pod "downwardapi-volume-92f04700-30d9-4e57-a02e-a04529ff843e" satisfied condition "Succeeded or Failed"
    May  9 16:38:40.072: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod downwardapi-volume-92f04700-30d9-4e57-a02e-a04529ff843e container client-container: <nil>
    STEP: delete the pod 05/09/23 16:38:40.124
    May  9 16:38:40.143: INFO: Waiting for pod downwardapi-volume-92f04700-30d9-4e57-a02e-a04529ff843e to disappear
    May  9 16:38:40.148: INFO: Pod downwardapi-volume-92f04700-30d9-4e57-a02e-a04529ff843e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  9 16:38:40.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7835" for this suite. 05/09/23 16:38:40.154
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:38:40.164
May  9 16:38:40.164: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename endpointslice 05/09/23 16:38:40.165
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:40.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:40.185
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
May  9 16:38:42.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9006" for this suite. 05/09/23 16:38:42.266
------------------------------
â€¢ [2.109 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:38:40.164
    May  9 16:38:40.164: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename endpointslice 05/09/23 16:38:40.165
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:40.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:40.185
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    May  9 16:38:42.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9006" for this suite. 05/09/23 16:38:42.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:38:42.275
May  9 16:38:42.275: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubectl 05/09/23 16:38:42.276
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:42.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:42.295
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 05/09/23 16:38:42.3
May  9 16:38:42.300: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May  9 16:38:42.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 create -f -'
May  9 16:38:42.517: INFO: stderr: ""
May  9 16:38:42.517: INFO: stdout: "service/agnhost-replica created\n"
May  9 16:38:42.517: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May  9 16:38:42.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 create -f -'
May  9 16:38:42.730: INFO: stderr: ""
May  9 16:38:42.730: INFO: stdout: "service/agnhost-primary created\n"
May  9 16:38:42.730: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May  9 16:38:42.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 create -f -'
May  9 16:38:42.952: INFO: stderr: ""
May  9 16:38:42.952: INFO: stdout: "service/frontend created\n"
May  9 16:38:42.952: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May  9 16:38:42.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 create -f -'
May  9 16:38:43.187: INFO: stderr: ""
May  9 16:38:43.187: INFO: stdout: "deployment.apps/frontend created\n"
May  9 16:38:43.187: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May  9 16:38:43.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 create -f -'
May  9 16:38:43.405: INFO: stderr: ""
May  9 16:38:43.406: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May  9 16:38:43.406: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May  9 16:38:43.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 create -f -'
May  9 16:38:43.624: INFO: stderr: ""
May  9 16:38:43.624: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 05/09/23 16:38:43.624
May  9 16:38:43.624: INFO: Waiting for all frontend pods to be Running.
May  9 16:38:48.674: INFO: Waiting for frontend to serve content.
May  9 16:38:48.691: INFO: Trying to add a new entry to the guestbook.
May  9 16:38:48.709: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 05/09/23 16:38:48.725
May  9 16:38:48.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 delete --grace-period=0 --force -f -'
May  9 16:38:48.841: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  9 16:38:48.841: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 05/09/23 16:38:48.841
May  9 16:38:48.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 delete --grace-period=0 --force -f -'
May  9 16:38:48.945: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  9 16:38:48.945: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 05/09/23 16:38:48.945
May  9 16:38:48.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 delete --grace-period=0 --force -f -'
May  9 16:38:49.041: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  9 16:38:49.041: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 05/09/23 16:38:49.041
May  9 16:38:49.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 delete --grace-period=0 --force -f -'
May  9 16:38:49.124: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  9 16:38:49.124: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 05/09/23 16:38:49.124
May  9 16:38:49.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 delete --grace-period=0 --force -f -'
May  9 16:38:49.220: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  9 16:38:49.220: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 05/09/23 16:38:49.221
May  9 16:38:49.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 delete --grace-period=0 --force -f -'
May  9 16:38:49.332: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  9 16:38:49.332: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  9 16:38:49.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4169" for this suite. 05/09/23 16:38:49.338
------------------------------
â€¢ [SLOW TEST] [7.087 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:38:42.275
    May  9 16:38:42.275: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubectl 05/09/23 16:38:42.276
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:42.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:42.295
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 05/09/23 16:38:42.3
    May  9 16:38:42.300: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    May  9 16:38:42.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 create -f -'
    May  9 16:38:42.517: INFO: stderr: ""
    May  9 16:38:42.517: INFO: stdout: "service/agnhost-replica created\n"
    May  9 16:38:42.517: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    May  9 16:38:42.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 create -f -'
    May  9 16:38:42.730: INFO: stderr: ""
    May  9 16:38:42.730: INFO: stdout: "service/agnhost-primary created\n"
    May  9 16:38:42.730: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    May  9 16:38:42.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 create -f -'
    May  9 16:38:42.952: INFO: stderr: ""
    May  9 16:38:42.952: INFO: stdout: "service/frontend created\n"
    May  9 16:38:42.952: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    May  9 16:38:42.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 create -f -'
    May  9 16:38:43.187: INFO: stderr: ""
    May  9 16:38:43.187: INFO: stdout: "deployment.apps/frontend created\n"
    May  9 16:38:43.187: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    May  9 16:38:43.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 create -f -'
    May  9 16:38:43.405: INFO: stderr: ""
    May  9 16:38:43.406: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    May  9 16:38:43.406: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    May  9 16:38:43.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 create -f -'
    May  9 16:38:43.624: INFO: stderr: ""
    May  9 16:38:43.624: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 05/09/23 16:38:43.624
    May  9 16:38:43.624: INFO: Waiting for all frontend pods to be Running.
    May  9 16:38:48.674: INFO: Waiting for frontend to serve content.
    May  9 16:38:48.691: INFO: Trying to add a new entry to the guestbook.
    May  9 16:38:48.709: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 05/09/23 16:38:48.725
    May  9 16:38:48.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 delete --grace-period=0 --force -f -'
    May  9 16:38:48.841: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  9 16:38:48.841: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 05/09/23 16:38:48.841
    May  9 16:38:48.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 delete --grace-period=0 --force -f -'
    May  9 16:38:48.945: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  9 16:38:48.945: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 05/09/23 16:38:48.945
    May  9 16:38:48.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 delete --grace-period=0 --force -f -'
    May  9 16:38:49.041: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  9 16:38:49.041: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 05/09/23 16:38:49.041
    May  9 16:38:49.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 delete --grace-period=0 --force -f -'
    May  9 16:38:49.124: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  9 16:38:49.124: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 05/09/23 16:38:49.124
    May  9 16:38:49.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 delete --grace-period=0 --force -f -'
    May  9 16:38:49.220: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  9 16:38:49.220: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 05/09/23 16:38:49.221
    May  9 16:38:49.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4169 delete --grace-period=0 --force -f -'
    May  9 16:38:49.332: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  9 16:38:49.332: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  9 16:38:49.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4169" for this suite. 05/09/23 16:38:49.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:38:49.363
May  9 16:38:49.363: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename job 05/09/23 16:38:49.364
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:49.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:49.387
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 05/09/23 16:38:49.392
STEP: Ensuring active pods == parallelism 05/09/23 16:38:49.398
STEP: Orphaning one of the Job's Pods 05/09/23 16:38:51.405
May  9 16:38:51.927: INFO: Successfully updated pod "adopt-release-b965n"
STEP: Checking that the Job readopts the Pod 05/09/23 16:38:51.927
May  9 16:38:51.927: INFO: Waiting up to 15m0s for pod "adopt-release-b965n" in namespace "job-9620" to be "adopted"
May  9 16:38:51.933: INFO: Pod "adopt-release-b965n": Phase="Running", Reason="", readiness=true. Elapsed: 5.29879ms
May  9 16:38:53.939: INFO: Pod "adopt-release-b965n": Phase="Running", Reason="", readiness=true. Elapsed: 2.011848778s
May  9 16:38:53.939: INFO: Pod "adopt-release-b965n" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 05/09/23 16:38:53.939
May  9 16:38:54.453: INFO: Successfully updated pod "adopt-release-b965n"
STEP: Checking that the Job releases the Pod 05/09/23 16:38:54.453
May  9 16:38:54.453: INFO: Waiting up to 15m0s for pod "adopt-release-b965n" in namespace "job-9620" to be "released"
May  9 16:38:54.457: INFO: Pod "adopt-release-b965n": Phase="Running", Reason="", readiness=true. Elapsed: 3.63293ms
May  9 16:38:56.464: INFO: Pod "adopt-release-b965n": Phase="Running", Reason="", readiness=true. Elapsed: 2.010502001s
May  9 16:38:56.464: INFO: Pod "adopt-release-b965n" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May  9 16:38:56.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9620" for this suite. 05/09/23 16:38:56.472
------------------------------
â€¢ [SLOW TEST] [7.118 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:38:49.363
    May  9 16:38:49.363: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename job 05/09/23 16:38:49.364
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:49.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:49.387
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 05/09/23 16:38:49.392
    STEP: Ensuring active pods == parallelism 05/09/23 16:38:49.398
    STEP: Orphaning one of the Job's Pods 05/09/23 16:38:51.405
    May  9 16:38:51.927: INFO: Successfully updated pod "adopt-release-b965n"
    STEP: Checking that the Job readopts the Pod 05/09/23 16:38:51.927
    May  9 16:38:51.927: INFO: Waiting up to 15m0s for pod "adopt-release-b965n" in namespace "job-9620" to be "adopted"
    May  9 16:38:51.933: INFO: Pod "adopt-release-b965n": Phase="Running", Reason="", readiness=true. Elapsed: 5.29879ms
    May  9 16:38:53.939: INFO: Pod "adopt-release-b965n": Phase="Running", Reason="", readiness=true. Elapsed: 2.011848778s
    May  9 16:38:53.939: INFO: Pod "adopt-release-b965n" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 05/09/23 16:38:53.939
    May  9 16:38:54.453: INFO: Successfully updated pod "adopt-release-b965n"
    STEP: Checking that the Job releases the Pod 05/09/23 16:38:54.453
    May  9 16:38:54.453: INFO: Waiting up to 15m0s for pod "adopt-release-b965n" in namespace "job-9620" to be "released"
    May  9 16:38:54.457: INFO: Pod "adopt-release-b965n": Phase="Running", Reason="", readiness=true. Elapsed: 3.63293ms
    May  9 16:38:56.464: INFO: Pod "adopt-release-b965n": Phase="Running", Reason="", readiness=true. Elapsed: 2.010502001s
    May  9 16:38:56.464: INFO: Pod "adopt-release-b965n" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May  9 16:38:56.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9620" for this suite. 05/09/23 16:38:56.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:38:56.482
May  9 16:38:56.482: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename security-context-test 05/09/23 16:38:56.483
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:56.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:56.506
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
May  9 16:38:56.524: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-b728e5cb-c6af-451d-b35a-de1daaaa8d7e" in namespace "security-context-test-4220" to be "Succeeded or Failed"
May  9 16:38:56.530: INFO: Pod "alpine-nnp-false-b728e5cb-c6af-451d-b35a-de1daaaa8d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.740166ms
May  9 16:38:58.537: INFO: Pod "alpine-nnp-false-b728e5cb-c6af-451d-b35a-de1daaaa8d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013165483s
May  9 16:39:00.537: INFO: Pod "alpine-nnp-false-b728e5cb-c6af-451d-b35a-de1daaaa8d7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012629885s
May  9 16:39:00.537: INFO: Pod "alpine-nnp-false-b728e5cb-c6af-451d-b35a-de1daaaa8d7e" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May  9 16:39:00.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4220" for this suite. 05/09/23 16:39:00.558
------------------------------
â€¢ [4.094 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:38:56.482
    May  9 16:38:56.482: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename security-context-test 05/09/23 16:38:56.483
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:38:56.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:38:56.506
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    May  9 16:38:56.524: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-b728e5cb-c6af-451d-b35a-de1daaaa8d7e" in namespace "security-context-test-4220" to be "Succeeded or Failed"
    May  9 16:38:56.530: INFO: Pod "alpine-nnp-false-b728e5cb-c6af-451d-b35a-de1daaaa8d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.740166ms
    May  9 16:38:58.537: INFO: Pod "alpine-nnp-false-b728e5cb-c6af-451d-b35a-de1daaaa8d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013165483s
    May  9 16:39:00.537: INFO: Pod "alpine-nnp-false-b728e5cb-c6af-451d-b35a-de1daaaa8d7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012629885s
    May  9 16:39:00.537: INFO: Pod "alpine-nnp-false-b728e5cb-c6af-451d-b35a-de1daaaa8d7e" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May  9 16:39:00.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4220" for this suite. 05/09/23 16:39:00.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:39:00.577
May  9 16:39:00.577: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename container-runtime 05/09/23 16:39:00.578
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:39:00.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:39:00.612
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 05/09/23 16:39:00.616
STEP: wait for the container to reach Succeeded 05/09/23 16:39:00.626
STEP: get the container status 05/09/23 16:39:05.671
STEP: the container should be terminated 05/09/23 16:39:05.677
STEP: the termination message should be set 05/09/23 16:39:05.677
May  9 16:39:05.677: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 05/09/23 16:39:05.677
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May  9 16:39:05.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4260" for this suite. 05/09/23 16:39:05.701
------------------------------
â€¢ [SLOW TEST] [5.134 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:39:00.577
    May  9 16:39:00.577: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename container-runtime 05/09/23 16:39:00.578
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:39:00.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:39:00.612
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 05/09/23 16:39:00.616
    STEP: wait for the container to reach Succeeded 05/09/23 16:39:00.626
    STEP: get the container status 05/09/23 16:39:05.671
    STEP: the container should be terminated 05/09/23 16:39:05.677
    STEP: the termination message should be set 05/09/23 16:39:05.677
    May  9 16:39:05.677: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 05/09/23 16:39:05.677
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May  9 16:39:05.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4260" for this suite. 05/09/23 16:39:05.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:39:05.713
May  9 16:39:05.713: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename replication-controller 05/09/23 16:39:05.714
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:39:05.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:39:05.74
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-b676v" 05/09/23 16:39:05.744
May  9 16:39:05.751: INFO: Get Replication Controller "e2e-rc-b676v" to confirm replicas
May  9 16:39:06.759: INFO: Get Replication Controller "e2e-rc-b676v" to confirm replicas
May  9 16:39:06.764: INFO: Found 1 replicas for "e2e-rc-b676v" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-b676v" 05/09/23 16:39:06.764
STEP: Updating a scale subresource 05/09/23 16:39:06.768
STEP: Verifying replicas where modified for replication controller "e2e-rc-b676v" 05/09/23 16:39:06.775
May  9 16:39:06.775: INFO: Get Replication Controller "e2e-rc-b676v" to confirm replicas
May  9 16:39:07.779: INFO: Get Replication Controller "e2e-rc-b676v" to confirm replicas
May  9 16:39:07.785: INFO: Found 2 replicas for "e2e-rc-b676v" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May  9 16:39:07.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9362" for this suite. 05/09/23 16:39:07.791
------------------------------
â€¢ [2.087 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:39:05.713
    May  9 16:39:05.713: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename replication-controller 05/09/23 16:39:05.714
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:39:05.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:39:05.74
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-b676v" 05/09/23 16:39:05.744
    May  9 16:39:05.751: INFO: Get Replication Controller "e2e-rc-b676v" to confirm replicas
    May  9 16:39:06.759: INFO: Get Replication Controller "e2e-rc-b676v" to confirm replicas
    May  9 16:39:06.764: INFO: Found 1 replicas for "e2e-rc-b676v" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-b676v" 05/09/23 16:39:06.764
    STEP: Updating a scale subresource 05/09/23 16:39:06.768
    STEP: Verifying replicas where modified for replication controller "e2e-rc-b676v" 05/09/23 16:39:06.775
    May  9 16:39:06.775: INFO: Get Replication Controller "e2e-rc-b676v" to confirm replicas
    May  9 16:39:07.779: INFO: Get Replication Controller "e2e-rc-b676v" to confirm replicas
    May  9 16:39:07.785: INFO: Found 2 replicas for "e2e-rc-b676v" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May  9 16:39:07.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9362" for this suite. 05/09/23 16:39:07.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:39:07.801
May  9 16:39:07.801: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename downward-api 05/09/23 16:39:07.802
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:39:07.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:39:07.822
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 05/09/23 16:39:07.826
May  9 16:39:07.838: INFO: Waiting up to 5m0s for pod "downward-api-72c7350d-ef5b-4720-b359-371549300900" in namespace "downward-api-363" to be "Succeeded or Failed"
May  9 16:39:07.846: INFO: Pod "downward-api-72c7350d-ef5b-4720-b359-371549300900": Phase="Pending", Reason="", readiness=false. Elapsed: 7.985598ms
May  9 16:39:09.852: INFO: Pod "downward-api-72c7350d-ef5b-4720-b359-371549300900": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014158112s
May  9 16:39:11.855: INFO: Pod "downward-api-72c7350d-ef5b-4720-b359-371549300900": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016556477s
STEP: Saw pod success 05/09/23 16:39:11.855
May  9 16:39:11.855: INFO: Pod "downward-api-72c7350d-ef5b-4720-b359-371549300900" satisfied condition "Succeeded or Failed"
May  9 16:39:11.860: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downward-api-72c7350d-ef5b-4720-b359-371549300900 container dapi-container: <nil>
STEP: delete the pod 05/09/23 16:39:11.872
May  9 16:39:11.888: INFO: Waiting for pod downward-api-72c7350d-ef5b-4720-b359-371549300900 to disappear
May  9 16:39:11.891: INFO: Pod downward-api-72c7350d-ef5b-4720-b359-371549300900 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May  9 16:39:11.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-363" for this suite. 05/09/23 16:39:11.899
------------------------------
â€¢ [4.104 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:39:07.801
    May  9 16:39:07.801: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename downward-api 05/09/23 16:39:07.802
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:39:07.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:39:07.822
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 05/09/23 16:39:07.826
    May  9 16:39:07.838: INFO: Waiting up to 5m0s for pod "downward-api-72c7350d-ef5b-4720-b359-371549300900" in namespace "downward-api-363" to be "Succeeded or Failed"
    May  9 16:39:07.846: INFO: Pod "downward-api-72c7350d-ef5b-4720-b359-371549300900": Phase="Pending", Reason="", readiness=false. Elapsed: 7.985598ms
    May  9 16:39:09.852: INFO: Pod "downward-api-72c7350d-ef5b-4720-b359-371549300900": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014158112s
    May  9 16:39:11.855: INFO: Pod "downward-api-72c7350d-ef5b-4720-b359-371549300900": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016556477s
    STEP: Saw pod success 05/09/23 16:39:11.855
    May  9 16:39:11.855: INFO: Pod "downward-api-72c7350d-ef5b-4720-b359-371549300900" satisfied condition "Succeeded or Failed"
    May  9 16:39:11.860: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downward-api-72c7350d-ef5b-4720-b359-371549300900 container dapi-container: <nil>
    STEP: delete the pod 05/09/23 16:39:11.872
    May  9 16:39:11.888: INFO: Waiting for pod downward-api-72c7350d-ef5b-4720-b359-371549300900 to disappear
    May  9 16:39:11.891: INFO: Pod downward-api-72c7350d-ef5b-4720-b359-371549300900 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May  9 16:39:11.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-363" for this suite. 05/09/23 16:39:11.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:39:11.906
May  9 16:39:11.906: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename watch 05/09/23 16:39:11.907
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:39:11.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:39:11.927
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 05/09/23 16:39:11.931
STEP: creating a new configmap 05/09/23 16:39:11.933
STEP: modifying the configmap once 05/09/23 16:39:11.938
STEP: closing the watch once it receives two notifications 05/09/23 16:39:11.948
May  9 16:39:11.948: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2543  3c7eb223-2c77-4abc-a1a3-d861dfe59630 318094875 0 2023-05-09 16:39:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-09 16:39:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 16:39:11.948: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2543  3c7eb223-2c77-4abc-a1a3-d861dfe59630 318094879 0 2023-05-09 16:39:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-09 16:39:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 05/09/23 16:39:11.948
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 05/09/23 16:39:11.957
STEP: deleting the configmap 05/09/23 16:39:11.959
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 05/09/23 16:39:11.968
May  9 16:39:11.968: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2543  3c7eb223-2c77-4abc-a1a3-d861dfe59630 318094884 0 2023-05-09 16:39:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-09 16:39:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 16:39:11.968: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2543  3c7eb223-2c77-4abc-a1a3-d861dfe59630 318094890 0 2023-05-09 16:39:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-09 16:39:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May  9 16:39:11.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2543" for this suite. 05/09/23 16:39:11.98
------------------------------
â€¢ [0.083 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:39:11.906
    May  9 16:39:11.906: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename watch 05/09/23 16:39:11.907
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:39:11.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:39:11.927
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 05/09/23 16:39:11.931
    STEP: creating a new configmap 05/09/23 16:39:11.933
    STEP: modifying the configmap once 05/09/23 16:39:11.938
    STEP: closing the watch once it receives two notifications 05/09/23 16:39:11.948
    May  9 16:39:11.948: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2543  3c7eb223-2c77-4abc-a1a3-d861dfe59630 318094875 0 2023-05-09 16:39:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-09 16:39:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 16:39:11.948: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2543  3c7eb223-2c77-4abc-a1a3-d861dfe59630 318094879 0 2023-05-09 16:39:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-09 16:39:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 05/09/23 16:39:11.948
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 05/09/23 16:39:11.957
    STEP: deleting the configmap 05/09/23 16:39:11.959
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 05/09/23 16:39:11.968
    May  9 16:39:11.968: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2543  3c7eb223-2c77-4abc-a1a3-d861dfe59630 318094884 0 2023-05-09 16:39:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-09 16:39:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 16:39:11.968: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2543  3c7eb223-2c77-4abc-a1a3-d861dfe59630 318094890 0 2023-05-09 16:39:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-09 16:39:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May  9 16:39:11.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2543" for this suite. 05/09/23 16:39:11.98
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:39:11.994
May  9 16:39:11.995: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename sched-pred 05/09/23 16:39:11.995
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:39:12.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:39:12.029
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
May  9 16:39:12.034: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  9 16:39:12.045: INFO: Waiting for terminating namespaces to be deleted...
May  9 16:39:12.050: INFO: 
Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 before test
May  9 16:39:12.062: INFO: adopt-release-4lql6 from job-9620 started at 2023-05-09 16:38:55 +0000 UTC (1 container statuses recorded)
May  9 16:39:12.062: INFO: 	Container c ready: true, restart count 0
May  9 16:39:12.062: INFO: adopt-release-b965n from job-9620 started at 2023-05-09 16:38:49 +0000 UTC (1 container statuses recorded)
May  9 16:39:12.062: INFO: 	Container c ready: true, restart count 0
May  9 16:39:12.062: INFO: canal-zcmck from kube-system started at 2023-05-09 08:48:05 +0000 UTC (2 container statuses recorded)
May  9 16:39:12.062: INFO: 	Container calico-node ready: true, restart count 2
May  9 16:39:12.062: INFO: 	Container kube-flannel ready: true, restart count 0
May  9 16:39:12.062: INFO: coredns-996c5dbc5-kcx4b from kube-system started at 2023-05-09 15:55:37 +0000 UTC (1 container statuses recorded)
May  9 16:39:12.062: INFO: 	Container coredns ready: true, restart count 0
May  9 16:39:12.062: INFO: kube-proxy-q8nhz from kube-system started at 2023-05-09 08:48:05 +0000 UTC (1 container statuses recorded)
May  9 16:39:12.062: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 16:39:12.062: INFO: wormhole-xj4hj from kube-system started at 2023-05-09 08:48:05 +0000 UTC (1 container statuses recorded)
May  9 16:39:12.062: INFO: 	Container wormhole ready: true, restart count 0
May  9 16:39:12.062: INFO: e2e-rc-b676v-6bncd from replication-controller-9362 started at 2023-05-09 16:39:06 +0000 UTC (1 container statuses recorded)
May  9 16:39:12.062: INFO: 	Container httpd ready: true, restart count 0
May  9 16:39:12.062: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-69c6h from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
May  9 16:39:12.062: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 16:39:12.062: INFO: 	Container systemd-logs ready: true, restart count 0
May  9 16:39:12.062: INFO: 
Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 before test
May  9 16:39:12.073: INFO: rs-f6twm from disruption-3357 started at 2023-05-09 16:38:00 +0000 UTC (1 container statuses recorded)
May  9 16:39:12.073: INFO: 	Container donothing ready: false, restart count 0
May  9 16:39:12.073: INFO: adopt-release-xs8sf from job-9620 started at 2023-05-09 16:38:49 +0000 UTC (1 container statuses recorded)
May  9 16:39:12.073: INFO: 	Container c ready: true, restart count 0
May  9 16:39:12.073: INFO: calico-kube-controllers-6cffbf7894-zrprz from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
May  9 16:39:12.073: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  9 16:39:12.073: INFO: canal-22bj2 from kube-system started at 2023-05-09 08:47:59 +0000 UTC (2 container statuses recorded)
May  9 16:39:12.073: INFO: 	Container calico-node ready: true, restart count 2
May  9 16:39:12.073: INFO: 	Container kube-flannel ready: true, restart count 0
May  9 16:39:12.073: INFO: coredns-996c5dbc5-24wst from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
May  9 16:39:12.073: INFO: 	Container coredns ready: true, restart count 0
May  9 16:39:12.073: INFO: kube-dns-autoscaler-789d47d664-hhx85 from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
May  9 16:39:12.073: INFO: 	Container autoscaler ready: true, restart count 0
May  9 16:39:12.073: INFO: kube-proxy-r9g6q from kube-system started at 2023-05-09 08:47:59 +0000 UTC (1 container statuses recorded)
May  9 16:39:12.073: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 16:39:12.073: INFO: metrics-server-5f9c95d78-hk5zz from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
May  9 16:39:12.073: INFO: 	Container metrics-server ready: true, restart count 0
May  9 16:39:12.073: INFO: wormhole-hnkjk from kube-system started at 2023-05-09 08:47:59 +0000 UTC (1 container statuses recorded)
May  9 16:39:12.073: INFO: 	Container wormhole ready: true, restart count 0
May  9 16:39:12.073: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-2x6mg from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
May  9 16:39:12.073: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 16:39:12.073: INFO: 	Container systemd-logs ready: true, restart count 0
May  9 16:39:12.073: INFO: 
Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 before test
May  9 16:39:12.084: INFO: canal-jl6h7 from kube-system started at 2023-05-09 08:48:03 +0000 UTC (2 container statuses recorded)
May  9 16:39:12.084: INFO: 	Container calico-node ready: true, restart count 2
May  9 16:39:12.084: INFO: 	Container kube-flannel ready: true, restart count 0
May  9 16:39:12.084: INFO: kube-proxy-qqc8l from kube-system started at 2023-05-09 08:48:03 +0000 UTC (1 container statuses recorded)
May  9 16:39:12.084: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 16:39:12.084: INFO: wormhole-825jz from kube-system started at 2023-05-09 08:48:03 +0000 UTC (1 container statuses recorded)
May  9 16:39:12.084: INFO: 	Container wormhole ready: true, restart count 0
May  9 16:39:12.084: INFO: e2e-rc-b676v-qvdnh from replication-controller-9362 started at 2023-05-09 16:39:05 +0000 UTC (1 container statuses recorded)
May  9 16:39:12.084: INFO: 	Container httpd ready: true, restart count 0
May  9 16:39:12.084: INFO: sonobuoy from sonobuoy started at 2023-05-09 15:24:58 +0000 UTC (1 container statuses recorded)
May  9 16:39:12.084: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  9 16:39:12.084: INFO: sonobuoy-e2e-job-d96e230b174b4cad from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
May  9 16:39:12.084: INFO: 	Container e2e ready: true, restart count 0
May  9 16:39:12.084: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 16:39:12.084: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-zvq2k from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
May  9 16:39:12.084: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 16:39:12.084: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 05/09/23 16:39:12.116
STEP: verifying the node has the label node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 05/09/23 16:39:12.136
STEP: verifying the node has the label node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 05/09/23 16:39:12.152
May  9 16:39:12.179: INFO: Pod rs-f6twm requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
May  9 16:39:12.179: INFO: Pod adopt-release-4lql6 requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
May  9 16:39:12.179: INFO: Pod adopt-release-b965n requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
May  9 16:39:12.179: INFO: Pod adopt-release-xs8sf requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
May  9 16:39:12.179: INFO: Pod calico-kube-controllers-6cffbf7894-zrprz requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
May  9 16:39:12.179: INFO: Pod canal-22bj2 requesting resource cpu=250m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
May  9 16:39:12.179: INFO: Pod canal-jl6h7 requesting resource cpu=250m on Node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62
May  9 16:39:12.179: INFO: Pod canal-zcmck requesting resource cpu=250m on Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
May  9 16:39:12.179: INFO: Pod coredns-996c5dbc5-24wst requesting resource cpu=100m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
May  9 16:39:12.179: INFO: Pod coredns-996c5dbc5-kcx4b requesting resource cpu=100m on Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
May  9 16:39:12.179: INFO: Pod kube-dns-autoscaler-789d47d664-hhx85 requesting resource cpu=20m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
May  9 16:39:12.179: INFO: Pod kube-proxy-q8nhz requesting resource cpu=100m on Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
May  9 16:39:12.179: INFO: Pod kube-proxy-qqc8l requesting resource cpu=100m on Node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62
May  9 16:39:12.179: INFO: Pod kube-proxy-r9g6q requesting resource cpu=100m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
May  9 16:39:12.179: INFO: Pod metrics-server-5f9c95d78-hk5zz requesting resource cpu=100m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
May  9 16:39:12.179: INFO: Pod wormhole-825jz requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62
May  9 16:39:12.179: INFO: Pod wormhole-hnkjk requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
May  9 16:39:12.179: INFO: Pod wormhole-xj4hj requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
May  9 16:39:12.179: INFO: Pod e2e-rc-b676v-6bncd requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
May  9 16:39:12.179: INFO: Pod e2e-rc-b676v-qvdnh requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62
May  9 16:39:12.179: INFO: Pod sonobuoy requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62
May  9 16:39:12.179: INFO: Pod sonobuoy-e2e-job-d96e230b174b4cad requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62
May  9 16:39:12.179: INFO: Pod sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-2x6mg requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
May  9 16:39:12.179: INFO: Pod sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-69c6h requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
May  9 16:39:12.179: INFO: Pod sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-zvq2k requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62
STEP: Starting Pods to consume most of the cluster CPU. 05/09/23 16:39:12.179
May  9 16:39:12.179: INFO: Creating a pod which consumes cpu=2366m on Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
May  9 16:39:12.190: INFO: Creating a pod which consumes cpu=2282m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
May  9 16:39:12.201: INFO: Creating a pod which consumes cpu=2436m on Node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62
May  9 16:39:12.206: INFO: Waiting up to 5m0s for pod "filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b" in namespace "sched-pred-5476" to be "running"
May  9 16:39:12.212: INFO: Pod "filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.23351ms
May  9 16:39:14.218: INFO: Pod "filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b": Phase="Running", Reason="", readiness=true. Elapsed: 2.011739877s
May  9 16:39:14.218: INFO: Pod "filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b" satisfied condition "running"
May  9 16:39:14.218: INFO: Waiting up to 5m0s for pod "filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4" in namespace "sched-pred-5476" to be "running"
May  9 16:39:14.223: INFO: Pod "filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4": Phase="Running", Reason="", readiness=true. Elapsed: 4.785448ms
May  9 16:39:14.223: INFO: Pod "filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4" satisfied condition "running"
May  9 16:39:14.223: INFO: Waiting up to 5m0s for pod "filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b" in namespace "sched-pred-5476" to be "running"
May  9 16:39:14.228: INFO: Pod "filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b": Phase="Running", Reason="", readiness=true. Elapsed: 5.264604ms
May  9 16:39:14.228: INFO: Pod "filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 05/09/23 16:39:14.228
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b.175d871331c196f8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5476/filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b to nodepool-8cc7f47e-9b0c-4801-88-node-f76f62] 05/09/23 16:39:14.235
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b.175d8713534656df], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 05/09/23 16:39:14.235
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b.175d87136ce972bb], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 430.093789ms (430.105181ms including waiting)] 05/09/23 16:39:14.235
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b.175d87136df0e521], Reason = [Created], Message = [Created container filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b] 05/09/23 16:39:14.235
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b.175d8713736315db], Reason = [Started], Message = [Started container filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b] 05/09/23 16:39:14.235
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b.175d871330993af7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5476/filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b to nodepool-8cc7f47e-9b0c-4801-88-node-7ad816] 05/09/23 16:39:14.235
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b.175d871353c0b9e0], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 05/09/23 16:39:14.235
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b.175d871366f6bff0], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 322.273459ms (322.285063ms including waiting)] 05/09/23 16:39:14.235
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b.175d8713679d9175], Reason = [Created], Message = [Created container filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b] 05/09/23 16:39:14.235
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b.175d87136cfc73a6], Reason = [Started], Message = [Started container filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b] 05/09/23 16:39:14.235
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4.175d8713310fe6ed], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5476/filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4 to nodepool-8cc7f47e-9b0c-4801-88-node-bbade7] 05/09/23 16:39:14.235
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4.175d871353a45adb], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 05/09/23 16:39:14.235
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4.175d8713670fdb0c], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 325.77629ms (325.791388ms including waiting)] 05/09/23 16:39:14.236
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4.175d871367a242db], Reason = [Created], Message = [Created container filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4] 05/09/23 16:39:14.236
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4.175d87136da33f73], Reason = [Started], Message = [Started container filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4] 05/09/23 16:39:14.236
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.175d8713aaedaa33], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 05/09/23 16:39:14.254
STEP: removing the label node off the node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 05/09/23 16:39:15.256
STEP: verifying the node doesn't have the label node 05/09/23 16:39:15.28
STEP: removing the label node off the node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 05/09/23 16:39:15.286
STEP: verifying the node doesn't have the label node 05/09/23 16:39:15.304
STEP: removing the label node off the node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 05/09/23 16:39:15.311
STEP: verifying the node doesn't have the label node 05/09/23 16:39:15.329
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 16:39:15.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5476" for this suite. 05/09/23 16:39:15.342
------------------------------
â€¢ [3.357 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:39:11.994
    May  9 16:39:11.995: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename sched-pred 05/09/23 16:39:11.995
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:39:12.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:39:12.029
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    May  9 16:39:12.034: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  9 16:39:12.045: INFO: Waiting for terminating namespaces to be deleted...
    May  9 16:39:12.050: INFO: 
    Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 before test
    May  9 16:39:12.062: INFO: adopt-release-4lql6 from job-9620 started at 2023-05-09 16:38:55 +0000 UTC (1 container statuses recorded)
    May  9 16:39:12.062: INFO: 	Container c ready: true, restart count 0
    May  9 16:39:12.062: INFO: adopt-release-b965n from job-9620 started at 2023-05-09 16:38:49 +0000 UTC (1 container statuses recorded)
    May  9 16:39:12.062: INFO: 	Container c ready: true, restart count 0
    May  9 16:39:12.062: INFO: canal-zcmck from kube-system started at 2023-05-09 08:48:05 +0000 UTC (2 container statuses recorded)
    May  9 16:39:12.062: INFO: 	Container calico-node ready: true, restart count 2
    May  9 16:39:12.062: INFO: 	Container kube-flannel ready: true, restart count 0
    May  9 16:39:12.062: INFO: coredns-996c5dbc5-kcx4b from kube-system started at 2023-05-09 15:55:37 +0000 UTC (1 container statuses recorded)
    May  9 16:39:12.062: INFO: 	Container coredns ready: true, restart count 0
    May  9 16:39:12.062: INFO: kube-proxy-q8nhz from kube-system started at 2023-05-09 08:48:05 +0000 UTC (1 container statuses recorded)
    May  9 16:39:12.062: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 16:39:12.062: INFO: wormhole-xj4hj from kube-system started at 2023-05-09 08:48:05 +0000 UTC (1 container statuses recorded)
    May  9 16:39:12.062: INFO: 	Container wormhole ready: true, restart count 0
    May  9 16:39:12.062: INFO: e2e-rc-b676v-6bncd from replication-controller-9362 started at 2023-05-09 16:39:06 +0000 UTC (1 container statuses recorded)
    May  9 16:39:12.062: INFO: 	Container httpd ready: true, restart count 0
    May  9 16:39:12.062: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-69c6h from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
    May  9 16:39:12.062: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 16:39:12.062: INFO: 	Container systemd-logs ready: true, restart count 0
    May  9 16:39:12.062: INFO: 
    Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 before test
    May  9 16:39:12.073: INFO: rs-f6twm from disruption-3357 started at 2023-05-09 16:38:00 +0000 UTC (1 container statuses recorded)
    May  9 16:39:12.073: INFO: 	Container donothing ready: false, restart count 0
    May  9 16:39:12.073: INFO: adopt-release-xs8sf from job-9620 started at 2023-05-09 16:38:49 +0000 UTC (1 container statuses recorded)
    May  9 16:39:12.073: INFO: 	Container c ready: true, restart count 0
    May  9 16:39:12.073: INFO: calico-kube-controllers-6cffbf7894-zrprz from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
    May  9 16:39:12.073: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  9 16:39:12.073: INFO: canal-22bj2 from kube-system started at 2023-05-09 08:47:59 +0000 UTC (2 container statuses recorded)
    May  9 16:39:12.073: INFO: 	Container calico-node ready: true, restart count 2
    May  9 16:39:12.073: INFO: 	Container kube-flannel ready: true, restart count 0
    May  9 16:39:12.073: INFO: coredns-996c5dbc5-24wst from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
    May  9 16:39:12.073: INFO: 	Container coredns ready: true, restart count 0
    May  9 16:39:12.073: INFO: kube-dns-autoscaler-789d47d664-hhx85 from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
    May  9 16:39:12.073: INFO: 	Container autoscaler ready: true, restart count 0
    May  9 16:39:12.073: INFO: kube-proxy-r9g6q from kube-system started at 2023-05-09 08:47:59 +0000 UTC (1 container statuses recorded)
    May  9 16:39:12.073: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 16:39:12.073: INFO: metrics-server-5f9c95d78-hk5zz from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
    May  9 16:39:12.073: INFO: 	Container metrics-server ready: true, restart count 0
    May  9 16:39:12.073: INFO: wormhole-hnkjk from kube-system started at 2023-05-09 08:47:59 +0000 UTC (1 container statuses recorded)
    May  9 16:39:12.073: INFO: 	Container wormhole ready: true, restart count 0
    May  9 16:39:12.073: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-2x6mg from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
    May  9 16:39:12.073: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 16:39:12.073: INFO: 	Container systemd-logs ready: true, restart count 0
    May  9 16:39:12.073: INFO: 
    Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 before test
    May  9 16:39:12.084: INFO: canal-jl6h7 from kube-system started at 2023-05-09 08:48:03 +0000 UTC (2 container statuses recorded)
    May  9 16:39:12.084: INFO: 	Container calico-node ready: true, restart count 2
    May  9 16:39:12.084: INFO: 	Container kube-flannel ready: true, restart count 0
    May  9 16:39:12.084: INFO: kube-proxy-qqc8l from kube-system started at 2023-05-09 08:48:03 +0000 UTC (1 container statuses recorded)
    May  9 16:39:12.084: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 16:39:12.084: INFO: wormhole-825jz from kube-system started at 2023-05-09 08:48:03 +0000 UTC (1 container statuses recorded)
    May  9 16:39:12.084: INFO: 	Container wormhole ready: true, restart count 0
    May  9 16:39:12.084: INFO: e2e-rc-b676v-qvdnh from replication-controller-9362 started at 2023-05-09 16:39:05 +0000 UTC (1 container statuses recorded)
    May  9 16:39:12.084: INFO: 	Container httpd ready: true, restart count 0
    May  9 16:39:12.084: INFO: sonobuoy from sonobuoy started at 2023-05-09 15:24:58 +0000 UTC (1 container statuses recorded)
    May  9 16:39:12.084: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  9 16:39:12.084: INFO: sonobuoy-e2e-job-d96e230b174b4cad from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
    May  9 16:39:12.084: INFO: 	Container e2e ready: true, restart count 0
    May  9 16:39:12.084: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 16:39:12.084: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-zvq2k from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
    May  9 16:39:12.084: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 16:39:12.084: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 05/09/23 16:39:12.116
    STEP: verifying the node has the label node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 05/09/23 16:39:12.136
    STEP: verifying the node has the label node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 05/09/23 16:39:12.152
    May  9 16:39:12.179: INFO: Pod rs-f6twm requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
    May  9 16:39:12.179: INFO: Pod adopt-release-4lql6 requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
    May  9 16:39:12.179: INFO: Pod adopt-release-b965n requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
    May  9 16:39:12.179: INFO: Pod adopt-release-xs8sf requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
    May  9 16:39:12.179: INFO: Pod calico-kube-controllers-6cffbf7894-zrprz requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
    May  9 16:39:12.179: INFO: Pod canal-22bj2 requesting resource cpu=250m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
    May  9 16:39:12.179: INFO: Pod canal-jl6h7 requesting resource cpu=250m on Node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62
    May  9 16:39:12.179: INFO: Pod canal-zcmck requesting resource cpu=250m on Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
    May  9 16:39:12.179: INFO: Pod coredns-996c5dbc5-24wst requesting resource cpu=100m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
    May  9 16:39:12.179: INFO: Pod coredns-996c5dbc5-kcx4b requesting resource cpu=100m on Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
    May  9 16:39:12.179: INFO: Pod kube-dns-autoscaler-789d47d664-hhx85 requesting resource cpu=20m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
    May  9 16:39:12.179: INFO: Pod kube-proxy-q8nhz requesting resource cpu=100m on Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
    May  9 16:39:12.179: INFO: Pod kube-proxy-qqc8l requesting resource cpu=100m on Node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62
    May  9 16:39:12.179: INFO: Pod kube-proxy-r9g6q requesting resource cpu=100m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
    May  9 16:39:12.179: INFO: Pod metrics-server-5f9c95d78-hk5zz requesting resource cpu=100m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
    May  9 16:39:12.179: INFO: Pod wormhole-825jz requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62
    May  9 16:39:12.179: INFO: Pod wormhole-hnkjk requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
    May  9 16:39:12.179: INFO: Pod wormhole-xj4hj requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
    May  9 16:39:12.179: INFO: Pod e2e-rc-b676v-6bncd requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
    May  9 16:39:12.179: INFO: Pod e2e-rc-b676v-qvdnh requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62
    May  9 16:39:12.179: INFO: Pod sonobuoy requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62
    May  9 16:39:12.179: INFO: Pod sonobuoy-e2e-job-d96e230b174b4cad requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62
    May  9 16:39:12.179: INFO: Pod sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-2x6mg requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
    May  9 16:39:12.179: INFO: Pod sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-69c6h requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
    May  9 16:39:12.179: INFO: Pod sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-zvq2k requesting resource cpu=0m on Node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62
    STEP: Starting Pods to consume most of the cluster CPU. 05/09/23 16:39:12.179
    May  9 16:39:12.179: INFO: Creating a pod which consumes cpu=2366m on Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816
    May  9 16:39:12.190: INFO: Creating a pod which consumes cpu=2282m on Node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7
    May  9 16:39:12.201: INFO: Creating a pod which consumes cpu=2436m on Node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62
    May  9 16:39:12.206: INFO: Waiting up to 5m0s for pod "filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b" in namespace "sched-pred-5476" to be "running"
    May  9 16:39:12.212: INFO: Pod "filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.23351ms
    May  9 16:39:14.218: INFO: Pod "filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b": Phase="Running", Reason="", readiness=true. Elapsed: 2.011739877s
    May  9 16:39:14.218: INFO: Pod "filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b" satisfied condition "running"
    May  9 16:39:14.218: INFO: Waiting up to 5m0s for pod "filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4" in namespace "sched-pred-5476" to be "running"
    May  9 16:39:14.223: INFO: Pod "filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4": Phase="Running", Reason="", readiness=true. Elapsed: 4.785448ms
    May  9 16:39:14.223: INFO: Pod "filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4" satisfied condition "running"
    May  9 16:39:14.223: INFO: Waiting up to 5m0s for pod "filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b" in namespace "sched-pred-5476" to be "running"
    May  9 16:39:14.228: INFO: Pod "filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b": Phase="Running", Reason="", readiness=true. Elapsed: 5.264604ms
    May  9 16:39:14.228: INFO: Pod "filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 05/09/23 16:39:14.228
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b.175d871331c196f8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5476/filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b to nodepool-8cc7f47e-9b0c-4801-88-node-f76f62] 05/09/23 16:39:14.235
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b.175d8713534656df], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 05/09/23 16:39:14.235
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b.175d87136ce972bb], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 430.093789ms (430.105181ms including waiting)] 05/09/23 16:39:14.235
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b.175d87136df0e521], Reason = [Created], Message = [Created container filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b] 05/09/23 16:39:14.235
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b.175d8713736315db], Reason = [Started], Message = [Started container filler-pod-897a70b8-e163-4d8c-a1dd-f299802f9f4b] 05/09/23 16:39:14.235
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b.175d871330993af7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5476/filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b to nodepool-8cc7f47e-9b0c-4801-88-node-7ad816] 05/09/23 16:39:14.235
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b.175d871353c0b9e0], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 05/09/23 16:39:14.235
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b.175d871366f6bff0], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 322.273459ms (322.285063ms including waiting)] 05/09/23 16:39:14.235
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b.175d8713679d9175], Reason = [Created], Message = [Created container filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b] 05/09/23 16:39:14.235
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b.175d87136cfc73a6], Reason = [Started], Message = [Started container filler-pod-d137ab7e-3d7f-46c0-b089-ae2f9f9b180b] 05/09/23 16:39:14.235
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4.175d8713310fe6ed], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5476/filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4 to nodepool-8cc7f47e-9b0c-4801-88-node-bbade7] 05/09/23 16:39:14.235
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4.175d871353a45adb], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 05/09/23 16:39:14.235
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4.175d8713670fdb0c], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 325.77629ms (325.791388ms including waiting)] 05/09/23 16:39:14.236
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4.175d871367a242db], Reason = [Created], Message = [Created container filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4] 05/09/23 16:39:14.236
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4.175d87136da33f73], Reason = [Started], Message = [Started container filler-pod-db98d216-1e31-4372-a484-25aca49d1ea4] 05/09/23 16:39:14.236
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.175d8713aaedaa33], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 05/09/23 16:39:14.254
    STEP: removing the label node off the node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 05/09/23 16:39:15.256
    STEP: verifying the node doesn't have the label node 05/09/23 16:39:15.28
    STEP: removing the label node off the node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 05/09/23 16:39:15.286
    STEP: verifying the node doesn't have the label node 05/09/23 16:39:15.304
    STEP: removing the label node off the node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 05/09/23 16:39:15.311
    STEP: verifying the node doesn't have the label node 05/09/23 16:39:15.329
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 16:39:15.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5476" for this suite. 05/09/23 16:39:15.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:39:15.352
May  9 16:39:15.352: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename gc 05/09/23 16:39:15.353
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:39:15.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:39:15.381
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 05/09/23 16:39:15.392
STEP: delete the rc 05/09/23 16:39:20.476
STEP: wait for the rc to be deleted 05/09/23 16:39:20.56
May  9 16:39:22.064: INFO: 85 pods remaining
May  9 16:39:22.064: INFO: 80 pods has nil DeletionTimestamp
May  9 16:39:22.064: INFO: 
May  9 16:39:22.905: INFO: 64 pods remaining
May  9 16:39:22.905: INFO: 64 pods has nil DeletionTimestamp
May  9 16:39:22.905: INFO: 
May  9 16:39:23.690: INFO: 59 pods remaining
May  9 16:39:23.690: INFO: 59 pods has nil DeletionTimestamp
May  9 16:39:23.691: INFO: 
May  9 16:39:24.680: INFO: 40 pods remaining
May  9 16:39:24.680: INFO: 40 pods has nil DeletionTimestamp
May  9 16:39:24.680: INFO: 
May  9 16:39:25.579: INFO: 30 pods remaining
May  9 16:39:25.579: INFO: 30 pods has nil DeletionTimestamp
May  9 16:39:25.579: INFO: 
May  9 16:39:26.772: INFO: 20 pods remaining
May  9 16:39:26.772: INFO: 20 pods has nil DeletionTimestamp
May  9 16:39:26.772: INFO: 
STEP: Gathering metrics 05/09/23 16:39:27.577
W0509 16:39:27.601019      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  9 16:39:27.601: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May  9 16:39:27.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5712" for this suite. 05/09/23 16:39:27.606
------------------------------
â€¢ [SLOW TEST] [12.323 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:39:15.352
    May  9 16:39:15.352: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename gc 05/09/23 16:39:15.353
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:39:15.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:39:15.381
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 05/09/23 16:39:15.392
    STEP: delete the rc 05/09/23 16:39:20.476
    STEP: wait for the rc to be deleted 05/09/23 16:39:20.56
    May  9 16:39:22.064: INFO: 85 pods remaining
    May  9 16:39:22.064: INFO: 80 pods has nil DeletionTimestamp
    May  9 16:39:22.064: INFO: 
    May  9 16:39:22.905: INFO: 64 pods remaining
    May  9 16:39:22.905: INFO: 64 pods has nil DeletionTimestamp
    May  9 16:39:22.905: INFO: 
    May  9 16:39:23.690: INFO: 59 pods remaining
    May  9 16:39:23.690: INFO: 59 pods has nil DeletionTimestamp
    May  9 16:39:23.691: INFO: 
    May  9 16:39:24.680: INFO: 40 pods remaining
    May  9 16:39:24.680: INFO: 40 pods has nil DeletionTimestamp
    May  9 16:39:24.680: INFO: 
    May  9 16:39:25.579: INFO: 30 pods remaining
    May  9 16:39:25.579: INFO: 30 pods has nil DeletionTimestamp
    May  9 16:39:25.579: INFO: 
    May  9 16:39:26.772: INFO: 20 pods remaining
    May  9 16:39:26.772: INFO: 20 pods has nil DeletionTimestamp
    May  9 16:39:26.772: INFO: 
    STEP: Gathering metrics 05/09/23 16:39:27.577
    W0509 16:39:27.601019      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May  9 16:39:27.601: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May  9 16:39:27.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5712" for this suite. 05/09/23 16:39:27.606
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:39:27.675
May  9 16:39:27.676: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename webhook 05/09/23 16:39:27.676
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:39:27.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:39:27.698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/09/23 16:39:27.769
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:39:28.39
STEP: Deploying the webhook pod 05/09/23 16:39:28.413
STEP: Wait for the deployment to be ready 05/09/23 16:39:28.424
May  9 16:39:28.435: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  9 16:39:30.457: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 39, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 39, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 39, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 39, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  9 16:39:32.464: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 39, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 39, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 39, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 39, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/09/23 16:39:34.463
STEP: Verifying the service has paired with the endpoint 05/09/23 16:39:34.477
May  9 16:39:35.477: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
May  9 16:39:35.482: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Registering the custom resource webhook via the AdmissionRegistration API 05/09/23 16:39:35.993
STEP: Creating a custom resource that should be denied by the webhook 05/09/23 16:39:36.016
STEP: Creating a custom resource whose deletion would be denied by the webhook 05/09/23 16:39:38.509
STEP: Updating the custom resource with disallowed data should be denied 05/09/23 16:39:38.518
STEP: Deleting the custom resource should be denied 05/09/23 16:39:38.538
STEP: Remove the offending key and value from the custom resource data 05/09/23 16:39:38.55
STEP: Deleting the updated custom resource should be successful 05/09/23 16:39:38.566
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:39:39.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4554" for this suite. 05/09/23 16:39:39.166
STEP: Destroying namespace "webhook-4554-markers" for this suite. 05/09/23 16:39:39.176
------------------------------
â€¢ [SLOW TEST] [11.510 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:39:27.675
    May  9 16:39:27.676: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename webhook 05/09/23 16:39:27.676
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:39:27.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:39:27.698
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/09/23 16:39:27.769
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:39:28.39
    STEP: Deploying the webhook pod 05/09/23 16:39:28.413
    STEP: Wait for the deployment to be ready 05/09/23 16:39:28.424
    May  9 16:39:28.435: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May  9 16:39:30.457: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 39, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 39, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 39, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 39, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  9 16:39:32.464: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 16, 39, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 39, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 16, 39, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 16, 39, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/09/23 16:39:34.463
    STEP: Verifying the service has paired with the endpoint 05/09/23 16:39:34.477
    May  9 16:39:35.477: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    May  9 16:39:35.482: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 05/09/23 16:39:35.993
    STEP: Creating a custom resource that should be denied by the webhook 05/09/23 16:39:36.016
    STEP: Creating a custom resource whose deletion would be denied by the webhook 05/09/23 16:39:38.509
    STEP: Updating the custom resource with disallowed data should be denied 05/09/23 16:39:38.518
    STEP: Deleting the custom resource should be denied 05/09/23 16:39:38.538
    STEP: Remove the offending key and value from the custom resource data 05/09/23 16:39:38.55
    STEP: Deleting the updated custom resource should be successful 05/09/23 16:39:38.566
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:39:39.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4554" for this suite. 05/09/23 16:39:39.166
    STEP: Destroying namespace "webhook-4554-markers" for this suite. 05/09/23 16:39:39.176
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:39:39.187
May  9 16:39:39.187: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename container-probe 05/09/23 16:39:39.188
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:39:39.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:39:39.211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
May  9 16:39:39.231: INFO: Waiting up to 5m0s for pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994" in namespace "container-probe-31" to be "running and ready"
May  9 16:39:39.237: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Pending", Reason="", readiness=false. Elapsed: 5.886107ms
May  9 16:39:39.237: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:39:41.244: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 2.013361807s
May  9 16:39:41.245: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
May  9 16:39:43.242: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 4.010980311s
May  9 16:39:43.242: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
May  9 16:39:45.244: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 6.012493945s
May  9 16:39:45.244: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
May  9 16:39:47.243: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 8.011958528s
May  9 16:39:47.243: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
May  9 16:39:49.242: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 10.011017823s
May  9 16:39:49.242: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
May  9 16:39:51.254: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 12.022573433s
May  9 16:39:51.254: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
May  9 16:39:53.244: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 14.012838354s
May  9 16:39:53.244: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
May  9 16:39:55.243: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 16.011618617s
May  9 16:39:55.243: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
May  9 16:39:57.244: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 18.013135159s
May  9 16:39:57.244: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
May  9 16:39:59.245: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 20.013411518s
May  9 16:39:59.245: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
May  9 16:40:01.244: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=true. Elapsed: 22.01262289s
May  9 16:40:01.244: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = true)
May  9 16:40:01.244: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994" satisfied condition "running and ready"
May  9 16:40:01.249: INFO: Container started at 2023-05-09 16:39:40 +0000 UTC, pod became ready at 2023-05-09 16:39:59 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May  9 16:40:01.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-31" for this suite. 05/09/23 16:40:01.256
------------------------------
â€¢ [SLOW TEST] [22.079 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:39:39.187
    May  9 16:39:39.187: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename container-probe 05/09/23 16:39:39.188
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:39:39.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:39:39.211
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    May  9 16:39:39.231: INFO: Waiting up to 5m0s for pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994" in namespace "container-probe-31" to be "running and ready"
    May  9 16:39:39.237: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Pending", Reason="", readiness=false. Elapsed: 5.886107ms
    May  9 16:39:39.237: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:39:41.244: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 2.013361807s
    May  9 16:39:41.245: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
    May  9 16:39:43.242: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 4.010980311s
    May  9 16:39:43.242: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
    May  9 16:39:45.244: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 6.012493945s
    May  9 16:39:45.244: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
    May  9 16:39:47.243: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 8.011958528s
    May  9 16:39:47.243: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
    May  9 16:39:49.242: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 10.011017823s
    May  9 16:39:49.242: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
    May  9 16:39:51.254: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 12.022573433s
    May  9 16:39:51.254: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
    May  9 16:39:53.244: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 14.012838354s
    May  9 16:39:53.244: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
    May  9 16:39:55.243: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 16.011618617s
    May  9 16:39:55.243: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
    May  9 16:39:57.244: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 18.013135159s
    May  9 16:39:57.244: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
    May  9 16:39:59.245: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=false. Elapsed: 20.013411518s
    May  9 16:39:59.245: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = false)
    May  9 16:40:01.244: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994": Phase="Running", Reason="", readiness=true. Elapsed: 22.01262289s
    May  9 16:40:01.244: INFO: The phase of Pod test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994 is Running (Ready = true)
    May  9 16:40:01.244: INFO: Pod "test-webserver-0aa30e00-4ead-46c7-bf2b-2b5dad753994" satisfied condition "running and ready"
    May  9 16:40:01.249: INFO: Container started at 2023-05-09 16:39:40 +0000 UTC, pod became ready at 2023-05-09 16:39:59 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May  9 16:40:01.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-31" for this suite. 05/09/23 16:40:01.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:40:01.267
May  9 16:40:01.267: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename cronjob 05/09/23 16:40:01.268
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:40:01.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:40:01.288
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 05/09/23 16:40:01.291
STEP: Ensuring a job is scheduled 05/09/23 16:40:01.299
STEP: Ensuring exactly one is scheduled 05/09/23 16:41:01.307
STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/09/23 16:41:01.312
STEP: Ensuring the job is replaced with a new one 05/09/23 16:41:01.317
STEP: Removing cronjob 05/09/23 16:42:01.324
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May  9 16:42:01.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9004" for this suite. 05/09/23 16:42:01.339
------------------------------
â€¢ [SLOW TEST] [120.079 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:40:01.267
    May  9 16:40:01.267: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename cronjob 05/09/23 16:40:01.268
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:40:01.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:40:01.288
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 05/09/23 16:40:01.291
    STEP: Ensuring a job is scheduled 05/09/23 16:40:01.299
    STEP: Ensuring exactly one is scheduled 05/09/23 16:41:01.307
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/09/23 16:41:01.312
    STEP: Ensuring the job is replaced with a new one 05/09/23 16:41:01.317
    STEP: Removing cronjob 05/09/23 16:42:01.324
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May  9 16:42:01.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9004" for this suite. 05/09/23 16:42:01.339
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:42:01.346
May  9 16:42:01.346: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubectl 05/09/23 16:42:01.347
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:01.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:01.367
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 05/09/23 16:42:01.372
May  9 16:42:01.372: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-269 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 05/09/23 16:42:01.435
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  9 16:42:01.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-269" for this suite. 05/09/23 16:42:01.466
------------------------------
â€¢ [0.133 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:42:01.346
    May  9 16:42:01.346: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubectl 05/09/23 16:42:01.347
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:01.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:01.367
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 05/09/23 16:42:01.372
    May  9 16:42:01.372: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-269 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 05/09/23 16:42:01.435
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  9 16:42:01.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-269" for this suite. 05/09/23 16:42:01.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:42:01.482
May  9 16:42:01.482: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename configmap 05/09/23 16:42:01.483
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:01.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:01.517
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-08bdd183-0948-4016-b7c0-df5929da58d2 05/09/23 16:42:01.523
STEP: Creating a pod to test consume configMaps 05/09/23 16:42:01.535
May  9 16:42:01.554: INFO: Waiting up to 5m0s for pod "pod-configmaps-7f501612-774c-421a-af52-c33f8b27dcb2" in namespace "configmap-6708" to be "Succeeded or Failed"
May  9 16:42:01.563: INFO: Pod "pod-configmaps-7f501612-774c-421a-af52-c33f8b27dcb2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.918806ms
May  9 16:42:03.570: INFO: Pod "pod-configmaps-7f501612-774c-421a-af52-c33f8b27dcb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015948883s
May  9 16:42:05.573: INFO: Pod "pod-configmaps-7f501612-774c-421a-af52-c33f8b27dcb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018876635s
STEP: Saw pod success 05/09/23 16:42:05.573
May  9 16:42:05.573: INFO: Pod "pod-configmaps-7f501612-774c-421a-af52-c33f8b27dcb2" satisfied condition "Succeeded or Failed"
May  9 16:42:05.578: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-configmaps-7f501612-774c-421a-af52-c33f8b27dcb2 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 16:42:05.635
May  9 16:42:05.652: INFO: Waiting for pod pod-configmaps-7f501612-774c-421a-af52-c33f8b27dcb2 to disappear
May  9 16:42:05.656: INFO: Pod pod-configmaps-7f501612-774c-421a-af52-c33f8b27dcb2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  9 16:42:05.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6708" for this suite. 05/09/23 16:42:05.663
------------------------------
â€¢ [4.191 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:42:01.482
    May  9 16:42:01.482: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename configmap 05/09/23 16:42:01.483
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:01.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:01.517
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-08bdd183-0948-4016-b7c0-df5929da58d2 05/09/23 16:42:01.523
    STEP: Creating a pod to test consume configMaps 05/09/23 16:42:01.535
    May  9 16:42:01.554: INFO: Waiting up to 5m0s for pod "pod-configmaps-7f501612-774c-421a-af52-c33f8b27dcb2" in namespace "configmap-6708" to be "Succeeded or Failed"
    May  9 16:42:01.563: INFO: Pod "pod-configmaps-7f501612-774c-421a-af52-c33f8b27dcb2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.918806ms
    May  9 16:42:03.570: INFO: Pod "pod-configmaps-7f501612-774c-421a-af52-c33f8b27dcb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015948883s
    May  9 16:42:05.573: INFO: Pod "pod-configmaps-7f501612-774c-421a-af52-c33f8b27dcb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018876635s
    STEP: Saw pod success 05/09/23 16:42:05.573
    May  9 16:42:05.573: INFO: Pod "pod-configmaps-7f501612-774c-421a-af52-c33f8b27dcb2" satisfied condition "Succeeded or Failed"
    May  9 16:42:05.578: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-configmaps-7f501612-774c-421a-af52-c33f8b27dcb2 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 16:42:05.635
    May  9 16:42:05.652: INFO: Waiting for pod pod-configmaps-7f501612-774c-421a-af52-c33f8b27dcb2 to disappear
    May  9 16:42:05.656: INFO: Pod pod-configmaps-7f501612-774c-421a-af52-c33f8b27dcb2 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  9 16:42:05.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6708" for this suite. 05/09/23 16:42:05.663
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:42:05.673
May  9 16:42:05.674: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename runtimeclass 05/09/23 16:42:05.675
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:05.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:05.694
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
May  9 16:42:05.715: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7427 to be scheduled
May  9 16:42:05.720: INFO: 1 pods are not scheduled: [runtimeclass-7427/test-runtimeclass-runtimeclass-7427-preconfigured-handler-m4khl(7e266ca8-04e0-4f08-b243-3b147e351680)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May  9 16:42:07.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7427" for this suite. 05/09/23 16:42:07.742
------------------------------
â€¢ [2.076 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:42:05.673
    May  9 16:42:05.674: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename runtimeclass 05/09/23 16:42:05.675
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:05.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:05.694
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    May  9 16:42:05.715: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7427 to be scheduled
    May  9 16:42:05.720: INFO: 1 pods are not scheduled: [runtimeclass-7427/test-runtimeclass-runtimeclass-7427-preconfigured-handler-m4khl(7e266ca8-04e0-4f08-b243-3b147e351680)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May  9 16:42:07.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7427" for this suite. 05/09/23 16:42:07.742
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:42:07.751
May  9 16:42:07.751: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename downward-api 05/09/23 16:42:07.752
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:07.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:07.77
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 05/09/23 16:42:07.774
May  9 16:42:07.785: INFO: Waiting up to 5m0s for pod "downward-api-6c2c28ad-6c8b-4331-8edd-493a0801cec0" in namespace "downward-api-5917" to be "Succeeded or Failed"
May  9 16:42:07.792: INFO: Pod "downward-api-6c2c28ad-6c8b-4331-8edd-493a0801cec0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.256333ms
May  9 16:42:09.798: INFO: Pod "downward-api-6c2c28ad-6c8b-4331-8edd-493a0801cec0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013389878s
May  9 16:42:11.799: INFO: Pod "downward-api-6c2c28ad-6c8b-4331-8edd-493a0801cec0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014372769s
STEP: Saw pod success 05/09/23 16:42:11.799
May  9 16:42:11.799: INFO: Pod "downward-api-6c2c28ad-6c8b-4331-8edd-493a0801cec0" satisfied condition "Succeeded or Failed"
May  9 16:42:11.807: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod downward-api-6c2c28ad-6c8b-4331-8edd-493a0801cec0 container dapi-container: <nil>
STEP: delete the pod 05/09/23 16:42:11.881
May  9 16:42:11.915: INFO: Waiting for pod downward-api-6c2c28ad-6c8b-4331-8edd-493a0801cec0 to disappear
May  9 16:42:11.920: INFO: Pod downward-api-6c2c28ad-6c8b-4331-8edd-493a0801cec0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May  9 16:42:11.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5917" for this suite. 05/09/23 16:42:11.926
------------------------------
â€¢ [4.184 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:42:07.751
    May  9 16:42:07.751: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename downward-api 05/09/23 16:42:07.752
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:07.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:07.77
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 05/09/23 16:42:07.774
    May  9 16:42:07.785: INFO: Waiting up to 5m0s for pod "downward-api-6c2c28ad-6c8b-4331-8edd-493a0801cec0" in namespace "downward-api-5917" to be "Succeeded or Failed"
    May  9 16:42:07.792: INFO: Pod "downward-api-6c2c28ad-6c8b-4331-8edd-493a0801cec0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.256333ms
    May  9 16:42:09.798: INFO: Pod "downward-api-6c2c28ad-6c8b-4331-8edd-493a0801cec0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013389878s
    May  9 16:42:11.799: INFO: Pod "downward-api-6c2c28ad-6c8b-4331-8edd-493a0801cec0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014372769s
    STEP: Saw pod success 05/09/23 16:42:11.799
    May  9 16:42:11.799: INFO: Pod "downward-api-6c2c28ad-6c8b-4331-8edd-493a0801cec0" satisfied condition "Succeeded or Failed"
    May  9 16:42:11.807: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod downward-api-6c2c28ad-6c8b-4331-8edd-493a0801cec0 container dapi-container: <nil>
    STEP: delete the pod 05/09/23 16:42:11.881
    May  9 16:42:11.915: INFO: Waiting for pod downward-api-6c2c28ad-6c8b-4331-8edd-493a0801cec0 to disappear
    May  9 16:42:11.920: INFO: Pod downward-api-6c2c28ad-6c8b-4331-8edd-493a0801cec0 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May  9 16:42:11.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5917" for this suite. 05/09/23 16:42:11.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:42:11.94
May  9 16:42:11.940: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename emptydir 05/09/23 16:42:11.941
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:11.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:11.966
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 05/09/23 16:42:11.971
May  9 16:42:11.990: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-78f596c9-b5f6-4eeb-9acf-e1b00823eb46" in namespace "emptydir-5897" to be "running"
May  9 16:42:11.998: INFO: Pod "pod-sharedvolume-78f596c9-b5f6-4eeb-9acf-e1b00823eb46": Phase="Pending", Reason="", readiness=false. Elapsed: 7.941455ms
May  9 16:42:14.006: INFO: Pod "pod-sharedvolume-78f596c9-b5f6-4eeb-9acf-e1b00823eb46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016500105s
May  9 16:42:16.007: INFO: Pod "pod-sharedvolume-78f596c9-b5f6-4eeb-9acf-e1b00823eb46": Phase="Running", Reason="", readiness=false. Elapsed: 4.016995257s
May  9 16:42:16.007: INFO: Pod "pod-sharedvolume-78f596c9-b5f6-4eeb-9acf-e1b00823eb46" satisfied condition "running"
STEP: Reading file content from the nginx-container 05/09/23 16:42:16.007
May  9 16:42:16.007: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5897 PodName:pod-sharedvolume-78f596c9-b5f6-4eeb-9acf-e1b00823eb46 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:42:16.007: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:42:16.008: INFO: ExecWithOptions: Clientset creation
May  9 16:42:16.008: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/emptydir-5897/pods/pod-sharedvolume-78f596c9-b5f6-4eeb-9acf-e1b00823eb46/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
May  9 16:42:16.131: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  9 16:42:16.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5897" for this suite. 05/09/23 16:42:16.137
------------------------------
â€¢ [4.209 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:42:11.94
    May  9 16:42:11.940: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename emptydir 05/09/23 16:42:11.941
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:11.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:11.966
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 05/09/23 16:42:11.971
    May  9 16:42:11.990: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-78f596c9-b5f6-4eeb-9acf-e1b00823eb46" in namespace "emptydir-5897" to be "running"
    May  9 16:42:11.998: INFO: Pod "pod-sharedvolume-78f596c9-b5f6-4eeb-9acf-e1b00823eb46": Phase="Pending", Reason="", readiness=false. Elapsed: 7.941455ms
    May  9 16:42:14.006: INFO: Pod "pod-sharedvolume-78f596c9-b5f6-4eeb-9acf-e1b00823eb46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016500105s
    May  9 16:42:16.007: INFO: Pod "pod-sharedvolume-78f596c9-b5f6-4eeb-9acf-e1b00823eb46": Phase="Running", Reason="", readiness=false. Elapsed: 4.016995257s
    May  9 16:42:16.007: INFO: Pod "pod-sharedvolume-78f596c9-b5f6-4eeb-9acf-e1b00823eb46" satisfied condition "running"
    STEP: Reading file content from the nginx-container 05/09/23 16:42:16.007
    May  9 16:42:16.007: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5897 PodName:pod-sharedvolume-78f596c9-b5f6-4eeb-9acf-e1b00823eb46 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:42:16.007: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:42:16.008: INFO: ExecWithOptions: Clientset creation
    May  9 16:42:16.008: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/emptydir-5897/pods/pod-sharedvolume-78f596c9-b5f6-4eeb-9acf-e1b00823eb46/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    May  9 16:42:16.131: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  9 16:42:16.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5897" for this suite. 05/09/23 16:42:16.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:42:16.152
May  9 16:42:16.152: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename cronjob 05/09/23 16:42:16.153
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:16.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:16.176
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 05/09/23 16:42:16.181
STEP: creating 05/09/23 16:42:16.181
STEP: getting 05/09/23 16:42:16.187
STEP: listing 05/09/23 16:42:16.192
STEP: watching 05/09/23 16:42:16.197
May  9 16:42:16.197: INFO: starting watch
STEP: cluster-wide listing 05/09/23 16:42:16.199
STEP: cluster-wide watching 05/09/23 16:42:16.204
May  9 16:42:16.204: INFO: starting watch
STEP: patching 05/09/23 16:42:16.207
STEP: updating 05/09/23 16:42:16.217
May  9 16:42:16.231: INFO: waiting for watch events with expected annotations
May  9 16:42:16.231: INFO: saw patched and updated annotations
STEP: patching /status 05/09/23 16:42:16.231
STEP: updating /status 05/09/23 16:42:16.239
STEP: get /status 05/09/23 16:42:16.255
STEP: deleting 05/09/23 16:42:16.262
STEP: deleting a collection 05/09/23 16:42:16.279
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May  9 16:42:16.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7854" for this suite. 05/09/23 16:42:16.302
------------------------------
â€¢ [0.159 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:42:16.152
    May  9 16:42:16.152: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename cronjob 05/09/23 16:42:16.153
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:16.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:16.176
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 05/09/23 16:42:16.181
    STEP: creating 05/09/23 16:42:16.181
    STEP: getting 05/09/23 16:42:16.187
    STEP: listing 05/09/23 16:42:16.192
    STEP: watching 05/09/23 16:42:16.197
    May  9 16:42:16.197: INFO: starting watch
    STEP: cluster-wide listing 05/09/23 16:42:16.199
    STEP: cluster-wide watching 05/09/23 16:42:16.204
    May  9 16:42:16.204: INFO: starting watch
    STEP: patching 05/09/23 16:42:16.207
    STEP: updating 05/09/23 16:42:16.217
    May  9 16:42:16.231: INFO: waiting for watch events with expected annotations
    May  9 16:42:16.231: INFO: saw patched and updated annotations
    STEP: patching /status 05/09/23 16:42:16.231
    STEP: updating /status 05/09/23 16:42:16.239
    STEP: get /status 05/09/23 16:42:16.255
    STEP: deleting 05/09/23 16:42:16.262
    STEP: deleting a collection 05/09/23 16:42:16.279
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May  9 16:42:16.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7854" for this suite. 05/09/23 16:42:16.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:42:16.312
May  9 16:42:16.312: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 16:42:16.313
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:16.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:16.334
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
May  9 16:42:16.339: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 05/09/23 16:42:19.454
May  9 16:42:19.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 --namespace=crd-publish-openapi-8640 create -f -'
May  9 16:42:20.302: INFO: stderr: ""
May  9 16:42:20.303: INFO: stdout: "e2e-test-crd-publish-openapi-9252-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May  9 16:42:20.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 --namespace=crd-publish-openapi-8640 delete e2e-test-crd-publish-openapi-9252-crds test-foo'
May  9 16:42:20.389: INFO: stderr: ""
May  9 16:42:20.389: INFO: stdout: "e2e-test-crd-publish-openapi-9252-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May  9 16:42:20.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 --namespace=crd-publish-openapi-8640 apply -f -'
May  9 16:42:20.602: INFO: stderr: ""
May  9 16:42:20.602: INFO: stdout: "e2e-test-crd-publish-openapi-9252-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May  9 16:42:20.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 --namespace=crd-publish-openapi-8640 delete e2e-test-crd-publish-openapi-9252-crds test-foo'
May  9 16:42:20.690: INFO: stderr: ""
May  9 16:42:20.690: INFO: stdout: "e2e-test-crd-publish-openapi-9252-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 05/09/23 16:42:20.69
May  9 16:42:20.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 --namespace=crd-publish-openapi-8640 create -f -'
May  9 16:42:20.905: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 05/09/23 16:42:20.905
May  9 16:42:20.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 --namespace=crd-publish-openapi-8640 create -f -'
May  9 16:42:21.107: INFO: rc: 1
May  9 16:42:21.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 --namespace=crd-publish-openapi-8640 apply -f -'
May  9 16:42:21.378: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 05/09/23 16:42:21.378
May  9 16:42:21.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 --namespace=crd-publish-openapi-8640 create -f -'
May  9 16:42:21.581: INFO: rc: 1
May  9 16:42:21.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 --namespace=crd-publish-openapi-8640 apply -f -'
May  9 16:42:21.811: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 05/09/23 16:42:21.811
May  9 16:42:21.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 explain e2e-test-crd-publish-openapi-9252-crds'
May  9 16:42:22.010: INFO: stderr: ""
May  9 16:42:22.010: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9252-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 05/09/23 16:42:22.01
May  9 16:42:22.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 explain e2e-test-crd-publish-openapi-9252-crds.metadata'
May  9 16:42:22.230: INFO: stderr: ""
May  9 16:42:22.230: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9252-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May  9 16:42:22.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 explain e2e-test-crd-publish-openapi-9252-crds.spec'
May  9 16:42:22.418: INFO: stderr: ""
May  9 16:42:22.418: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9252-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May  9 16:42:22.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 explain e2e-test-crd-publish-openapi-9252-crds.spec.bars'
May  9 16:42:22.651: INFO: stderr: ""
May  9 16:42:22.651: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9252-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 05/09/23 16:42:22.651
May  9 16:42:22.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 explain e2e-test-crd-publish-openapi-9252-crds.spec.bars2'
May  9 16:42:22.871: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:42:24.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8640" for this suite. 05/09/23 16:42:24.764
------------------------------
â€¢ [SLOW TEST] [8.460 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:42:16.312
    May  9 16:42:16.312: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 16:42:16.313
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:16.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:16.334
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    May  9 16:42:16.339: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 05/09/23 16:42:19.454
    May  9 16:42:19.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 --namespace=crd-publish-openapi-8640 create -f -'
    May  9 16:42:20.302: INFO: stderr: ""
    May  9 16:42:20.303: INFO: stdout: "e2e-test-crd-publish-openapi-9252-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    May  9 16:42:20.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 --namespace=crd-publish-openapi-8640 delete e2e-test-crd-publish-openapi-9252-crds test-foo'
    May  9 16:42:20.389: INFO: stderr: ""
    May  9 16:42:20.389: INFO: stdout: "e2e-test-crd-publish-openapi-9252-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    May  9 16:42:20.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 --namespace=crd-publish-openapi-8640 apply -f -'
    May  9 16:42:20.602: INFO: stderr: ""
    May  9 16:42:20.602: INFO: stdout: "e2e-test-crd-publish-openapi-9252-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    May  9 16:42:20.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 --namespace=crd-publish-openapi-8640 delete e2e-test-crd-publish-openapi-9252-crds test-foo'
    May  9 16:42:20.690: INFO: stderr: ""
    May  9 16:42:20.690: INFO: stdout: "e2e-test-crd-publish-openapi-9252-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 05/09/23 16:42:20.69
    May  9 16:42:20.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 --namespace=crd-publish-openapi-8640 create -f -'
    May  9 16:42:20.905: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 05/09/23 16:42:20.905
    May  9 16:42:20.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 --namespace=crd-publish-openapi-8640 create -f -'
    May  9 16:42:21.107: INFO: rc: 1
    May  9 16:42:21.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 --namespace=crd-publish-openapi-8640 apply -f -'
    May  9 16:42:21.378: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 05/09/23 16:42:21.378
    May  9 16:42:21.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 --namespace=crd-publish-openapi-8640 create -f -'
    May  9 16:42:21.581: INFO: rc: 1
    May  9 16:42:21.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 --namespace=crd-publish-openapi-8640 apply -f -'
    May  9 16:42:21.811: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 05/09/23 16:42:21.811
    May  9 16:42:21.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 explain e2e-test-crd-publish-openapi-9252-crds'
    May  9 16:42:22.010: INFO: stderr: ""
    May  9 16:42:22.010: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9252-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 05/09/23 16:42:22.01
    May  9 16:42:22.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 explain e2e-test-crd-publish-openapi-9252-crds.metadata'
    May  9 16:42:22.230: INFO: stderr: ""
    May  9 16:42:22.230: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9252-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    May  9 16:42:22.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 explain e2e-test-crd-publish-openapi-9252-crds.spec'
    May  9 16:42:22.418: INFO: stderr: ""
    May  9 16:42:22.418: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9252-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    May  9 16:42:22.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 explain e2e-test-crd-publish-openapi-9252-crds.spec.bars'
    May  9 16:42:22.651: INFO: stderr: ""
    May  9 16:42:22.651: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9252-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 05/09/23 16:42:22.651
    May  9 16:42:22.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-8640 explain e2e-test-crd-publish-openapi-9252-crds.spec.bars2'
    May  9 16:42:22.871: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:42:24.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8640" for this suite. 05/09/23 16:42:24.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:42:24.777
May  9 16:42:24.777: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename watch 05/09/23 16:42:24.778
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:24.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:24.804
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 05/09/23 16:42:24.808
STEP: creating a new configmap 05/09/23 16:42:24.81
STEP: modifying the configmap once 05/09/23 16:42:24.816
STEP: changing the label value of the configmap 05/09/23 16:42:24.827
STEP: Expecting to observe a delete notification for the watched object 05/09/23 16:42:24.838
May  9 16:42:24.838: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1233  dc457c8b-9d80-4d5d-85aa-d1e977be944a 318115661 0 2023-05-09 16:42:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 16:42:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 16:42:24.838: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1233  dc457c8b-9d80-4d5d-85aa-d1e977be944a 318115663 0 2023-05-09 16:42:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 16:42:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 16:42:24.838: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1233  dc457c8b-9d80-4d5d-85aa-d1e977be944a 318115665 0 2023-05-09 16:42:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 16:42:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 05/09/23 16:42:24.838
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 05/09/23 16:42:24.851
STEP: changing the label value of the configmap back 05/09/23 16:42:34.852
STEP: modifying the configmap a third time 05/09/23 16:42:34.881
STEP: deleting the configmap 05/09/23 16:42:34.9
STEP: Expecting to observe an add notification for the watched object when the label value was restored 05/09/23 16:42:34.917
May  9 16:42:34.917: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1233  dc457c8b-9d80-4d5d-85aa-d1e977be944a 318116564 0 2023-05-09 16:42:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 16:42:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 16:42:34.917: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1233  dc457c8b-9d80-4d5d-85aa-d1e977be944a 318116567 0 2023-05-09 16:42:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 16:42:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 16:42:34.917: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1233  dc457c8b-9d80-4d5d-85aa-d1e977be944a 318116571 0 2023-05-09 16:42:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 16:42:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May  9 16:42:34.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1233" for this suite. 05/09/23 16:42:34.924
------------------------------
â€¢ [SLOW TEST] [10.156 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:42:24.777
    May  9 16:42:24.777: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename watch 05/09/23 16:42:24.778
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:24.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:24.804
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 05/09/23 16:42:24.808
    STEP: creating a new configmap 05/09/23 16:42:24.81
    STEP: modifying the configmap once 05/09/23 16:42:24.816
    STEP: changing the label value of the configmap 05/09/23 16:42:24.827
    STEP: Expecting to observe a delete notification for the watched object 05/09/23 16:42:24.838
    May  9 16:42:24.838: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1233  dc457c8b-9d80-4d5d-85aa-d1e977be944a 318115661 0 2023-05-09 16:42:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 16:42:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 16:42:24.838: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1233  dc457c8b-9d80-4d5d-85aa-d1e977be944a 318115663 0 2023-05-09 16:42:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 16:42:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 16:42:24.838: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1233  dc457c8b-9d80-4d5d-85aa-d1e977be944a 318115665 0 2023-05-09 16:42:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 16:42:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 05/09/23 16:42:24.838
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 05/09/23 16:42:24.851
    STEP: changing the label value of the configmap back 05/09/23 16:42:34.852
    STEP: modifying the configmap a third time 05/09/23 16:42:34.881
    STEP: deleting the configmap 05/09/23 16:42:34.9
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 05/09/23 16:42:34.917
    May  9 16:42:34.917: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1233  dc457c8b-9d80-4d5d-85aa-d1e977be944a 318116564 0 2023-05-09 16:42:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 16:42:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 16:42:34.917: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1233  dc457c8b-9d80-4d5d-85aa-d1e977be944a 318116567 0 2023-05-09 16:42:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 16:42:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 16:42:34.917: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1233  dc457c8b-9d80-4d5d-85aa-d1e977be944a 318116571 0 2023-05-09 16:42:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-09 16:42:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May  9 16:42:34.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1233" for this suite. 05/09/23 16:42:34.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:42:34.935
May  9 16:42:34.935: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename services 05/09/23 16:42:34.936
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:34.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:34.963
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-1882 05/09/23 16:42:34.968
STEP: creating service affinity-clusterip-transition in namespace services-1882 05/09/23 16:42:34.968
STEP: creating replication controller affinity-clusterip-transition in namespace services-1882 05/09/23 16:42:34.981
I0509 16:42:34.990324      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1882, replica count: 3
I0509 16:42:38.042286      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 16:42:38.053: INFO: Creating new exec pod
May  9 16:42:38.059: INFO: Waiting up to 5m0s for pod "execpod-affinitywlrs4" in namespace "services-1882" to be "running"
May  9 16:42:38.064: INFO: Pod "execpod-affinitywlrs4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.435452ms
May  9 16:42:40.071: INFO: Pod "execpod-affinitywlrs4": Phase="Running", Reason="", readiness=true. Elapsed: 2.012464882s
May  9 16:42:40.071: INFO: Pod "execpod-affinitywlrs4" satisfied condition "running"
May  9 16:42:41.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-1882 exec execpod-affinitywlrs4 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
May  9 16:42:41.293: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May  9 16:42:41.293: INFO: stdout: ""
May  9 16:42:41.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-1882 exec execpod-affinitywlrs4 -- /bin/sh -x -c nc -v -z -w 2 10.3.143.12 80'
May  9 16:42:41.514: INFO: stderr: "+ nc -v -z -w 2 10.3.143.12 80\nConnection to 10.3.143.12 80 port [tcp/http] succeeded!\n"
May  9 16:42:41.514: INFO: stdout: ""
May  9 16:42:41.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-1882 exec execpod-affinitywlrs4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.143.12:80/ ; done'
May  9 16:42:41.819: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n"
May  9 16:42:41.819: INFO: stdout: "\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-pw7n5\naffinity-clusterip-transition-mttjk\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-pw7n5\naffinity-clusterip-transition-pw7n5\naffinity-clusterip-transition-mttjk\naffinity-clusterip-transition-pw7n5\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mttjk\naffinity-clusterip-transition-mttjk\naffinity-clusterip-transition-mttjk\naffinity-clusterip-transition-pw7n5\naffinity-clusterip-transition-mttjk"
May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-pw7n5
May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mttjk
May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-pw7n5
May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-pw7n5
May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mttjk
May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-pw7n5
May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mttjk
May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mttjk
May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mttjk
May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-pw7n5
May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mttjk
May  9 16:42:41.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-1882 exec execpod-affinitywlrs4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.143.12:80/ ; done'
May  9 16:42:42.124: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n"
May  9 16:42:42.124: INFO: stdout: "\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds"
May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
May  9 16:42:42.124: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1882, will wait for the garbage collector to delete the pods 05/09/23 16:42:42.141
May  9 16:42:42.204: INFO: Deleting ReplicationController affinity-clusterip-transition took: 8.198621ms
May  9 16:42:42.304: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.54206ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  9 16:42:44.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1882" for this suite. 05/09/23 16:42:44.636
------------------------------
â€¢ [SLOW TEST] [9.709 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:42:34.935
    May  9 16:42:34.935: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename services 05/09/23 16:42:34.936
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:34.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:34.963
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-1882 05/09/23 16:42:34.968
    STEP: creating service affinity-clusterip-transition in namespace services-1882 05/09/23 16:42:34.968
    STEP: creating replication controller affinity-clusterip-transition in namespace services-1882 05/09/23 16:42:34.981
    I0509 16:42:34.990324      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1882, replica count: 3
    I0509 16:42:38.042286      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 16:42:38.053: INFO: Creating new exec pod
    May  9 16:42:38.059: INFO: Waiting up to 5m0s for pod "execpod-affinitywlrs4" in namespace "services-1882" to be "running"
    May  9 16:42:38.064: INFO: Pod "execpod-affinitywlrs4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.435452ms
    May  9 16:42:40.071: INFO: Pod "execpod-affinitywlrs4": Phase="Running", Reason="", readiness=true. Elapsed: 2.012464882s
    May  9 16:42:40.071: INFO: Pod "execpod-affinitywlrs4" satisfied condition "running"
    May  9 16:42:41.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-1882 exec execpod-affinitywlrs4 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    May  9 16:42:41.293: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    May  9 16:42:41.293: INFO: stdout: ""
    May  9 16:42:41.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-1882 exec execpod-affinitywlrs4 -- /bin/sh -x -c nc -v -z -w 2 10.3.143.12 80'
    May  9 16:42:41.514: INFO: stderr: "+ nc -v -z -w 2 10.3.143.12 80\nConnection to 10.3.143.12 80 port [tcp/http] succeeded!\n"
    May  9 16:42:41.514: INFO: stdout: ""
    May  9 16:42:41.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-1882 exec execpod-affinitywlrs4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.143.12:80/ ; done'
    May  9 16:42:41.819: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n"
    May  9 16:42:41.819: INFO: stdout: "\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-pw7n5\naffinity-clusterip-transition-mttjk\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-pw7n5\naffinity-clusterip-transition-pw7n5\naffinity-clusterip-transition-mttjk\naffinity-clusterip-transition-pw7n5\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mttjk\naffinity-clusterip-transition-mttjk\naffinity-clusterip-transition-mttjk\naffinity-clusterip-transition-pw7n5\naffinity-clusterip-transition-mttjk"
    May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-pw7n5
    May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mttjk
    May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-pw7n5
    May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-pw7n5
    May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mttjk
    May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-pw7n5
    May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mttjk
    May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mttjk
    May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mttjk
    May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-pw7n5
    May  9 16:42:41.819: INFO: Received response from host: affinity-clusterip-transition-mttjk
    May  9 16:42:41.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-1882 exec execpod-affinitywlrs4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.143.12:80/ ; done'
    May  9 16:42:42.124: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.143.12:80/\n"
    May  9 16:42:42.124: INFO: stdout: "\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds\naffinity-clusterip-transition-mwqds"
    May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:42.124: INFO: Received response from host: affinity-clusterip-transition-mwqds
    May  9 16:42:42.124: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1882, will wait for the garbage collector to delete the pods 05/09/23 16:42:42.141
    May  9 16:42:42.204: INFO: Deleting ReplicationController affinity-clusterip-transition took: 8.198621ms
    May  9 16:42:42.304: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.54206ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  9 16:42:44.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1882" for this suite. 05/09/23 16:42:44.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:42:44.645
May  9 16:42:44.645: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename subpath 05/09/23 16:42:44.646
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:44.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:44.665
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/09/23 16:42:44.669
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-7cq9 05/09/23 16:42:44.682
STEP: Creating a pod to test atomic-volume-subpath 05/09/23 16:42:44.683
May  9 16:42:44.690: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-7cq9" in namespace "subpath-5042" to be "Succeeded or Failed"
May  9 16:42:44.695: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.455357ms
May  9 16:42:46.739: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 2.048476782s
May  9 16:42:48.702: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 4.011517294s
May  9 16:42:50.703: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 6.012450118s
May  9 16:42:52.703: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 8.012108994s
May  9 16:42:54.702: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 10.011252744s
May  9 16:42:56.702: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 12.011962299s
May  9 16:42:58.702: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 14.011846048s
May  9 16:43:00.705: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 16.014712449s
May  9 16:43:02.702: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 18.011257958s
May  9 16:43:04.704: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 20.013139637s
May  9 16:43:06.703: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=false. Elapsed: 22.012292088s
May  9 16:43:08.702: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012015619s
STEP: Saw pod success 05/09/23 16:43:08.703
May  9 16:43:08.703: INFO: Pod "pod-subpath-test-projected-7cq9" satisfied condition "Succeeded or Failed"
May  9 16:43:08.708: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-subpath-test-projected-7cq9 container test-container-subpath-projected-7cq9: <nil>
STEP: delete the pod 05/09/23 16:43:08.727
May  9 16:43:08.746: INFO: Waiting for pod pod-subpath-test-projected-7cq9 to disappear
May  9 16:43:08.751: INFO: Pod pod-subpath-test-projected-7cq9 no longer exists
STEP: Deleting pod pod-subpath-test-projected-7cq9 05/09/23 16:43:08.751
May  9 16:43:08.752: INFO: Deleting pod "pod-subpath-test-projected-7cq9" in namespace "subpath-5042"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May  9 16:43:08.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5042" for this suite. 05/09/23 16:43:08.765
------------------------------
â€¢ [SLOW TEST] [24.130 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:42:44.645
    May  9 16:42:44.645: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename subpath 05/09/23 16:42:44.646
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:42:44.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:42:44.665
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/09/23 16:42:44.669
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-7cq9 05/09/23 16:42:44.682
    STEP: Creating a pod to test atomic-volume-subpath 05/09/23 16:42:44.683
    May  9 16:42:44.690: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-7cq9" in namespace "subpath-5042" to be "Succeeded or Failed"
    May  9 16:42:44.695: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.455357ms
    May  9 16:42:46.739: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 2.048476782s
    May  9 16:42:48.702: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 4.011517294s
    May  9 16:42:50.703: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 6.012450118s
    May  9 16:42:52.703: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 8.012108994s
    May  9 16:42:54.702: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 10.011252744s
    May  9 16:42:56.702: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 12.011962299s
    May  9 16:42:58.702: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 14.011846048s
    May  9 16:43:00.705: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 16.014712449s
    May  9 16:43:02.702: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 18.011257958s
    May  9 16:43:04.704: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=true. Elapsed: 20.013139637s
    May  9 16:43:06.703: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Running", Reason="", readiness=false. Elapsed: 22.012292088s
    May  9 16:43:08.702: INFO: Pod "pod-subpath-test-projected-7cq9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012015619s
    STEP: Saw pod success 05/09/23 16:43:08.703
    May  9 16:43:08.703: INFO: Pod "pod-subpath-test-projected-7cq9" satisfied condition "Succeeded or Failed"
    May  9 16:43:08.708: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-subpath-test-projected-7cq9 container test-container-subpath-projected-7cq9: <nil>
    STEP: delete the pod 05/09/23 16:43:08.727
    May  9 16:43:08.746: INFO: Waiting for pod pod-subpath-test-projected-7cq9 to disappear
    May  9 16:43:08.751: INFO: Pod pod-subpath-test-projected-7cq9 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-7cq9 05/09/23 16:43:08.751
    May  9 16:43:08.752: INFO: Deleting pod "pod-subpath-test-projected-7cq9" in namespace "subpath-5042"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May  9 16:43:08.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5042" for this suite. 05/09/23 16:43:08.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:43:08.777
May  9 16:43:08.777: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename pods 05/09/23 16:43:08.777
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:43:08.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:43:08.798
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
May  9 16:43:08.803: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: creating the pod 05/09/23 16:43:08.803
STEP: submitting the pod to kubernetes 05/09/23 16:43:08.803
May  9 16:43:08.819: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-9483a5f1-d00d-4108-90b5-94157d1fec21" in namespace "pods-722" to be "running and ready"
May  9 16:43:08.825: INFO: Pod "pod-exec-websocket-9483a5f1-d00d-4108-90b5-94157d1fec21": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049277ms
May  9 16:43:08.825: INFO: The phase of Pod pod-exec-websocket-9483a5f1-d00d-4108-90b5-94157d1fec21 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:43:10.832: INFO: Pod "pod-exec-websocket-9483a5f1-d00d-4108-90b5-94157d1fec21": Phase="Running", Reason="", readiness=true. Elapsed: 2.013257372s
May  9 16:43:10.832: INFO: The phase of Pod pod-exec-websocket-9483a5f1-d00d-4108-90b5-94157d1fec21 is Running (Ready = true)
May  9 16:43:10.832: INFO: Pod "pod-exec-websocket-9483a5f1-d00d-4108-90b5-94157d1fec21" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  9 16:43:10.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-722" for this suite. 05/09/23 16:43:10.993
------------------------------
â€¢ [2.226 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:43:08.777
    May  9 16:43:08.777: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename pods 05/09/23 16:43:08.777
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:43:08.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:43:08.798
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    May  9 16:43:08.803: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: creating the pod 05/09/23 16:43:08.803
    STEP: submitting the pod to kubernetes 05/09/23 16:43:08.803
    May  9 16:43:08.819: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-9483a5f1-d00d-4108-90b5-94157d1fec21" in namespace "pods-722" to be "running and ready"
    May  9 16:43:08.825: INFO: Pod "pod-exec-websocket-9483a5f1-d00d-4108-90b5-94157d1fec21": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049277ms
    May  9 16:43:08.825: INFO: The phase of Pod pod-exec-websocket-9483a5f1-d00d-4108-90b5-94157d1fec21 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:43:10.832: INFO: Pod "pod-exec-websocket-9483a5f1-d00d-4108-90b5-94157d1fec21": Phase="Running", Reason="", readiness=true. Elapsed: 2.013257372s
    May  9 16:43:10.832: INFO: The phase of Pod pod-exec-websocket-9483a5f1-d00d-4108-90b5-94157d1fec21 is Running (Ready = true)
    May  9 16:43:10.832: INFO: Pod "pod-exec-websocket-9483a5f1-d00d-4108-90b5-94157d1fec21" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  9 16:43:10.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-722" for this suite. 05/09/23 16:43:10.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:43:11.004
May  9 16:43:11.004: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 16:43:11.005
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:43:11.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:43:11.026
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
May  9 16:43:11.031: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/09/23 16:43:13.332
May  9 16:43:13.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5139 --namespace=crd-publish-openapi-5139 create -f -'
May  9 16:43:14.625: INFO: stderr: ""
May  9 16:43:14.625: INFO: stdout: "e2e-test-crd-publish-openapi-9207-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May  9 16:43:14.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5139 --namespace=crd-publish-openapi-5139 delete e2e-test-crd-publish-openapi-9207-crds test-cr'
May  9 16:43:14.719: INFO: stderr: ""
May  9 16:43:14.719: INFO: stdout: "e2e-test-crd-publish-openapi-9207-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May  9 16:43:14.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5139 --namespace=crd-publish-openapi-5139 apply -f -'
May  9 16:43:14.924: INFO: stderr: ""
May  9 16:43:14.924: INFO: stdout: "e2e-test-crd-publish-openapi-9207-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May  9 16:43:14.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5139 --namespace=crd-publish-openapi-5139 delete e2e-test-crd-publish-openapi-9207-crds test-cr'
May  9 16:43:15.011: INFO: stderr: ""
May  9 16:43:15.011: INFO: stdout: "e2e-test-crd-publish-openapi-9207-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 05/09/23 16:43:15.011
May  9 16:43:15.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5139 explain e2e-test-crd-publish-openapi-9207-crds'
May  9 16:43:15.215: INFO: stderr: ""
May  9 16:43:15.215: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9207-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:43:17.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5139" for this suite. 05/09/23 16:43:17.09
------------------------------
â€¢ [SLOW TEST] [6.095 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:43:11.004
    May  9 16:43:11.004: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 16:43:11.005
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:43:11.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:43:11.026
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    May  9 16:43:11.031: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/09/23 16:43:13.332
    May  9 16:43:13.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5139 --namespace=crd-publish-openapi-5139 create -f -'
    May  9 16:43:14.625: INFO: stderr: ""
    May  9 16:43:14.625: INFO: stdout: "e2e-test-crd-publish-openapi-9207-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    May  9 16:43:14.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5139 --namespace=crd-publish-openapi-5139 delete e2e-test-crd-publish-openapi-9207-crds test-cr'
    May  9 16:43:14.719: INFO: stderr: ""
    May  9 16:43:14.719: INFO: stdout: "e2e-test-crd-publish-openapi-9207-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    May  9 16:43:14.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5139 --namespace=crd-publish-openapi-5139 apply -f -'
    May  9 16:43:14.924: INFO: stderr: ""
    May  9 16:43:14.924: INFO: stdout: "e2e-test-crd-publish-openapi-9207-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    May  9 16:43:14.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5139 --namespace=crd-publish-openapi-5139 delete e2e-test-crd-publish-openapi-9207-crds test-cr'
    May  9 16:43:15.011: INFO: stderr: ""
    May  9 16:43:15.011: INFO: stdout: "e2e-test-crd-publish-openapi-9207-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 05/09/23 16:43:15.011
    May  9 16:43:15.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5139 explain e2e-test-crd-publish-openapi-9207-crds'
    May  9 16:43:15.215: INFO: stderr: ""
    May  9 16:43:15.215: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9207-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:43:17.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5139" for this suite. 05/09/23 16:43:17.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:43:17.105
May  9 16:43:17.105: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename podtemplate 05/09/23 16:43:17.106
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:43:17.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:43:17.126
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
May  9 16:43:17.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-5659" for this suite. 05/09/23 16:43:17.176
------------------------------
â€¢ [0.079 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:43:17.105
    May  9 16:43:17.105: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename podtemplate 05/09/23 16:43:17.106
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:43:17.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:43:17.126
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    May  9 16:43:17.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-5659" for this suite. 05/09/23 16:43:17.176
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:43:17.186
May  9 16:43:17.186: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename pods 05/09/23 16:43:17.186
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:43:17.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:43:17.207
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 05/09/23 16:43:17.227
STEP: watching for Pod to be ready 05/09/23 16:43:17.248
May  9 16:43:17.258: INFO: observed Pod pod-test in namespace pods-4090 in phase Pending with labels: map[test-pod-static:true] & conditions []
May  9 16:43:17.258: INFO: observed Pod pod-test in namespace pods-4090 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC  }]
May  9 16:43:17.272: INFO: observed Pod pod-test in namespace pods-4090 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC  }]
May  9 16:43:17.726: INFO: observed Pod pod-test in namespace pods-4090 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC  }]
May  9 16:43:18.602: INFO: Found Pod pod-test in namespace pods-4090 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 05/09/23 16:43:18.607
STEP: getting the Pod and ensuring that it's patched 05/09/23 16:43:18.62
STEP: replacing the Pod's status Ready condition to False 05/09/23 16:43:18.624
STEP: check the Pod again to ensure its Ready conditions are False 05/09/23 16:43:18.647
STEP: deleting the Pod via a Collection with a LabelSelector 05/09/23 16:43:18.648
STEP: watching for the Pod to be deleted 05/09/23 16:43:18.66
May  9 16:43:18.662: INFO: observed event type MODIFIED
May  9 16:43:20.612: INFO: observed event type MODIFIED
May  9 16:43:20.884: INFO: observed event type MODIFIED
May  9 16:43:21.614: INFO: observed event type MODIFIED
May  9 16:43:21.623: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  9 16:43:21.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4090" for this suite. 05/09/23 16:43:21.639
------------------------------
â€¢ [4.460 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:43:17.186
    May  9 16:43:17.186: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename pods 05/09/23 16:43:17.186
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:43:17.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:43:17.207
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 05/09/23 16:43:17.227
    STEP: watching for Pod to be ready 05/09/23 16:43:17.248
    May  9 16:43:17.258: INFO: observed Pod pod-test in namespace pods-4090 in phase Pending with labels: map[test-pod-static:true] & conditions []
    May  9 16:43:17.258: INFO: observed Pod pod-test in namespace pods-4090 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC  }]
    May  9 16:43:17.272: INFO: observed Pod pod-test in namespace pods-4090 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC  }]
    May  9 16:43:17.726: INFO: observed Pod pod-test in namespace pods-4090 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC  }]
    May  9 16:43:18.602: INFO: Found Pod pod-test in namespace pods-4090 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:43:17 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 05/09/23 16:43:18.607
    STEP: getting the Pod and ensuring that it's patched 05/09/23 16:43:18.62
    STEP: replacing the Pod's status Ready condition to False 05/09/23 16:43:18.624
    STEP: check the Pod again to ensure its Ready conditions are False 05/09/23 16:43:18.647
    STEP: deleting the Pod via a Collection with a LabelSelector 05/09/23 16:43:18.648
    STEP: watching for the Pod to be deleted 05/09/23 16:43:18.66
    May  9 16:43:18.662: INFO: observed event type MODIFIED
    May  9 16:43:20.612: INFO: observed event type MODIFIED
    May  9 16:43:20.884: INFO: observed event type MODIFIED
    May  9 16:43:21.614: INFO: observed event type MODIFIED
    May  9 16:43:21.623: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  9 16:43:21.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4090" for this suite. 05/09/23 16:43:21.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:43:21.649
May  9 16:43:21.650: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename proxy 05/09/23 16:43:21.65
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:43:21.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:43:21.673
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 05/09/23 16:43:21.691
STEP: creating replication controller proxy-service-zffb7 in namespace proxy-7234 05/09/23 16:43:21.692
I0509 16:43:21.699036      21 runners.go:193] Created replication controller with name: proxy-service-zffb7, namespace: proxy-7234, replica count: 1
I0509 16:43:22.749640      21 runners.go:193] proxy-service-zffb7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0509 16:43:23.750654      21 runners.go:193] proxy-service-zffb7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 16:43:23.757: INFO: setup took 2.079972029s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 05/09/23 16:43:23.757
May  9 16:43:23.768: INFO: (0) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 9.992521ms)
May  9 16:43:23.775: INFO: (0) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 17.090942ms)
May  9 16:43:23.775: INFO: (0) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 16.970467ms)
May  9 16:43:23.778: INFO: (0) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 20.068487ms)
May  9 16:43:23.778: INFO: (0) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 20.069751ms)
May  9 16:43:23.778: INFO: (0) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 20.191226ms)
May  9 16:43:23.778: INFO: (0) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 20.337027ms)
May  9 16:43:23.784: INFO: (0) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 26.093744ms)
May  9 16:43:23.784: INFO: (0) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 26.317755ms)
May  9 16:43:23.784: INFO: (0) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 26.540772ms)
May  9 16:43:23.784: INFO: (0) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 26.412906ms)
May  9 16:43:23.784: INFO: (0) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 26.303353ms)
May  9 16:43:23.784: INFO: (0) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 26.244102ms)
May  9 16:43:23.786: INFO: (0) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 27.608978ms)
May  9 16:43:23.786: INFO: (0) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 27.858683ms)
May  9 16:43:23.786: INFO: (0) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 27.675349ms)
May  9 16:43:23.816: INFO: (1) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 30.415935ms)
May  9 16:43:23.816: INFO: (1) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 30.453265ms)
May  9 16:43:23.816: INFO: (1) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 30.708561ms)
May  9 16:43:23.816: INFO: (1) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 30.559946ms)
May  9 16:43:23.816: INFO: (1) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 30.62826ms)
May  9 16:43:23.816: INFO: (1) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 30.687128ms)
May  9 16:43:23.816: INFO: (1) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 30.577508ms)
May  9 16:43:23.817: INFO: (1) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 30.577297ms)
May  9 16:43:23.817: INFO: (1) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 30.595851ms)
May  9 16:43:23.817: INFO: (1) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 30.598288ms)
May  9 16:43:23.817: INFO: (1) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 30.696083ms)
May  9 16:43:23.817: INFO: (1) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 30.759553ms)
May  9 16:43:23.817: INFO: (1) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 31.113194ms)
May  9 16:43:23.818: INFO: (1) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 32.162086ms)
May  9 16:43:23.820: INFO: (1) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 33.641504ms)
May  9 16:43:23.820: INFO: (1) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 33.74423ms)
May  9 16:43:23.833: INFO: (2) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 13.411896ms)
May  9 16:43:23.833: INFO: (2) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 13.238223ms)
May  9 16:43:23.833: INFO: (2) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 13.310438ms)
May  9 16:43:23.833: INFO: (2) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 13.305702ms)
May  9 16:43:23.833: INFO: (2) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 13.407455ms)
May  9 16:43:23.833: INFO: (2) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 13.370594ms)
May  9 16:43:23.833: INFO: (2) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 13.30352ms)
May  9 16:43:23.833: INFO: (2) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 13.429703ms)
May  9 16:43:23.833: INFO: (2) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 13.555613ms)
May  9 16:43:23.834: INFO: (2) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 13.955895ms)
May  9 16:43:23.834: INFO: (2) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 13.918061ms)
May  9 16:43:23.834: INFO: (2) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 14.596394ms)
May  9 16:43:23.834: INFO: (2) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 14.602918ms)
May  9 16:43:23.844: INFO: (2) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 24.329145ms)
May  9 16:43:23.844: INFO: (2) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 24.361721ms)
May  9 16:43:23.844: INFO: (2) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 24.368768ms)
May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 9.508263ms)
May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 9.524837ms)
May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 9.482431ms)
May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 9.58124ms)
May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 9.682256ms)
May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 9.527327ms)
May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 9.757484ms)
May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 9.571125ms)
May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 9.735012ms)
May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 9.574421ms)
May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 9.629754ms)
May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 9.942157ms)
May  9 16:43:23.855: INFO: (3) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 10.3443ms)
May  9 16:43:23.858: INFO: (3) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 13.938623ms)
May  9 16:43:23.858: INFO: (3) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 13.923033ms)
May  9 16:43:23.859: INFO: (3) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 14.738357ms)
May  9 16:43:23.867: INFO: (4) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 7.50948ms)
May  9 16:43:23.869: INFO: (4) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 9.598969ms)
May  9 16:43:23.869: INFO: (4) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 9.736803ms)
May  9 16:43:23.869: INFO: (4) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 9.643395ms)
May  9 16:43:23.869: INFO: (4) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 9.73054ms)
May  9 16:43:23.869: INFO: (4) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 9.559079ms)
May  9 16:43:23.872: INFO: (4) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 12.801912ms)
May  9 16:43:23.872: INFO: (4) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 12.934958ms)
May  9 16:43:23.873: INFO: (4) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 13.220037ms)
May  9 16:43:23.873: INFO: (4) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 13.185171ms)
May  9 16:43:23.873: INFO: (4) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 13.196899ms)
May  9 16:43:23.873: INFO: (4) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 13.149496ms)
May  9 16:43:23.873: INFO: (4) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 13.283373ms)
May  9 16:43:23.873: INFO: (4) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 13.262052ms)
May  9 16:43:23.873: INFO: (4) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 13.322584ms)
May  9 16:43:23.873: INFO: (4) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 13.331853ms)
May  9 16:43:23.883: INFO: (5) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 10.063465ms)
May  9 16:43:23.883: INFO: (5) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 10.178647ms)
May  9 16:43:23.883: INFO: (5) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 10.085245ms)
May  9 16:43:23.883: INFO: (5) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 10.119082ms)
May  9 16:43:23.884: INFO: (5) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 10.715691ms)
May  9 16:43:23.884: INFO: (5) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 10.71112ms)
May  9 16:43:23.884: INFO: (5) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 10.803267ms)
May  9 16:43:23.884: INFO: (5) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 10.928123ms)
May  9 16:43:23.884: INFO: (5) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 10.793447ms)
May  9 16:43:23.884: INFO: (5) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 10.784788ms)
May  9 16:43:23.884: INFO: (5) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 11.15722ms)
May  9 16:43:23.884: INFO: (5) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 11.320786ms)
May  9 16:43:23.886: INFO: (5) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 13.324326ms)
May  9 16:43:23.886: INFO: (5) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 13.387856ms)
May  9 16:43:23.886: INFO: (5) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 13.442495ms)
May  9 16:43:23.887: INFO: (5) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 14.252273ms)
May  9 16:43:23.895: INFO: (6) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 7.341327ms)
May  9 16:43:23.895: INFO: (6) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 7.496261ms)
May  9 16:43:23.895: INFO: (6) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 7.452236ms)
May  9 16:43:23.895: INFO: (6) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 7.232702ms)
May  9 16:43:23.896: INFO: (6) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 8.029148ms)
May  9 16:43:23.897: INFO: (6) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 8.754438ms)
May  9 16:43:23.897: INFO: (6) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 8.887333ms)
May  9 16:43:23.897: INFO: (6) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 8.534266ms)
May  9 16:43:23.897: INFO: (6) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 8.688297ms)
May  9 16:43:23.897: INFO: (6) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 9.091273ms)
May  9 16:43:23.897: INFO: (6) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 9.124212ms)
May  9 16:43:23.897: INFO: (6) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 9.239699ms)
May  9 16:43:23.897: INFO: (6) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 9.395289ms)
May  9 16:43:23.898: INFO: (6) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 10.731248ms)
May  9 16:43:23.902: INFO: (6) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 13.347576ms)
May  9 16:43:23.902: INFO: (6) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 13.385938ms)
May  9 16:43:23.912: INFO: (7) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 10.389607ms)
May  9 16:43:23.914: INFO: (7) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 12.354119ms)
May  9 16:43:23.914: INFO: (7) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 12.376213ms)
May  9 16:43:23.914: INFO: (7) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 12.460808ms)
May  9 16:43:23.914: INFO: (7) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 12.471839ms)
May  9 16:43:23.914: INFO: (7) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 12.494439ms)
May  9 16:43:23.914: INFO: (7) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 12.59299ms)
May  9 16:43:23.914: INFO: (7) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 12.487786ms)
May  9 16:43:23.914: INFO: (7) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 12.498604ms)
May  9 16:43:23.915: INFO: (7) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 12.78232ms)
May  9 16:43:23.915: INFO: (7) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 12.908969ms)
May  9 16:43:23.915: INFO: (7) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 13.003127ms)
May  9 16:43:23.915: INFO: (7) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 13.255653ms)
May  9 16:43:23.915: INFO: (7) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 13.474571ms)
May  9 16:43:23.917: INFO: (7) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 15.328356ms)
May  9 16:43:23.918: INFO: (7) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 15.827403ms)
May  9 16:43:23.961: INFO: (8) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 42.78319ms)
May  9 16:43:23.961: INFO: (8) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 42.718776ms)
May  9 16:43:23.961: INFO: (8) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 42.734084ms)
May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 44.185742ms)
May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 44.313643ms)
May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 44.401759ms)
May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 44.296535ms)
May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 44.448582ms)
May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 44.305477ms)
May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 44.298058ms)
May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 44.436324ms)
May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 44.358004ms)
May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 44.373995ms)
May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 44.419659ms)
May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 44.622361ms)
May  9 16:43:23.963: INFO: (8) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 45.008506ms)
May  9 16:43:23.976: INFO: (9) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 12.954331ms)
May  9 16:43:23.983: INFO: (9) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 19.715217ms)
May  9 16:43:23.983: INFO: (9) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 20.446819ms)
May  9 16:43:23.983: INFO: (9) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 20.238667ms)
May  9 16:43:23.984: INFO: (9) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 20.331179ms)
May  9 16:43:24.001: INFO: (9) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 38.267676ms)
May  9 16:43:24.001: INFO: (9) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 38.140505ms)
May  9 16:43:24.001: INFO: (9) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 38.084557ms)
May  9 16:43:24.001: INFO: (9) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 38.160907ms)
May  9 16:43:24.003: INFO: (9) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 39.513165ms)
May  9 16:43:24.003: INFO: (9) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 39.393111ms)
May  9 16:43:24.003: INFO: (9) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 39.443605ms)
May  9 16:43:24.003: INFO: (9) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 39.595592ms)
May  9 16:43:24.003: INFO: (9) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 39.576934ms)
May  9 16:43:24.004: INFO: (9) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 41.147119ms)
May  9 16:43:24.004: INFO: (9) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 41.050434ms)
May  9 16:43:24.014: INFO: (10) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 9.384457ms)
May  9 16:43:24.014: INFO: (10) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 9.398642ms)
May  9 16:43:24.014: INFO: (10) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 9.568081ms)
May  9 16:43:24.014: INFO: (10) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 9.068846ms)
May  9 16:43:24.014: INFO: (10) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 9.59468ms)
May  9 16:43:24.014: INFO: (10) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 9.553675ms)
May  9 16:43:24.014: INFO: (10) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 9.372294ms)
May  9 16:43:24.014: INFO: (10) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 9.652861ms)
May  9 16:43:24.015: INFO: (10) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 10.263933ms)
May  9 16:43:24.015: INFO: (10) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 10.52243ms)
May  9 16:43:24.015: INFO: (10) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 10.328754ms)
May  9 16:43:24.015: INFO: (10) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 10.258168ms)
May  9 16:43:24.015: INFO: (10) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 10.4231ms)
May  9 16:43:24.018: INFO: (10) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 13.03347ms)
May  9 16:43:24.018: INFO: (10) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 13.410033ms)
May  9 16:43:24.018: INFO: (10) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 13.681417ms)
May  9 16:43:24.033: INFO: (11) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 14.372611ms)
May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 20.237835ms)
May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 20.076608ms)
May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 20.124905ms)
May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 20.196841ms)
May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 20.2865ms)
May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 20.327645ms)
May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 20.293504ms)
May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 20.195566ms)
May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 20.236555ms)
May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 20.374206ms)
May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 20.243662ms)
May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 20.295936ms)
May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 20.360855ms)
May  9 16:43:24.042: INFO: (11) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 23.049865ms)
May  9 16:43:24.042: INFO: (11) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 23.400275ms)
May  9 16:43:24.055: INFO: (12) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 13.12502ms)
May  9 16:43:24.055: INFO: (12) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 13.234278ms)
May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 16.606051ms)
May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 16.690976ms)
May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 16.73076ms)
May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 16.707223ms)
May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 16.750687ms)
May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 16.860589ms)
May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 16.789625ms)
May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 16.866357ms)
May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 16.801475ms)
May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 16.845457ms)
May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 16.753866ms)
May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 16.869245ms)
May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 16.772473ms)
May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 16.860039ms)
May  9 16:43:24.068: INFO: (13) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 8.970693ms)
May  9 16:43:24.072: INFO: (13) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 12.352355ms)
May  9 16:43:24.072: INFO: (13) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 12.483306ms)
May  9 16:43:24.072: INFO: (13) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 12.602115ms)
May  9 16:43:24.077: INFO: (13) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 17.921541ms)
May  9 16:43:24.077: INFO: (13) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 18.040589ms)
May  9 16:43:24.079: INFO: (13) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 19.3038ms)
May  9 16:43:24.079: INFO: (13) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 19.281346ms)
May  9 16:43:24.080: INFO: (13) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 20.257106ms)
May  9 16:43:24.080: INFO: (13) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 20.299511ms)
May  9 16:43:24.080: INFO: (13) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 20.408715ms)
May  9 16:43:24.080: INFO: (13) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 20.323034ms)
May  9 16:43:24.080: INFO: (13) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 20.514221ms)
May  9 16:43:24.080: INFO: (13) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 20.602461ms)
May  9 16:43:24.083: INFO: (13) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 23.725327ms)
May  9 16:43:24.087: INFO: (13) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 27.651667ms)
May  9 16:43:24.093: INFO: (14) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 5.887624ms)
May  9 16:43:24.093: INFO: (14) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 5.861455ms)
May  9 16:43:24.094: INFO: (14) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 6.598429ms)
May  9 16:43:24.094: INFO: (14) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 6.530758ms)
May  9 16:43:24.099: INFO: (14) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 11.668032ms)
May  9 16:43:24.099: INFO: (14) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 12.052107ms)
May  9 16:43:24.099: INFO: (14) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 12.310551ms)
May  9 16:43:24.103: INFO: (14) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 15.484591ms)
May  9 16:43:24.103: INFO: (14) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 15.806172ms)
May  9 16:43:24.103: INFO: (14) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 15.618202ms)
May  9 16:43:24.103: INFO: (14) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 15.492524ms)
May  9 16:43:24.103: INFO: (14) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 15.589556ms)
May  9 16:43:24.103: INFO: (14) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 15.77019ms)
May  9 16:43:24.103: INFO: (14) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 16.230898ms)
May  9 16:43:24.104: INFO: (14) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 16.443811ms)
May  9 16:43:24.104: INFO: (14) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 16.279683ms)
May  9 16:43:24.112: INFO: (15) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 8.25913ms)
May  9 16:43:24.112: INFO: (15) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 8.282573ms)
May  9 16:43:24.112: INFO: (15) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 8.195162ms)
May  9 16:43:24.113: INFO: (15) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 9.005501ms)
May  9 16:43:24.113: INFO: (15) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 9.08477ms)
May  9 16:43:24.114: INFO: (15) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 10.148414ms)
May  9 16:43:24.114: INFO: (15) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 10.179676ms)
May  9 16:43:24.114: INFO: (15) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 10.251288ms)
May  9 16:43:24.114: INFO: (15) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 10.318523ms)
May  9 16:43:24.114: INFO: (15) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 10.104291ms)
May  9 16:43:24.114: INFO: (15) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 10.319754ms)
May  9 16:43:24.118: INFO: (15) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 14.032467ms)
May  9 16:43:24.118: INFO: (15) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 13.971574ms)
May  9 16:43:24.118: INFO: (15) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 13.985079ms)
May  9 16:43:24.118: INFO: (15) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 14.143122ms)
May  9 16:43:24.118: INFO: (15) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 14.00187ms)
May  9 16:43:24.128: INFO: (16) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 10.088288ms)
May  9 16:43:24.128: INFO: (16) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 10.086695ms)
May  9 16:43:24.128: INFO: (16) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 10.039769ms)
May  9 16:43:24.128: INFO: (16) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 10.175453ms)
May  9 16:43:24.128: INFO: (16) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 10.124843ms)
May  9 16:43:24.128: INFO: (16) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 10.148543ms)
May  9 16:43:24.128: INFO: (16) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 10.307637ms)
May  9 16:43:24.133: INFO: (16) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 15.400275ms)
May  9 16:43:24.133: INFO: (16) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 15.506489ms)
May  9 16:43:24.133: INFO: (16) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 15.392356ms)
May  9 16:43:24.133: INFO: (16) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 15.447747ms)
May  9 16:43:24.135: INFO: (16) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 17.322342ms)
May  9 16:43:24.135: INFO: (16) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 17.218242ms)
May  9 16:43:24.135: INFO: (16) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 17.254386ms)
May  9 16:43:24.135: INFO: (16) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 17.308443ms)
May  9 16:43:24.135: INFO: (16) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 17.170614ms)
May  9 16:43:24.161: INFO: (17) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 25.083906ms)
May  9 16:43:24.161: INFO: (17) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 25.237391ms)
May  9 16:43:24.161: INFO: (17) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 25.266948ms)
May  9 16:43:24.161: INFO: (17) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 25.334292ms)
May  9 16:43:24.161: INFO: (17) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 25.33709ms)
May  9 16:43:24.161: INFO: (17) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 25.404256ms)
May  9 16:43:24.161: INFO: (17) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 25.396302ms)
May  9 16:43:24.162: INFO: (17) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 26.812555ms)
May  9 16:43:24.162: INFO: (17) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 26.883353ms)
May  9 16:43:24.163: INFO: (17) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 27.711903ms)
May  9 16:43:24.163: INFO: (17) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 27.834518ms)
May  9 16:43:24.163: INFO: (17) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 27.712382ms)
May  9 16:43:24.163: INFO: (17) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 27.785068ms)
May  9 16:43:24.164: INFO: (17) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 28.479119ms)
May  9 16:43:24.164: INFO: (17) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 28.566685ms)
May  9 16:43:24.164: INFO: (17) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 28.509921ms)
May  9 16:43:24.172: INFO: (18) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 7.23894ms)
May  9 16:43:24.175: INFO: (18) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 10.205449ms)
May  9 16:43:24.175: INFO: (18) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 10.331436ms)
May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 11.476204ms)
May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 11.442244ms)
May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 11.611338ms)
May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 11.577058ms)
May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 11.468793ms)
May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 11.659842ms)
May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 11.561332ms)
May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 11.560469ms)
May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 12.020152ms)
May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 11.903017ms)
May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 12.111343ms)
May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 11.977257ms)
May  9 16:43:24.183: INFO: (18) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 18.432309ms)
May  9 16:43:24.193: INFO: (19) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 10.413316ms)
May  9 16:43:24.193: INFO: (19) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 10.348657ms)
May  9 16:43:24.193: INFO: (19) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 10.561896ms)
May  9 16:43:24.193: INFO: (19) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 10.579645ms)
May  9 16:43:24.193: INFO: (19) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 10.483381ms)
May  9 16:43:24.193: INFO: (19) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 10.53261ms)
May  9 16:43:24.194: INFO: (19) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 10.463029ms)
May  9 16:43:24.194: INFO: (19) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 10.408103ms)
May  9 16:43:24.194: INFO: (19) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 10.708944ms)
May  9 16:43:24.195: INFO: (19) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 12.029162ms)
May  9 16:43:24.195: INFO: (19) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 12.246033ms)
May  9 16:43:24.195: INFO: (19) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 12.276994ms)
May  9 16:43:24.196: INFO: (19) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 12.557033ms)
May  9 16:43:24.196: INFO: (19) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 12.745443ms)
May  9 16:43:24.196: INFO: (19) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 13.262154ms)
May  9 16:43:24.196: INFO: (19) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 13.390969ms)
STEP: deleting ReplicationController proxy-service-zffb7 in namespace proxy-7234, will wait for the garbage collector to delete the pods 05/09/23 16:43:24.196
May  9 16:43:24.260: INFO: Deleting ReplicationController proxy-service-zffb7 took: 8.27193ms
May  9 16:43:24.361: INFO: Terminating ReplicationController proxy-service-zffb7 pods took: 100.957027ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
May  9 16:43:26.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-7234" for this suite. 05/09/23 16:43:26.67
------------------------------
â€¢ [SLOW TEST] [5.028 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:43:21.649
    May  9 16:43:21.650: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename proxy 05/09/23 16:43:21.65
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:43:21.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:43:21.673
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 05/09/23 16:43:21.691
    STEP: creating replication controller proxy-service-zffb7 in namespace proxy-7234 05/09/23 16:43:21.692
    I0509 16:43:21.699036      21 runners.go:193] Created replication controller with name: proxy-service-zffb7, namespace: proxy-7234, replica count: 1
    I0509 16:43:22.749640      21 runners.go:193] proxy-service-zffb7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0509 16:43:23.750654      21 runners.go:193] proxy-service-zffb7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 16:43:23.757: INFO: setup took 2.079972029s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 05/09/23 16:43:23.757
    May  9 16:43:23.768: INFO: (0) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 9.992521ms)
    May  9 16:43:23.775: INFO: (0) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 17.090942ms)
    May  9 16:43:23.775: INFO: (0) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 16.970467ms)
    May  9 16:43:23.778: INFO: (0) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 20.068487ms)
    May  9 16:43:23.778: INFO: (0) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 20.069751ms)
    May  9 16:43:23.778: INFO: (0) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 20.191226ms)
    May  9 16:43:23.778: INFO: (0) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 20.337027ms)
    May  9 16:43:23.784: INFO: (0) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 26.093744ms)
    May  9 16:43:23.784: INFO: (0) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 26.317755ms)
    May  9 16:43:23.784: INFO: (0) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 26.540772ms)
    May  9 16:43:23.784: INFO: (0) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 26.412906ms)
    May  9 16:43:23.784: INFO: (0) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 26.303353ms)
    May  9 16:43:23.784: INFO: (0) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 26.244102ms)
    May  9 16:43:23.786: INFO: (0) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 27.608978ms)
    May  9 16:43:23.786: INFO: (0) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 27.858683ms)
    May  9 16:43:23.786: INFO: (0) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 27.675349ms)
    May  9 16:43:23.816: INFO: (1) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 30.415935ms)
    May  9 16:43:23.816: INFO: (1) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 30.453265ms)
    May  9 16:43:23.816: INFO: (1) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 30.708561ms)
    May  9 16:43:23.816: INFO: (1) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 30.559946ms)
    May  9 16:43:23.816: INFO: (1) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 30.62826ms)
    May  9 16:43:23.816: INFO: (1) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 30.687128ms)
    May  9 16:43:23.816: INFO: (1) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 30.577508ms)
    May  9 16:43:23.817: INFO: (1) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 30.577297ms)
    May  9 16:43:23.817: INFO: (1) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 30.595851ms)
    May  9 16:43:23.817: INFO: (1) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 30.598288ms)
    May  9 16:43:23.817: INFO: (1) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 30.696083ms)
    May  9 16:43:23.817: INFO: (1) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 30.759553ms)
    May  9 16:43:23.817: INFO: (1) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 31.113194ms)
    May  9 16:43:23.818: INFO: (1) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 32.162086ms)
    May  9 16:43:23.820: INFO: (1) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 33.641504ms)
    May  9 16:43:23.820: INFO: (1) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 33.74423ms)
    May  9 16:43:23.833: INFO: (2) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 13.411896ms)
    May  9 16:43:23.833: INFO: (2) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 13.238223ms)
    May  9 16:43:23.833: INFO: (2) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 13.310438ms)
    May  9 16:43:23.833: INFO: (2) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 13.305702ms)
    May  9 16:43:23.833: INFO: (2) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 13.407455ms)
    May  9 16:43:23.833: INFO: (2) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 13.370594ms)
    May  9 16:43:23.833: INFO: (2) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 13.30352ms)
    May  9 16:43:23.833: INFO: (2) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 13.429703ms)
    May  9 16:43:23.833: INFO: (2) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 13.555613ms)
    May  9 16:43:23.834: INFO: (2) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 13.955895ms)
    May  9 16:43:23.834: INFO: (2) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 13.918061ms)
    May  9 16:43:23.834: INFO: (2) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 14.596394ms)
    May  9 16:43:23.834: INFO: (2) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 14.602918ms)
    May  9 16:43:23.844: INFO: (2) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 24.329145ms)
    May  9 16:43:23.844: INFO: (2) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 24.361721ms)
    May  9 16:43:23.844: INFO: (2) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 24.368768ms)
    May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 9.508263ms)
    May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 9.524837ms)
    May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 9.482431ms)
    May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 9.58124ms)
    May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 9.682256ms)
    May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 9.527327ms)
    May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 9.757484ms)
    May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 9.571125ms)
    May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 9.735012ms)
    May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 9.574421ms)
    May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 9.629754ms)
    May  9 16:43:23.854: INFO: (3) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 9.942157ms)
    May  9 16:43:23.855: INFO: (3) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 10.3443ms)
    May  9 16:43:23.858: INFO: (3) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 13.938623ms)
    May  9 16:43:23.858: INFO: (3) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 13.923033ms)
    May  9 16:43:23.859: INFO: (3) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 14.738357ms)
    May  9 16:43:23.867: INFO: (4) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 7.50948ms)
    May  9 16:43:23.869: INFO: (4) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 9.598969ms)
    May  9 16:43:23.869: INFO: (4) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 9.736803ms)
    May  9 16:43:23.869: INFO: (4) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 9.643395ms)
    May  9 16:43:23.869: INFO: (4) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 9.73054ms)
    May  9 16:43:23.869: INFO: (4) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 9.559079ms)
    May  9 16:43:23.872: INFO: (4) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 12.801912ms)
    May  9 16:43:23.872: INFO: (4) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 12.934958ms)
    May  9 16:43:23.873: INFO: (4) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 13.220037ms)
    May  9 16:43:23.873: INFO: (4) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 13.185171ms)
    May  9 16:43:23.873: INFO: (4) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 13.196899ms)
    May  9 16:43:23.873: INFO: (4) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 13.149496ms)
    May  9 16:43:23.873: INFO: (4) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 13.283373ms)
    May  9 16:43:23.873: INFO: (4) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 13.262052ms)
    May  9 16:43:23.873: INFO: (4) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 13.322584ms)
    May  9 16:43:23.873: INFO: (4) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 13.331853ms)
    May  9 16:43:23.883: INFO: (5) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 10.063465ms)
    May  9 16:43:23.883: INFO: (5) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 10.178647ms)
    May  9 16:43:23.883: INFO: (5) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 10.085245ms)
    May  9 16:43:23.883: INFO: (5) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 10.119082ms)
    May  9 16:43:23.884: INFO: (5) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 10.715691ms)
    May  9 16:43:23.884: INFO: (5) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 10.71112ms)
    May  9 16:43:23.884: INFO: (5) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 10.803267ms)
    May  9 16:43:23.884: INFO: (5) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 10.928123ms)
    May  9 16:43:23.884: INFO: (5) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 10.793447ms)
    May  9 16:43:23.884: INFO: (5) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 10.784788ms)
    May  9 16:43:23.884: INFO: (5) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 11.15722ms)
    May  9 16:43:23.884: INFO: (5) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 11.320786ms)
    May  9 16:43:23.886: INFO: (5) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 13.324326ms)
    May  9 16:43:23.886: INFO: (5) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 13.387856ms)
    May  9 16:43:23.886: INFO: (5) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 13.442495ms)
    May  9 16:43:23.887: INFO: (5) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 14.252273ms)
    May  9 16:43:23.895: INFO: (6) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 7.341327ms)
    May  9 16:43:23.895: INFO: (6) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 7.496261ms)
    May  9 16:43:23.895: INFO: (6) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 7.452236ms)
    May  9 16:43:23.895: INFO: (6) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 7.232702ms)
    May  9 16:43:23.896: INFO: (6) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 8.029148ms)
    May  9 16:43:23.897: INFO: (6) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 8.754438ms)
    May  9 16:43:23.897: INFO: (6) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 8.887333ms)
    May  9 16:43:23.897: INFO: (6) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 8.534266ms)
    May  9 16:43:23.897: INFO: (6) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 8.688297ms)
    May  9 16:43:23.897: INFO: (6) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 9.091273ms)
    May  9 16:43:23.897: INFO: (6) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 9.124212ms)
    May  9 16:43:23.897: INFO: (6) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 9.239699ms)
    May  9 16:43:23.897: INFO: (6) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 9.395289ms)
    May  9 16:43:23.898: INFO: (6) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 10.731248ms)
    May  9 16:43:23.902: INFO: (6) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 13.347576ms)
    May  9 16:43:23.902: INFO: (6) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 13.385938ms)
    May  9 16:43:23.912: INFO: (7) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 10.389607ms)
    May  9 16:43:23.914: INFO: (7) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 12.354119ms)
    May  9 16:43:23.914: INFO: (7) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 12.376213ms)
    May  9 16:43:23.914: INFO: (7) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 12.460808ms)
    May  9 16:43:23.914: INFO: (7) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 12.471839ms)
    May  9 16:43:23.914: INFO: (7) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 12.494439ms)
    May  9 16:43:23.914: INFO: (7) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 12.59299ms)
    May  9 16:43:23.914: INFO: (7) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 12.487786ms)
    May  9 16:43:23.914: INFO: (7) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 12.498604ms)
    May  9 16:43:23.915: INFO: (7) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 12.78232ms)
    May  9 16:43:23.915: INFO: (7) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 12.908969ms)
    May  9 16:43:23.915: INFO: (7) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 13.003127ms)
    May  9 16:43:23.915: INFO: (7) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 13.255653ms)
    May  9 16:43:23.915: INFO: (7) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 13.474571ms)
    May  9 16:43:23.917: INFO: (7) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 15.328356ms)
    May  9 16:43:23.918: INFO: (7) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 15.827403ms)
    May  9 16:43:23.961: INFO: (8) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 42.78319ms)
    May  9 16:43:23.961: INFO: (8) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 42.718776ms)
    May  9 16:43:23.961: INFO: (8) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 42.734084ms)
    May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 44.185742ms)
    May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 44.313643ms)
    May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 44.401759ms)
    May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 44.296535ms)
    May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 44.448582ms)
    May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 44.305477ms)
    May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 44.298058ms)
    May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 44.436324ms)
    May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 44.358004ms)
    May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 44.373995ms)
    May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 44.419659ms)
    May  9 16:43:23.962: INFO: (8) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 44.622361ms)
    May  9 16:43:23.963: INFO: (8) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 45.008506ms)
    May  9 16:43:23.976: INFO: (9) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 12.954331ms)
    May  9 16:43:23.983: INFO: (9) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 19.715217ms)
    May  9 16:43:23.983: INFO: (9) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 20.446819ms)
    May  9 16:43:23.983: INFO: (9) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 20.238667ms)
    May  9 16:43:23.984: INFO: (9) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 20.331179ms)
    May  9 16:43:24.001: INFO: (9) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 38.267676ms)
    May  9 16:43:24.001: INFO: (9) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 38.140505ms)
    May  9 16:43:24.001: INFO: (9) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 38.084557ms)
    May  9 16:43:24.001: INFO: (9) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 38.160907ms)
    May  9 16:43:24.003: INFO: (9) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 39.513165ms)
    May  9 16:43:24.003: INFO: (9) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 39.393111ms)
    May  9 16:43:24.003: INFO: (9) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 39.443605ms)
    May  9 16:43:24.003: INFO: (9) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 39.595592ms)
    May  9 16:43:24.003: INFO: (9) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 39.576934ms)
    May  9 16:43:24.004: INFO: (9) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 41.147119ms)
    May  9 16:43:24.004: INFO: (9) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 41.050434ms)
    May  9 16:43:24.014: INFO: (10) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 9.384457ms)
    May  9 16:43:24.014: INFO: (10) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 9.398642ms)
    May  9 16:43:24.014: INFO: (10) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 9.568081ms)
    May  9 16:43:24.014: INFO: (10) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 9.068846ms)
    May  9 16:43:24.014: INFO: (10) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 9.59468ms)
    May  9 16:43:24.014: INFO: (10) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 9.553675ms)
    May  9 16:43:24.014: INFO: (10) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 9.372294ms)
    May  9 16:43:24.014: INFO: (10) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 9.652861ms)
    May  9 16:43:24.015: INFO: (10) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 10.263933ms)
    May  9 16:43:24.015: INFO: (10) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 10.52243ms)
    May  9 16:43:24.015: INFO: (10) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 10.328754ms)
    May  9 16:43:24.015: INFO: (10) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 10.258168ms)
    May  9 16:43:24.015: INFO: (10) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 10.4231ms)
    May  9 16:43:24.018: INFO: (10) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 13.03347ms)
    May  9 16:43:24.018: INFO: (10) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 13.410033ms)
    May  9 16:43:24.018: INFO: (10) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 13.681417ms)
    May  9 16:43:24.033: INFO: (11) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 14.372611ms)
    May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 20.237835ms)
    May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 20.076608ms)
    May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 20.124905ms)
    May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 20.196841ms)
    May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 20.2865ms)
    May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 20.327645ms)
    May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 20.293504ms)
    May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 20.195566ms)
    May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 20.236555ms)
    May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 20.374206ms)
    May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 20.243662ms)
    May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 20.295936ms)
    May  9 16:43:24.039: INFO: (11) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 20.360855ms)
    May  9 16:43:24.042: INFO: (11) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 23.049865ms)
    May  9 16:43:24.042: INFO: (11) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 23.400275ms)
    May  9 16:43:24.055: INFO: (12) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 13.12502ms)
    May  9 16:43:24.055: INFO: (12) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 13.234278ms)
    May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 16.606051ms)
    May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 16.690976ms)
    May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 16.73076ms)
    May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 16.707223ms)
    May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 16.750687ms)
    May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 16.860589ms)
    May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 16.789625ms)
    May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 16.866357ms)
    May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 16.801475ms)
    May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 16.845457ms)
    May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 16.753866ms)
    May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 16.869245ms)
    May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 16.772473ms)
    May  9 16:43:24.059: INFO: (12) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 16.860039ms)
    May  9 16:43:24.068: INFO: (13) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 8.970693ms)
    May  9 16:43:24.072: INFO: (13) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 12.352355ms)
    May  9 16:43:24.072: INFO: (13) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 12.483306ms)
    May  9 16:43:24.072: INFO: (13) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 12.602115ms)
    May  9 16:43:24.077: INFO: (13) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 17.921541ms)
    May  9 16:43:24.077: INFO: (13) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 18.040589ms)
    May  9 16:43:24.079: INFO: (13) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 19.3038ms)
    May  9 16:43:24.079: INFO: (13) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 19.281346ms)
    May  9 16:43:24.080: INFO: (13) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 20.257106ms)
    May  9 16:43:24.080: INFO: (13) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 20.299511ms)
    May  9 16:43:24.080: INFO: (13) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 20.408715ms)
    May  9 16:43:24.080: INFO: (13) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 20.323034ms)
    May  9 16:43:24.080: INFO: (13) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 20.514221ms)
    May  9 16:43:24.080: INFO: (13) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 20.602461ms)
    May  9 16:43:24.083: INFO: (13) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 23.725327ms)
    May  9 16:43:24.087: INFO: (13) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 27.651667ms)
    May  9 16:43:24.093: INFO: (14) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 5.887624ms)
    May  9 16:43:24.093: INFO: (14) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 5.861455ms)
    May  9 16:43:24.094: INFO: (14) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 6.598429ms)
    May  9 16:43:24.094: INFO: (14) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 6.530758ms)
    May  9 16:43:24.099: INFO: (14) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 11.668032ms)
    May  9 16:43:24.099: INFO: (14) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 12.052107ms)
    May  9 16:43:24.099: INFO: (14) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 12.310551ms)
    May  9 16:43:24.103: INFO: (14) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 15.484591ms)
    May  9 16:43:24.103: INFO: (14) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 15.806172ms)
    May  9 16:43:24.103: INFO: (14) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 15.618202ms)
    May  9 16:43:24.103: INFO: (14) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 15.492524ms)
    May  9 16:43:24.103: INFO: (14) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 15.589556ms)
    May  9 16:43:24.103: INFO: (14) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 15.77019ms)
    May  9 16:43:24.103: INFO: (14) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 16.230898ms)
    May  9 16:43:24.104: INFO: (14) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 16.443811ms)
    May  9 16:43:24.104: INFO: (14) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 16.279683ms)
    May  9 16:43:24.112: INFO: (15) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 8.25913ms)
    May  9 16:43:24.112: INFO: (15) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 8.282573ms)
    May  9 16:43:24.112: INFO: (15) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 8.195162ms)
    May  9 16:43:24.113: INFO: (15) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 9.005501ms)
    May  9 16:43:24.113: INFO: (15) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 9.08477ms)
    May  9 16:43:24.114: INFO: (15) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 10.148414ms)
    May  9 16:43:24.114: INFO: (15) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 10.179676ms)
    May  9 16:43:24.114: INFO: (15) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 10.251288ms)
    May  9 16:43:24.114: INFO: (15) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 10.318523ms)
    May  9 16:43:24.114: INFO: (15) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 10.104291ms)
    May  9 16:43:24.114: INFO: (15) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 10.319754ms)
    May  9 16:43:24.118: INFO: (15) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 14.032467ms)
    May  9 16:43:24.118: INFO: (15) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 13.971574ms)
    May  9 16:43:24.118: INFO: (15) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 13.985079ms)
    May  9 16:43:24.118: INFO: (15) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 14.143122ms)
    May  9 16:43:24.118: INFO: (15) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 14.00187ms)
    May  9 16:43:24.128: INFO: (16) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 10.088288ms)
    May  9 16:43:24.128: INFO: (16) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 10.086695ms)
    May  9 16:43:24.128: INFO: (16) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 10.039769ms)
    May  9 16:43:24.128: INFO: (16) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 10.175453ms)
    May  9 16:43:24.128: INFO: (16) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 10.124843ms)
    May  9 16:43:24.128: INFO: (16) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 10.148543ms)
    May  9 16:43:24.128: INFO: (16) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 10.307637ms)
    May  9 16:43:24.133: INFO: (16) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 15.400275ms)
    May  9 16:43:24.133: INFO: (16) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 15.506489ms)
    May  9 16:43:24.133: INFO: (16) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 15.392356ms)
    May  9 16:43:24.133: INFO: (16) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 15.447747ms)
    May  9 16:43:24.135: INFO: (16) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 17.322342ms)
    May  9 16:43:24.135: INFO: (16) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 17.218242ms)
    May  9 16:43:24.135: INFO: (16) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 17.254386ms)
    May  9 16:43:24.135: INFO: (16) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 17.308443ms)
    May  9 16:43:24.135: INFO: (16) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 17.170614ms)
    May  9 16:43:24.161: INFO: (17) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 25.083906ms)
    May  9 16:43:24.161: INFO: (17) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 25.237391ms)
    May  9 16:43:24.161: INFO: (17) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 25.266948ms)
    May  9 16:43:24.161: INFO: (17) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 25.334292ms)
    May  9 16:43:24.161: INFO: (17) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 25.33709ms)
    May  9 16:43:24.161: INFO: (17) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 25.404256ms)
    May  9 16:43:24.161: INFO: (17) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 25.396302ms)
    May  9 16:43:24.162: INFO: (17) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 26.812555ms)
    May  9 16:43:24.162: INFO: (17) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 26.883353ms)
    May  9 16:43:24.163: INFO: (17) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 27.711903ms)
    May  9 16:43:24.163: INFO: (17) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 27.834518ms)
    May  9 16:43:24.163: INFO: (17) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 27.712382ms)
    May  9 16:43:24.163: INFO: (17) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 27.785068ms)
    May  9 16:43:24.164: INFO: (17) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 28.479119ms)
    May  9 16:43:24.164: INFO: (17) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 28.566685ms)
    May  9 16:43:24.164: INFO: (17) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 28.509921ms)
    May  9 16:43:24.172: INFO: (18) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 7.23894ms)
    May  9 16:43:24.175: INFO: (18) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 10.205449ms)
    May  9 16:43:24.175: INFO: (18) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 10.331436ms)
    May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 11.476204ms)
    May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 11.442244ms)
    May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 11.611338ms)
    May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 11.577058ms)
    May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 11.468793ms)
    May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 11.659842ms)
    May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 11.561332ms)
    May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 11.560469ms)
    May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 12.020152ms)
    May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 11.903017ms)
    May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 12.111343ms)
    May  9 16:43:24.176: INFO: (18) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 11.977257ms)
    May  9 16:43:24.183: INFO: (18) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 18.432309ms)
    May  9 16:43:24.193: INFO: (19) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:462/proxy/: tls qux (200; 10.413316ms)
    May  9 16:43:24.193: INFO: (19) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh/proxy/rewriteme">test</a> (200; 10.348657ms)
    May  9 16:43:24.193: INFO: (19) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname1/proxy/: foo (200; 10.561896ms)
    May  9 16:43:24.193: INFO: (19) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">test<... (200; 10.579645ms)
    May  9 16:43:24.193: INFO: (19) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 10.483381ms)
    May  9 16:43:24.193: INFO: (19) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:443/proxy/tlsrewritem... (200; 10.53261ms)
    May  9 16:43:24.194: INFO: (19) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:162/proxy/: bar (200; 10.463029ms)
    May  9 16:43:24.194: INFO: (19) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:1080/proxy/rewriteme">... (200; 10.408103ms)
    May  9 16:43:24.194: INFO: (19) /api/v1/namespaces/proxy-7234/pods/https:proxy-service-zffb7-zvrqh:460/proxy/: tls baz (200; 10.708944ms)
    May  9 16:43:24.195: INFO: (19) /api/v1/namespaces/proxy-7234/pods/proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 12.029162ms)
    May  9 16:43:24.195: INFO: (19) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname2/proxy/: tls qux (200; 12.246033ms)
    May  9 16:43:24.195: INFO: (19) /api/v1/namespaces/proxy-7234/pods/http:proxy-service-zffb7-zvrqh:160/proxy/: foo (200; 12.276994ms)
    May  9 16:43:24.196: INFO: (19) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname1/proxy/: foo (200; 12.557033ms)
    May  9 16:43:24.196: INFO: (19) /api/v1/namespaces/proxy-7234/services/proxy-service-zffb7:portname2/proxy/: bar (200; 12.745443ms)
    May  9 16:43:24.196: INFO: (19) /api/v1/namespaces/proxy-7234/services/http:proxy-service-zffb7:portname2/proxy/: bar (200; 13.262154ms)
    May  9 16:43:24.196: INFO: (19) /api/v1/namespaces/proxy-7234/services/https:proxy-service-zffb7:tlsportname1/proxy/: tls baz (200; 13.390969ms)
    STEP: deleting ReplicationController proxy-service-zffb7 in namespace proxy-7234, will wait for the garbage collector to delete the pods 05/09/23 16:43:24.196
    May  9 16:43:24.260: INFO: Deleting ReplicationController proxy-service-zffb7 took: 8.27193ms
    May  9 16:43:24.361: INFO: Terminating ReplicationController proxy-service-zffb7 pods took: 100.957027ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    May  9 16:43:26.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-7234" for this suite. 05/09/23 16:43:26.67
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:43:26.678
May  9 16:43:26.678: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename var-expansion 05/09/23 16:43:26.68
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:43:26.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:43:26.702
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 05/09/23 16:43:26.706
May  9 16:43:26.728: INFO: Waiting up to 5m0s for pod "var-expansion-625190a2-e1aa-4c74-87e8-9a6cd01f8417" in namespace "var-expansion-8132" to be "Succeeded or Failed"
May  9 16:43:26.786: INFO: Pod "var-expansion-625190a2-e1aa-4c74-87e8-9a6cd01f8417": Phase="Pending", Reason="", readiness=false. Elapsed: 58.346753ms
May  9 16:43:28.794: INFO: Pod "var-expansion-625190a2-e1aa-4c74-87e8-9a6cd01f8417": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066262052s
May  9 16:43:30.793: INFO: Pod "var-expansion-625190a2-e1aa-4c74-87e8-9a6cd01f8417": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065315512s
STEP: Saw pod success 05/09/23 16:43:30.793
May  9 16:43:30.793: INFO: Pod "var-expansion-625190a2-e1aa-4c74-87e8-9a6cd01f8417" satisfied condition "Succeeded or Failed"
May  9 16:43:30.799: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod var-expansion-625190a2-e1aa-4c74-87e8-9a6cd01f8417 container dapi-container: <nil>
STEP: delete the pod 05/09/23 16:43:30.811
May  9 16:43:30.828: INFO: Waiting for pod var-expansion-625190a2-e1aa-4c74-87e8-9a6cd01f8417 to disappear
May  9 16:43:30.834: INFO: Pod var-expansion-625190a2-e1aa-4c74-87e8-9a6cd01f8417 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May  9 16:43:30.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8132" for this suite. 05/09/23 16:43:30.844
------------------------------
â€¢ [4.175 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:43:26.678
    May  9 16:43:26.678: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename var-expansion 05/09/23 16:43:26.68
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:43:26.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:43:26.702
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 05/09/23 16:43:26.706
    May  9 16:43:26.728: INFO: Waiting up to 5m0s for pod "var-expansion-625190a2-e1aa-4c74-87e8-9a6cd01f8417" in namespace "var-expansion-8132" to be "Succeeded or Failed"
    May  9 16:43:26.786: INFO: Pod "var-expansion-625190a2-e1aa-4c74-87e8-9a6cd01f8417": Phase="Pending", Reason="", readiness=false. Elapsed: 58.346753ms
    May  9 16:43:28.794: INFO: Pod "var-expansion-625190a2-e1aa-4c74-87e8-9a6cd01f8417": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066262052s
    May  9 16:43:30.793: INFO: Pod "var-expansion-625190a2-e1aa-4c74-87e8-9a6cd01f8417": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065315512s
    STEP: Saw pod success 05/09/23 16:43:30.793
    May  9 16:43:30.793: INFO: Pod "var-expansion-625190a2-e1aa-4c74-87e8-9a6cd01f8417" satisfied condition "Succeeded or Failed"
    May  9 16:43:30.799: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod var-expansion-625190a2-e1aa-4c74-87e8-9a6cd01f8417 container dapi-container: <nil>
    STEP: delete the pod 05/09/23 16:43:30.811
    May  9 16:43:30.828: INFO: Waiting for pod var-expansion-625190a2-e1aa-4c74-87e8-9a6cd01f8417 to disappear
    May  9 16:43:30.834: INFO: Pod var-expansion-625190a2-e1aa-4c74-87e8-9a6cd01f8417 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May  9 16:43:30.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8132" for this suite. 05/09/23 16:43:30.844
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:43:30.854
May  9 16:43:30.854: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename svcaccounts 05/09/23 16:43:30.856
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:43:30.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:43:30.884
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
May  9 16:43:30.906: INFO: created pod
May  9 16:43:30.906: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6761" to be "Succeeded or Failed"
May  9 16:43:30.913: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.811903ms
May  9 16:43:32.920: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013764858s
May  9 16:43:34.924: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017877845s
STEP: Saw pod success 05/09/23 16:43:34.924
May  9 16:43:34.924: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
May  9 16:44:04.925: INFO: polling logs
May  9 16:44:04.943: INFO: Pod logs: 
I0509 16:43:31.959523       1 log.go:198] OK: Got token
I0509 16:43:31.959561       1 log.go:198] validating with in-cluster discovery
I0509 16:43:31.959878       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0509 16:43:31.959908       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6761:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683651211, NotBefore:1683650611, IssuedAt:1683650611, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6761", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a054058c-4d04-46e1-853c-fc5fea30cf06"}}}
I0509 16:43:31.993458       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0509 16:43:32.022853       1 log.go:198] OK: Validated signature on JWT
I0509 16:43:32.022991       1 log.go:198] OK: Got valid claims from token!
I0509 16:43:32.023028       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6761:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683651211, NotBefore:1683650611, IssuedAt:1683650611, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6761", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a054058c-4d04-46e1-853c-fc5fea30cf06"}}}

May  9 16:44:04.943: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May  9 16:44:04.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6761" for this suite. 05/09/23 16:44:04.959
------------------------------
â€¢ [SLOW TEST] [34.114 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:43:30.854
    May  9 16:43:30.854: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename svcaccounts 05/09/23 16:43:30.856
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:43:30.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:43:30.884
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    May  9 16:43:30.906: INFO: created pod
    May  9 16:43:30.906: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6761" to be "Succeeded or Failed"
    May  9 16:43:30.913: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.811903ms
    May  9 16:43:32.920: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013764858s
    May  9 16:43:34.924: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017877845s
    STEP: Saw pod success 05/09/23 16:43:34.924
    May  9 16:43:34.924: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    May  9 16:44:04.925: INFO: polling logs
    May  9 16:44:04.943: INFO: Pod logs: 
    I0509 16:43:31.959523       1 log.go:198] OK: Got token
    I0509 16:43:31.959561       1 log.go:198] validating with in-cluster discovery
    I0509 16:43:31.959878       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0509 16:43:31.959908       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6761:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683651211, NotBefore:1683650611, IssuedAt:1683650611, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6761", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a054058c-4d04-46e1-853c-fc5fea30cf06"}}}
    I0509 16:43:31.993458       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0509 16:43:32.022853       1 log.go:198] OK: Validated signature on JWT
    I0509 16:43:32.022991       1 log.go:198] OK: Got valid claims from token!
    I0509 16:43:32.023028       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6761:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683651211, NotBefore:1683650611, IssuedAt:1683650611, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6761", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a054058c-4d04-46e1-853c-fc5fea30cf06"}}}

    May  9 16:44:04.943: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May  9 16:44:04.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6761" for this suite. 05/09/23 16:44:04.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:44:04.968
May  9 16:44:04.968: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename pods 05/09/23 16:44:04.969
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:44:04.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:44:05
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
May  9 16:44:05.026: INFO: Waiting up to 5m0s for pod "server-envvars-5174b66b-5c56-4cf7-93a7-072355a9c5c3" in namespace "pods-7563" to be "running and ready"
May  9 16:44:05.035: INFO: Pod "server-envvars-5174b66b-5c56-4cf7-93a7-072355a9c5c3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.055446ms
May  9 16:44:05.035: INFO: The phase of Pod server-envvars-5174b66b-5c56-4cf7-93a7-072355a9c5c3 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:44:07.042: INFO: Pod "server-envvars-5174b66b-5c56-4cf7-93a7-072355a9c5c3": Phase="Running", Reason="", readiness=true. Elapsed: 2.015847362s
May  9 16:44:07.042: INFO: The phase of Pod server-envvars-5174b66b-5c56-4cf7-93a7-072355a9c5c3 is Running (Ready = true)
May  9 16:44:07.042: INFO: Pod "server-envvars-5174b66b-5c56-4cf7-93a7-072355a9c5c3" satisfied condition "running and ready"
May  9 16:44:07.069: INFO: Waiting up to 5m0s for pod "client-envvars-88a832c3-8841-4b38-8d25-03d163867270" in namespace "pods-7563" to be "Succeeded or Failed"
May  9 16:44:07.073: INFO: Pod "client-envvars-88a832c3-8841-4b38-8d25-03d163867270": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027696ms
May  9 16:44:09.097: INFO: Pod "client-envvars-88a832c3-8841-4b38-8d25-03d163867270": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02803325s
May  9 16:44:11.080: INFO: Pod "client-envvars-88a832c3-8841-4b38-8d25-03d163867270": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011192088s
STEP: Saw pod success 05/09/23 16:44:11.08
May  9 16:44:11.080: INFO: Pod "client-envvars-88a832c3-8841-4b38-8d25-03d163867270" satisfied condition "Succeeded or Failed"
May  9 16:44:11.086: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod client-envvars-88a832c3-8841-4b38-8d25-03d163867270 container env3cont: <nil>
STEP: delete the pod 05/09/23 16:44:11.098
May  9 16:44:11.116: INFO: Waiting for pod client-envvars-88a832c3-8841-4b38-8d25-03d163867270 to disappear
May  9 16:44:11.122: INFO: Pod client-envvars-88a832c3-8841-4b38-8d25-03d163867270 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  9 16:44:11.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7563" for this suite. 05/09/23 16:44:11.131
------------------------------
â€¢ [SLOW TEST] [6.174 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:44:04.968
    May  9 16:44:04.968: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename pods 05/09/23 16:44:04.969
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:44:04.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:44:05
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    May  9 16:44:05.026: INFO: Waiting up to 5m0s for pod "server-envvars-5174b66b-5c56-4cf7-93a7-072355a9c5c3" in namespace "pods-7563" to be "running and ready"
    May  9 16:44:05.035: INFO: Pod "server-envvars-5174b66b-5c56-4cf7-93a7-072355a9c5c3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.055446ms
    May  9 16:44:05.035: INFO: The phase of Pod server-envvars-5174b66b-5c56-4cf7-93a7-072355a9c5c3 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:44:07.042: INFO: Pod "server-envvars-5174b66b-5c56-4cf7-93a7-072355a9c5c3": Phase="Running", Reason="", readiness=true. Elapsed: 2.015847362s
    May  9 16:44:07.042: INFO: The phase of Pod server-envvars-5174b66b-5c56-4cf7-93a7-072355a9c5c3 is Running (Ready = true)
    May  9 16:44:07.042: INFO: Pod "server-envvars-5174b66b-5c56-4cf7-93a7-072355a9c5c3" satisfied condition "running and ready"
    May  9 16:44:07.069: INFO: Waiting up to 5m0s for pod "client-envvars-88a832c3-8841-4b38-8d25-03d163867270" in namespace "pods-7563" to be "Succeeded or Failed"
    May  9 16:44:07.073: INFO: Pod "client-envvars-88a832c3-8841-4b38-8d25-03d163867270": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027696ms
    May  9 16:44:09.097: INFO: Pod "client-envvars-88a832c3-8841-4b38-8d25-03d163867270": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02803325s
    May  9 16:44:11.080: INFO: Pod "client-envvars-88a832c3-8841-4b38-8d25-03d163867270": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011192088s
    STEP: Saw pod success 05/09/23 16:44:11.08
    May  9 16:44:11.080: INFO: Pod "client-envvars-88a832c3-8841-4b38-8d25-03d163867270" satisfied condition "Succeeded or Failed"
    May  9 16:44:11.086: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod client-envvars-88a832c3-8841-4b38-8d25-03d163867270 container env3cont: <nil>
    STEP: delete the pod 05/09/23 16:44:11.098
    May  9 16:44:11.116: INFO: Waiting for pod client-envvars-88a832c3-8841-4b38-8d25-03d163867270 to disappear
    May  9 16:44:11.122: INFO: Pod client-envvars-88a832c3-8841-4b38-8d25-03d163867270 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  9 16:44:11.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7563" for this suite. 05/09/23 16:44:11.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:44:11.143
May  9 16:44:11.143: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 16:44:11.144
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:44:11.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:44:11.17
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-0f06ce23-52ed-4df3-a86c-1df63ae7f249 05/09/23 16:44:11.181
STEP: Creating configMap with name cm-test-opt-upd-5365455d-199b-4ab5-81c6-188c08a4e71b 05/09/23 16:44:11.187
STEP: Creating the pod 05/09/23 16:44:11.194
May  9 16:44:11.206: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-be2d914b-194d-4da6-8800-e5c34bed7e35" in namespace "projected-2191" to be "running and ready"
May  9 16:44:11.217: INFO: Pod "pod-projected-configmaps-be2d914b-194d-4da6-8800-e5c34bed7e35": Phase="Pending", Reason="", readiness=false. Elapsed: 10.651462ms
May  9 16:44:11.217: INFO: The phase of Pod pod-projected-configmaps-be2d914b-194d-4da6-8800-e5c34bed7e35 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:44:13.225: INFO: Pod "pod-projected-configmaps-be2d914b-194d-4da6-8800-e5c34bed7e35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018983582s
May  9 16:44:13.225: INFO: The phase of Pod pod-projected-configmaps-be2d914b-194d-4da6-8800-e5c34bed7e35 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:44:15.246: INFO: Pod "pod-projected-configmaps-be2d914b-194d-4da6-8800-e5c34bed7e35": Phase="Running", Reason="", readiness=true. Elapsed: 4.040224933s
May  9 16:44:15.246: INFO: The phase of Pod pod-projected-configmaps-be2d914b-194d-4da6-8800-e5c34bed7e35 is Running (Ready = true)
May  9 16:44:15.246: INFO: Pod "pod-projected-configmaps-be2d914b-194d-4da6-8800-e5c34bed7e35" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-0f06ce23-52ed-4df3-a86c-1df63ae7f249 05/09/23 16:44:15.285
STEP: Updating configmap cm-test-opt-upd-5365455d-199b-4ab5-81c6-188c08a4e71b 05/09/23 16:44:15.295
STEP: Creating configMap with name cm-test-opt-create-078d3a74-eb29-41c4-ab92-b1ad1cf3dcfe 05/09/23 16:44:15.303
STEP: waiting to observe update in volume 05/09/23 16:44:15.31
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May  9 16:45:34.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2191" for this suite. 05/09/23 16:45:34.16
------------------------------
â€¢ [SLOW TEST] [83.027 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:44:11.143
    May  9 16:44:11.143: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 16:44:11.144
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:44:11.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:44:11.17
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-0f06ce23-52ed-4df3-a86c-1df63ae7f249 05/09/23 16:44:11.181
    STEP: Creating configMap with name cm-test-opt-upd-5365455d-199b-4ab5-81c6-188c08a4e71b 05/09/23 16:44:11.187
    STEP: Creating the pod 05/09/23 16:44:11.194
    May  9 16:44:11.206: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-be2d914b-194d-4da6-8800-e5c34bed7e35" in namespace "projected-2191" to be "running and ready"
    May  9 16:44:11.217: INFO: Pod "pod-projected-configmaps-be2d914b-194d-4da6-8800-e5c34bed7e35": Phase="Pending", Reason="", readiness=false. Elapsed: 10.651462ms
    May  9 16:44:11.217: INFO: The phase of Pod pod-projected-configmaps-be2d914b-194d-4da6-8800-e5c34bed7e35 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:44:13.225: INFO: Pod "pod-projected-configmaps-be2d914b-194d-4da6-8800-e5c34bed7e35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018983582s
    May  9 16:44:13.225: INFO: The phase of Pod pod-projected-configmaps-be2d914b-194d-4da6-8800-e5c34bed7e35 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:44:15.246: INFO: Pod "pod-projected-configmaps-be2d914b-194d-4da6-8800-e5c34bed7e35": Phase="Running", Reason="", readiness=true. Elapsed: 4.040224933s
    May  9 16:44:15.246: INFO: The phase of Pod pod-projected-configmaps-be2d914b-194d-4da6-8800-e5c34bed7e35 is Running (Ready = true)
    May  9 16:44:15.246: INFO: Pod "pod-projected-configmaps-be2d914b-194d-4da6-8800-e5c34bed7e35" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-0f06ce23-52ed-4df3-a86c-1df63ae7f249 05/09/23 16:44:15.285
    STEP: Updating configmap cm-test-opt-upd-5365455d-199b-4ab5-81c6-188c08a4e71b 05/09/23 16:44:15.295
    STEP: Creating configMap with name cm-test-opt-create-078d3a74-eb29-41c4-ab92-b1ad1cf3dcfe 05/09/23 16:44:15.303
    STEP: waiting to observe update in volume 05/09/23 16:44:15.31
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May  9 16:45:34.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2191" for this suite. 05/09/23 16:45:34.16
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:45:34.172
May  9 16:45:34.172: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename podtemplate 05/09/23 16:45:34.173
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:45:34.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:45:34.196
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 05/09/23 16:45:34.2
May  9 16:45:34.208: INFO: created test-podtemplate-1
May  9 16:45:34.214: INFO: created test-podtemplate-2
May  9 16:45:34.219: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 05/09/23 16:45:34.219
STEP: delete collection of pod templates 05/09/23 16:45:34.225
May  9 16:45:34.225: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 05/09/23 16:45:34.245
May  9 16:45:34.246: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
May  9 16:45:34.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8402" for this suite. 05/09/23 16:45:34.256
------------------------------
â€¢ [0.093 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:45:34.172
    May  9 16:45:34.172: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename podtemplate 05/09/23 16:45:34.173
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:45:34.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:45:34.196
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 05/09/23 16:45:34.2
    May  9 16:45:34.208: INFO: created test-podtemplate-1
    May  9 16:45:34.214: INFO: created test-podtemplate-2
    May  9 16:45:34.219: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 05/09/23 16:45:34.219
    STEP: delete collection of pod templates 05/09/23 16:45:34.225
    May  9 16:45:34.225: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 05/09/23 16:45:34.245
    May  9 16:45:34.246: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    May  9 16:45:34.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8402" for this suite. 05/09/23 16:45:34.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:45:34.265
May  9 16:45:34.265: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename job 05/09/23 16:45:34.266
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:45:34.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:45:34.284
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 05/09/23 16:45:34.288
STEP: Ensuring active pods == parallelism 05/09/23 16:45:34.295
STEP: delete a job 05/09/23 16:45:36.312
STEP: deleting Job.batch foo in namespace job-8726, will wait for the garbage collector to delete the pods 05/09/23 16:45:36.312
May  9 16:45:36.378: INFO: Deleting Job.batch foo took: 11.273557ms
May  9 16:45:36.478: INFO: Terminating Job.batch foo pods took: 100.594019ms
STEP: Ensuring job was deleted 05/09/23 16:46:09.079
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May  9 16:46:09.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8726" for this suite. 05/09/23 16:46:09.091
------------------------------
â€¢ [SLOW TEST] [34.835 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:45:34.265
    May  9 16:45:34.265: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename job 05/09/23 16:45:34.266
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:45:34.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:45:34.284
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 05/09/23 16:45:34.288
    STEP: Ensuring active pods == parallelism 05/09/23 16:45:34.295
    STEP: delete a job 05/09/23 16:45:36.312
    STEP: deleting Job.batch foo in namespace job-8726, will wait for the garbage collector to delete the pods 05/09/23 16:45:36.312
    May  9 16:45:36.378: INFO: Deleting Job.batch foo took: 11.273557ms
    May  9 16:45:36.478: INFO: Terminating Job.batch foo pods took: 100.594019ms
    STEP: Ensuring job was deleted 05/09/23 16:46:09.079
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May  9 16:46:09.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8726" for this suite. 05/09/23 16:46:09.091
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:46:09.101
May  9 16:46:09.101: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename limitrange 05/09/23 16:46:09.102
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:46:09.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:46:09.121
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-2rsgt" in namespace "limitrange-5973" 05/09/23 16:46:09.129
STEP: Creating another limitRange in another namespace 05/09/23 16:46:09.136
May  9 16:46:09.151: INFO: Namespace "e2e-limitrange-2rsgt-8854" created
May  9 16:46:09.151: INFO: Creating LimitRange "e2e-limitrange-2rsgt" in namespace "e2e-limitrange-2rsgt-8854"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-2rsgt" 05/09/23 16:46:09.16
May  9 16:46:09.166: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-2rsgt" in "limitrange-5973" namespace 05/09/23 16:46:09.166
May  9 16:46:09.174: INFO: LimitRange "e2e-limitrange-2rsgt" has been patched
STEP: Delete LimitRange "e2e-limitrange-2rsgt" by Collection with labelSelector: "e2e-limitrange-2rsgt=patched" 05/09/23 16:46:09.174
STEP: Confirm that the limitRange "e2e-limitrange-2rsgt" has been deleted 05/09/23 16:46:09.185
May  9 16:46:09.185: INFO: Requesting list of LimitRange to confirm quantity
May  9 16:46:09.189: INFO: Found 0 LimitRange with label "e2e-limitrange-2rsgt=patched"
May  9 16:46:09.190: INFO: LimitRange "e2e-limitrange-2rsgt" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-2rsgt" 05/09/23 16:46:09.19
May  9 16:46:09.195: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
May  9 16:46:09.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-5973" for this suite. 05/09/23 16:46:09.201
STEP: Destroying namespace "e2e-limitrange-2rsgt-8854" for this suite. 05/09/23 16:46:09.211
------------------------------
â€¢ [0.119 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:46:09.101
    May  9 16:46:09.101: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename limitrange 05/09/23 16:46:09.102
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:46:09.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:46:09.121
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-2rsgt" in namespace "limitrange-5973" 05/09/23 16:46:09.129
    STEP: Creating another limitRange in another namespace 05/09/23 16:46:09.136
    May  9 16:46:09.151: INFO: Namespace "e2e-limitrange-2rsgt-8854" created
    May  9 16:46:09.151: INFO: Creating LimitRange "e2e-limitrange-2rsgt" in namespace "e2e-limitrange-2rsgt-8854"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-2rsgt" 05/09/23 16:46:09.16
    May  9 16:46:09.166: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-2rsgt" in "limitrange-5973" namespace 05/09/23 16:46:09.166
    May  9 16:46:09.174: INFO: LimitRange "e2e-limitrange-2rsgt" has been patched
    STEP: Delete LimitRange "e2e-limitrange-2rsgt" by Collection with labelSelector: "e2e-limitrange-2rsgt=patched" 05/09/23 16:46:09.174
    STEP: Confirm that the limitRange "e2e-limitrange-2rsgt" has been deleted 05/09/23 16:46:09.185
    May  9 16:46:09.185: INFO: Requesting list of LimitRange to confirm quantity
    May  9 16:46:09.189: INFO: Found 0 LimitRange with label "e2e-limitrange-2rsgt=patched"
    May  9 16:46:09.190: INFO: LimitRange "e2e-limitrange-2rsgt" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-2rsgt" 05/09/23 16:46:09.19
    May  9 16:46:09.195: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    May  9 16:46:09.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-5973" for this suite. 05/09/23 16:46:09.201
    STEP: Destroying namespace "e2e-limitrange-2rsgt-8854" for this suite. 05/09/23 16:46:09.211
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:46:09.22
May  9 16:46:09.220: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename svc-latency 05/09/23 16:46:09.221
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:46:09.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:46:09.246
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
May  9 16:46:09.251: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5827 05/09/23 16:46:09.252
I0509 16:46:09.259199      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5827, replica count: 1
I0509 16:46:10.309671      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0509 16:46:11.310626      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 16:46:11.427: INFO: Created: latency-svc-qbp87
May  9 16:46:11.434: INFO: Got endpoints: latency-svc-qbp87 [23.030268ms]
May  9 16:46:11.448: INFO: Created: latency-svc-5vr4v
May  9 16:46:11.454: INFO: Got endpoints: latency-svc-5vr4v [19.957502ms]
May  9 16:46:11.457: INFO: Created: latency-svc-qpkxd
May  9 16:46:11.465: INFO: Created: latency-svc-mdtg6
May  9 16:46:11.465: INFO: Got endpoints: latency-svc-qpkxd [30.702614ms]
May  9 16:46:11.471: INFO: Got endpoints: latency-svc-mdtg6 [37.062093ms]
May  9 16:46:11.474: INFO: Created: latency-svc-4r52v
May  9 16:46:11.480: INFO: Got endpoints: latency-svc-4r52v [45.78423ms]
May  9 16:46:11.482: INFO: Created: latency-svc-c9wzn
May  9 16:46:11.488: INFO: Got endpoints: latency-svc-c9wzn [54.188359ms]
May  9 16:46:11.492: INFO: Created: latency-svc-hvkcm
May  9 16:46:11.498: INFO: Got endpoints: latency-svc-hvkcm [63.824026ms]
May  9 16:46:11.498: INFO: Created: latency-svc-vhwrx
May  9 16:46:11.502: INFO: Got endpoints: latency-svc-vhwrx [68.258526ms]
May  9 16:46:11.506: INFO: Created: latency-svc-x4v6f
May  9 16:46:11.510: INFO: Got endpoints: latency-svc-x4v6f [75.88395ms]
May  9 16:46:11.513: INFO: Created: latency-svc-bqzqx
May  9 16:46:11.515: INFO: Got endpoints: latency-svc-bqzqx [81.095838ms]
May  9 16:46:11.521: INFO: Created: latency-svc-rzbxv
May  9 16:46:11.528: INFO: Got endpoints: latency-svc-rzbxv [93.232104ms]
May  9 16:46:11.531: INFO: Created: latency-svc-bnns8
May  9 16:46:11.542: INFO: Got endpoints: latency-svc-bnns8 [107.96074ms]
May  9 16:46:11.543: INFO: Created: latency-svc-r96g2
May  9 16:46:11.547: INFO: Got endpoints: latency-svc-r96g2 [113.075712ms]
May  9 16:46:11.560: INFO: Created: latency-svc-ld9gw
May  9 16:46:11.568: INFO: Created: latency-svc-gfk4w
May  9 16:46:11.570: INFO: Got endpoints: latency-svc-ld9gw [136.243356ms]
May  9 16:46:11.574: INFO: Got endpoints: latency-svc-gfk4w [139.506473ms]
May  9 16:46:11.585: INFO: Created: latency-svc-qf5cw
May  9 16:46:11.590: INFO: Got endpoints: latency-svc-qf5cw [155.916827ms]
May  9 16:46:11.601: INFO: Created: latency-svc-xnwcr
May  9 16:46:11.603: INFO: Created: latency-svc-62j8t
May  9 16:46:11.604: INFO: Got endpoints: latency-svc-62j8t [149.801079ms]
May  9 16:46:11.610: INFO: Created: latency-svc-nsbnm
May  9 16:46:11.613: INFO: Got endpoints: latency-svc-xnwcr [147.315296ms]
May  9 16:46:11.619: INFO: Got endpoints: latency-svc-nsbnm [147.669371ms]
May  9 16:46:11.622: INFO: Created: latency-svc-k2cqn
May  9 16:46:11.629: INFO: Got endpoints: latency-svc-k2cqn [149.303231ms]
May  9 16:46:11.632: INFO: Created: latency-svc-c789v
May  9 16:46:11.635: INFO: Created: latency-svc-zrcc9
May  9 16:46:11.636: INFO: Got endpoints: latency-svc-c789v [147.864967ms]
May  9 16:46:11.649: INFO: Created: latency-svc-xqf48
May  9 16:46:11.649: INFO: Got endpoints: latency-svc-zrcc9 [151.152078ms]
May  9 16:46:11.659: INFO: Got endpoints: latency-svc-xqf48 [156.734122ms]
May  9 16:46:11.660: INFO: Created: latency-svc-kk7mf
May  9 16:46:11.664: INFO: Got endpoints: latency-svc-kk7mf [154.329068ms]
May  9 16:46:11.667: INFO: Created: latency-svc-pdf9r
May  9 16:46:11.673: INFO: Created: latency-svc-w2cj2
May  9 16:46:11.674: INFO: Got endpoints: latency-svc-pdf9r [158.111192ms]
May  9 16:46:11.679: INFO: Got endpoints: latency-svc-w2cj2 [151.032692ms]
May  9 16:46:11.684: INFO: Created: latency-svc-ns6g9
May  9 16:46:11.687: INFO: Got endpoints: latency-svc-ns6g9 [144.756767ms]
May  9 16:46:11.688: INFO: Created: latency-svc-ljfln
May  9 16:46:11.692: INFO: Got endpoints: latency-svc-ljfln [144.743184ms]
May  9 16:46:11.695: INFO: Created: latency-svc-qhjqt
May  9 16:46:11.702: INFO: Created: latency-svc-zs5wr
May  9 16:46:11.708: INFO: Got endpoints: latency-svc-qhjqt [138.0533ms]
May  9 16:46:11.708: INFO: Got endpoints: latency-svc-zs5wr [134.599115ms]
May  9 16:46:11.710: INFO: Created: latency-svc-ww7sl
May  9 16:46:11.714: INFO: Got endpoints: latency-svc-ww7sl [124.312813ms]
May  9 16:46:11.727: INFO: Created: latency-svc-tnx9f
May  9 16:46:11.732: INFO: Got endpoints: latency-svc-tnx9f [128.422317ms]
May  9 16:46:11.735: INFO: Created: latency-svc-tsjgx
May  9 16:46:11.739: INFO: Got endpoints: latency-svc-tsjgx [126.803401ms]
May  9 16:46:11.740: INFO: Created: latency-svc-r76q2
May  9 16:46:11.764: INFO: Got endpoints: latency-svc-r76q2 [145.132309ms]
May  9 16:46:11.768: INFO: Created: latency-svc-hp89b
May  9 16:46:11.776: INFO: Got endpoints: latency-svc-hp89b [146.586596ms]
May  9 16:46:11.778: INFO: Created: latency-svc-jw2gx
May  9 16:46:11.784: INFO: Got endpoints: latency-svc-jw2gx [147.391173ms]
May  9 16:46:11.786: INFO: Created: latency-svc-s4hmn
May  9 16:46:11.790: INFO: Got endpoints: latency-svc-s4hmn [140.42651ms]
May  9 16:46:11.792: INFO: Created: latency-svc-rgdg9
May  9 16:46:11.799: INFO: Created: latency-svc-8ksbd
May  9 16:46:11.811: INFO: Created: latency-svc-qfwb9
May  9 16:46:11.813: INFO: Created: latency-svc-kbdp8
May  9 16:46:11.819: INFO: Created: latency-svc-shvvp
May  9 16:46:11.829: INFO: Created: latency-svc-v5mvg
May  9 16:46:11.832: INFO: Got endpoints: latency-svc-rgdg9 [173.151945ms]
May  9 16:46:11.835: INFO: Created: latency-svc-xkprp
May  9 16:46:11.845: INFO: Created: latency-svc-7m4x2
May  9 16:46:11.852: INFO: Created: latency-svc-wc9zv
May  9 16:46:11.859: INFO: Created: latency-svc-tq9sw
May  9 16:46:11.863: INFO: Created: latency-svc-bd4zq
May  9 16:46:11.871: INFO: Created: latency-svc-kl4zq
May  9 16:46:11.877: INFO: Created: latency-svc-86swz
May  9 16:46:11.884: INFO: Created: latency-svc-px7r4
May  9 16:46:11.886: INFO: Got endpoints: latency-svc-8ksbd [221.418884ms]
May  9 16:46:11.896: INFO: Created: latency-svc-z5zrf
May  9 16:46:11.903: INFO: Created: latency-svc-fjk6d
May  9 16:46:11.910: INFO: Created: latency-svc-kc9p9
May  9 16:46:11.933: INFO: Got endpoints: latency-svc-qfwb9 [259.681455ms]
May  9 16:46:11.948: INFO: Created: latency-svc-4xv6b
May  9 16:46:11.983: INFO: Got endpoints: latency-svc-kbdp8 [304.264594ms]
May  9 16:46:12.002: INFO: Created: latency-svc-5ltj4
May  9 16:46:12.034: INFO: Got endpoints: latency-svc-shvvp [346.510277ms]
May  9 16:46:12.048: INFO: Created: latency-svc-zxg6n
May  9 16:46:12.083: INFO: Got endpoints: latency-svc-v5mvg [390.464989ms]
May  9 16:46:12.096: INFO: Created: latency-svc-r8ltr
May  9 16:46:12.140: INFO: Got endpoints: latency-svc-xkprp [431.999056ms]
May  9 16:46:12.158: INFO: Created: latency-svc-s9wcc
May  9 16:46:12.184: INFO: Got endpoints: latency-svc-7m4x2 [475.140995ms]
May  9 16:46:12.197: INFO: Created: latency-svc-xch99
May  9 16:46:12.244: INFO: Got endpoints: latency-svc-wc9zv [529.790152ms]
May  9 16:46:12.258: INFO: Created: latency-svc-pzlrd
May  9 16:46:12.285: INFO: Got endpoints: latency-svc-tq9sw [552.789226ms]
May  9 16:46:12.299: INFO: Created: latency-svc-2gs2b
May  9 16:46:12.337: INFO: Got endpoints: latency-svc-bd4zq [597.97309ms]
May  9 16:46:12.358: INFO: Created: latency-svc-vkk4f
May  9 16:46:12.384: INFO: Got endpoints: latency-svc-kl4zq [619.835602ms]
May  9 16:46:12.398: INFO: Created: latency-svc-vfpzg
May  9 16:46:12.445: INFO: Got endpoints: latency-svc-86swz [669.701779ms]
May  9 16:46:12.460: INFO: Created: latency-svc-vkjns
May  9 16:46:12.484: INFO: Got endpoints: latency-svc-px7r4 [700.579842ms]
May  9 16:46:12.499: INFO: Created: latency-svc-rdtpf
May  9 16:46:12.535: INFO: Got endpoints: latency-svc-z5zrf [744.839297ms]
May  9 16:46:12.549: INFO: Created: latency-svc-pcwrs
May  9 16:46:12.587: INFO: Got endpoints: latency-svc-fjk6d [754.835634ms]
May  9 16:46:12.612: INFO: Created: latency-svc-xmv96
May  9 16:46:12.634: INFO: Got endpoints: latency-svc-kc9p9 [748.521282ms]
May  9 16:46:12.651: INFO: Created: latency-svc-rmn4p
May  9 16:46:12.683: INFO: Got endpoints: latency-svc-4xv6b [749.777773ms]
May  9 16:46:12.696: INFO: Created: latency-svc-zmvnn
May  9 16:46:12.733: INFO: Got endpoints: latency-svc-5ltj4 [750.281764ms]
May  9 16:46:12.748: INFO: Created: latency-svc-kntbc
May  9 16:46:12.782: INFO: Got endpoints: latency-svc-zxg6n [747.947994ms]
May  9 16:46:12.795: INFO: Created: latency-svc-fgc59
May  9 16:46:12.835: INFO: Got endpoints: latency-svc-r8ltr [751.714021ms]
May  9 16:46:12.855: INFO: Created: latency-svc-rcx65
May  9 16:46:12.883: INFO: Got endpoints: latency-svc-s9wcc [742.396624ms]
May  9 16:46:12.895: INFO: Created: latency-svc-8bgtr
May  9 16:46:12.933: INFO: Got endpoints: latency-svc-xch99 [749.03791ms]
May  9 16:46:12.946: INFO: Created: latency-svc-cm4vn
May  9 16:46:12.983: INFO: Got endpoints: latency-svc-pzlrd [738.557029ms]
May  9 16:46:12.996: INFO: Created: latency-svc-npv64
May  9 16:46:13.034: INFO: Got endpoints: latency-svc-2gs2b [749.041158ms]
May  9 16:46:13.050: INFO: Created: latency-svc-n8mvr
May  9 16:46:13.082: INFO: Got endpoints: latency-svc-vkk4f [744.579535ms]
May  9 16:46:13.098: INFO: Created: latency-svc-grsj2
May  9 16:46:13.135: INFO: Got endpoints: latency-svc-vfpzg [750.665779ms]
May  9 16:46:13.147: INFO: Created: latency-svc-4qb5m
May  9 16:46:13.183: INFO: Got endpoints: latency-svc-vkjns [737.609712ms]
May  9 16:46:13.196: INFO: Created: latency-svc-rfnlq
May  9 16:46:13.232: INFO: Got endpoints: latency-svc-rdtpf [748.104598ms]
May  9 16:46:13.246: INFO: Created: latency-svc-685fp
May  9 16:46:13.290: INFO: Got endpoints: latency-svc-pcwrs [755.259288ms]
May  9 16:46:13.309: INFO: Created: latency-svc-schzz
May  9 16:46:13.333: INFO: Got endpoints: latency-svc-xmv96 [746.312334ms]
May  9 16:46:13.347: INFO: Created: latency-svc-h8ldx
May  9 16:46:13.382: INFO: Got endpoints: latency-svc-rmn4p [747.608915ms]
May  9 16:46:13.394: INFO: Created: latency-svc-vwkn6
May  9 16:46:13.433: INFO: Got endpoints: latency-svc-zmvnn [749.9534ms]
May  9 16:46:13.446: INFO: Created: latency-svc-tflxn
May  9 16:46:13.488: INFO: Got endpoints: latency-svc-kntbc [754.44581ms]
May  9 16:46:13.502: INFO: Created: latency-svc-zgr6n
May  9 16:46:13.532: INFO: Got endpoints: latency-svc-fgc59 [750.430523ms]
May  9 16:46:13.548: INFO: Created: latency-svc-xd2h2
May  9 16:46:13.584: INFO: Got endpoints: latency-svc-rcx65 [749.424557ms]
May  9 16:46:13.597: INFO: Created: latency-svc-8gfbd
May  9 16:46:13.633: INFO: Got endpoints: latency-svc-8bgtr [750.303052ms]
May  9 16:46:13.646: INFO: Created: latency-svc-2wdfq
May  9 16:46:13.683: INFO: Got endpoints: latency-svc-cm4vn [750.736126ms]
May  9 16:46:13.696: INFO: Created: latency-svc-w98xp
May  9 16:46:13.739: INFO: Got endpoints: latency-svc-npv64 [756.307651ms]
May  9 16:46:13.755: INFO: Created: latency-svc-dm5td
May  9 16:46:13.784: INFO: Got endpoints: latency-svc-n8mvr [749.741688ms]
May  9 16:46:13.797: INFO: Created: latency-svc-rgxcn
May  9 16:46:13.834: INFO: Got endpoints: latency-svc-grsj2 [751.618209ms]
May  9 16:46:13.847: INFO: Created: latency-svc-fjj68
May  9 16:46:13.884: INFO: Got endpoints: latency-svc-4qb5m [748.845787ms]
May  9 16:46:13.897: INFO: Created: latency-svc-tv6gs
May  9 16:46:13.953: INFO: Got endpoints: latency-svc-rfnlq [769.593142ms]
May  9 16:46:13.968: INFO: Created: latency-svc-mv784
May  9 16:46:13.984: INFO: Got endpoints: latency-svc-685fp [751.813948ms]
May  9 16:46:13.998: INFO: Created: latency-svc-h87p9
May  9 16:46:14.033: INFO: Got endpoints: latency-svc-schzz [743.044681ms]
May  9 16:46:14.047: INFO: Created: latency-svc-lgc7r
May  9 16:46:14.084: INFO: Got endpoints: latency-svc-h8ldx [750.937173ms]
May  9 16:46:14.100: INFO: Created: latency-svc-ms2kg
May  9 16:46:14.131: INFO: Got endpoints: latency-svc-vwkn6 [749.09178ms]
May  9 16:46:14.159: INFO: Created: latency-svc-7clml
May  9 16:46:14.192: INFO: Got endpoints: latency-svc-tflxn [758.676133ms]
May  9 16:46:14.205: INFO: Created: latency-svc-ngbct
May  9 16:46:14.243: INFO: Got endpoints: latency-svc-zgr6n [754.563768ms]
May  9 16:46:14.255: INFO: Created: latency-svc-46sjn
May  9 16:46:14.365: INFO: Got endpoints: latency-svc-xd2h2 [833.097434ms]
May  9 16:46:14.365: INFO: Got endpoints: latency-svc-8gfbd [781.391918ms]
May  9 16:46:14.377: INFO: Created: latency-svc-z25km
May  9 16:46:14.393: INFO: Got endpoints: latency-svc-2wdfq [759.530435ms]
May  9 16:46:14.396: INFO: Created: latency-svc-6pwpg
May  9 16:46:14.421: INFO: Created: latency-svc-8dl49
May  9 16:46:14.432: INFO: Got endpoints: latency-svc-w98xp [748.528232ms]
May  9 16:46:14.444: INFO: Created: latency-svc-jdb5b
May  9 16:46:14.482: INFO: Got endpoints: latency-svc-dm5td [742.787178ms]
May  9 16:46:14.494: INFO: Created: latency-svc-p6m5b
May  9 16:46:14.534: INFO: Got endpoints: latency-svc-fjj68 [700.363162ms]
May  9 16:46:14.559: INFO: Created: latency-svc-c8jjv
May  9 16:46:14.582: INFO: Got endpoints: latency-svc-rgxcn [798.441299ms]
May  9 16:46:14.624: INFO: Created: latency-svc-blfcd
May  9 16:46:14.633: INFO: Got endpoints: latency-svc-tv6gs [748.792046ms]
May  9 16:46:14.644: INFO: Created: latency-svc-dsp7h
May  9 16:46:14.681: INFO: Got endpoints: latency-svc-mv784 [727.973868ms]
May  9 16:46:14.693: INFO: Created: latency-svc-w9kls
May  9 16:46:14.732: INFO: Got endpoints: latency-svc-h87p9 [747.575201ms]
May  9 16:46:14.744: INFO: Created: latency-svc-f9b9l
May  9 16:46:14.783: INFO: Got endpoints: latency-svc-lgc7r [749.899372ms]
May  9 16:46:14.795: INFO: Created: latency-svc-zggld
May  9 16:46:14.849: INFO: Got endpoints: latency-svc-ms2kg [764.330261ms]
May  9 16:46:14.865: INFO: Created: latency-svc-c4xzv
May  9 16:46:14.884: INFO: Got endpoints: latency-svc-7clml [752.236626ms]
May  9 16:46:14.897: INFO: Created: latency-svc-cfqvf
May  9 16:46:14.934: INFO: Got endpoints: latency-svc-ngbct [741.697877ms]
May  9 16:46:14.947: INFO: Created: latency-svc-knznc
May  9 16:46:14.985: INFO: Got endpoints: latency-svc-46sjn [742.104598ms]
May  9 16:46:14.998: INFO: Created: latency-svc-j4hlx
May  9 16:46:15.036: INFO: Got endpoints: latency-svc-z25km [670.671938ms]
May  9 16:46:15.051: INFO: Created: latency-svc-pc7mf
May  9 16:46:15.087: INFO: Got endpoints: latency-svc-6pwpg [721.215055ms]
May  9 16:46:15.101: INFO: Created: latency-svc-xt4nl
May  9 16:46:15.133: INFO: Got endpoints: latency-svc-8dl49 [739.920422ms]
May  9 16:46:15.147: INFO: Created: latency-svc-7x2ts
May  9 16:46:15.182: INFO: Got endpoints: latency-svc-jdb5b [749.972397ms]
May  9 16:46:15.194: INFO: Created: latency-svc-lnr2d
May  9 16:46:15.234: INFO: Got endpoints: latency-svc-p6m5b [751.365115ms]
May  9 16:46:15.247: INFO: Created: latency-svc-wlhcd
May  9 16:46:15.282: INFO: Got endpoints: latency-svc-c8jjv [747.382767ms]
May  9 16:46:15.294: INFO: Created: latency-svc-zfxt7
May  9 16:46:15.334: INFO: Got endpoints: latency-svc-blfcd [751.611892ms]
May  9 16:46:15.346: INFO: Created: latency-svc-65qht
May  9 16:46:15.384: INFO: Got endpoints: latency-svc-dsp7h [751.540417ms]
May  9 16:46:15.400: INFO: Created: latency-svc-zhbrd
May  9 16:46:15.436: INFO: Got endpoints: latency-svc-w9kls [755.347447ms]
May  9 16:46:15.448: INFO: Created: latency-svc-72stz
May  9 16:46:15.485: INFO: Got endpoints: latency-svc-f9b9l [753.724986ms]
May  9 16:46:15.517: INFO: Created: latency-svc-9vwlv
May  9 16:46:15.536: INFO: Got endpoints: latency-svc-zggld [752.710101ms]
May  9 16:46:15.549: INFO: Created: latency-svc-4dvnb
May  9 16:46:15.582: INFO: Got endpoints: latency-svc-c4xzv [732.655908ms]
May  9 16:46:15.594: INFO: Created: latency-svc-smkj8
May  9 16:46:15.639: INFO: Got endpoints: latency-svc-cfqvf [755.020846ms]
May  9 16:46:15.652: INFO: Created: latency-svc-bmjzd
May  9 16:46:15.692: INFO: Got endpoints: latency-svc-knznc [758.570969ms]
May  9 16:46:15.709: INFO: Created: latency-svc-s7xn2
May  9 16:46:15.736: INFO: Got endpoints: latency-svc-j4hlx [751.276844ms]
May  9 16:46:15.750: INFO: Created: latency-svc-f6rb9
May  9 16:46:15.783: INFO: Got endpoints: latency-svc-pc7mf [746.679044ms]
May  9 16:46:15.794: INFO: Created: latency-svc-kzds8
May  9 16:46:15.833: INFO: Got endpoints: latency-svc-xt4nl [746.637266ms]
May  9 16:46:15.847: INFO: Created: latency-svc-vfhrn
May  9 16:46:15.881: INFO: Got endpoints: latency-svc-7x2ts [748.687102ms]
May  9 16:46:15.893: INFO: Created: latency-svc-vrvr9
May  9 16:46:15.933: INFO: Got endpoints: latency-svc-lnr2d [750.890356ms]
May  9 16:46:15.945: INFO: Created: latency-svc-jjrxf
May  9 16:46:15.983: INFO: Got endpoints: latency-svc-wlhcd [749.550043ms]
May  9 16:46:16.000: INFO: Created: latency-svc-nm9d4
May  9 16:46:16.035: INFO: Got endpoints: latency-svc-zfxt7 [753.424422ms]
May  9 16:46:16.050: INFO: Created: latency-svc-sr89n
May  9 16:46:16.085: INFO: Got endpoints: latency-svc-65qht [751.118136ms]
May  9 16:46:16.107: INFO: Created: latency-svc-ng69z
May  9 16:46:16.137: INFO: Got endpoints: latency-svc-zhbrd [752.813633ms]
May  9 16:46:16.155: INFO: Created: latency-svc-p44rl
May  9 16:46:16.187: INFO: Got endpoints: latency-svc-72stz [751.378457ms]
May  9 16:46:16.202: INFO: Created: latency-svc-bxd5g
May  9 16:46:16.234: INFO: Got endpoints: latency-svc-9vwlv [748.249793ms]
May  9 16:46:16.246: INFO: Created: latency-svc-wf4jz
May  9 16:46:16.284: INFO: Got endpoints: latency-svc-4dvnb [748.103972ms]
May  9 16:46:16.297: INFO: Created: latency-svc-9z72d
May  9 16:46:16.336: INFO: Got endpoints: latency-svc-smkj8 [753.897916ms]
May  9 16:46:16.385: INFO: Created: latency-svc-67mw5
May  9 16:46:16.385: INFO: Got endpoints: latency-svc-bmjzd [746.535816ms]
May  9 16:46:16.399: INFO: Created: latency-svc-khpng
May  9 16:46:16.435: INFO: Got endpoints: latency-svc-s7xn2 [743.136462ms]
May  9 16:46:16.450: INFO: Created: latency-svc-jzn42
May  9 16:46:16.481: INFO: Got endpoints: latency-svc-f6rb9 [744.827479ms]
May  9 16:46:16.495: INFO: Created: latency-svc-zq4df
May  9 16:46:16.533: INFO: Got endpoints: latency-svc-kzds8 [750.231265ms]
May  9 16:46:16.552: INFO: Created: latency-svc-vlsph
May  9 16:46:16.583: INFO: Got endpoints: latency-svc-vfhrn [749.269115ms]
May  9 16:46:16.596: INFO: Created: latency-svc-rvb2n
May  9 16:46:16.634: INFO: Got endpoints: latency-svc-vrvr9 [752.56131ms]
May  9 16:46:16.647: INFO: Created: latency-svc-xqk8c
May  9 16:46:16.684: INFO: Got endpoints: latency-svc-jjrxf [751.052959ms]
May  9 16:46:16.697: INFO: Created: latency-svc-xz5zd
May  9 16:46:16.756: INFO: Got endpoints: latency-svc-nm9d4 [773.258926ms]
May  9 16:46:16.785: INFO: Got endpoints: latency-svc-sr89n [749.948949ms]
May  9 16:46:16.794: INFO: Created: latency-svc-g7vvc
May  9 16:46:16.804: INFO: Created: latency-svc-lnhpk
May  9 16:46:16.835: INFO: Got endpoints: latency-svc-ng69z [749.621038ms]
May  9 16:46:16.848: INFO: Created: latency-svc-mbhwd
May  9 16:46:16.883: INFO: Got endpoints: latency-svc-p44rl [746.083784ms]
May  9 16:46:16.897: INFO: Created: latency-svc-54b4f
May  9 16:46:16.958: INFO: Got endpoints: latency-svc-bxd5g [770.703297ms]
May  9 16:46:16.972: INFO: Created: latency-svc-zzfks
May  9 16:46:16.982: INFO: Got endpoints: latency-svc-wf4jz [748.297192ms]
May  9 16:46:16.997: INFO: Created: latency-svc-x67nm
May  9 16:46:17.034: INFO: Got endpoints: latency-svc-9z72d [749.614581ms]
May  9 16:46:17.046: INFO: Created: latency-svc-dt7l7
May  9 16:46:17.082: INFO: Got endpoints: latency-svc-67mw5 [746.53884ms]
May  9 16:46:17.095: INFO: Created: latency-svc-jl7pc
May  9 16:46:17.133: INFO: Got endpoints: latency-svc-khpng [747.451751ms]
May  9 16:46:17.145: INFO: Created: latency-svc-dpjmn
May  9 16:46:17.184: INFO: Got endpoints: latency-svc-jzn42 [748.325941ms]
May  9 16:46:17.198: INFO: Created: latency-svc-n7hjw
May  9 16:46:17.234: INFO: Got endpoints: latency-svc-zq4df [752.655423ms]
May  9 16:46:17.247: INFO: Created: latency-svc-42z9d
May  9 16:46:17.287: INFO: Got endpoints: latency-svc-vlsph [754.213012ms]
May  9 16:46:17.315: INFO: Created: latency-svc-dhpwv
May  9 16:46:17.336: INFO: Got endpoints: latency-svc-rvb2n [752.97859ms]
May  9 16:46:17.352: INFO: Created: latency-svc-c4pb8
May  9 16:46:17.384: INFO: Got endpoints: latency-svc-xqk8c [749.43251ms]
May  9 16:46:17.399: INFO: Created: latency-svc-l2gp9
May  9 16:46:17.435: INFO: Got endpoints: latency-svc-xz5zd [751.521306ms]
May  9 16:46:17.450: INFO: Created: latency-svc-l8s62
May  9 16:46:17.484: INFO: Got endpoints: latency-svc-g7vvc [727.728426ms]
May  9 16:46:17.498: INFO: Created: latency-svc-m6chn
May  9 16:46:17.533: INFO: Got endpoints: latency-svc-lnhpk [747.684577ms]
May  9 16:46:17.547: INFO: Created: latency-svc-wx26g
May  9 16:46:17.590: INFO: Got endpoints: latency-svc-mbhwd [755.517412ms]
May  9 16:46:17.606: INFO: Created: latency-svc-phvxm
May  9 16:46:17.633: INFO: Got endpoints: latency-svc-54b4f [749.352347ms]
May  9 16:46:17.646: INFO: Created: latency-svc-f7n45
May  9 16:46:17.684: INFO: Got endpoints: latency-svc-zzfks [725.667295ms]
May  9 16:46:17.699: INFO: Created: latency-svc-dtqtk
May  9 16:46:17.733: INFO: Got endpoints: latency-svc-x67nm [750.551328ms]
May  9 16:46:17.746: INFO: Created: latency-svc-jhvhg
May  9 16:46:17.783: INFO: Got endpoints: latency-svc-dt7l7 [749.352119ms]
May  9 16:46:17.795: INFO: Created: latency-svc-j224q
May  9 16:46:17.833: INFO: Got endpoints: latency-svc-jl7pc [750.658786ms]
May  9 16:46:17.850: INFO: Created: latency-svc-nfq7k
May  9 16:46:17.891: INFO: Got endpoints: latency-svc-dpjmn [757.866608ms]
May  9 16:46:17.908: INFO: Created: latency-svc-5pzt4
May  9 16:46:17.934: INFO: Got endpoints: latency-svc-n7hjw [749.988219ms]
May  9 16:46:17.948: INFO: Created: latency-svc-4wzsm
May  9 16:46:17.984: INFO: Got endpoints: latency-svc-42z9d [749.906177ms]
May  9 16:46:17.997: INFO: Created: latency-svc-9xgqk
May  9 16:46:18.036: INFO: Got endpoints: latency-svc-dhpwv [748.50979ms]
May  9 16:46:18.057: INFO: Created: latency-svc-x7272
May  9 16:46:18.084: INFO: Got endpoints: latency-svc-c4pb8 [747.961665ms]
May  9 16:46:18.105: INFO: Created: latency-svc-wrkjj
May  9 16:46:18.133: INFO: Got endpoints: latency-svc-l2gp9 [749.012137ms]
May  9 16:46:18.148: INFO: Created: latency-svc-trvp5
May  9 16:46:18.186: INFO: Got endpoints: latency-svc-l8s62 [750.924095ms]
May  9 16:46:18.202: INFO: Created: latency-svc-7jmvd
May  9 16:46:18.237: INFO: Got endpoints: latency-svc-m6chn [752.940331ms]
May  9 16:46:18.263: INFO: Created: latency-svc-l882p
May  9 16:46:18.284: INFO: Got endpoints: latency-svc-wx26g [751.284293ms]
May  9 16:46:18.296: INFO: Created: latency-svc-rrh28
May  9 16:46:18.339: INFO: Got endpoints: latency-svc-phvxm [748.576238ms]
May  9 16:46:18.353: INFO: Created: latency-svc-zld8c
May  9 16:46:18.382: INFO: Got endpoints: latency-svc-f7n45 [749.28674ms]
May  9 16:46:18.396: INFO: Created: latency-svc-95tsm
May  9 16:46:18.431: INFO: Got endpoints: latency-svc-dtqtk [747.398322ms]
May  9 16:46:18.446: INFO: Created: latency-svc-bwlbv
May  9 16:46:18.486: INFO: Got endpoints: latency-svc-jhvhg [753.062863ms]
May  9 16:46:18.499: INFO: Created: latency-svc-s6h5z
May  9 16:46:18.532: INFO: Got endpoints: latency-svc-j224q [748.521136ms]
May  9 16:46:18.546: INFO: Created: latency-svc-vftf9
May  9 16:46:18.583: INFO: Got endpoints: latency-svc-nfq7k [750.338385ms]
May  9 16:46:18.601: INFO: Created: latency-svc-8p6ks
May  9 16:46:18.632: INFO: Got endpoints: latency-svc-5pzt4 [741.863889ms]
May  9 16:46:18.646: INFO: Created: latency-svc-hnnxr
May  9 16:46:18.683: INFO: Got endpoints: latency-svc-4wzsm [748.843738ms]
May  9 16:46:18.695: INFO: Created: latency-svc-5qtzf
May  9 16:46:18.735: INFO: Got endpoints: latency-svc-9xgqk [751.128141ms]
May  9 16:46:18.748: INFO: Created: latency-svc-x9n8b
May  9 16:46:18.786: INFO: Got endpoints: latency-svc-x7272 [749.378948ms]
May  9 16:46:18.805: INFO: Created: latency-svc-s9w6w
May  9 16:46:18.883: INFO: Got endpoints: latency-svc-wrkjj [799.481424ms]
May  9 16:46:18.887: INFO: Got endpoints: latency-svc-trvp5 [754.589433ms]
May  9 16:46:18.972: INFO: Got endpoints: latency-svc-7jmvd [785.310566ms]
May  9 16:46:18.972: INFO: Created: latency-svc-wlz2z
May  9 16:46:18.981: INFO: Created: latency-svc-cw6gw
May  9 16:46:18.986: INFO: Got endpoints: latency-svc-l882p [748.549995ms]
May  9 16:46:19.060: INFO: Created: latency-svc-96v7j
May  9 16:46:19.066: INFO: Got endpoints: latency-svc-rrh28 [781.417006ms]
May  9 16:46:19.071: INFO: Created: latency-svc-7nrv7
May  9 16:46:19.080: INFO: Created: latency-svc-db9z7
May  9 16:46:19.084: INFO: Got endpoints: latency-svc-zld8c [745.405431ms]
May  9 16:46:19.100: INFO: Created: latency-svc-2qsbv
May  9 16:46:19.139: INFO: Got endpoints: latency-svc-95tsm [757.165018ms]
May  9 16:46:19.156: INFO: Created: latency-svc-k462q
May  9 16:46:19.182: INFO: Got endpoints: latency-svc-bwlbv [750.148714ms]
May  9 16:46:19.201: INFO: Created: latency-svc-nk99s
May  9 16:46:19.232: INFO: Got endpoints: latency-svc-s6h5z [746.081241ms]
May  9 16:46:19.245: INFO: Created: latency-svc-kf8r2
May  9 16:46:19.282: INFO: Got endpoints: latency-svc-vftf9 [750.102309ms]
May  9 16:46:19.333: INFO: Got endpoints: latency-svc-8p6ks [749.751628ms]
May  9 16:46:19.386: INFO: Got endpoints: latency-svc-hnnxr [753.106889ms]
May  9 16:46:19.433: INFO: Got endpoints: latency-svc-5qtzf [750.658777ms]
May  9 16:46:19.482: INFO: Got endpoints: latency-svc-x9n8b [747.208893ms]
May  9 16:46:19.534: INFO: Got endpoints: latency-svc-s9w6w [748.545275ms]
May  9 16:46:19.581: INFO: Got endpoints: latency-svc-wlz2z [697.66469ms]
May  9 16:46:19.631: INFO: Got endpoints: latency-svc-cw6gw [744.124384ms]
May  9 16:46:19.683: INFO: Got endpoints: latency-svc-96v7j [711.212276ms]
May  9 16:46:19.734: INFO: Got endpoints: latency-svc-7nrv7 [747.791297ms]
May  9 16:46:19.783: INFO: Got endpoints: latency-svc-db9z7 [717.153941ms]
May  9 16:46:19.834: INFO: Got endpoints: latency-svc-2qsbv [749.763969ms]
May  9 16:46:19.884: INFO: Got endpoints: latency-svc-k462q [745.205105ms]
May  9 16:46:19.934: INFO: Got endpoints: latency-svc-nk99s [752.547356ms]
May  9 16:46:19.994: INFO: Got endpoints: latency-svc-kf8r2 [761.91186ms]
May  9 16:46:19.994: INFO: Latencies: [19.957502ms 30.702614ms 37.062093ms 45.78423ms 54.188359ms 63.824026ms 68.258526ms 75.88395ms 81.095838ms 93.232104ms 107.96074ms 113.075712ms 124.312813ms 126.803401ms 128.422317ms 134.599115ms 136.243356ms 138.0533ms 139.506473ms 140.42651ms 144.743184ms 144.756767ms 145.132309ms 146.586596ms 147.315296ms 147.391173ms 147.669371ms 147.864967ms 149.303231ms 149.801079ms 151.032692ms 151.152078ms 154.329068ms 155.916827ms 156.734122ms 158.111192ms 173.151945ms 221.418884ms 259.681455ms 304.264594ms 346.510277ms 390.464989ms 431.999056ms 475.140995ms 529.790152ms 552.789226ms 597.97309ms 619.835602ms 669.701779ms 670.671938ms 697.66469ms 700.363162ms 700.579842ms 711.212276ms 717.153941ms 721.215055ms 725.667295ms 727.728426ms 727.973868ms 732.655908ms 737.609712ms 738.557029ms 739.920422ms 741.697877ms 741.863889ms 742.104598ms 742.396624ms 742.787178ms 743.044681ms 743.136462ms 744.124384ms 744.579535ms 744.827479ms 744.839297ms 745.205105ms 745.405431ms 746.081241ms 746.083784ms 746.312334ms 746.535816ms 746.53884ms 746.637266ms 746.679044ms 747.208893ms 747.382767ms 747.398322ms 747.451751ms 747.575201ms 747.608915ms 747.684577ms 747.791297ms 747.947994ms 747.961665ms 748.103972ms 748.104598ms 748.249793ms 748.297192ms 748.325941ms 748.50979ms 748.521136ms 748.521282ms 748.528232ms 748.545275ms 748.549995ms 748.576238ms 748.687102ms 748.792046ms 748.843738ms 748.845787ms 749.012137ms 749.03791ms 749.041158ms 749.09178ms 749.269115ms 749.28674ms 749.352119ms 749.352347ms 749.378948ms 749.424557ms 749.43251ms 749.550043ms 749.614581ms 749.621038ms 749.741688ms 749.751628ms 749.763969ms 749.777773ms 749.899372ms 749.906177ms 749.948949ms 749.9534ms 749.972397ms 749.988219ms 750.102309ms 750.148714ms 750.231265ms 750.281764ms 750.303052ms 750.338385ms 750.430523ms 750.551328ms 750.658777ms 750.658786ms 750.665779ms 750.736126ms 750.890356ms 750.924095ms 750.937173ms 751.052959ms 751.118136ms 751.128141ms 751.276844ms 751.284293ms 751.365115ms 751.378457ms 751.521306ms 751.540417ms 751.611892ms 751.618209ms 751.714021ms 751.813948ms 752.236626ms 752.547356ms 752.56131ms 752.655423ms 752.710101ms 752.813633ms 752.940331ms 752.97859ms 753.062863ms 753.106889ms 753.424422ms 753.724986ms 753.897916ms 754.213012ms 754.44581ms 754.563768ms 754.589433ms 754.835634ms 755.020846ms 755.259288ms 755.347447ms 755.517412ms 756.307651ms 757.165018ms 757.866608ms 758.570969ms 758.676133ms 759.530435ms 761.91186ms 764.330261ms 769.593142ms 770.703297ms 773.258926ms 781.391918ms 781.417006ms 785.310566ms 798.441299ms 799.481424ms 833.097434ms]
May  9 16:46:19.994: INFO: 50 %ile: 748.521282ms
May  9 16:46:19.994: INFO: 90 %ile: 755.259288ms
May  9 16:46:19.994: INFO: 99 %ile: 799.481424ms
May  9 16:46:19.994: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
May  9 16:46:19.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-5827" for this suite. 05/09/23 16:46:20.003
------------------------------
â€¢ [SLOW TEST] [10.792 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:46:09.22
    May  9 16:46:09.220: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename svc-latency 05/09/23 16:46:09.221
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:46:09.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:46:09.246
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    May  9 16:46:09.251: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-5827 05/09/23 16:46:09.252
    I0509 16:46:09.259199      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5827, replica count: 1
    I0509 16:46:10.309671      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0509 16:46:11.310626      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 16:46:11.427: INFO: Created: latency-svc-qbp87
    May  9 16:46:11.434: INFO: Got endpoints: latency-svc-qbp87 [23.030268ms]
    May  9 16:46:11.448: INFO: Created: latency-svc-5vr4v
    May  9 16:46:11.454: INFO: Got endpoints: latency-svc-5vr4v [19.957502ms]
    May  9 16:46:11.457: INFO: Created: latency-svc-qpkxd
    May  9 16:46:11.465: INFO: Created: latency-svc-mdtg6
    May  9 16:46:11.465: INFO: Got endpoints: latency-svc-qpkxd [30.702614ms]
    May  9 16:46:11.471: INFO: Got endpoints: latency-svc-mdtg6 [37.062093ms]
    May  9 16:46:11.474: INFO: Created: latency-svc-4r52v
    May  9 16:46:11.480: INFO: Got endpoints: latency-svc-4r52v [45.78423ms]
    May  9 16:46:11.482: INFO: Created: latency-svc-c9wzn
    May  9 16:46:11.488: INFO: Got endpoints: latency-svc-c9wzn [54.188359ms]
    May  9 16:46:11.492: INFO: Created: latency-svc-hvkcm
    May  9 16:46:11.498: INFO: Got endpoints: latency-svc-hvkcm [63.824026ms]
    May  9 16:46:11.498: INFO: Created: latency-svc-vhwrx
    May  9 16:46:11.502: INFO: Got endpoints: latency-svc-vhwrx [68.258526ms]
    May  9 16:46:11.506: INFO: Created: latency-svc-x4v6f
    May  9 16:46:11.510: INFO: Got endpoints: latency-svc-x4v6f [75.88395ms]
    May  9 16:46:11.513: INFO: Created: latency-svc-bqzqx
    May  9 16:46:11.515: INFO: Got endpoints: latency-svc-bqzqx [81.095838ms]
    May  9 16:46:11.521: INFO: Created: latency-svc-rzbxv
    May  9 16:46:11.528: INFO: Got endpoints: latency-svc-rzbxv [93.232104ms]
    May  9 16:46:11.531: INFO: Created: latency-svc-bnns8
    May  9 16:46:11.542: INFO: Got endpoints: latency-svc-bnns8 [107.96074ms]
    May  9 16:46:11.543: INFO: Created: latency-svc-r96g2
    May  9 16:46:11.547: INFO: Got endpoints: latency-svc-r96g2 [113.075712ms]
    May  9 16:46:11.560: INFO: Created: latency-svc-ld9gw
    May  9 16:46:11.568: INFO: Created: latency-svc-gfk4w
    May  9 16:46:11.570: INFO: Got endpoints: latency-svc-ld9gw [136.243356ms]
    May  9 16:46:11.574: INFO: Got endpoints: latency-svc-gfk4w [139.506473ms]
    May  9 16:46:11.585: INFO: Created: latency-svc-qf5cw
    May  9 16:46:11.590: INFO: Got endpoints: latency-svc-qf5cw [155.916827ms]
    May  9 16:46:11.601: INFO: Created: latency-svc-xnwcr
    May  9 16:46:11.603: INFO: Created: latency-svc-62j8t
    May  9 16:46:11.604: INFO: Got endpoints: latency-svc-62j8t [149.801079ms]
    May  9 16:46:11.610: INFO: Created: latency-svc-nsbnm
    May  9 16:46:11.613: INFO: Got endpoints: latency-svc-xnwcr [147.315296ms]
    May  9 16:46:11.619: INFO: Got endpoints: latency-svc-nsbnm [147.669371ms]
    May  9 16:46:11.622: INFO: Created: latency-svc-k2cqn
    May  9 16:46:11.629: INFO: Got endpoints: latency-svc-k2cqn [149.303231ms]
    May  9 16:46:11.632: INFO: Created: latency-svc-c789v
    May  9 16:46:11.635: INFO: Created: latency-svc-zrcc9
    May  9 16:46:11.636: INFO: Got endpoints: latency-svc-c789v [147.864967ms]
    May  9 16:46:11.649: INFO: Created: latency-svc-xqf48
    May  9 16:46:11.649: INFO: Got endpoints: latency-svc-zrcc9 [151.152078ms]
    May  9 16:46:11.659: INFO: Got endpoints: latency-svc-xqf48 [156.734122ms]
    May  9 16:46:11.660: INFO: Created: latency-svc-kk7mf
    May  9 16:46:11.664: INFO: Got endpoints: latency-svc-kk7mf [154.329068ms]
    May  9 16:46:11.667: INFO: Created: latency-svc-pdf9r
    May  9 16:46:11.673: INFO: Created: latency-svc-w2cj2
    May  9 16:46:11.674: INFO: Got endpoints: latency-svc-pdf9r [158.111192ms]
    May  9 16:46:11.679: INFO: Got endpoints: latency-svc-w2cj2 [151.032692ms]
    May  9 16:46:11.684: INFO: Created: latency-svc-ns6g9
    May  9 16:46:11.687: INFO: Got endpoints: latency-svc-ns6g9 [144.756767ms]
    May  9 16:46:11.688: INFO: Created: latency-svc-ljfln
    May  9 16:46:11.692: INFO: Got endpoints: latency-svc-ljfln [144.743184ms]
    May  9 16:46:11.695: INFO: Created: latency-svc-qhjqt
    May  9 16:46:11.702: INFO: Created: latency-svc-zs5wr
    May  9 16:46:11.708: INFO: Got endpoints: latency-svc-qhjqt [138.0533ms]
    May  9 16:46:11.708: INFO: Got endpoints: latency-svc-zs5wr [134.599115ms]
    May  9 16:46:11.710: INFO: Created: latency-svc-ww7sl
    May  9 16:46:11.714: INFO: Got endpoints: latency-svc-ww7sl [124.312813ms]
    May  9 16:46:11.727: INFO: Created: latency-svc-tnx9f
    May  9 16:46:11.732: INFO: Got endpoints: latency-svc-tnx9f [128.422317ms]
    May  9 16:46:11.735: INFO: Created: latency-svc-tsjgx
    May  9 16:46:11.739: INFO: Got endpoints: latency-svc-tsjgx [126.803401ms]
    May  9 16:46:11.740: INFO: Created: latency-svc-r76q2
    May  9 16:46:11.764: INFO: Got endpoints: latency-svc-r76q2 [145.132309ms]
    May  9 16:46:11.768: INFO: Created: latency-svc-hp89b
    May  9 16:46:11.776: INFO: Got endpoints: latency-svc-hp89b [146.586596ms]
    May  9 16:46:11.778: INFO: Created: latency-svc-jw2gx
    May  9 16:46:11.784: INFO: Got endpoints: latency-svc-jw2gx [147.391173ms]
    May  9 16:46:11.786: INFO: Created: latency-svc-s4hmn
    May  9 16:46:11.790: INFO: Got endpoints: latency-svc-s4hmn [140.42651ms]
    May  9 16:46:11.792: INFO: Created: latency-svc-rgdg9
    May  9 16:46:11.799: INFO: Created: latency-svc-8ksbd
    May  9 16:46:11.811: INFO: Created: latency-svc-qfwb9
    May  9 16:46:11.813: INFO: Created: latency-svc-kbdp8
    May  9 16:46:11.819: INFO: Created: latency-svc-shvvp
    May  9 16:46:11.829: INFO: Created: latency-svc-v5mvg
    May  9 16:46:11.832: INFO: Got endpoints: latency-svc-rgdg9 [173.151945ms]
    May  9 16:46:11.835: INFO: Created: latency-svc-xkprp
    May  9 16:46:11.845: INFO: Created: latency-svc-7m4x2
    May  9 16:46:11.852: INFO: Created: latency-svc-wc9zv
    May  9 16:46:11.859: INFO: Created: latency-svc-tq9sw
    May  9 16:46:11.863: INFO: Created: latency-svc-bd4zq
    May  9 16:46:11.871: INFO: Created: latency-svc-kl4zq
    May  9 16:46:11.877: INFO: Created: latency-svc-86swz
    May  9 16:46:11.884: INFO: Created: latency-svc-px7r4
    May  9 16:46:11.886: INFO: Got endpoints: latency-svc-8ksbd [221.418884ms]
    May  9 16:46:11.896: INFO: Created: latency-svc-z5zrf
    May  9 16:46:11.903: INFO: Created: latency-svc-fjk6d
    May  9 16:46:11.910: INFO: Created: latency-svc-kc9p9
    May  9 16:46:11.933: INFO: Got endpoints: latency-svc-qfwb9 [259.681455ms]
    May  9 16:46:11.948: INFO: Created: latency-svc-4xv6b
    May  9 16:46:11.983: INFO: Got endpoints: latency-svc-kbdp8 [304.264594ms]
    May  9 16:46:12.002: INFO: Created: latency-svc-5ltj4
    May  9 16:46:12.034: INFO: Got endpoints: latency-svc-shvvp [346.510277ms]
    May  9 16:46:12.048: INFO: Created: latency-svc-zxg6n
    May  9 16:46:12.083: INFO: Got endpoints: latency-svc-v5mvg [390.464989ms]
    May  9 16:46:12.096: INFO: Created: latency-svc-r8ltr
    May  9 16:46:12.140: INFO: Got endpoints: latency-svc-xkprp [431.999056ms]
    May  9 16:46:12.158: INFO: Created: latency-svc-s9wcc
    May  9 16:46:12.184: INFO: Got endpoints: latency-svc-7m4x2 [475.140995ms]
    May  9 16:46:12.197: INFO: Created: latency-svc-xch99
    May  9 16:46:12.244: INFO: Got endpoints: latency-svc-wc9zv [529.790152ms]
    May  9 16:46:12.258: INFO: Created: latency-svc-pzlrd
    May  9 16:46:12.285: INFO: Got endpoints: latency-svc-tq9sw [552.789226ms]
    May  9 16:46:12.299: INFO: Created: latency-svc-2gs2b
    May  9 16:46:12.337: INFO: Got endpoints: latency-svc-bd4zq [597.97309ms]
    May  9 16:46:12.358: INFO: Created: latency-svc-vkk4f
    May  9 16:46:12.384: INFO: Got endpoints: latency-svc-kl4zq [619.835602ms]
    May  9 16:46:12.398: INFO: Created: latency-svc-vfpzg
    May  9 16:46:12.445: INFO: Got endpoints: latency-svc-86swz [669.701779ms]
    May  9 16:46:12.460: INFO: Created: latency-svc-vkjns
    May  9 16:46:12.484: INFO: Got endpoints: latency-svc-px7r4 [700.579842ms]
    May  9 16:46:12.499: INFO: Created: latency-svc-rdtpf
    May  9 16:46:12.535: INFO: Got endpoints: latency-svc-z5zrf [744.839297ms]
    May  9 16:46:12.549: INFO: Created: latency-svc-pcwrs
    May  9 16:46:12.587: INFO: Got endpoints: latency-svc-fjk6d [754.835634ms]
    May  9 16:46:12.612: INFO: Created: latency-svc-xmv96
    May  9 16:46:12.634: INFO: Got endpoints: latency-svc-kc9p9 [748.521282ms]
    May  9 16:46:12.651: INFO: Created: latency-svc-rmn4p
    May  9 16:46:12.683: INFO: Got endpoints: latency-svc-4xv6b [749.777773ms]
    May  9 16:46:12.696: INFO: Created: latency-svc-zmvnn
    May  9 16:46:12.733: INFO: Got endpoints: latency-svc-5ltj4 [750.281764ms]
    May  9 16:46:12.748: INFO: Created: latency-svc-kntbc
    May  9 16:46:12.782: INFO: Got endpoints: latency-svc-zxg6n [747.947994ms]
    May  9 16:46:12.795: INFO: Created: latency-svc-fgc59
    May  9 16:46:12.835: INFO: Got endpoints: latency-svc-r8ltr [751.714021ms]
    May  9 16:46:12.855: INFO: Created: latency-svc-rcx65
    May  9 16:46:12.883: INFO: Got endpoints: latency-svc-s9wcc [742.396624ms]
    May  9 16:46:12.895: INFO: Created: latency-svc-8bgtr
    May  9 16:46:12.933: INFO: Got endpoints: latency-svc-xch99 [749.03791ms]
    May  9 16:46:12.946: INFO: Created: latency-svc-cm4vn
    May  9 16:46:12.983: INFO: Got endpoints: latency-svc-pzlrd [738.557029ms]
    May  9 16:46:12.996: INFO: Created: latency-svc-npv64
    May  9 16:46:13.034: INFO: Got endpoints: latency-svc-2gs2b [749.041158ms]
    May  9 16:46:13.050: INFO: Created: latency-svc-n8mvr
    May  9 16:46:13.082: INFO: Got endpoints: latency-svc-vkk4f [744.579535ms]
    May  9 16:46:13.098: INFO: Created: latency-svc-grsj2
    May  9 16:46:13.135: INFO: Got endpoints: latency-svc-vfpzg [750.665779ms]
    May  9 16:46:13.147: INFO: Created: latency-svc-4qb5m
    May  9 16:46:13.183: INFO: Got endpoints: latency-svc-vkjns [737.609712ms]
    May  9 16:46:13.196: INFO: Created: latency-svc-rfnlq
    May  9 16:46:13.232: INFO: Got endpoints: latency-svc-rdtpf [748.104598ms]
    May  9 16:46:13.246: INFO: Created: latency-svc-685fp
    May  9 16:46:13.290: INFO: Got endpoints: latency-svc-pcwrs [755.259288ms]
    May  9 16:46:13.309: INFO: Created: latency-svc-schzz
    May  9 16:46:13.333: INFO: Got endpoints: latency-svc-xmv96 [746.312334ms]
    May  9 16:46:13.347: INFO: Created: latency-svc-h8ldx
    May  9 16:46:13.382: INFO: Got endpoints: latency-svc-rmn4p [747.608915ms]
    May  9 16:46:13.394: INFO: Created: latency-svc-vwkn6
    May  9 16:46:13.433: INFO: Got endpoints: latency-svc-zmvnn [749.9534ms]
    May  9 16:46:13.446: INFO: Created: latency-svc-tflxn
    May  9 16:46:13.488: INFO: Got endpoints: latency-svc-kntbc [754.44581ms]
    May  9 16:46:13.502: INFO: Created: latency-svc-zgr6n
    May  9 16:46:13.532: INFO: Got endpoints: latency-svc-fgc59 [750.430523ms]
    May  9 16:46:13.548: INFO: Created: latency-svc-xd2h2
    May  9 16:46:13.584: INFO: Got endpoints: latency-svc-rcx65 [749.424557ms]
    May  9 16:46:13.597: INFO: Created: latency-svc-8gfbd
    May  9 16:46:13.633: INFO: Got endpoints: latency-svc-8bgtr [750.303052ms]
    May  9 16:46:13.646: INFO: Created: latency-svc-2wdfq
    May  9 16:46:13.683: INFO: Got endpoints: latency-svc-cm4vn [750.736126ms]
    May  9 16:46:13.696: INFO: Created: latency-svc-w98xp
    May  9 16:46:13.739: INFO: Got endpoints: latency-svc-npv64 [756.307651ms]
    May  9 16:46:13.755: INFO: Created: latency-svc-dm5td
    May  9 16:46:13.784: INFO: Got endpoints: latency-svc-n8mvr [749.741688ms]
    May  9 16:46:13.797: INFO: Created: latency-svc-rgxcn
    May  9 16:46:13.834: INFO: Got endpoints: latency-svc-grsj2 [751.618209ms]
    May  9 16:46:13.847: INFO: Created: latency-svc-fjj68
    May  9 16:46:13.884: INFO: Got endpoints: latency-svc-4qb5m [748.845787ms]
    May  9 16:46:13.897: INFO: Created: latency-svc-tv6gs
    May  9 16:46:13.953: INFO: Got endpoints: latency-svc-rfnlq [769.593142ms]
    May  9 16:46:13.968: INFO: Created: latency-svc-mv784
    May  9 16:46:13.984: INFO: Got endpoints: latency-svc-685fp [751.813948ms]
    May  9 16:46:13.998: INFO: Created: latency-svc-h87p9
    May  9 16:46:14.033: INFO: Got endpoints: latency-svc-schzz [743.044681ms]
    May  9 16:46:14.047: INFO: Created: latency-svc-lgc7r
    May  9 16:46:14.084: INFO: Got endpoints: latency-svc-h8ldx [750.937173ms]
    May  9 16:46:14.100: INFO: Created: latency-svc-ms2kg
    May  9 16:46:14.131: INFO: Got endpoints: latency-svc-vwkn6 [749.09178ms]
    May  9 16:46:14.159: INFO: Created: latency-svc-7clml
    May  9 16:46:14.192: INFO: Got endpoints: latency-svc-tflxn [758.676133ms]
    May  9 16:46:14.205: INFO: Created: latency-svc-ngbct
    May  9 16:46:14.243: INFO: Got endpoints: latency-svc-zgr6n [754.563768ms]
    May  9 16:46:14.255: INFO: Created: latency-svc-46sjn
    May  9 16:46:14.365: INFO: Got endpoints: latency-svc-xd2h2 [833.097434ms]
    May  9 16:46:14.365: INFO: Got endpoints: latency-svc-8gfbd [781.391918ms]
    May  9 16:46:14.377: INFO: Created: latency-svc-z25km
    May  9 16:46:14.393: INFO: Got endpoints: latency-svc-2wdfq [759.530435ms]
    May  9 16:46:14.396: INFO: Created: latency-svc-6pwpg
    May  9 16:46:14.421: INFO: Created: latency-svc-8dl49
    May  9 16:46:14.432: INFO: Got endpoints: latency-svc-w98xp [748.528232ms]
    May  9 16:46:14.444: INFO: Created: latency-svc-jdb5b
    May  9 16:46:14.482: INFO: Got endpoints: latency-svc-dm5td [742.787178ms]
    May  9 16:46:14.494: INFO: Created: latency-svc-p6m5b
    May  9 16:46:14.534: INFO: Got endpoints: latency-svc-fjj68 [700.363162ms]
    May  9 16:46:14.559: INFO: Created: latency-svc-c8jjv
    May  9 16:46:14.582: INFO: Got endpoints: latency-svc-rgxcn [798.441299ms]
    May  9 16:46:14.624: INFO: Created: latency-svc-blfcd
    May  9 16:46:14.633: INFO: Got endpoints: latency-svc-tv6gs [748.792046ms]
    May  9 16:46:14.644: INFO: Created: latency-svc-dsp7h
    May  9 16:46:14.681: INFO: Got endpoints: latency-svc-mv784 [727.973868ms]
    May  9 16:46:14.693: INFO: Created: latency-svc-w9kls
    May  9 16:46:14.732: INFO: Got endpoints: latency-svc-h87p9 [747.575201ms]
    May  9 16:46:14.744: INFO: Created: latency-svc-f9b9l
    May  9 16:46:14.783: INFO: Got endpoints: latency-svc-lgc7r [749.899372ms]
    May  9 16:46:14.795: INFO: Created: latency-svc-zggld
    May  9 16:46:14.849: INFO: Got endpoints: latency-svc-ms2kg [764.330261ms]
    May  9 16:46:14.865: INFO: Created: latency-svc-c4xzv
    May  9 16:46:14.884: INFO: Got endpoints: latency-svc-7clml [752.236626ms]
    May  9 16:46:14.897: INFO: Created: latency-svc-cfqvf
    May  9 16:46:14.934: INFO: Got endpoints: latency-svc-ngbct [741.697877ms]
    May  9 16:46:14.947: INFO: Created: latency-svc-knznc
    May  9 16:46:14.985: INFO: Got endpoints: latency-svc-46sjn [742.104598ms]
    May  9 16:46:14.998: INFO: Created: latency-svc-j4hlx
    May  9 16:46:15.036: INFO: Got endpoints: latency-svc-z25km [670.671938ms]
    May  9 16:46:15.051: INFO: Created: latency-svc-pc7mf
    May  9 16:46:15.087: INFO: Got endpoints: latency-svc-6pwpg [721.215055ms]
    May  9 16:46:15.101: INFO: Created: latency-svc-xt4nl
    May  9 16:46:15.133: INFO: Got endpoints: latency-svc-8dl49 [739.920422ms]
    May  9 16:46:15.147: INFO: Created: latency-svc-7x2ts
    May  9 16:46:15.182: INFO: Got endpoints: latency-svc-jdb5b [749.972397ms]
    May  9 16:46:15.194: INFO: Created: latency-svc-lnr2d
    May  9 16:46:15.234: INFO: Got endpoints: latency-svc-p6m5b [751.365115ms]
    May  9 16:46:15.247: INFO: Created: latency-svc-wlhcd
    May  9 16:46:15.282: INFO: Got endpoints: latency-svc-c8jjv [747.382767ms]
    May  9 16:46:15.294: INFO: Created: latency-svc-zfxt7
    May  9 16:46:15.334: INFO: Got endpoints: latency-svc-blfcd [751.611892ms]
    May  9 16:46:15.346: INFO: Created: latency-svc-65qht
    May  9 16:46:15.384: INFO: Got endpoints: latency-svc-dsp7h [751.540417ms]
    May  9 16:46:15.400: INFO: Created: latency-svc-zhbrd
    May  9 16:46:15.436: INFO: Got endpoints: latency-svc-w9kls [755.347447ms]
    May  9 16:46:15.448: INFO: Created: latency-svc-72stz
    May  9 16:46:15.485: INFO: Got endpoints: latency-svc-f9b9l [753.724986ms]
    May  9 16:46:15.517: INFO: Created: latency-svc-9vwlv
    May  9 16:46:15.536: INFO: Got endpoints: latency-svc-zggld [752.710101ms]
    May  9 16:46:15.549: INFO: Created: latency-svc-4dvnb
    May  9 16:46:15.582: INFO: Got endpoints: latency-svc-c4xzv [732.655908ms]
    May  9 16:46:15.594: INFO: Created: latency-svc-smkj8
    May  9 16:46:15.639: INFO: Got endpoints: latency-svc-cfqvf [755.020846ms]
    May  9 16:46:15.652: INFO: Created: latency-svc-bmjzd
    May  9 16:46:15.692: INFO: Got endpoints: latency-svc-knznc [758.570969ms]
    May  9 16:46:15.709: INFO: Created: latency-svc-s7xn2
    May  9 16:46:15.736: INFO: Got endpoints: latency-svc-j4hlx [751.276844ms]
    May  9 16:46:15.750: INFO: Created: latency-svc-f6rb9
    May  9 16:46:15.783: INFO: Got endpoints: latency-svc-pc7mf [746.679044ms]
    May  9 16:46:15.794: INFO: Created: latency-svc-kzds8
    May  9 16:46:15.833: INFO: Got endpoints: latency-svc-xt4nl [746.637266ms]
    May  9 16:46:15.847: INFO: Created: latency-svc-vfhrn
    May  9 16:46:15.881: INFO: Got endpoints: latency-svc-7x2ts [748.687102ms]
    May  9 16:46:15.893: INFO: Created: latency-svc-vrvr9
    May  9 16:46:15.933: INFO: Got endpoints: latency-svc-lnr2d [750.890356ms]
    May  9 16:46:15.945: INFO: Created: latency-svc-jjrxf
    May  9 16:46:15.983: INFO: Got endpoints: latency-svc-wlhcd [749.550043ms]
    May  9 16:46:16.000: INFO: Created: latency-svc-nm9d4
    May  9 16:46:16.035: INFO: Got endpoints: latency-svc-zfxt7 [753.424422ms]
    May  9 16:46:16.050: INFO: Created: latency-svc-sr89n
    May  9 16:46:16.085: INFO: Got endpoints: latency-svc-65qht [751.118136ms]
    May  9 16:46:16.107: INFO: Created: latency-svc-ng69z
    May  9 16:46:16.137: INFO: Got endpoints: latency-svc-zhbrd [752.813633ms]
    May  9 16:46:16.155: INFO: Created: latency-svc-p44rl
    May  9 16:46:16.187: INFO: Got endpoints: latency-svc-72stz [751.378457ms]
    May  9 16:46:16.202: INFO: Created: latency-svc-bxd5g
    May  9 16:46:16.234: INFO: Got endpoints: latency-svc-9vwlv [748.249793ms]
    May  9 16:46:16.246: INFO: Created: latency-svc-wf4jz
    May  9 16:46:16.284: INFO: Got endpoints: latency-svc-4dvnb [748.103972ms]
    May  9 16:46:16.297: INFO: Created: latency-svc-9z72d
    May  9 16:46:16.336: INFO: Got endpoints: latency-svc-smkj8 [753.897916ms]
    May  9 16:46:16.385: INFO: Created: latency-svc-67mw5
    May  9 16:46:16.385: INFO: Got endpoints: latency-svc-bmjzd [746.535816ms]
    May  9 16:46:16.399: INFO: Created: latency-svc-khpng
    May  9 16:46:16.435: INFO: Got endpoints: latency-svc-s7xn2 [743.136462ms]
    May  9 16:46:16.450: INFO: Created: latency-svc-jzn42
    May  9 16:46:16.481: INFO: Got endpoints: latency-svc-f6rb9 [744.827479ms]
    May  9 16:46:16.495: INFO: Created: latency-svc-zq4df
    May  9 16:46:16.533: INFO: Got endpoints: latency-svc-kzds8 [750.231265ms]
    May  9 16:46:16.552: INFO: Created: latency-svc-vlsph
    May  9 16:46:16.583: INFO: Got endpoints: latency-svc-vfhrn [749.269115ms]
    May  9 16:46:16.596: INFO: Created: latency-svc-rvb2n
    May  9 16:46:16.634: INFO: Got endpoints: latency-svc-vrvr9 [752.56131ms]
    May  9 16:46:16.647: INFO: Created: latency-svc-xqk8c
    May  9 16:46:16.684: INFO: Got endpoints: latency-svc-jjrxf [751.052959ms]
    May  9 16:46:16.697: INFO: Created: latency-svc-xz5zd
    May  9 16:46:16.756: INFO: Got endpoints: latency-svc-nm9d4 [773.258926ms]
    May  9 16:46:16.785: INFO: Got endpoints: latency-svc-sr89n [749.948949ms]
    May  9 16:46:16.794: INFO: Created: latency-svc-g7vvc
    May  9 16:46:16.804: INFO: Created: latency-svc-lnhpk
    May  9 16:46:16.835: INFO: Got endpoints: latency-svc-ng69z [749.621038ms]
    May  9 16:46:16.848: INFO: Created: latency-svc-mbhwd
    May  9 16:46:16.883: INFO: Got endpoints: latency-svc-p44rl [746.083784ms]
    May  9 16:46:16.897: INFO: Created: latency-svc-54b4f
    May  9 16:46:16.958: INFO: Got endpoints: latency-svc-bxd5g [770.703297ms]
    May  9 16:46:16.972: INFO: Created: latency-svc-zzfks
    May  9 16:46:16.982: INFO: Got endpoints: latency-svc-wf4jz [748.297192ms]
    May  9 16:46:16.997: INFO: Created: latency-svc-x67nm
    May  9 16:46:17.034: INFO: Got endpoints: latency-svc-9z72d [749.614581ms]
    May  9 16:46:17.046: INFO: Created: latency-svc-dt7l7
    May  9 16:46:17.082: INFO: Got endpoints: latency-svc-67mw5 [746.53884ms]
    May  9 16:46:17.095: INFO: Created: latency-svc-jl7pc
    May  9 16:46:17.133: INFO: Got endpoints: latency-svc-khpng [747.451751ms]
    May  9 16:46:17.145: INFO: Created: latency-svc-dpjmn
    May  9 16:46:17.184: INFO: Got endpoints: latency-svc-jzn42 [748.325941ms]
    May  9 16:46:17.198: INFO: Created: latency-svc-n7hjw
    May  9 16:46:17.234: INFO: Got endpoints: latency-svc-zq4df [752.655423ms]
    May  9 16:46:17.247: INFO: Created: latency-svc-42z9d
    May  9 16:46:17.287: INFO: Got endpoints: latency-svc-vlsph [754.213012ms]
    May  9 16:46:17.315: INFO: Created: latency-svc-dhpwv
    May  9 16:46:17.336: INFO: Got endpoints: latency-svc-rvb2n [752.97859ms]
    May  9 16:46:17.352: INFO: Created: latency-svc-c4pb8
    May  9 16:46:17.384: INFO: Got endpoints: latency-svc-xqk8c [749.43251ms]
    May  9 16:46:17.399: INFO: Created: latency-svc-l2gp9
    May  9 16:46:17.435: INFO: Got endpoints: latency-svc-xz5zd [751.521306ms]
    May  9 16:46:17.450: INFO: Created: latency-svc-l8s62
    May  9 16:46:17.484: INFO: Got endpoints: latency-svc-g7vvc [727.728426ms]
    May  9 16:46:17.498: INFO: Created: latency-svc-m6chn
    May  9 16:46:17.533: INFO: Got endpoints: latency-svc-lnhpk [747.684577ms]
    May  9 16:46:17.547: INFO: Created: latency-svc-wx26g
    May  9 16:46:17.590: INFO: Got endpoints: latency-svc-mbhwd [755.517412ms]
    May  9 16:46:17.606: INFO: Created: latency-svc-phvxm
    May  9 16:46:17.633: INFO: Got endpoints: latency-svc-54b4f [749.352347ms]
    May  9 16:46:17.646: INFO: Created: latency-svc-f7n45
    May  9 16:46:17.684: INFO: Got endpoints: latency-svc-zzfks [725.667295ms]
    May  9 16:46:17.699: INFO: Created: latency-svc-dtqtk
    May  9 16:46:17.733: INFO: Got endpoints: latency-svc-x67nm [750.551328ms]
    May  9 16:46:17.746: INFO: Created: latency-svc-jhvhg
    May  9 16:46:17.783: INFO: Got endpoints: latency-svc-dt7l7 [749.352119ms]
    May  9 16:46:17.795: INFO: Created: latency-svc-j224q
    May  9 16:46:17.833: INFO: Got endpoints: latency-svc-jl7pc [750.658786ms]
    May  9 16:46:17.850: INFO: Created: latency-svc-nfq7k
    May  9 16:46:17.891: INFO: Got endpoints: latency-svc-dpjmn [757.866608ms]
    May  9 16:46:17.908: INFO: Created: latency-svc-5pzt4
    May  9 16:46:17.934: INFO: Got endpoints: latency-svc-n7hjw [749.988219ms]
    May  9 16:46:17.948: INFO: Created: latency-svc-4wzsm
    May  9 16:46:17.984: INFO: Got endpoints: latency-svc-42z9d [749.906177ms]
    May  9 16:46:17.997: INFO: Created: latency-svc-9xgqk
    May  9 16:46:18.036: INFO: Got endpoints: latency-svc-dhpwv [748.50979ms]
    May  9 16:46:18.057: INFO: Created: latency-svc-x7272
    May  9 16:46:18.084: INFO: Got endpoints: latency-svc-c4pb8 [747.961665ms]
    May  9 16:46:18.105: INFO: Created: latency-svc-wrkjj
    May  9 16:46:18.133: INFO: Got endpoints: latency-svc-l2gp9 [749.012137ms]
    May  9 16:46:18.148: INFO: Created: latency-svc-trvp5
    May  9 16:46:18.186: INFO: Got endpoints: latency-svc-l8s62 [750.924095ms]
    May  9 16:46:18.202: INFO: Created: latency-svc-7jmvd
    May  9 16:46:18.237: INFO: Got endpoints: latency-svc-m6chn [752.940331ms]
    May  9 16:46:18.263: INFO: Created: latency-svc-l882p
    May  9 16:46:18.284: INFO: Got endpoints: latency-svc-wx26g [751.284293ms]
    May  9 16:46:18.296: INFO: Created: latency-svc-rrh28
    May  9 16:46:18.339: INFO: Got endpoints: latency-svc-phvxm [748.576238ms]
    May  9 16:46:18.353: INFO: Created: latency-svc-zld8c
    May  9 16:46:18.382: INFO: Got endpoints: latency-svc-f7n45 [749.28674ms]
    May  9 16:46:18.396: INFO: Created: latency-svc-95tsm
    May  9 16:46:18.431: INFO: Got endpoints: latency-svc-dtqtk [747.398322ms]
    May  9 16:46:18.446: INFO: Created: latency-svc-bwlbv
    May  9 16:46:18.486: INFO: Got endpoints: latency-svc-jhvhg [753.062863ms]
    May  9 16:46:18.499: INFO: Created: latency-svc-s6h5z
    May  9 16:46:18.532: INFO: Got endpoints: latency-svc-j224q [748.521136ms]
    May  9 16:46:18.546: INFO: Created: latency-svc-vftf9
    May  9 16:46:18.583: INFO: Got endpoints: latency-svc-nfq7k [750.338385ms]
    May  9 16:46:18.601: INFO: Created: latency-svc-8p6ks
    May  9 16:46:18.632: INFO: Got endpoints: latency-svc-5pzt4 [741.863889ms]
    May  9 16:46:18.646: INFO: Created: latency-svc-hnnxr
    May  9 16:46:18.683: INFO: Got endpoints: latency-svc-4wzsm [748.843738ms]
    May  9 16:46:18.695: INFO: Created: latency-svc-5qtzf
    May  9 16:46:18.735: INFO: Got endpoints: latency-svc-9xgqk [751.128141ms]
    May  9 16:46:18.748: INFO: Created: latency-svc-x9n8b
    May  9 16:46:18.786: INFO: Got endpoints: latency-svc-x7272 [749.378948ms]
    May  9 16:46:18.805: INFO: Created: latency-svc-s9w6w
    May  9 16:46:18.883: INFO: Got endpoints: latency-svc-wrkjj [799.481424ms]
    May  9 16:46:18.887: INFO: Got endpoints: latency-svc-trvp5 [754.589433ms]
    May  9 16:46:18.972: INFO: Got endpoints: latency-svc-7jmvd [785.310566ms]
    May  9 16:46:18.972: INFO: Created: latency-svc-wlz2z
    May  9 16:46:18.981: INFO: Created: latency-svc-cw6gw
    May  9 16:46:18.986: INFO: Got endpoints: latency-svc-l882p [748.549995ms]
    May  9 16:46:19.060: INFO: Created: latency-svc-96v7j
    May  9 16:46:19.066: INFO: Got endpoints: latency-svc-rrh28 [781.417006ms]
    May  9 16:46:19.071: INFO: Created: latency-svc-7nrv7
    May  9 16:46:19.080: INFO: Created: latency-svc-db9z7
    May  9 16:46:19.084: INFO: Got endpoints: latency-svc-zld8c [745.405431ms]
    May  9 16:46:19.100: INFO: Created: latency-svc-2qsbv
    May  9 16:46:19.139: INFO: Got endpoints: latency-svc-95tsm [757.165018ms]
    May  9 16:46:19.156: INFO: Created: latency-svc-k462q
    May  9 16:46:19.182: INFO: Got endpoints: latency-svc-bwlbv [750.148714ms]
    May  9 16:46:19.201: INFO: Created: latency-svc-nk99s
    May  9 16:46:19.232: INFO: Got endpoints: latency-svc-s6h5z [746.081241ms]
    May  9 16:46:19.245: INFO: Created: latency-svc-kf8r2
    May  9 16:46:19.282: INFO: Got endpoints: latency-svc-vftf9 [750.102309ms]
    May  9 16:46:19.333: INFO: Got endpoints: latency-svc-8p6ks [749.751628ms]
    May  9 16:46:19.386: INFO: Got endpoints: latency-svc-hnnxr [753.106889ms]
    May  9 16:46:19.433: INFO: Got endpoints: latency-svc-5qtzf [750.658777ms]
    May  9 16:46:19.482: INFO: Got endpoints: latency-svc-x9n8b [747.208893ms]
    May  9 16:46:19.534: INFO: Got endpoints: latency-svc-s9w6w [748.545275ms]
    May  9 16:46:19.581: INFO: Got endpoints: latency-svc-wlz2z [697.66469ms]
    May  9 16:46:19.631: INFO: Got endpoints: latency-svc-cw6gw [744.124384ms]
    May  9 16:46:19.683: INFO: Got endpoints: latency-svc-96v7j [711.212276ms]
    May  9 16:46:19.734: INFO: Got endpoints: latency-svc-7nrv7 [747.791297ms]
    May  9 16:46:19.783: INFO: Got endpoints: latency-svc-db9z7 [717.153941ms]
    May  9 16:46:19.834: INFO: Got endpoints: latency-svc-2qsbv [749.763969ms]
    May  9 16:46:19.884: INFO: Got endpoints: latency-svc-k462q [745.205105ms]
    May  9 16:46:19.934: INFO: Got endpoints: latency-svc-nk99s [752.547356ms]
    May  9 16:46:19.994: INFO: Got endpoints: latency-svc-kf8r2 [761.91186ms]
    May  9 16:46:19.994: INFO: Latencies: [19.957502ms 30.702614ms 37.062093ms 45.78423ms 54.188359ms 63.824026ms 68.258526ms 75.88395ms 81.095838ms 93.232104ms 107.96074ms 113.075712ms 124.312813ms 126.803401ms 128.422317ms 134.599115ms 136.243356ms 138.0533ms 139.506473ms 140.42651ms 144.743184ms 144.756767ms 145.132309ms 146.586596ms 147.315296ms 147.391173ms 147.669371ms 147.864967ms 149.303231ms 149.801079ms 151.032692ms 151.152078ms 154.329068ms 155.916827ms 156.734122ms 158.111192ms 173.151945ms 221.418884ms 259.681455ms 304.264594ms 346.510277ms 390.464989ms 431.999056ms 475.140995ms 529.790152ms 552.789226ms 597.97309ms 619.835602ms 669.701779ms 670.671938ms 697.66469ms 700.363162ms 700.579842ms 711.212276ms 717.153941ms 721.215055ms 725.667295ms 727.728426ms 727.973868ms 732.655908ms 737.609712ms 738.557029ms 739.920422ms 741.697877ms 741.863889ms 742.104598ms 742.396624ms 742.787178ms 743.044681ms 743.136462ms 744.124384ms 744.579535ms 744.827479ms 744.839297ms 745.205105ms 745.405431ms 746.081241ms 746.083784ms 746.312334ms 746.535816ms 746.53884ms 746.637266ms 746.679044ms 747.208893ms 747.382767ms 747.398322ms 747.451751ms 747.575201ms 747.608915ms 747.684577ms 747.791297ms 747.947994ms 747.961665ms 748.103972ms 748.104598ms 748.249793ms 748.297192ms 748.325941ms 748.50979ms 748.521136ms 748.521282ms 748.528232ms 748.545275ms 748.549995ms 748.576238ms 748.687102ms 748.792046ms 748.843738ms 748.845787ms 749.012137ms 749.03791ms 749.041158ms 749.09178ms 749.269115ms 749.28674ms 749.352119ms 749.352347ms 749.378948ms 749.424557ms 749.43251ms 749.550043ms 749.614581ms 749.621038ms 749.741688ms 749.751628ms 749.763969ms 749.777773ms 749.899372ms 749.906177ms 749.948949ms 749.9534ms 749.972397ms 749.988219ms 750.102309ms 750.148714ms 750.231265ms 750.281764ms 750.303052ms 750.338385ms 750.430523ms 750.551328ms 750.658777ms 750.658786ms 750.665779ms 750.736126ms 750.890356ms 750.924095ms 750.937173ms 751.052959ms 751.118136ms 751.128141ms 751.276844ms 751.284293ms 751.365115ms 751.378457ms 751.521306ms 751.540417ms 751.611892ms 751.618209ms 751.714021ms 751.813948ms 752.236626ms 752.547356ms 752.56131ms 752.655423ms 752.710101ms 752.813633ms 752.940331ms 752.97859ms 753.062863ms 753.106889ms 753.424422ms 753.724986ms 753.897916ms 754.213012ms 754.44581ms 754.563768ms 754.589433ms 754.835634ms 755.020846ms 755.259288ms 755.347447ms 755.517412ms 756.307651ms 757.165018ms 757.866608ms 758.570969ms 758.676133ms 759.530435ms 761.91186ms 764.330261ms 769.593142ms 770.703297ms 773.258926ms 781.391918ms 781.417006ms 785.310566ms 798.441299ms 799.481424ms 833.097434ms]
    May  9 16:46:19.994: INFO: 50 %ile: 748.521282ms
    May  9 16:46:19.994: INFO: 90 %ile: 755.259288ms
    May  9 16:46:19.994: INFO: 99 %ile: 799.481424ms
    May  9 16:46:19.994: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    May  9 16:46:19.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-5827" for this suite. 05/09/23 16:46:20.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:46:20.013
May  9 16:46:20.013: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename pods 05/09/23 16:46:20.013
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:46:20.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:46:20.037
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 05/09/23 16:46:20.041
May  9 16:46:20.053: INFO: created test-pod-1
May  9 16:46:20.060: INFO: created test-pod-2
May  9 16:46:20.068: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 05/09/23 16:46:20.068
May  9 16:46:20.068: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4683' to be running and ready
May  9 16:46:20.088: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  9 16:46:20.088: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  9 16:46:20.088: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  9 16:46:20.088: INFO: 0 / 3 pods in namespace 'pods-4683' are running and ready (0 seconds elapsed)
May  9 16:46:20.088: INFO: expected 0 pod replicas in namespace 'pods-4683', 0 are Running and Ready.
May  9 16:46:20.088: INFO: POD         NODE                                        PHASE    GRACE  CONDITIONS
May  9 16:46:20.088: INFO: test-pod-1  nodepool-8cc7f47e-9b0c-4801-88-node-7ad816  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:46:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:46:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:46:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:46:20 +0000 UTC  }]
May  9 16:46:20.088: INFO: test-pod-2  nodepool-8cc7f47e-9b0c-4801-88-node-7ad816  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:46:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:46:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:46:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:46:20 +0000 UTC  }]
May  9 16:46:20.088: INFO: test-pod-3  nodepool-8cc7f47e-9b0c-4801-88-node-f76f62  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:46:20 +0000 UTC  }]
May  9 16:46:20.088: INFO: 
May  9 16:46:22.117: INFO: 3 / 3 pods in namespace 'pods-4683' are running and ready (2 seconds elapsed)
May  9 16:46:22.117: INFO: expected 0 pod replicas in namespace 'pods-4683', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 05/09/23 16:46:22.147
May  9 16:46:22.153: INFO: Pod quantity 3 is different from expected quantity 0
May  9 16:46:23.160: INFO: Pod quantity 3 is different from expected quantity 0
May  9 16:46:24.161: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  9 16:46:25.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4683" for this suite. 05/09/23 16:46:25.167
------------------------------
â€¢ [SLOW TEST] [5.163 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:46:20.013
    May  9 16:46:20.013: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename pods 05/09/23 16:46:20.013
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:46:20.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:46:20.037
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 05/09/23 16:46:20.041
    May  9 16:46:20.053: INFO: created test-pod-1
    May  9 16:46:20.060: INFO: created test-pod-2
    May  9 16:46:20.068: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 05/09/23 16:46:20.068
    May  9 16:46:20.068: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4683' to be running and ready
    May  9 16:46:20.088: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  9 16:46:20.088: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  9 16:46:20.088: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  9 16:46:20.088: INFO: 0 / 3 pods in namespace 'pods-4683' are running and ready (0 seconds elapsed)
    May  9 16:46:20.088: INFO: expected 0 pod replicas in namespace 'pods-4683', 0 are Running and Ready.
    May  9 16:46:20.088: INFO: POD         NODE                                        PHASE    GRACE  CONDITIONS
    May  9 16:46:20.088: INFO: test-pod-1  nodepool-8cc7f47e-9b0c-4801-88-node-7ad816  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:46:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:46:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:46:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:46:20 +0000 UTC  }]
    May  9 16:46:20.088: INFO: test-pod-2  nodepool-8cc7f47e-9b0c-4801-88-node-7ad816  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:46:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:46:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:46:20 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:46:20 +0000 UTC  }]
    May  9 16:46:20.088: INFO: test-pod-3  nodepool-8cc7f47e-9b0c-4801-88-node-f76f62  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-09 16:46:20 +0000 UTC  }]
    May  9 16:46:20.088: INFO: 
    May  9 16:46:22.117: INFO: 3 / 3 pods in namespace 'pods-4683' are running and ready (2 seconds elapsed)
    May  9 16:46:22.117: INFO: expected 0 pod replicas in namespace 'pods-4683', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 05/09/23 16:46:22.147
    May  9 16:46:22.153: INFO: Pod quantity 3 is different from expected quantity 0
    May  9 16:46:23.160: INFO: Pod quantity 3 is different from expected quantity 0
    May  9 16:46:24.161: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  9 16:46:25.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4683" for this suite. 05/09/23 16:46:25.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:46:25.176
May  9 16:46:25.176: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename replicaset 05/09/23 16:46:25.177
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:46:25.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:46:25.227
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
May  9 16:46:25.267: INFO: Creating ReplicaSet my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee
May  9 16:46:25.282: INFO: Pod name my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee: Found 0 pods out of 1
May  9 16:46:30.289: INFO: Pod name my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee: Found 1 pods out of 1
May  9 16:46:30.289: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee" is running
May  9 16:46:30.289: INFO: Waiting up to 5m0s for pod "my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee-mmw52" in namespace "replicaset-6257" to be "running"
May  9 16:46:30.403: INFO: Pod "my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee-mmw52": Phase="Running", Reason="", readiness=true. Elapsed: 114.172948ms
May  9 16:46:30.403: INFO: Pod "my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee-mmw52" satisfied condition "running"
May  9 16:46:30.403: INFO: Pod "my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee-mmw52" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 16:46:25 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 16:46:27 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 16:46:27 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 16:46:25 +0000 UTC Reason: Message:}])
May  9 16:46:30.403: INFO: Trying to dial the pod
May  9 16:46:35.425: INFO: Controller my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee: Got expected result from replica 1 [my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee-mmw52]: "my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee-mmw52", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May  9 16:46:35.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6257" for this suite. 05/09/23 16:46:35.434
------------------------------
â€¢ [SLOW TEST] [10.267 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:46:25.176
    May  9 16:46:25.176: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename replicaset 05/09/23 16:46:25.177
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:46:25.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:46:25.227
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    May  9 16:46:25.267: INFO: Creating ReplicaSet my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee
    May  9 16:46:25.282: INFO: Pod name my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee: Found 0 pods out of 1
    May  9 16:46:30.289: INFO: Pod name my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee: Found 1 pods out of 1
    May  9 16:46:30.289: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee" is running
    May  9 16:46:30.289: INFO: Waiting up to 5m0s for pod "my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee-mmw52" in namespace "replicaset-6257" to be "running"
    May  9 16:46:30.403: INFO: Pod "my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee-mmw52": Phase="Running", Reason="", readiness=true. Elapsed: 114.172948ms
    May  9 16:46:30.403: INFO: Pod "my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee-mmw52" satisfied condition "running"
    May  9 16:46:30.403: INFO: Pod "my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee-mmw52" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 16:46:25 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 16:46:27 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 16:46:27 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-09 16:46:25 +0000 UTC Reason: Message:}])
    May  9 16:46:30.403: INFO: Trying to dial the pod
    May  9 16:46:35.425: INFO: Controller my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee: Got expected result from replica 1 [my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee-mmw52]: "my-hostname-basic-3de4fe98-8753-46d3-9e28-edb36533e1ee-mmw52", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May  9 16:46:35.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6257" for this suite. 05/09/23 16:46:35.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:46:35.444
May  9 16:46:35.444: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename pod-network-test 05/09/23 16:46:35.444
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:46:35.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:46:35.465
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-36 05/09/23 16:46:35.469
STEP: creating a selector 05/09/23 16:46:35.469
STEP: Creating the service pods in kubernetes 05/09/23 16:46:35.469
May  9 16:46:35.469: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  9 16:46:35.510: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-36" to be "running and ready"
May  9 16:46:35.514: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069391ms
May  9 16:46:35.514: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:46:37.528: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01856333s
May  9 16:46:37.528: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:46:39.525: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015435386s
May  9 16:46:39.525: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:46:41.522: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.012570946s
May  9 16:46:41.522: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:46:43.520: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010434511s
May  9 16:46:43.520: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:46:45.521: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011863799s
May  9 16:46:45.521: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:46:47.521: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011796405s
May  9 16:46:47.521: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:46:49.522: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.012334328s
May  9 16:46:49.522: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:46:51.521: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.011633314s
May  9 16:46:51.521: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:46:53.521: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011016455s
May  9 16:46:53.521: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:46:55.524: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014774879s
May  9 16:46:55.524: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:46:57.521: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011230401s
May  9 16:46:57.521: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  9 16:46:57.521: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  9 16:46:57.526: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-36" to be "running and ready"
May  9 16:46:57.532: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.012372ms
May  9 16:46:57.532: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  9 16:46:57.532: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  9 16:46:57.538: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-36" to be "running and ready"
May  9 16:46:57.544: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.021565ms
May  9 16:46:57.544: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  9 16:46:57.544: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/09/23 16:46:57.548
May  9 16:46:57.563: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-36" to be "running"
May  9 16:46:57.569: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.957278ms
May  9 16:46:59.575: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012517184s
May  9 16:46:59.575: INFO: Pod "test-container-pod" satisfied condition "running"
May  9 16:46:59.582: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-36" to be "running"
May  9 16:46:59.586: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.622956ms
May  9 16:46:59.586: INFO: Pod "host-test-container-pod" satisfied condition "running"
May  9 16:46:59.592: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May  9 16:46:59.592: INFO: Going to poll 10.2.1.125 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May  9 16:46:59.599: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.1.125:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-36 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:46:59.599: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:46:59.600: INFO: ExecWithOptions: Clientset creation
May  9 16:46:59.600: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-36/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.2.1.125%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  9 16:46:59.742: INFO: Found all 1 expected endpoints: [netserver-0]
May  9 16:46:59.742: INFO: Going to poll 10.2.0.232 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May  9 16:46:59.748: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.0.232:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-36 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:46:59.748: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:46:59.748: INFO: ExecWithOptions: Clientset creation
May  9 16:46:59.748: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-36/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.2.0.232%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  9 16:46:59.873: INFO: Found all 1 expected endpoints: [netserver-1]
May  9 16:46:59.873: INFO: Going to poll 10.2.2.230 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May  9 16:46:59.879: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.2.230:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-36 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:46:59.879: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:46:59.880: INFO: ExecWithOptions: Clientset creation
May  9 16:46:59.880: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-36/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.2.2.230%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  9 16:47:00.013: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
May  9 16:47:00.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-36" for this suite. 05/09/23 16:47:00.02
------------------------------
â€¢ [SLOW TEST] [24.589 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:46:35.444
    May  9 16:46:35.444: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename pod-network-test 05/09/23 16:46:35.444
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:46:35.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:46:35.465
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-36 05/09/23 16:46:35.469
    STEP: creating a selector 05/09/23 16:46:35.469
    STEP: Creating the service pods in kubernetes 05/09/23 16:46:35.469
    May  9 16:46:35.469: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  9 16:46:35.510: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-36" to be "running and ready"
    May  9 16:46:35.514: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069391ms
    May  9 16:46:35.514: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:46:37.528: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01856333s
    May  9 16:46:37.528: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:46:39.525: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015435386s
    May  9 16:46:39.525: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:46:41.522: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.012570946s
    May  9 16:46:41.522: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:46:43.520: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010434511s
    May  9 16:46:43.520: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:46:45.521: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011863799s
    May  9 16:46:45.521: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:46:47.521: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011796405s
    May  9 16:46:47.521: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:46:49.522: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.012334328s
    May  9 16:46:49.522: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:46:51.521: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.011633314s
    May  9 16:46:51.521: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:46:53.521: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011016455s
    May  9 16:46:53.521: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:46:55.524: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014774879s
    May  9 16:46:55.524: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:46:57.521: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011230401s
    May  9 16:46:57.521: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  9 16:46:57.521: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  9 16:46:57.526: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-36" to be "running and ready"
    May  9 16:46:57.532: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.012372ms
    May  9 16:46:57.532: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  9 16:46:57.532: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  9 16:46:57.538: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-36" to be "running and ready"
    May  9 16:46:57.544: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.021565ms
    May  9 16:46:57.544: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  9 16:46:57.544: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/09/23 16:46:57.548
    May  9 16:46:57.563: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-36" to be "running"
    May  9 16:46:57.569: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.957278ms
    May  9 16:46:59.575: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012517184s
    May  9 16:46:59.575: INFO: Pod "test-container-pod" satisfied condition "running"
    May  9 16:46:59.582: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-36" to be "running"
    May  9 16:46:59.586: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.622956ms
    May  9 16:46:59.586: INFO: Pod "host-test-container-pod" satisfied condition "running"
    May  9 16:46:59.592: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May  9 16:46:59.592: INFO: Going to poll 10.2.1.125 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    May  9 16:46:59.599: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.1.125:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-36 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:46:59.599: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:46:59.600: INFO: ExecWithOptions: Clientset creation
    May  9 16:46:59.600: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-36/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.2.1.125%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  9 16:46:59.742: INFO: Found all 1 expected endpoints: [netserver-0]
    May  9 16:46:59.742: INFO: Going to poll 10.2.0.232 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    May  9 16:46:59.748: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.0.232:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-36 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:46:59.748: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:46:59.748: INFO: ExecWithOptions: Clientset creation
    May  9 16:46:59.748: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-36/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.2.0.232%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  9 16:46:59.873: INFO: Found all 1 expected endpoints: [netserver-1]
    May  9 16:46:59.873: INFO: Going to poll 10.2.2.230 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    May  9 16:46:59.879: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.2.230:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-36 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:46:59.879: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:46:59.880: INFO: ExecWithOptions: Clientset creation
    May  9 16:46:59.880: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-36/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.2.2.230%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  9 16:47:00.013: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    May  9 16:47:00.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-36" for this suite. 05/09/23 16:47:00.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:47:00.035
May  9 16:47:00.035: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename namespaces 05/09/23 16:47:00.036
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:47:00.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:47:00.055
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 05/09/23 16:47:00.06
STEP: patching the Namespace 05/09/23 16:47:00.075
STEP: get the Namespace and ensuring it has the label 05/09/23 16:47:00.082
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 16:47:00.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-360" for this suite. 05/09/23 16:47:00.092
STEP: Destroying namespace "nspatchtest-fdba72b0-b4c0-4af6-926e-060c5c23ab8a-4258" for this suite. 05/09/23 16:47:00.104
------------------------------
â€¢ [0.085 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:47:00.035
    May  9 16:47:00.035: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename namespaces 05/09/23 16:47:00.036
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:47:00.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:47:00.055
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 05/09/23 16:47:00.06
    STEP: patching the Namespace 05/09/23 16:47:00.075
    STEP: get the Namespace and ensuring it has the label 05/09/23 16:47:00.082
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 16:47:00.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-360" for this suite. 05/09/23 16:47:00.092
    STEP: Destroying namespace "nspatchtest-fdba72b0-b4c0-4af6-926e-060c5c23ab8a-4258" for this suite. 05/09/23 16:47:00.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:47:00.121
May  9 16:47:00.121: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename deployment 05/09/23 16:47:00.122
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:47:00.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:47:00.153
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 05/09/23 16:47:00.166
STEP: waiting for Deployment to be created 05/09/23 16:47:00.172
STEP: waiting for all Replicas to be Ready 05/09/23 16:47:00.176
May  9 16:47:00.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  9 16:47:00.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  9 16:47:00.201: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  9 16:47:00.201: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  9 16:47:00.222: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  9 16:47:00.222: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  9 16:47:00.256: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  9 16:47:00.256: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  9 16:47:02.132: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May  9 16:47:02.132: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May  9 16:47:02.171: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 05/09/23 16:47:02.171
W0509 16:47:02.180683      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May  9 16:47:02.182: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 05/09/23 16:47:02.182
May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0
May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0
May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0
May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0
May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0
May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0
May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0
May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0
May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
May  9 16:47:02.197: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
May  9 16:47:02.197: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
May  9 16:47:02.219: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
May  9 16:47:02.219: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
May  9 16:47:02.237: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
May  9 16:47:02.237: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
May  9 16:47:03.954: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
May  9 16:47:03.954: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
May  9 16:47:03.992: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
STEP: listing Deployments 05/09/23 16:47:03.993
May  9 16:47:03.998: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 05/09/23 16:47:03.998
May  9 16:47:04.015: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 05/09/23 16:47:04.015
May  9 16:47:04.024: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  9 16:47:04.027: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  9 16:47:04.055: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  9 16:47:04.075: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  9 16:47:04.088: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  9 16:47:05.184: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  9 16:47:06.148: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
May  9 16:47:06.183: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  9 16:47:06.198: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  9 16:47:08.582: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 05/09/23 16:47:08.609
STEP: fetching the DeploymentStatus 05/09/23 16:47:08.621
May  9 16:47:08.628: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
May  9 16:47:08.628: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
May  9 16:47:08.628: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
May  9 16:47:08.629: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
May  9 16:47:08.629: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
May  9 16:47:08.629: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
May  9 16:47:08.629: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 3
May  9 16:47:08.629: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
May  9 16:47:08.629: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
May  9 16:47:08.629: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 3
STEP: deleting the Deployment 05/09/23 16:47:08.629
May  9 16:47:08.643: INFO: observed event type MODIFIED
May  9 16:47:08.644: INFO: observed event type MODIFIED
May  9 16:47:08.644: INFO: observed event type MODIFIED
May  9 16:47:08.644: INFO: observed event type MODIFIED
May  9 16:47:08.644: INFO: observed event type MODIFIED
May  9 16:47:08.644: INFO: observed event type MODIFIED
May  9 16:47:08.644: INFO: observed event type MODIFIED
May  9 16:47:08.644: INFO: observed event type MODIFIED
May  9 16:47:08.644: INFO: observed event type MODIFIED
May  9 16:47:08.644: INFO: observed event type MODIFIED
May  9 16:47:08.644: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  9 16:47:08.660: INFO: Log out all the ReplicaSets if there is no deployment created
May  9 16:47:08.665: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-180  6d59de9f-2c9a-42fc-85dd-773cac91bc9d 318145910 2 2023-05-09 16:47:04 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 5dd45c58-d3cf-4768-b123-4666df6850ea 0xc003bebec7 0xc003bebec8}] [] [{kube-controller-manager Update apps/v1 2023-05-09 16:47:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5dd45c58-d3cf-4768-b123-4666df6850ea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:47:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bebf50 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

May  9 16:47:08.671: INFO: pod: "test-deployment-7b7876f9d6-8xlt4":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-8xlt4 test-deployment-7b7876f9d6- deployment-180  6a438acc-d908-42d1-b795-3f5bbd439cf9 318145617 0 2023-05-09 16:47:04 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:3c1c5ae5361f96ffbea006c0bd0c0ff34bb392633bb86de3936b4282746fcc02 cni.projectcalico.org/podIP:10.2.2.232/32 cni.projectcalico.org/podIPs:10.2.2.232/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 6d59de9f-2c9a-42fc-85dd-773cac91bc9d 0xc00452fc47 0xc00452fc48}] [] [{calico Update v1 2023-05-09 16:47:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 16:47:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d59de9f-2c9a-42fc-85dd-773cac91bc9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 16:47:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.2.232\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kgqlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kgqlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:10.2.2.232,StartTime:2023-05-09 16:47:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 16:47:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cdba977fda47660b4640e4d9bda311b01633edfc3bdad5a5ae29803a305bb1e4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.2.232,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May  9 16:47:08.671: INFO: pod: "test-deployment-7b7876f9d6-x2dw4":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-x2dw4 test-deployment-7b7876f9d6- deployment-180  4d69760e-2e64-4483-a198-5cbcb4637cdc 318145909 0 2023-05-09 16:47:06 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:9e9ed2b2e7488c4742bdc205cfdc9ea1e917a5776813353fd0d9f1195b652f37 cni.projectcalico.org/podIP:10.2.1.129/32 cni.projectcalico.org/podIPs:10.2.1.129/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 6d59de9f-2c9a-42fc-85dd-773cac91bc9d 0xc00452fe77 0xc00452fe78}] [] [{kube-controller-manager Update v1 2023-05-09 16:47:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d59de9f-2c9a-42fc-85dd-773cac91bc9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 16:47:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 16:47:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.129\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dpd7p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dpd7p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:10.2.1.129,StartTime:2023-05-09 16:47:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 16:47:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://172cbbf76c05847f55a638d37de55df487dbb558b76e51c675912a8feac1d4a8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.129,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May  9 16:47:08.671: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-180  4596805d-6553-4e8d-9cf1-e171b12341b0 318145919 4 2023-05-09 16:47:02 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 5dd45c58-d3cf-4768-b123-4666df6850ea 0xc003bebfc7 0xc003bebfc8}] [] [{kube-controller-manager Update apps/v1 2023-05-09 16:47:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5dd45c58-d3cf-4768-b123-4666df6850ea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:47:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fa0050 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May  9 16:47:08.678: INFO: pod: "test-deployment-7df74c55ff-2gpcb":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-2gpcb test-deployment-7df74c55ff- deployment-180  ea760c67-b68d-4373-98ff-7d64b88c4aa4 318145915 0 2023-05-09 16:47:02 +0000 UTC 2023-05-09 16:47:09 +0000 UTC 0xc003dc54b0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:6ef604d85eed2d74ada810e5a3e49add385bbc9ae1c8aeacb2b90a74ce518089 cni.projectcalico.org/podIP:10.2.0.233/32 cni.projectcalico.org/podIPs:10.2.0.233/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 4596805d-6553-4e8d-9cf1-e171b12341b0 0xc003dc5507 0xc003dc5508}] [] [{calico Update v1 2023-05-09 16:47:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 16:47:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4596805d-6553-4e8d-9cf1-e171b12341b0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 16:47:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.0.233\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kfk79,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kfk79,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:10.2.0.233,StartTime:2023-05-09 16:47:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 16:47:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://11928abc06c3be6fcbc17e3eaa4ddb7045087ae0ad3271cc5d85c749655ac7e4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.0.233,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May  9 16:47:08.678: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-180  a39d5ea3-c5c2-4ad5-bd8d-399c1aa5446e 318145240 3 2023-05-09 16:47:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 5dd45c58-d3cf-4768-b123-4666df6850ea 0xc003fa00c7 0xc003fa00c8}] [] [{kube-controller-manager Update apps/v1 2023-05-09 16:47:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5dd45c58-d3cf-4768-b123-4666df6850ea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:47:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fa0150 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May  9 16:47:08.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-180" for this suite. 05/09/23 16:47:08.69
------------------------------
â€¢ [SLOW TEST] [8.580 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:47:00.121
    May  9 16:47:00.121: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename deployment 05/09/23 16:47:00.122
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:47:00.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:47:00.153
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 05/09/23 16:47:00.166
    STEP: waiting for Deployment to be created 05/09/23 16:47:00.172
    STEP: waiting for all Replicas to be Ready 05/09/23 16:47:00.176
    May  9 16:47:00.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  9 16:47:00.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  9 16:47:00.201: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  9 16:47:00.201: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  9 16:47:00.222: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  9 16:47:00.222: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  9 16:47:00.256: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  9 16:47:00.256: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  9 16:47:02.132: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    May  9 16:47:02.132: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    May  9 16:47:02.171: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 05/09/23 16:47:02.171
    W0509 16:47:02.180683      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May  9 16:47:02.182: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 05/09/23 16:47:02.182
    May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0
    May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0
    May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0
    May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0
    May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0
    May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0
    May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0
    May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 0
    May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
    May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
    May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
    May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
    May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
    May  9 16:47:02.186: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
    May  9 16:47:02.197: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
    May  9 16:47:02.197: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
    May  9 16:47:02.219: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
    May  9 16:47:02.219: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
    May  9 16:47:02.237: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
    May  9 16:47:02.237: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
    May  9 16:47:03.954: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
    May  9 16:47:03.954: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
    May  9 16:47:03.992: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
    STEP: listing Deployments 05/09/23 16:47:03.993
    May  9 16:47:03.998: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 05/09/23 16:47:03.998
    May  9 16:47:04.015: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 05/09/23 16:47:04.015
    May  9 16:47:04.024: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  9 16:47:04.027: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  9 16:47:04.055: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  9 16:47:04.075: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  9 16:47:04.088: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  9 16:47:05.184: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May  9 16:47:06.148: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    May  9 16:47:06.183: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May  9 16:47:06.198: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May  9 16:47:08.582: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 05/09/23 16:47:08.609
    STEP: fetching the DeploymentStatus 05/09/23 16:47:08.621
    May  9 16:47:08.628: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
    May  9 16:47:08.628: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
    May  9 16:47:08.628: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
    May  9 16:47:08.629: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
    May  9 16:47:08.629: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 1
    May  9 16:47:08.629: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
    May  9 16:47:08.629: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 3
    May  9 16:47:08.629: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
    May  9 16:47:08.629: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 2
    May  9 16:47:08.629: INFO: observed Deployment test-deployment in namespace deployment-180 with ReadyReplicas 3
    STEP: deleting the Deployment 05/09/23 16:47:08.629
    May  9 16:47:08.643: INFO: observed event type MODIFIED
    May  9 16:47:08.644: INFO: observed event type MODIFIED
    May  9 16:47:08.644: INFO: observed event type MODIFIED
    May  9 16:47:08.644: INFO: observed event type MODIFIED
    May  9 16:47:08.644: INFO: observed event type MODIFIED
    May  9 16:47:08.644: INFO: observed event type MODIFIED
    May  9 16:47:08.644: INFO: observed event type MODIFIED
    May  9 16:47:08.644: INFO: observed event type MODIFIED
    May  9 16:47:08.644: INFO: observed event type MODIFIED
    May  9 16:47:08.644: INFO: observed event type MODIFIED
    May  9 16:47:08.644: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  9 16:47:08.660: INFO: Log out all the ReplicaSets if there is no deployment created
    May  9 16:47:08.665: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-180  6d59de9f-2c9a-42fc-85dd-773cac91bc9d 318145910 2 2023-05-09 16:47:04 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 5dd45c58-d3cf-4768-b123-4666df6850ea 0xc003bebec7 0xc003bebec8}] [] [{kube-controller-manager Update apps/v1 2023-05-09 16:47:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5dd45c58-d3cf-4768-b123-4666df6850ea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:47:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bebf50 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    May  9 16:47:08.671: INFO: pod: "test-deployment-7b7876f9d6-8xlt4":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-8xlt4 test-deployment-7b7876f9d6- deployment-180  6a438acc-d908-42d1-b795-3f5bbd439cf9 318145617 0 2023-05-09 16:47:04 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:3c1c5ae5361f96ffbea006c0bd0c0ff34bb392633bb86de3936b4282746fcc02 cni.projectcalico.org/podIP:10.2.2.232/32 cni.projectcalico.org/podIPs:10.2.2.232/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 6d59de9f-2c9a-42fc-85dd-773cac91bc9d 0xc00452fc47 0xc00452fc48}] [] [{calico Update v1 2023-05-09 16:47:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 16:47:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d59de9f-2c9a-42fc-85dd-773cac91bc9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 16:47:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.2.232\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kgqlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kgqlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.94.118,PodIP:10.2.2.232,StartTime:2023-05-09 16:47:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 16:47:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cdba977fda47660b4640e4d9bda311b01633edfc3bdad5a5ae29803a305bb1e4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.2.232,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May  9 16:47:08.671: INFO: pod: "test-deployment-7b7876f9d6-x2dw4":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-x2dw4 test-deployment-7b7876f9d6- deployment-180  4d69760e-2e64-4483-a198-5cbcb4637cdc 318145909 0 2023-05-09 16:47:06 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:9e9ed2b2e7488c4742bdc205cfdc9ea1e917a5776813353fd0d9f1195b652f37 cni.projectcalico.org/podIP:10.2.1.129/32 cni.projectcalico.org/podIPs:10.2.1.129/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 6d59de9f-2c9a-42fc-85dd-773cac91bc9d 0xc00452fe77 0xc00452fe78}] [] [{kube-controller-manager Update v1 2023-05-09 16:47:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d59de9f-2c9a-42fc-85dd-773cac91bc9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-09 16:47:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-09 16:47:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.129\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dpd7p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dpd7p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:10.2.1.129,StartTime:2023-05-09 16:47:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 16:47:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://172cbbf76c05847f55a638d37de55df487dbb558b76e51c675912a8feac1d4a8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.129,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May  9 16:47:08.671: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-180  4596805d-6553-4e8d-9cf1-e171b12341b0 318145919 4 2023-05-09 16:47:02 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 5dd45c58-d3cf-4768-b123-4666df6850ea 0xc003bebfc7 0xc003bebfc8}] [] [{kube-controller-manager Update apps/v1 2023-05-09 16:47:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5dd45c58-d3cf-4768-b123-4666df6850ea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:47:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fa0050 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    May  9 16:47:08.678: INFO: pod: "test-deployment-7df74c55ff-2gpcb":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-2gpcb test-deployment-7df74c55ff- deployment-180  ea760c67-b68d-4373-98ff-7d64b88c4aa4 318145915 0 2023-05-09 16:47:02 +0000 UTC 2023-05-09 16:47:09 +0000 UTC 0xc003dc54b0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:6ef604d85eed2d74ada810e5a3e49add385bbc9ae1c8aeacb2b90a74ce518089 cni.projectcalico.org/podIP:10.2.0.233/32 cni.projectcalico.org/podIPs:10.2.0.233/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 4596805d-6553-4e8d-9cf1-e171b12341b0 0xc003dc5507 0xc003dc5508}] [] [{calico Update v1 2023-05-09 16:47:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 16:47:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4596805d-6553-4e8d-9cf1-e171b12341b0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 16:47:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.0.233\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kfk79,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kfk79,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-bbade7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:47:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.93.170,PodIP:10.2.0.233,StartTime:2023-05-09 16:47:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 16:47:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://11928abc06c3be6fcbc17e3eaa4ddb7045087ae0ad3271cc5d85c749655ac7e4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.0.233,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May  9 16:47:08.678: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-180  a39d5ea3-c5c2-4ad5-bd8d-399c1aa5446e 318145240 3 2023-05-09 16:47:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 5dd45c58-d3cf-4768-b123-4666df6850ea 0xc003fa00c7 0xc003fa00c8}] [] [{kube-controller-manager Update apps/v1 2023-05-09 16:47:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5dd45c58-d3cf-4768-b123-4666df6850ea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:47:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fa0150 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May  9 16:47:08.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-180" for this suite. 05/09/23 16:47:08.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:47:08.704
May  9 16:47:08.704: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubectl 05/09/23 16:47:08.705
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:47:08.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:47:08.735
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 05/09/23 16:47:08.739
May  9 16:47:08.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6145 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
May  9 16:47:08.853: INFO: stderr: ""
May  9 16:47:08.853: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 05/09/23 16:47:08.853
May  9 16:47:08.853: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May  9 16:47:08.853: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6145" to be "running and ready, or succeeded"
May  9 16:47:08.859: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.98731ms
May  9 16:47:08.859: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'nodepool-8cc7f47e-9b0c-4801-88-node-7ad816' to be 'Running' but was 'Pending'
May  9 16:47:10.868: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014732443s
May  9 16:47:10.868: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'nodepool-8cc7f47e-9b0c-4801-88-node-7ad816' to be 'Running' but was 'Pending'
May  9 16:47:12.866: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.012243435s
May  9 16:47:12.866: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May  9 16:47:12.866: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 05/09/23 16:47:12.866
May  9 16:47:12.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6145 logs logs-generator logs-generator'
May  9 16:47:13.034: INFO: stderr: ""
May  9 16:47:13.034: INFO: stdout: "I0509 16:47:10.345510       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/lvv 380\nI0509 16:47:10.545775       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/22r 533\nI0509 16:47:10.746130       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/9k98 544\nI0509 16:47:10.946515       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/xrs9 389\nI0509 16:47:11.145902       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/mmx 425\nI0509 16:47:11.346292       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/75q 242\nI0509 16:47:11.545603       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/m67 598\nI0509 16:47:11.745962       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/mp6 270\nI0509 16:47:11.946392       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/qhn7 239\nI0509 16:47:12.145699       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/cmz4 356\nI0509 16:47:12.346102       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/k24 316\nI0509 16:47:12.545642       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/wrl 433\nI0509 16:47:12.746080       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/nhx 240\nI0509 16:47:12.946482       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/8bw2 279\n"
STEP: limiting log lines 05/09/23 16:47:13.034
May  9 16:47:13.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6145 logs logs-generator logs-generator --tail=1'
May  9 16:47:13.145: INFO: stderr: ""
May  9 16:47:13.145: INFO: stdout: "I0509 16:47:12.946482       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/8bw2 279\n"
May  9 16:47:13.145: INFO: got output "I0509 16:47:12.946482       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/8bw2 279\n"
STEP: limiting log bytes 05/09/23 16:47:13.145
May  9 16:47:13.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6145 logs logs-generator logs-generator --limit-bytes=1'
May  9 16:47:13.249: INFO: stderr: ""
May  9 16:47:13.249: INFO: stdout: "I"
May  9 16:47:13.249: INFO: got output "I"
STEP: exposing timestamps 05/09/23 16:47:13.249
May  9 16:47:13.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6145 logs logs-generator logs-generator --tail=1 --timestamps'
May  9 16:47:13.356: INFO: stderr: ""
May  9 16:47:13.356: INFO: stdout: "2023-05-09T16:47:13.346211361Z I0509 16:47:13.346095       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/87t 247\n"
May  9 16:47:13.356: INFO: got output "2023-05-09T16:47:13.346211361Z I0509 16:47:13.346095       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/87t 247\n"
STEP: restricting to a time range 05/09/23 16:47:13.356
May  9 16:47:15.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6145 logs logs-generator logs-generator --since=1s'
May  9 16:47:15.955: INFO: stderr: ""
May  9 16:47:15.955: INFO: stdout: "I0509 16:47:15.145764       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/qn8 388\nI0509 16:47:15.346156       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/8qd 225\nI0509 16:47:15.546592       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/6sdz 368\nI0509 16:47:15.745977       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/kube-system/pods/p9xz 406\nI0509 16:47:15.946404       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/kcs8 253\n"
May  9 16:47:15.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6145 logs logs-generator logs-generator --since=24h'
May  9 16:47:16.058: INFO: stderr: ""
May  9 16:47:16.058: INFO: stdout: "I0509 16:47:10.345510       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/lvv 380\nI0509 16:47:10.545775       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/22r 533\nI0509 16:47:10.746130       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/9k98 544\nI0509 16:47:10.946515       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/xrs9 389\nI0509 16:47:11.145902       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/mmx 425\nI0509 16:47:11.346292       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/75q 242\nI0509 16:47:11.545603       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/m67 598\nI0509 16:47:11.745962       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/mp6 270\nI0509 16:47:11.946392       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/qhn7 239\nI0509 16:47:12.145699       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/cmz4 356\nI0509 16:47:12.346102       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/k24 316\nI0509 16:47:12.545642       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/wrl 433\nI0509 16:47:12.746080       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/nhx 240\nI0509 16:47:12.946482       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/8bw2 279\nI0509 16:47:13.145747       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/q4nw 210\nI0509 16:47:13.346095       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/87t 247\nI0509 16:47:13.546482       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/78dr 245\nI0509 16:47:13.745928       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/2lj 278\nI0509 16:47:13.946332       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/8srt 267\nI0509 16:47:14.145645       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/8fm 254\nI0509 16:47:14.346037       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/xnw 350\nI0509 16:47:14.548678       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/kkjl 481\nI0509 16:47:14.746020       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/q7r 269\nI0509 16:47:14.946437       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/kdnz 363\nI0509 16:47:15.145764       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/qn8 388\nI0509 16:47:15.346156       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/8qd 225\nI0509 16:47:15.546592       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/6sdz 368\nI0509 16:47:15.745977       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/kube-system/pods/p9xz 406\nI0509 16:47:15.946404       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/kcs8 253\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
May  9 16:47:16.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6145 delete pod logs-generator'
May  9 16:47:17.277: INFO: stderr: ""
May  9 16:47:17.277: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  9 16:47:17.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6145" for this suite. 05/09/23 16:47:17.314
------------------------------
â€¢ [SLOW TEST] [8.633 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:47:08.704
    May  9 16:47:08.704: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubectl 05/09/23 16:47:08.705
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:47:08.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:47:08.735
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 05/09/23 16:47:08.739
    May  9 16:47:08.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6145 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    May  9 16:47:08.853: INFO: stderr: ""
    May  9 16:47:08.853: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 05/09/23 16:47:08.853
    May  9 16:47:08.853: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    May  9 16:47:08.853: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6145" to be "running and ready, or succeeded"
    May  9 16:47:08.859: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.98731ms
    May  9 16:47:08.859: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'nodepool-8cc7f47e-9b0c-4801-88-node-7ad816' to be 'Running' but was 'Pending'
    May  9 16:47:10.868: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014732443s
    May  9 16:47:10.868: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'nodepool-8cc7f47e-9b0c-4801-88-node-7ad816' to be 'Running' but was 'Pending'
    May  9 16:47:12.866: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.012243435s
    May  9 16:47:12.866: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    May  9 16:47:12.866: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 05/09/23 16:47:12.866
    May  9 16:47:12.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6145 logs logs-generator logs-generator'
    May  9 16:47:13.034: INFO: stderr: ""
    May  9 16:47:13.034: INFO: stdout: "I0509 16:47:10.345510       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/lvv 380\nI0509 16:47:10.545775       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/22r 533\nI0509 16:47:10.746130       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/9k98 544\nI0509 16:47:10.946515       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/xrs9 389\nI0509 16:47:11.145902       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/mmx 425\nI0509 16:47:11.346292       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/75q 242\nI0509 16:47:11.545603       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/m67 598\nI0509 16:47:11.745962       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/mp6 270\nI0509 16:47:11.946392       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/qhn7 239\nI0509 16:47:12.145699       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/cmz4 356\nI0509 16:47:12.346102       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/k24 316\nI0509 16:47:12.545642       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/wrl 433\nI0509 16:47:12.746080       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/nhx 240\nI0509 16:47:12.946482       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/8bw2 279\n"
    STEP: limiting log lines 05/09/23 16:47:13.034
    May  9 16:47:13.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6145 logs logs-generator logs-generator --tail=1'
    May  9 16:47:13.145: INFO: stderr: ""
    May  9 16:47:13.145: INFO: stdout: "I0509 16:47:12.946482       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/8bw2 279\n"
    May  9 16:47:13.145: INFO: got output "I0509 16:47:12.946482       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/8bw2 279\n"
    STEP: limiting log bytes 05/09/23 16:47:13.145
    May  9 16:47:13.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6145 logs logs-generator logs-generator --limit-bytes=1'
    May  9 16:47:13.249: INFO: stderr: ""
    May  9 16:47:13.249: INFO: stdout: "I"
    May  9 16:47:13.249: INFO: got output "I"
    STEP: exposing timestamps 05/09/23 16:47:13.249
    May  9 16:47:13.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6145 logs logs-generator logs-generator --tail=1 --timestamps'
    May  9 16:47:13.356: INFO: stderr: ""
    May  9 16:47:13.356: INFO: stdout: "2023-05-09T16:47:13.346211361Z I0509 16:47:13.346095       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/87t 247\n"
    May  9 16:47:13.356: INFO: got output "2023-05-09T16:47:13.346211361Z I0509 16:47:13.346095       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/87t 247\n"
    STEP: restricting to a time range 05/09/23 16:47:13.356
    May  9 16:47:15.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6145 logs logs-generator logs-generator --since=1s'
    May  9 16:47:15.955: INFO: stderr: ""
    May  9 16:47:15.955: INFO: stdout: "I0509 16:47:15.145764       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/qn8 388\nI0509 16:47:15.346156       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/8qd 225\nI0509 16:47:15.546592       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/6sdz 368\nI0509 16:47:15.745977       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/kube-system/pods/p9xz 406\nI0509 16:47:15.946404       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/kcs8 253\n"
    May  9 16:47:15.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6145 logs logs-generator logs-generator --since=24h'
    May  9 16:47:16.058: INFO: stderr: ""
    May  9 16:47:16.058: INFO: stdout: "I0509 16:47:10.345510       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/lvv 380\nI0509 16:47:10.545775       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/22r 533\nI0509 16:47:10.746130       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/9k98 544\nI0509 16:47:10.946515       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/xrs9 389\nI0509 16:47:11.145902       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/mmx 425\nI0509 16:47:11.346292       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/75q 242\nI0509 16:47:11.545603       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/m67 598\nI0509 16:47:11.745962       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/mp6 270\nI0509 16:47:11.946392       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/qhn7 239\nI0509 16:47:12.145699       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/cmz4 356\nI0509 16:47:12.346102       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/k24 316\nI0509 16:47:12.545642       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/wrl 433\nI0509 16:47:12.746080       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/nhx 240\nI0509 16:47:12.946482       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/8bw2 279\nI0509 16:47:13.145747       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/q4nw 210\nI0509 16:47:13.346095       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/87t 247\nI0509 16:47:13.546482       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/78dr 245\nI0509 16:47:13.745928       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/2lj 278\nI0509 16:47:13.946332       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/8srt 267\nI0509 16:47:14.145645       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/8fm 254\nI0509 16:47:14.346037       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/xnw 350\nI0509 16:47:14.548678       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/kkjl 481\nI0509 16:47:14.746020       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/q7r 269\nI0509 16:47:14.946437       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/kdnz 363\nI0509 16:47:15.145764       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/qn8 388\nI0509 16:47:15.346156       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/8qd 225\nI0509 16:47:15.546592       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/6sdz 368\nI0509 16:47:15.745977       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/kube-system/pods/p9xz 406\nI0509 16:47:15.946404       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/kcs8 253\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    May  9 16:47:16.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6145 delete pod logs-generator'
    May  9 16:47:17.277: INFO: stderr: ""
    May  9 16:47:17.277: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  9 16:47:17.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6145" for this suite. 05/09/23 16:47:17.314
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:47:17.337
May  9 16:47:17.337: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename secrets 05/09/23 16:47:17.338
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:47:17.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:47:17.361
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-e120ccac-db5d-4763-9a84-65ca6fbab025 05/09/23 16:47:17.369
STEP: Creating a pod to test consume secrets 05/09/23 16:47:17.377
May  9 16:47:17.388: INFO: Waiting up to 5m0s for pod "pod-secrets-a47133a4-1c3a-46e5-b120-a84da03d5fe7" in namespace "secrets-6172" to be "Succeeded or Failed"
May  9 16:47:17.393: INFO: Pod "pod-secrets-a47133a4-1c3a-46e5-b120-a84da03d5fe7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.148916ms
May  9 16:47:19.401: INFO: Pod "pod-secrets-a47133a4-1c3a-46e5-b120-a84da03d5fe7": Phase="Running", Reason="", readiness=false. Elapsed: 2.012965721s
May  9 16:47:21.399: INFO: Pod "pod-secrets-a47133a4-1c3a-46e5-b120-a84da03d5fe7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011763621s
STEP: Saw pod success 05/09/23 16:47:21.399
May  9 16:47:21.400: INFO: Pod "pod-secrets-a47133a4-1c3a-46e5-b120-a84da03d5fe7" satisfied condition "Succeeded or Failed"
May  9 16:47:21.405: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-secrets-a47133a4-1c3a-46e5-b120-a84da03d5fe7 container secret-volume-test: <nil>
STEP: delete the pod 05/09/23 16:47:21.416
May  9 16:47:21.431: INFO: Waiting for pod pod-secrets-a47133a4-1c3a-46e5-b120-a84da03d5fe7 to disappear
May  9 16:47:21.435: INFO: Pod pod-secrets-a47133a4-1c3a-46e5-b120-a84da03d5fe7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May  9 16:47:21.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6172" for this suite. 05/09/23 16:47:21.443
------------------------------
â€¢ [4.114 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:47:17.337
    May  9 16:47:17.337: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename secrets 05/09/23 16:47:17.338
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:47:17.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:47:17.361
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-e120ccac-db5d-4763-9a84-65ca6fbab025 05/09/23 16:47:17.369
    STEP: Creating a pod to test consume secrets 05/09/23 16:47:17.377
    May  9 16:47:17.388: INFO: Waiting up to 5m0s for pod "pod-secrets-a47133a4-1c3a-46e5-b120-a84da03d5fe7" in namespace "secrets-6172" to be "Succeeded or Failed"
    May  9 16:47:17.393: INFO: Pod "pod-secrets-a47133a4-1c3a-46e5-b120-a84da03d5fe7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.148916ms
    May  9 16:47:19.401: INFO: Pod "pod-secrets-a47133a4-1c3a-46e5-b120-a84da03d5fe7": Phase="Running", Reason="", readiness=false. Elapsed: 2.012965721s
    May  9 16:47:21.399: INFO: Pod "pod-secrets-a47133a4-1c3a-46e5-b120-a84da03d5fe7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011763621s
    STEP: Saw pod success 05/09/23 16:47:21.399
    May  9 16:47:21.400: INFO: Pod "pod-secrets-a47133a4-1c3a-46e5-b120-a84da03d5fe7" satisfied condition "Succeeded or Failed"
    May  9 16:47:21.405: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-secrets-a47133a4-1c3a-46e5-b120-a84da03d5fe7 container secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 16:47:21.416
    May  9 16:47:21.431: INFO: Waiting for pod pod-secrets-a47133a4-1c3a-46e5-b120-a84da03d5fe7 to disappear
    May  9 16:47:21.435: INFO: Pod pod-secrets-a47133a4-1c3a-46e5-b120-a84da03d5fe7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May  9 16:47:21.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6172" for this suite. 05/09/23 16:47:21.443
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:47:21.453
May  9 16:47:21.453: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename tables 05/09/23 16:47:21.454
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:47:21.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:47:21.483
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
May  9 16:47:21.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-7932" for this suite. 05/09/23 16:47:21.497
------------------------------
â€¢ [0.054 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:47:21.453
    May  9 16:47:21.453: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename tables 05/09/23 16:47:21.454
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:47:21.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:47:21.483
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    May  9 16:47:21.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-7932" for this suite. 05/09/23 16:47:21.497
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:47:21.508
May  9 16:47:21.508: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 16:47:21.509
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:47:21.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:47:21.528
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-f1e00951-83ee-4ce1-8161-f3473594191a 05/09/23 16:47:21.533
STEP: Creating a pod to test consume configMaps 05/09/23 16:47:21.539
May  9 16:47:21.552: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-82d988b0-1f3a-4f70-8814-d061d7593660" in namespace "projected-4159" to be "Succeeded or Failed"
May  9 16:47:21.558: INFO: Pod "pod-projected-configmaps-82d988b0-1f3a-4f70-8814-d061d7593660": Phase="Pending", Reason="", readiness=false. Elapsed: 5.903304ms
May  9 16:47:23.565: INFO: Pod "pod-projected-configmaps-82d988b0-1f3a-4f70-8814-d061d7593660": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013664817s
May  9 16:47:25.566: INFO: Pod "pod-projected-configmaps-82d988b0-1f3a-4f70-8814-d061d7593660": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014352106s
STEP: Saw pod success 05/09/23 16:47:25.566
May  9 16:47:25.566: INFO: Pod "pod-projected-configmaps-82d988b0-1f3a-4f70-8814-d061d7593660" satisfied condition "Succeeded or Failed"
May  9 16:47:25.572: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-configmaps-82d988b0-1f3a-4f70-8814-d061d7593660 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 16:47:25.584
May  9 16:47:25.602: INFO: Waiting for pod pod-projected-configmaps-82d988b0-1f3a-4f70-8814-d061d7593660 to disappear
May  9 16:47:25.607: INFO: Pod pod-projected-configmaps-82d988b0-1f3a-4f70-8814-d061d7593660 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May  9 16:47:25.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4159" for this suite. 05/09/23 16:47:25.614
------------------------------
â€¢ [4.115 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:47:21.508
    May  9 16:47:21.508: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 16:47:21.509
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:47:21.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:47:21.528
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-f1e00951-83ee-4ce1-8161-f3473594191a 05/09/23 16:47:21.533
    STEP: Creating a pod to test consume configMaps 05/09/23 16:47:21.539
    May  9 16:47:21.552: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-82d988b0-1f3a-4f70-8814-d061d7593660" in namespace "projected-4159" to be "Succeeded or Failed"
    May  9 16:47:21.558: INFO: Pod "pod-projected-configmaps-82d988b0-1f3a-4f70-8814-d061d7593660": Phase="Pending", Reason="", readiness=false. Elapsed: 5.903304ms
    May  9 16:47:23.565: INFO: Pod "pod-projected-configmaps-82d988b0-1f3a-4f70-8814-d061d7593660": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013664817s
    May  9 16:47:25.566: INFO: Pod "pod-projected-configmaps-82d988b0-1f3a-4f70-8814-d061d7593660": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014352106s
    STEP: Saw pod success 05/09/23 16:47:25.566
    May  9 16:47:25.566: INFO: Pod "pod-projected-configmaps-82d988b0-1f3a-4f70-8814-d061d7593660" satisfied condition "Succeeded or Failed"
    May  9 16:47:25.572: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-configmaps-82d988b0-1f3a-4f70-8814-d061d7593660 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 16:47:25.584
    May  9 16:47:25.602: INFO: Waiting for pod pod-projected-configmaps-82d988b0-1f3a-4f70-8814-d061d7593660 to disappear
    May  9 16:47:25.607: INFO: Pod pod-projected-configmaps-82d988b0-1f3a-4f70-8814-d061d7593660 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May  9 16:47:25.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4159" for this suite. 05/09/23 16:47:25.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:47:25.629
May  9 16:47:25.629: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename var-expansion 05/09/23 16:47:25.63
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:47:25.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:47:25.652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 05/09/23 16:47:25.656
May  9 16:47:25.666: INFO: Waiting up to 2m0s for pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379" in namespace "var-expansion-9614" to be "running"
May  9 16:47:25.672: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 5.732058ms
May  9 16:47:27.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014075174s
May  9 16:47:29.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012536748s
May  9 16:47:31.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013146313s
May  9 16:47:33.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013287567s
May  9 16:47:35.678: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012326048s
May  9 16:47:37.678: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011939706s
May  9 16:47:39.678: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012323191s
May  9 16:47:41.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013586536s
May  9 16:47:43.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01377004s
May  9 16:47:45.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 20.0136304s
May  9 16:47:47.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 22.012706504s
May  9 16:47:49.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 24.0138507s
May  9 16:47:51.687: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 26.020841412s
May  9 16:47:53.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 28.013323504s
May  9 16:47:55.684: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 30.017430671s
May  9 16:47:57.677: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 32.01096314s
May  9 16:47:59.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 34.013870752s
May  9 16:48:01.681: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 36.014924853s
May  9 16:48:03.682: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 38.016145272s
May  9 16:48:05.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 40.014010018s
May  9 16:48:07.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 42.013405418s
May  9 16:48:09.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 44.013074087s
May  9 16:48:11.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 46.013724335s
May  9 16:48:13.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 48.01307033s
May  9 16:48:15.682: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 50.016013953s
May  9 16:48:17.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 52.013642278s
May  9 16:48:19.678: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 54.011823651s
May  9 16:48:21.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 56.013037172s
May  9 16:48:23.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 58.013214554s
May  9 16:48:25.681: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.015222714s
May  9 16:48:27.678: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011711823s
May  9 16:48:29.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.01425099s
May  9 16:48:31.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012433325s
May  9 16:48:33.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.012505703s
May  9 16:48:35.678: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.012328031s
May  9 16:48:37.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.013413901s
May  9 16:48:39.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.013407351s
May  9 16:48:41.678: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.01209375s
May  9 16:48:43.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.012774888s
May  9 16:48:45.678: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.011507074s
May  9 16:48:47.678: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.01186053s
May  9 16:48:49.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.013757997s
May  9 16:48:51.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.012415145s
May  9 16:48:53.682: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.015485587s
May  9 16:48:55.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.013675735s
May  9 16:48:57.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.012704479s
May  9 16:48:59.681: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.015338536s
May  9 16:49:01.681: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.014588399s
May  9 16:49:03.681: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.015007246s
May  9 16:49:05.683: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.016704144s
May  9 16:49:07.681: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.014651507s
May  9 16:49:09.682: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.016263881s
May  9 16:49:11.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.012895128s
May  9 16:49:13.681: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.014442082s
May  9 16:49:15.707: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.04115878s
May  9 16:49:17.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.012710311s
May  9 16:49:19.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.013715322s
May  9 16:49:21.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.01348256s
May  9 16:49:23.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.012743374s
May  9 16:49:25.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013264111s
May  9 16:49:25.685: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.018696859s
STEP: updating the pod 05/09/23 16:49:25.685
May  9 16:49:26.203: INFO: Successfully updated pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379"
STEP: waiting for pod running 05/09/23 16:49:26.203
May  9 16:49:26.203: INFO: Waiting up to 2m0s for pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379" in namespace "var-expansion-9614" to be "running"
May  9 16:49:26.207: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 4.61374ms
May  9 16:49:28.217: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Running", Reason="", readiness=true. Elapsed: 2.014193017s
May  9 16:49:28.217: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379" satisfied condition "running"
STEP: deleting the pod gracefully 05/09/23 16:49:28.217
May  9 16:49:28.217: INFO: Deleting pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379" in namespace "var-expansion-9614"
May  9 16:49:28.228: INFO: Wait up to 5m0s for pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May  9 16:50:00.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9614" for this suite. 05/09/23 16:50:00.253
------------------------------
â€¢ [SLOW TEST] [154.633 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:47:25.629
    May  9 16:47:25.629: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename var-expansion 05/09/23 16:47:25.63
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:47:25.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:47:25.652
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 05/09/23 16:47:25.656
    May  9 16:47:25.666: INFO: Waiting up to 2m0s for pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379" in namespace "var-expansion-9614" to be "running"
    May  9 16:47:25.672: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 5.732058ms
    May  9 16:47:27.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014075174s
    May  9 16:47:29.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012536748s
    May  9 16:47:31.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013146313s
    May  9 16:47:33.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013287567s
    May  9 16:47:35.678: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012326048s
    May  9 16:47:37.678: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011939706s
    May  9 16:47:39.678: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012323191s
    May  9 16:47:41.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013586536s
    May  9 16:47:43.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01377004s
    May  9 16:47:45.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 20.0136304s
    May  9 16:47:47.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 22.012706504s
    May  9 16:47:49.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 24.0138507s
    May  9 16:47:51.687: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 26.020841412s
    May  9 16:47:53.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 28.013323504s
    May  9 16:47:55.684: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 30.017430671s
    May  9 16:47:57.677: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 32.01096314s
    May  9 16:47:59.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 34.013870752s
    May  9 16:48:01.681: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 36.014924853s
    May  9 16:48:03.682: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 38.016145272s
    May  9 16:48:05.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 40.014010018s
    May  9 16:48:07.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 42.013405418s
    May  9 16:48:09.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 44.013074087s
    May  9 16:48:11.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 46.013724335s
    May  9 16:48:13.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 48.01307033s
    May  9 16:48:15.682: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 50.016013953s
    May  9 16:48:17.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 52.013642278s
    May  9 16:48:19.678: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 54.011823651s
    May  9 16:48:21.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 56.013037172s
    May  9 16:48:23.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 58.013214554s
    May  9 16:48:25.681: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.015222714s
    May  9 16:48:27.678: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011711823s
    May  9 16:48:29.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.01425099s
    May  9 16:48:31.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012433325s
    May  9 16:48:33.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.012505703s
    May  9 16:48:35.678: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.012328031s
    May  9 16:48:37.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.013413901s
    May  9 16:48:39.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.013407351s
    May  9 16:48:41.678: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.01209375s
    May  9 16:48:43.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.012774888s
    May  9 16:48:45.678: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.011507074s
    May  9 16:48:47.678: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.01186053s
    May  9 16:48:49.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.013757997s
    May  9 16:48:51.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.012415145s
    May  9 16:48:53.682: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.015485587s
    May  9 16:48:55.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.013675735s
    May  9 16:48:57.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.012704479s
    May  9 16:48:59.681: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.015338536s
    May  9 16:49:01.681: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.014588399s
    May  9 16:49:03.681: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.015007246s
    May  9 16:49:05.683: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.016704144s
    May  9 16:49:07.681: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.014651507s
    May  9 16:49:09.682: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.016263881s
    May  9 16:49:11.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.012895128s
    May  9 16:49:13.681: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.014442082s
    May  9 16:49:15.707: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.04115878s
    May  9 16:49:17.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.012710311s
    May  9 16:49:19.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.013715322s
    May  9 16:49:21.680: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.01348256s
    May  9 16:49:23.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.012743374s
    May  9 16:49:25.679: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013264111s
    May  9 16:49:25.685: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.018696859s
    STEP: updating the pod 05/09/23 16:49:25.685
    May  9 16:49:26.203: INFO: Successfully updated pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379"
    STEP: waiting for pod running 05/09/23 16:49:26.203
    May  9 16:49:26.203: INFO: Waiting up to 2m0s for pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379" in namespace "var-expansion-9614" to be "running"
    May  9 16:49:26.207: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Pending", Reason="", readiness=false. Elapsed: 4.61374ms
    May  9 16:49:28.217: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379": Phase="Running", Reason="", readiness=true. Elapsed: 2.014193017s
    May  9 16:49:28.217: INFO: Pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379" satisfied condition "running"
    STEP: deleting the pod gracefully 05/09/23 16:49:28.217
    May  9 16:49:28.217: INFO: Deleting pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379" in namespace "var-expansion-9614"
    May  9 16:49:28.228: INFO: Wait up to 5m0s for pod "var-expansion-69fe334b-d2f9-425d-a916-9dbbcdc70379" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May  9 16:50:00.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9614" for this suite. 05/09/23 16:50:00.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:50:00.263
May  9 16:50:00.263: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubelet-test 05/09/23 16:50:00.263
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:00.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:00.3
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May  9 16:50:00.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9769" for this suite. 05/09/23 16:50:00.338
------------------------------
â€¢ [0.083 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:50:00.263
    May  9 16:50:00.263: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubelet-test 05/09/23 16:50:00.263
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:00.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:00.3
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May  9 16:50:00.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9769" for this suite. 05/09/23 16:50:00.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:50:00.347
May  9 16:50:00.348: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 16:50:00.348
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:00.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:00.388
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 05/09/23 16:50:00.392
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 05/09/23 16:50:00.394
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 05/09/23 16:50:00.394
STEP: fetching the /apis/apiextensions.k8s.io discovery document 05/09/23 16:50:00.394
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 05/09/23 16:50:00.396
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 05/09/23 16:50:00.396
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 05/09/23 16:50:00.398
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:50:00.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1307" for this suite. 05/09/23 16:50:00.404
------------------------------
â€¢ [0.080 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:50:00.347
    May  9 16:50:00.348: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 16:50:00.348
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:00.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:00.388
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 05/09/23 16:50:00.392
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 05/09/23 16:50:00.394
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 05/09/23 16:50:00.394
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 05/09/23 16:50:00.394
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 05/09/23 16:50:00.396
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 05/09/23 16:50:00.396
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 05/09/23 16:50:00.398
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:50:00.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1307" for this suite. 05/09/23 16:50:00.404
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:50:00.431
May  9 16:50:00.431: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename container-lifecycle-hook 05/09/23 16:50:00.432
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:00.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:00.453
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 05/09/23 16:50:00.478
May  9 16:50:00.489: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1786" to be "running and ready"
May  9 16:50:00.493: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.44298ms
May  9 16:50:00.494: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  9 16:50:02.501: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.0123877s
May  9 16:50:02.501: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  9 16:50:02.501: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 05/09/23 16:50:02.506
May  9 16:50:02.515: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-1786" to be "running and ready"
May  9 16:50:02.520: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.131853ms
May  9 16:50:02.520: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
May  9 16:50:04.545: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.029529941s
May  9 16:50:04.545: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
May  9 16:50:04.545: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 05/09/23 16:50:04.551
May  9 16:50:04.561: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  9 16:50:04.567: INFO: Pod pod-with-prestop-http-hook still exists
May  9 16:50:06.568: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  9 16:50:06.574: INFO: Pod pod-with-prestop-http-hook still exists
May  9 16:50:08.567: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  9 16:50:08.573: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 05/09/23 16:50:08.573
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
May  9 16:50:08.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1786" for this suite. 05/09/23 16:50:08.639
------------------------------
â€¢ [SLOW TEST] [8.217 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:50:00.431
    May  9 16:50:00.431: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/09/23 16:50:00.432
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:00.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:00.453
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 05/09/23 16:50:00.478
    May  9 16:50:00.489: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1786" to be "running and ready"
    May  9 16:50:00.493: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.44298ms
    May  9 16:50:00.494: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:50:02.501: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.0123877s
    May  9 16:50:02.501: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  9 16:50:02.501: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 05/09/23 16:50:02.506
    May  9 16:50:02.515: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-1786" to be "running and ready"
    May  9 16:50:02.520: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.131853ms
    May  9 16:50:02.520: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:50:04.545: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.029529941s
    May  9 16:50:04.545: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    May  9 16:50:04.545: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 05/09/23 16:50:04.551
    May  9 16:50:04.561: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May  9 16:50:04.567: INFO: Pod pod-with-prestop-http-hook still exists
    May  9 16:50:06.568: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May  9 16:50:06.574: INFO: Pod pod-with-prestop-http-hook still exists
    May  9 16:50:08.567: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May  9 16:50:08.573: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 05/09/23 16:50:08.573
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    May  9 16:50:08.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1786" for this suite. 05/09/23 16:50:08.639
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:50:08.649
May  9 16:50:08.649: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename webhook 05/09/23 16:50:08.65
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:08.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:08.674
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/09/23 16:50:08.694
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:50:09.029
STEP: Deploying the webhook pod 05/09/23 16:50:09.042
STEP: Wait for the deployment to be ready 05/09/23 16:50:09.055
May  9 16:50:09.063: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 16:50:11.079
STEP: Verifying the service has paired with the endpoint 05/09/23 16:50:11.093
May  9 16:50:12.094: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 05/09/23 16:50:12.176
STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 16:50:12.218
STEP: Deleting the collection of validation webhooks 05/09/23 16:50:12.255
STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 16:50:12.332
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:50:12.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6826" for this suite. 05/09/23 16:50:12.407
STEP: Destroying namespace "webhook-6826-markers" for this suite. 05/09/23 16:50:12.416
------------------------------
â€¢ [3.776 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:50:08.649
    May  9 16:50:08.649: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename webhook 05/09/23 16:50:08.65
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:08.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:08.674
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/09/23 16:50:08.694
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:50:09.029
    STEP: Deploying the webhook pod 05/09/23 16:50:09.042
    STEP: Wait for the deployment to be ready 05/09/23 16:50:09.055
    May  9 16:50:09.063: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 16:50:11.079
    STEP: Verifying the service has paired with the endpoint 05/09/23 16:50:11.093
    May  9 16:50:12.094: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 05/09/23 16:50:12.176
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 16:50:12.218
    STEP: Deleting the collection of validation webhooks 05/09/23 16:50:12.255
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/09/23 16:50:12.332
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:50:12.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6826" for this suite. 05/09/23 16:50:12.407
    STEP: Destroying namespace "webhook-6826-markers" for this suite. 05/09/23 16:50:12.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:50:12.427
May  9 16:50:12.427: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename runtimeclass 05/09/23 16:50:12.428
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:12.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:12.449
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-1940-delete-me 05/09/23 16:50:12.46
STEP: Waiting for the RuntimeClass to disappear 05/09/23 16:50:12.469
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May  9 16:50:12.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1940" for this suite. 05/09/23 16:50:12.494
------------------------------
â€¢ [0.076 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:50:12.427
    May  9 16:50:12.427: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename runtimeclass 05/09/23 16:50:12.428
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:12.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:12.449
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-1940-delete-me 05/09/23 16:50:12.46
    STEP: Waiting for the RuntimeClass to disappear 05/09/23 16:50:12.469
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May  9 16:50:12.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1940" for this suite. 05/09/23 16:50:12.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:50:12.506
May  9 16:50:12.506: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename namespaces 05/09/23 16:50:12.506
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:12.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:12.529
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-n8hhp" 05/09/23 16:50:12.533
May  9 16:50:12.549: INFO: Namespace "e2e-ns-n8hhp-9132" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-n8hhp-9132" 05/09/23 16:50:12.549
May  9 16:50:12.560: INFO: Namespace "e2e-ns-n8hhp-9132" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-n8hhp-9132" 05/09/23 16:50:12.56
May  9 16:50:12.570: INFO: Namespace "e2e-ns-n8hhp-9132" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 16:50:12.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1125" for this suite. 05/09/23 16:50:12.576
STEP: Destroying namespace "e2e-ns-n8hhp-9132" for this suite. 05/09/23 16:50:12.585
------------------------------
â€¢ [0.090 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:50:12.506
    May  9 16:50:12.506: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename namespaces 05/09/23 16:50:12.506
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:12.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:12.529
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-n8hhp" 05/09/23 16:50:12.533
    May  9 16:50:12.549: INFO: Namespace "e2e-ns-n8hhp-9132" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-n8hhp-9132" 05/09/23 16:50:12.549
    May  9 16:50:12.560: INFO: Namespace "e2e-ns-n8hhp-9132" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-n8hhp-9132" 05/09/23 16:50:12.56
    May  9 16:50:12.570: INFO: Namespace "e2e-ns-n8hhp-9132" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 16:50:12.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1125" for this suite. 05/09/23 16:50:12.576
    STEP: Destroying namespace "e2e-ns-n8hhp-9132" for this suite. 05/09/23 16:50:12.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:50:12.597
May  9 16:50:12.597: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 16:50:12.597
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:12.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:12.62
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 05/09/23 16:50:12.624
May  9 16:50:12.625: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 05/09/23 16:50:20.511
May  9 16:50:20.511: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:50:23.224: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:50:31.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9246" for this suite. 05/09/23 16:50:31.297
------------------------------
â€¢ [SLOW TEST] [18.711 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:50:12.597
    May  9 16:50:12.597: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 16:50:12.597
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:12.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:12.62
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 05/09/23 16:50:12.624
    May  9 16:50:12.625: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 05/09/23 16:50:20.511
    May  9 16:50:20.511: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:50:23.224: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:50:31.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9246" for this suite. 05/09/23 16:50:31.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:50:31.31
May  9 16:50:31.310: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename svcaccounts 05/09/23 16:50:31.311
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:31.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:31.337
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  05/09/23 16:50:31.341
May  9 16:50:31.367: INFO: Waiting up to 5m0s for pod "test-pod-8a546869-3080-406d-b245-044cda701687" in namespace "svcaccounts-8539" to be "Succeeded or Failed"
May  9 16:50:31.371: INFO: Pod "test-pod-8a546869-3080-406d-b245-044cda701687": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005424ms
May  9 16:50:33.377: INFO: Pod "test-pod-8a546869-3080-406d-b245-044cda701687": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010597987s
May  9 16:50:35.379: INFO: Pod "test-pod-8a546869-3080-406d-b245-044cda701687": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012482142s
STEP: Saw pod success 05/09/23 16:50:35.379
May  9 16:50:35.380: INFO: Pod "test-pod-8a546869-3080-406d-b245-044cda701687" satisfied condition "Succeeded or Failed"
May  9 16:50:35.386: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod test-pod-8a546869-3080-406d-b245-044cda701687 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 16:50:35.443
May  9 16:50:35.459: INFO: Waiting for pod test-pod-8a546869-3080-406d-b245-044cda701687 to disappear
May  9 16:50:35.463: INFO: Pod test-pod-8a546869-3080-406d-b245-044cda701687 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May  9 16:50:35.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8539" for this suite. 05/09/23 16:50:35.47
------------------------------
â€¢ [4.171 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:50:31.31
    May  9 16:50:31.310: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename svcaccounts 05/09/23 16:50:31.311
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:31.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:31.337
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  05/09/23 16:50:31.341
    May  9 16:50:31.367: INFO: Waiting up to 5m0s for pod "test-pod-8a546869-3080-406d-b245-044cda701687" in namespace "svcaccounts-8539" to be "Succeeded or Failed"
    May  9 16:50:31.371: INFO: Pod "test-pod-8a546869-3080-406d-b245-044cda701687": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005424ms
    May  9 16:50:33.377: INFO: Pod "test-pod-8a546869-3080-406d-b245-044cda701687": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010597987s
    May  9 16:50:35.379: INFO: Pod "test-pod-8a546869-3080-406d-b245-044cda701687": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012482142s
    STEP: Saw pod success 05/09/23 16:50:35.379
    May  9 16:50:35.380: INFO: Pod "test-pod-8a546869-3080-406d-b245-044cda701687" satisfied condition "Succeeded or Failed"
    May  9 16:50:35.386: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod test-pod-8a546869-3080-406d-b245-044cda701687 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 16:50:35.443
    May  9 16:50:35.459: INFO: Waiting for pod test-pod-8a546869-3080-406d-b245-044cda701687 to disappear
    May  9 16:50:35.463: INFO: Pod test-pod-8a546869-3080-406d-b245-044cda701687 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May  9 16:50:35.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8539" for this suite. 05/09/23 16:50:35.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:50:35.485
May  9 16:50:35.485: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename job 05/09/23 16:50:35.486
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:35.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:35.514
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 05/09/23 16:50:35.519
STEP: Ensuring job reaches completions 05/09/23 16:50:35.527
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May  9 16:50:47.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4899" for this suite. 05/09/23 16:50:47.539
------------------------------
â€¢ [SLOW TEST] [12.065 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:50:35.485
    May  9 16:50:35.485: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename job 05/09/23 16:50:35.486
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:35.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:35.514
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 05/09/23 16:50:35.519
    STEP: Ensuring job reaches completions 05/09/23 16:50:35.527
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May  9 16:50:47.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4899" for this suite. 05/09/23 16:50:47.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:50:47.551
May  9 16:50:47.551: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename statefulset 05/09/23 16:50:47.552
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:47.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:47.579
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2338 05/09/23 16:50:47.584
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-2338 05/09/23 16:50:47.597
May  9 16:50:47.616: INFO: Found 0 stateful pods, waiting for 1
May  9 16:50:57.623: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 05/09/23 16:50:57.637
STEP: updating a scale subresource 05/09/23 16:50:57.641
STEP: verifying the statefulset Spec.Replicas was modified 05/09/23 16:50:57.648
STEP: Patch a scale subresource 05/09/23 16:50:57.652
STEP: verifying the statefulset Spec.Replicas was modified 05/09/23 16:50:57.66
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May  9 16:50:57.666: INFO: Deleting all statefulset in ns statefulset-2338
May  9 16:50:57.671: INFO: Scaling statefulset ss to 0
May  9 16:51:07.705: INFO: Waiting for statefulset status.replicas updated to 0
May  9 16:51:07.710: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May  9 16:51:07.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2338" for this suite. 05/09/23 16:51:07.743
------------------------------
â€¢ [SLOW TEST] [20.204 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:50:47.551
    May  9 16:50:47.551: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename statefulset 05/09/23 16:50:47.552
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:50:47.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:50:47.579
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2338 05/09/23 16:50:47.584
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-2338 05/09/23 16:50:47.597
    May  9 16:50:47.616: INFO: Found 0 stateful pods, waiting for 1
    May  9 16:50:57.623: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 05/09/23 16:50:57.637
    STEP: updating a scale subresource 05/09/23 16:50:57.641
    STEP: verifying the statefulset Spec.Replicas was modified 05/09/23 16:50:57.648
    STEP: Patch a scale subresource 05/09/23 16:50:57.652
    STEP: verifying the statefulset Spec.Replicas was modified 05/09/23 16:50:57.66
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May  9 16:50:57.666: INFO: Deleting all statefulset in ns statefulset-2338
    May  9 16:50:57.671: INFO: Scaling statefulset ss to 0
    May  9 16:51:07.705: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 16:51:07.710: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May  9 16:51:07.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2338" for this suite. 05/09/23 16:51:07.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:51:07.755
May  9 16:51:07.755: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename services 05/09/23 16:51:07.756
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:51:07.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:51:07.78
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 05/09/23 16:51:07.784
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  9 16:51:07.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5436" for this suite. 05/09/23 16:51:07.803
------------------------------
â€¢ [0.057 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:51:07.755
    May  9 16:51:07.755: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename services 05/09/23 16:51:07.756
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:51:07.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:51:07.78
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 05/09/23 16:51:07.784
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  9 16:51:07.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5436" for this suite. 05/09/23 16:51:07.803
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:51:07.813
May  9 16:51:07.813: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename endpointslicemirroring 05/09/23 16:51:07.814
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:51:07.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:51:07.835
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 05/09/23 16:51:07.856
May  9 16:51:07.865: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 05/09/23 16:51:09.875
May  9 16:51:09.895: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 05/09/23 16:51:11.901
May  9 16:51:11.915: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
May  9 16:51:13.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-6527" for this suite. 05/09/23 16:51:13.928
------------------------------
â€¢ [SLOW TEST] [6.123 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:51:07.813
    May  9 16:51:07.813: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename endpointslicemirroring 05/09/23 16:51:07.814
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:51:07.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:51:07.835
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 05/09/23 16:51:07.856
    May  9 16:51:07.865: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 05/09/23 16:51:09.875
    May  9 16:51:09.895: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 05/09/23 16:51:11.901
    May  9 16:51:11.915: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    May  9 16:51:13.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-6527" for this suite. 05/09/23 16:51:13.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:51:13.939
May  9 16:51:13.939: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename webhook 05/09/23 16:51:13.94
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:51:13.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:51:13.963
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/09/23 16:51:13.988
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:51:14.295
STEP: Deploying the webhook pod 05/09/23 16:51:14.309
STEP: Wait for the deployment to be ready 05/09/23 16:51:14.323
May  9 16:51:14.334: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 16:51:16.351
STEP: Verifying the service has paired with the endpoint 05/09/23 16:51:16.365
May  9 16:51:17.366: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 05/09/23 16:51:17.373
STEP: Registering slow webhook via the AdmissionRegistration API 05/09/23 16:51:17.373
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 05/09/23 16:51:17.396
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 05/09/23 16:51:18.409
STEP: Registering slow webhook via the AdmissionRegistration API 05/09/23 16:51:18.409
STEP: Having no error when timeout is longer than webhook latency 05/09/23 16:51:19.449
STEP: Registering slow webhook via the AdmissionRegistration API 05/09/23 16:51:19.449
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 05/09/23 16:51:24.5
STEP: Registering slow webhook via the AdmissionRegistration API 05/09/23 16:51:24.5
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:51:29.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1253" for this suite. 05/09/23 16:51:29.606
STEP: Destroying namespace "webhook-1253-markers" for this suite. 05/09/23 16:51:29.613
------------------------------
â€¢ [SLOW TEST] [15.685 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:51:13.939
    May  9 16:51:13.939: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename webhook 05/09/23 16:51:13.94
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:51:13.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:51:13.963
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/09/23 16:51:13.988
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:51:14.295
    STEP: Deploying the webhook pod 05/09/23 16:51:14.309
    STEP: Wait for the deployment to be ready 05/09/23 16:51:14.323
    May  9 16:51:14.334: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 16:51:16.351
    STEP: Verifying the service has paired with the endpoint 05/09/23 16:51:16.365
    May  9 16:51:17.366: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 05/09/23 16:51:17.373
    STEP: Registering slow webhook via the AdmissionRegistration API 05/09/23 16:51:17.373
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 05/09/23 16:51:17.396
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 05/09/23 16:51:18.409
    STEP: Registering slow webhook via the AdmissionRegistration API 05/09/23 16:51:18.409
    STEP: Having no error when timeout is longer than webhook latency 05/09/23 16:51:19.449
    STEP: Registering slow webhook via the AdmissionRegistration API 05/09/23 16:51:19.449
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 05/09/23 16:51:24.5
    STEP: Registering slow webhook via the AdmissionRegistration API 05/09/23 16:51:24.5
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:51:29.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1253" for this suite. 05/09/23 16:51:29.606
    STEP: Destroying namespace "webhook-1253-markers" for this suite. 05/09/23 16:51:29.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:51:29.625
May  9 16:51:29.625: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename webhook 05/09/23 16:51:29.625
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:51:29.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:51:29.661
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/09/23 16:51:29.682
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:51:30.063
STEP: Deploying the webhook pod 05/09/23 16:51:30.072
STEP: Wait for the deployment to be ready 05/09/23 16:51:30.106
May  9 16:51:30.114: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 16:51:32.139
STEP: Verifying the service has paired with the endpoint 05/09/23 16:51:32.152
May  9 16:51:33.152: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 05/09/23 16:51:33.159
STEP: Creating a custom resource definition that should be denied by the webhook 05/09/23 16:51:33.187
May  9 16:51:33.187: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:51:33.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6711" for this suite. 05/09/23 16:51:33.273
STEP: Destroying namespace "webhook-6711-markers" for this suite. 05/09/23 16:51:33.28
------------------------------
â€¢ [3.678 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:51:29.625
    May  9 16:51:29.625: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename webhook 05/09/23 16:51:29.625
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:51:29.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:51:29.661
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/09/23 16:51:29.682
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 16:51:30.063
    STEP: Deploying the webhook pod 05/09/23 16:51:30.072
    STEP: Wait for the deployment to be ready 05/09/23 16:51:30.106
    May  9 16:51:30.114: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 16:51:32.139
    STEP: Verifying the service has paired with the endpoint 05/09/23 16:51:32.152
    May  9 16:51:33.152: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 05/09/23 16:51:33.159
    STEP: Creating a custom resource definition that should be denied by the webhook 05/09/23 16:51:33.187
    May  9 16:51:33.187: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:51:33.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6711" for this suite. 05/09/23 16:51:33.273
    STEP: Destroying namespace "webhook-6711-markers" for this suite. 05/09/23 16:51:33.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:51:33.309
May  9 16:51:33.309: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename emptydir-wrapper 05/09/23 16:51:33.31
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:51:33.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:51:33.334
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 05/09/23 16:51:33.338
STEP: Creating RC which spawns configmap-volume pods 05/09/23 16:51:33.689
May  9 16:51:33.705: INFO: Pod name wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f: Found 0 pods out of 5
May  9 16:51:38.718: INFO: Pod name wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/09/23 16:51:38.718
May  9 16:51:38.718: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-9ldqf" in namespace "emptydir-wrapper-6205" to be "running"
May  9 16:51:38.727: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-9ldqf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.672794ms
May  9 16:51:40.736: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-9ldqf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018678046s
May  9 16:51:42.737: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-9ldqf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019326649s
May  9 16:51:44.737: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-9ldqf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018686893s
May  9 16:51:46.736: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-9ldqf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018380008s
May  9 16:51:48.737: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-9ldqf": Phase="Running", Reason="", readiness=true. Elapsed: 10.019393675s
May  9 16:51:48.737: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-9ldqf" satisfied condition "running"
May  9 16:51:48.737: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-b57kq" in namespace "emptydir-wrapper-6205" to be "running"
May  9 16:51:48.746: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-b57kq": Phase="Running", Reason="", readiness=true. Elapsed: 8.58536ms
May  9 16:51:48.746: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-b57kq" satisfied condition "running"
May  9 16:51:48.746: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-dm7gq" in namespace "emptydir-wrapper-6205" to be "running"
May  9 16:51:48.755: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-dm7gq": Phase="Pending", Reason="", readiness=false. Elapsed: 9.5397ms
May  9 16:51:50.765: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-dm7gq": Phase="Running", Reason="", readiness=true. Elapsed: 2.018992128s
May  9 16:51:50.765: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-dm7gq" satisfied condition "running"
May  9 16:51:50.765: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-r9lxx" in namespace "emptydir-wrapper-6205" to be "running"
May  9 16:51:50.771: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-r9lxx": Phase="Running", Reason="", readiness=true. Elapsed: 5.479036ms
May  9 16:51:50.771: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-r9lxx" satisfied condition "running"
May  9 16:51:50.771: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-ztw2q" in namespace "emptydir-wrapper-6205" to be "running"
May  9 16:51:50.776: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-ztw2q": Phase="Running", Reason="", readiness=true. Elapsed: 5.0078ms
May  9 16:51:50.776: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-ztw2q" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f in namespace emptydir-wrapper-6205, will wait for the garbage collector to delete the pods 05/09/23 16:51:50.776
May  9 16:51:50.842: INFO: Deleting ReplicationController wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f took: 10.452797ms
May  9 16:51:50.943: INFO: Terminating ReplicationController wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f pods took: 100.877677ms
STEP: Creating RC which spawns configmap-volume pods 05/09/23 16:51:53.752
May  9 16:51:53.776: INFO: Pod name wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440: Found 0 pods out of 5
May  9 16:51:58.791: INFO: Pod name wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/09/23 16:51:58.791
May  9 16:51:58.791: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-4g5kp" in namespace "emptydir-wrapper-6205" to be "running"
May  9 16:51:58.797: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-4g5kp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.239353ms
May  9 16:52:00.877: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-4g5kp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.086175184s
May  9 16:52:02.810: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-4g5kp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019268517s
May  9 16:52:04.808: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-4g5kp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016865388s
May  9 16:52:06.807: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-4g5kp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015520927s
May  9 16:52:08.820: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-4g5kp": Phase="Running", Reason="", readiness=true. Elapsed: 10.029264726s
May  9 16:52:08.821: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-4g5kp" satisfied condition "running"
May  9 16:52:08.821: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-5ck6w" in namespace "emptydir-wrapper-6205" to be "running"
May  9 16:52:08.827: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-5ck6w": Phase="Running", Reason="", readiness=true. Elapsed: 6.937254ms
May  9 16:52:08.828: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-5ck6w" satisfied condition "running"
May  9 16:52:08.828: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-77lql" in namespace "emptydir-wrapper-6205" to be "running"
May  9 16:52:08.835: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-77lql": Phase="Running", Reason="", readiness=true. Elapsed: 7.218825ms
May  9 16:52:08.835: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-77lql" satisfied condition "running"
May  9 16:52:08.835: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-nqd24" in namespace "emptydir-wrapper-6205" to be "running"
May  9 16:52:08.843: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-nqd24": Phase="Running", Reason="", readiness=true. Elapsed: 8.219388ms
May  9 16:52:08.843: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-nqd24" satisfied condition "running"
May  9 16:52:08.843: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-tb6hp" in namespace "emptydir-wrapper-6205" to be "running"
May  9 16:52:08.850: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-tb6hp": Phase="Running", Reason="", readiness=true. Elapsed: 7.303098ms
May  9 16:52:08.850: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-tb6hp" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440 in namespace emptydir-wrapper-6205, will wait for the garbage collector to delete the pods 05/09/23 16:52:08.851
May  9 16:52:08.920: INFO: Deleting ReplicationController wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440 took: 11.259526ms
May  9 16:52:09.021: INFO: Terminating ReplicationController wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440 pods took: 101.042313ms
STEP: Creating RC which spawns configmap-volume pods 05/09/23 16:52:12.228
May  9 16:52:12.249: INFO: Pod name wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37: Found 0 pods out of 5
May  9 16:52:17.261: INFO: Pod name wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/09/23 16:52:17.261
May  9 16:52:17.261: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-99vzc" in namespace "emptydir-wrapper-6205" to be "running"
May  9 16:52:17.269: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-99vzc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.628401ms
May  9 16:52:19.276: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-99vzc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014776508s
May  9 16:52:21.280: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-99vzc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018903091s
May  9 16:52:23.277: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-99vzc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015125882s
May  9 16:52:25.278: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-99vzc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016816734s
May  9 16:52:27.277: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-99vzc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015424097s
May  9 16:52:29.278: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-99vzc": Phase="Running", Reason="", readiness=true. Elapsed: 12.016895227s
May  9 16:52:29.278: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-99vzc" satisfied condition "running"
May  9 16:52:29.278: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-cmgg6" in namespace "emptydir-wrapper-6205" to be "running"
May  9 16:52:29.284: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-cmgg6": Phase="Running", Reason="", readiness=true. Elapsed: 5.541961ms
May  9 16:52:29.284: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-cmgg6" satisfied condition "running"
May  9 16:52:29.284: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-hxhrb" in namespace "emptydir-wrapper-6205" to be "running"
May  9 16:52:29.290: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-hxhrb": Phase="Running", Reason="", readiness=true. Elapsed: 6.083211ms
May  9 16:52:29.290: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-hxhrb" satisfied condition "running"
May  9 16:52:29.290: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-vn5ng" in namespace "emptydir-wrapper-6205" to be "running"
May  9 16:52:29.298: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-vn5ng": Phase="Running", Reason="", readiness=true. Elapsed: 7.420622ms
May  9 16:52:29.298: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-vn5ng" satisfied condition "running"
May  9 16:52:29.298: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-wqp9v" in namespace "emptydir-wrapper-6205" to be "running"
May  9 16:52:29.303: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-wqp9v": Phase="Running", Reason="", readiness=true. Elapsed: 5.292789ms
May  9 16:52:29.303: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-wqp9v" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37 in namespace emptydir-wrapper-6205, will wait for the garbage collector to delete the pods 05/09/23 16:52:29.303
May  9 16:52:29.372: INFO: Deleting ReplicationController wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37 took: 11.617127ms
May  9 16:52:29.473: INFO: Terminating ReplicationController wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37 pods took: 100.624943ms
STEP: Cleaning up the configMaps 05/09/23 16:52:32.573
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
May  9 16:52:33.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-6205" for this suite. 05/09/23 16:52:33.041
------------------------------
â€¢ [SLOW TEST] [59.741 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:51:33.309
    May  9 16:51:33.309: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename emptydir-wrapper 05/09/23 16:51:33.31
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:51:33.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:51:33.334
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 05/09/23 16:51:33.338
    STEP: Creating RC which spawns configmap-volume pods 05/09/23 16:51:33.689
    May  9 16:51:33.705: INFO: Pod name wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f: Found 0 pods out of 5
    May  9 16:51:38.718: INFO: Pod name wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/09/23 16:51:38.718
    May  9 16:51:38.718: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-9ldqf" in namespace "emptydir-wrapper-6205" to be "running"
    May  9 16:51:38.727: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-9ldqf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.672794ms
    May  9 16:51:40.736: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-9ldqf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018678046s
    May  9 16:51:42.737: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-9ldqf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019326649s
    May  9 16:51:44.737: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-9ldqf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018686893s
    May  9 16:51:46.736: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-9ldqf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018380008s
    May  9 16:51:48.737: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-9ldqf": Phase="Running", Reason="", readiness=true. Elapsed: 10.019393675s
    May  9 16:51:48.737: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-9ldqf" satisfied condition "running"
    May  9 16:51:48.737: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-b57kq" in namespace "emptydir-wrapper-6205" to be "running"
    May  9 16:51:48.746: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-b57kq": Phase="Running", Reason="", readiness=true. Elapsed: 8.58536ms
    May  9 16:51:48.746: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-b57kq" satisfied condition "running"
    May  9 16:51:48.746: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-dm7gq" in namespace "emptydir-wrapper-6205" to be "running"
    May  9 16:51:48.755: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-dm7gq": Phase="Pending", Reason="", readiness=false. Elapsed: 9.5397ms
    May  9 16:51:50.765: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-dm7gq": Phase="Running", Reason="", readiness=true. Elapsed: 2.018992128s
    May  9 16:51:50.765: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-dm7gq" satisfied condition "running"
    May  9 16:51:50.765: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-r9lxx" in namespace "emptydir-wrapper-6205" to be "running"
    May  9 16:51:50.771: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-r9lxx": Phase="Running", Reason="", readiness=true. Elapsed: 5.479036ms
    May  9 16:51:50.771: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-r9lxx" satisfied condition "running"
    May  9 16:51:50.771: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-ztw2q" in namespace "emptydir-wrapper-6205" to be "running"
    May  9 16:51:50.776: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-ztw2q": Phase="Running", Reason="", readiness=true. Elapsed: 5.0078ms
    May  9 16:51:50.776: INFO: Pod "wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f-ztw2q" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f in namespace emptydir-wrapper-6205, will wait for the garbage collector to delete the pods 05/09/23 16:51:50.776
    May  9 16:51:50.842: INFO: Deleting ReplicationController wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f took: 10.452797ms
    May  9 16:51:50.943: INFO: Terminating ReplicationController wrapped-volume-race-cb985c78-ffc7-40d5-ad21-343d8f71ae8f pods took: 100.877677ms
    STEP: Creating RC which spawns configmap-volume pods 05/09/23 16:51:53.752
    May  9 16:51:53.776: INFO: Pod name wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440: Found 0 pods out of 5
    May  9 16:51:58.791: INFO: Pod name wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/09/23 16:51:58.791
    May  9 16:51:58.791: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-4g5kp" in namespace "emptydir-wrapper-6205" to be "running"
    May  9 16:51:58.797: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-4g5kp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.239353ms
    May  9 16:52:00.877: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-4g5kp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.086175184s
    May  9 16:52:02.810: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-4g5kp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019268517s
    May  9 16:52:04.808: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-4g5kp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016865388s
    May  9 16:52:06.807: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-4g5kp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015520927s
    May  9 16:52:08.820: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-4g5kp": Phase="Running", Reason="", readiness=true. Elapsed: 10.029264726s
    May  9 16:52:08.821: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-4g5kp" satisfied condition "running"
    May  9 16:52:08.821: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-5ck6w" in namespace "emptydir-wrapper-6205" to be "running"
    May  9 16:52:08.827: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-5ck6w": Phase="Running", Reason="", readiness=true. Elapsed: 6.937254ms
    May  9 16:52:08.828: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-5ck6w" satisfied condition "running"
    May  9 16:52:08.828: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-77lql" in namespace "emptydir-wrapper-6205" to be "running"
    May  9 16:52:08.835: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-77lql": Phase="Running", Reason="", readiness=true. Elapsed: 7.218825ms
    May  9 16:52:08.835: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-77lql" satisfied condition "running"
    May  9 16:52:08.835: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-nqd24" in namespace "emptydir-wrapper-6205" to be "running"
    May  9 16:52:08.843: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-nqd24": Phase="Running", Reason="", readiness=true. Elapsed: 8.219388ms
    May  9 16:52:08.843: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-nqd24" satisfied condition "running"
    May  9 16:52:08.843: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-tb6hp" in namespace "emptydir-wrapper-6205" to be "running"
    May  9 16:52:08.850: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-tb6hp": Phase="Running", Reason="", readiness=true. Elapsed: 7.303098ms
    May  9 16:52:08.850: INFO: Pod "wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440-tb6hp" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440 in namespace emptydir-wrapper-6205, will wait for the garbage collector to delete the pods 05/09/23 16:52:08.851
    May  9 16:52:08.920: INFO: Deleting ReplicationController wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440 took: 11.259526ms
    May  9 16:52:09.021: INFO: Terminating ReplicationController wrapped-volume-race-ee3f8c92-bf8b-4971-b56b-bff590cc0440 pods took: 101.042313ms
    STEP: Creating RC which spawns configmap-volume pods 05/09/23 16:52:12.228
    May  9 16:52:12.249: INFO: Pod name wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37: Found 0 pods out of 5
    May  9 16:52:17.261: INFO: Pod name wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/09/23 16:52:17.261
    May  9 16:52:17.261: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-99vzc" in namespace "emptydir-wrapper-6205" to be "running"
    May  9 16:52:17.269: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-99vzc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.628401ms
    May  9 16:52:19.276: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-99vzc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014776508s
    May  9 16:52:21.280: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-99vzc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018903091s
    May  9 16:52:23.277: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-99vzc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015125882s
    May  9 16:52:25.278: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-99vzc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016816734s
    May  9 16:52:27.277: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-99vzc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015424097s
    May  9 16:52:29.278: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-99vzc": Phase="Running", Reason="", readiness=true. Elapsed: 12.016895227s
    May  9 16:52:29.278: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-99vzc" satisfied condition "running"
    May  9 16:52:29.278: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-cmgg6" in namespace "emptydir-wrapper-6205" to be "running"
    May  9 16:52:29.284: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-cmgg6": Phase="Running", Reason="", readiness=true. Elapsed: 5.541961ms
    May  9 16:52:29.284: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-cmgg6" satisfied condition "running"
    May  9 16:52:29.284: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-hxhrb" in namespace "emptydir-wrapper-6205" to be "running"
    May  9 16:52:29.290: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-hxhrb": Phase="Running", Reason="", readiness=true. Elapsed: 6.083211ms
    May  9 16:52:29.290: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-hxhrb" satisfied condition "running"
    May  9 16:52:29.290: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-vn5ng" in namespace "emptydir-wrapper-6205" to be "running"
    May  9 16:52:29.298: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-vn5ng": Phase="Running", Reason="", readiness=true. Elapsed: 7.420622ms
    May  9 16:52:29.298: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-vn5ng" satisfied condition "running"
    May  9 16:52:29.298: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-wqp9v" in namespace "emptydir-wrapper-6205" to be "running"
    May  9 16:52:29.303: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-wqp9v": Phase="Running", Reason="", readiness=true. Elapsed: 5.292789ms
    May  9 16:52:29.303: INFO: Pod "wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37-wqp9v" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37 in namespace emptydir-wrapper-6205, will wait for the garbage collector to delete the pods 05/09/23 16:52:29.303
    May  9 16:52:29.372: INFO: Deleting ReplicationController wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37 took: 11.617127ms
    May  9 16:52:29.473: INFO: Terminating ReplicationController wrapped-volume-race-68adbb5f-9598-4377-a49a-82e449f6ef37 pods took: 100.624943ms
    STEP: Cleaning up the configMaps 05/09/23 16:52:32.573
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    May  9 16:52:33.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-6205" for this suite. 05/09/23 16:52:33.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:52:33.052
May  9 16:52:33.053: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename emptydir-wrapper 05/09/23 16:52:33.054
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:52:33.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:52:33.075
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
May  9 16:52:33.100: INFO: Waiting up to 5m0s for pod "pod-secrets-c420d03d-431b-4d60-ac0a-d84044d2a137" in namespace "emptydir-wrapper-7480" to be "running and ready"
May  9 16:52:33.106: INFO: Pod "pod-secrets-c420d03d-431b-4d60-ac0a-d84044d2a137": Phase="Pending", Reason="", readiness=false. Elapsed: 5.448017ms
May  9 16:52:33.106: INFO: The phase of Pod pod-secrets-c420d03d-431b-4d60-ac0a-d84044d2a137 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:52:35.114: INFO: Pod "pod-secrets-c420d03d-431b-4d60-ac0a-d84044d2a137": Phase="Running", Reason="", readiness=true. Elapsed: 2.012973338s
May  9 16:52:35.114: INFO: The phase of Pod pod-secrets-c420d03d-431b-4d60-ac0a-d84044d2a137 is Running (Ready = true)
May  9 16:52:35.114: INFO: Pod "pod-secrets-c420d03d-431b-4d60-ac0a-d84044d2a137" satisfied condition "running and ready"
STEP: Cleaning up the secret 05/09/23 16:52:35.122
STEP: Cleaning up the configmap 05/09/23 16:52:35.131
STEP: Cleaning up the pod 05/09/23 16:52:35.143
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
May  9 16:52:35.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-7480" for this suite. 05/09/23 16:52:35.166
------------------------------
â€¢ [2.135 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:52:33.052
    May  9 16:52:33.053: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename emptydir-wrapper 05/09/23 16:52:33.054
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:52:33.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:52:33.075
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    May  9 16:52:33.100: INFO: Waiting up to 5m0s for pod "pod-secrets-c420d03d-431b-4d60-ac0a-d84044d2a137" in namespace "emptydir-wrapper-7480" to be "running and ready"
    May  9 16:52:33.106: INFO: Pod "pod-secrets-c420d03d-431b-4d60-ac0a-d84044d2a137": Phase="Pending", Reason="", readiness=false. Elapsed: 5.448017ms
    May  9 16:52:33.106: INFO: The phase of Pod pod-secrets-c420d03d-431b-4d60-ac0a-d84044d2a137 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:52:35.114: INFO: Pod "pod-secrets-c420d03d-431b-4d60-ac0a-d84044d2a137": Phase="Running", Reason="", readiness=true. Elapsed: 2.012973338s
    May  9 16:52:35.114: INFO: The phase of Pod pod-secrets-c420d03d-431b-4d60-ac0a-d84044d2a137 is Running (Ready = true)
    May  9 16:52:35.114: INFO: Pod "pod-secrets-c420d03d-431b-4d60-ac0a-d84044d2a137" satisfied condition "running and ready"
    STEP: Cleaning up the secret 05/09/23 16:52:35.122
    STEP: Cleaning up the configmap 05/09/23 16:52:35.131
    STEP: Cleaning up the pod 05/09/23 16:52:35.143
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    May  9 16:52:35.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-7480" for this suite. 05/09/23 16:52:35.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:52:35.189
May  9 16:52:35.189: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename services 05/09/23 16:52:35.19
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:52:35.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:52:35.211
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1061 05/09/23 16:52:35.216
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/09/23 16:52:35.242
STEP: creating service externalsvc in namespace services-1061 05/09/23 16:52:35.242
STEP: creating replication controller externalsvc in namespace services-1061 05/09/23 16:52:35.257
I0509 16:52:35.266453      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1061, replica count: 2
I0509 16:52:38.317114      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 05/09/23 16:52:38.322
May  9 16:52:38.348: INFO: Creating new exec pod
May  9 16:52:38.355: INFO: Waiting up to 5m0s for pod "execpodvp6vr" in namespace "services-1061" to be "running"
May  9 16:52:38.360: INFO: Pod "execpodvp6vr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.812329ms
May  9 16:52:40.365: INFO: Pod "execpodvp6vr": Phase="Running", Reason="", readiness=true. Elapsed: 2.010535643s
May  9 16:52:40.366: INFO: Pod "execpodvp6vr" satisfied condition "running"
May  9 16:52:40.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-1061 exec execpodvp6vr -- /bin/sh -x -c nslookup nodeport-service.services-1061.svc.cluster.local'
May  9 16:52:40.610: INFO: stderr: "+ nslookup nodeport-service.services-1061.svc.cluster.local\n"
May  9 16:52:40.610: INFO: stdout: "Server:\t\t10.3.0.10\nAddress:\t10.3.0.10#53\n\nnodeport-service.services-1061.svc.cluster.local\tcanonical name = externalsvc.services-1061.svc.cluster.local.\nName:\texternalsvc.services-1061.svc.cluster.local\nAddress: 10.3.249.23\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1061, will wait for the garbage collector to delete the pods 05/09/23 16:52:40.61
May  9 16:52:40.674: INFO: Deleting ReplicationController externalsvc took: 8.063477ms
May  9 16:52:40.774: INFO: Terminating ReplicationController externalsvc pods took: 100.206535ms
May  9 16:52:42.698: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  9 16:52:42.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1061" for this suite. 05/09/23 16:52:42.726
------------------------------
â€¢ [SLOW TEST] [7.548 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:52:35.189
    May  9 16:52:35.189: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename services 05/09/23 16:52:35.19
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:52:35.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:52:35.211
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-1061 05/09/23 16:52:35.216
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/09/23 16:52:35.242
    STEP: creating service externalsvc in namespace services-1061 05/09/23 16:52:35.242
    STEP: creating replication controller externalsvc in namespace services-1061 05/09/23 16:52:35.257
    I0509 16:52:35.266453      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1061, replica count: 2
    I0509 16:52:38.317114      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 05/09/23 16:52:38.322
    May  9 16:52:38.348: INFO: Creating new exec pod
    May  9 16:52:38.355: INFO: Waiting up to 5m0s for pod "execpodvp6vr" in namespace "services-1061" to be "running"
    May  9 16:52:38.360: INFO: Pod "execpodvp6vr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.812329ms
    May  9 16:52:40.365: INFO: Pod "execpodvp6vr": Phase="Running", Reason="", readiness=true. Elapsed: 2.010535643s
    May  9 16:52:40.366: INFO: Pod "execpodvp6vr" satisfied condition "running"
    May  9 16:52:40.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-1061 exec execpodvp6vr -- /bin/sh -x -c nslookup nodeport-service.services-1061.svc.cluster.local'
    May  9 16:52:40.610: INFO: stderr: "+ nslookup nodeport-service.services-1061.svc.cluster.local\n"
    May  9 16:52:40.610: INFO: stdout: "Server:\t\t10.3.0.10\nAddress:\t10.3.0.10#53\n\nnodeport-service.services-1061.svc.cluster.local\tcanonical name = externalsvc.services-1061.svc.cluster.local.\nName:\texternalsvc.services-1061.svc.cluster.local\nAddress: 10.3.249.23\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1061, will wait for the garbage collector to delete the pods 05/09/23 16:52:40.61
    May  9 16:52:40.674: INFO: Deleting ReplicationController externalsvc took: 8.063477ms
    May  9 16:52:40.774: INFO: Terminating ReplicationController externalsvc pods took: 100.206535ms
    May  9 16:52:42.698: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  9 16:52:42.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1061" for this suite. 05/09/23 16:52:42.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:52:42.737
May  9 16:52:42.737: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename deployment 05/09/23 16:52:42.738
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:52:42.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:52:42.758
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
May  9 16:52:42.762: INFO: Creating deployment "test-recreate-deployment"
May  9 16:52:42.770: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May  9 16:52:42.779: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May  9 16:52:44.797: INFO: Waiting deployment "test-recreate-deployment" to complete
May  9 16:52:44.802: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May  9 16:52:44.814: INFO: Updating deployment test-recreate-deployment
May  9 16:52:44.814: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  9 16:52:44.903: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-505  40ebeb88-e6ca-4b44-b921-7c7032a4f6d3 318179821 2 2023-05-09 16:52:42 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-09 16:52:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:52:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00590c6d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-09 16:52:44 +0000 UTC,LastTransitionTime:2023-05-09 16:52:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-05-09 16:52:44 +0000 UTC,LastTransitionTime:2023-05-09 16:52:42 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May  9 16:52:44.908: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-505  c9af7725-ad84-4595-bbf1-8ed77eca7728 318179817 1 2023-05-09 16:52:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 40ebeb88-e6ca-4b44-b921-7c7032a4f6d3 0xc00590cba0 0xc00590cba1}] [] [{kube-controller-manager Update apps/v1 2023-05-09 16:52:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40ebeb88-e6ca-4b44-b921-7c7032a4f6d3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:52:44 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00590cc38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  9 16:52:44.908: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May  9 16:52:44.908: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-505  45bf7c4d-f0ea-4529-a178-e573166390d9 318179809 2 2023-05-09 16:52:42 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 40ebeb88-e6ca-4b44-b921-7c7032a4f6d3 0xc00590ca87 0xc00590ca88}] [] [{kube-controller-manager Update apps/v1 2023-05-09 16:52:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40ebeb88-e6ca-4b44-b921-7c7032a4f6d3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:52:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00590cb38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  9 16:52:44.916: INFO: Pod "test-recreate-deployment-cff6dc657-2hlvz" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-2hlvz test-recreate-deployment-cff6dc657- deployment-505  99e62e55-9adb-4280-bba6-958ebf5fc3c0 318179822 0 2023-05-09 16:52:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 c9af7725-ad84-4595-bbf1-8ed77eca7728 0xc00590d0b0 0xc00590d0b1}] [] [{kube-controller-manager Update v1 2023-05-09 16:52:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9af7725-ad84-4595-bbf1-8ed77eca7728\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 16:52:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pr9ff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pr9ff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:52:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:52:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:52:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:52:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:,StartTime:2023-05-09 16:52:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May  9 16:52:44.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-505" for this suite. 05/09/23 16:52:44.923
------------------------------
â€¢ [2.197 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:52:42.737
    May  9 16:52:42.737: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename deployment 05/09/23 16:52:42.738
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:52:42.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:52:42.758
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    May  9 16:52:42.762: INFO: Creating deployment "test-recreate-deployment"
    May  9 16:52:42.770: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    May  9 16:52:42.779: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    May  9 16:52:44.797: INFO: Waiting deployment "test-recreate-deployment" to complete
    May  9 16:52:44.802: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    May  9 16:52:44.814: INFO: Updating deployment test-recreate-deployment
    May  9 16:52:44.814: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  9 16:52:44.903: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-505  40ebeb88-e6ca-4b44-b921-7c7032a4f6d3 318179821 2 2023-05-09 16:52:42 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-09 16:52:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:52:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00590c6d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-09 16:52:44 +0000 UTC,LastTransitionTime:2023-05-09 16:52:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-05-09 16:52:44 +0000 UTC,LastTransitionTime:2023-05-09 16:52:42 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    May  9 16:52:44.908: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-505  c9af7725-ad84-4595-bbf1-8ed77eca7728 318179817 1 2023-05-09 16:52:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 40ebeb88-e6ca-4b44-b921-7c7032a4f6d3 0xc00590cba0 0xc00590cba1}] [] [{kube-controller-manager Update apps/v1 2023-05-09 16:52:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40ebeb88-e6ca-4b44-b921-7c7032a4f6d3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:52:44 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00590cc38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  9 16:52:44.908: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    May  9 16:52:44.908: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-505  45bf7c4d-f0ea-4529-a178-e573166390d9 318179809 2 2023-05-09 16:52:42 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 40ebeb88-e6ca-4b44-b921-7c7032a4f6d3 0xc00590ca87 0xc00590ca88}] [] [{kube-controller-manager Update apps/v1 2023-05-09 16:52:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40ebeb88-e6ca-4b44-b921-7c7032a4f6d3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 16:52:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00590cb38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  9 16:52:44.916: INFO: Pod "test-recreate-deployment-cff6dc657-2hlvz" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-2hlvz test-recreate-deployment-cff6dc657- deployment-505  99e62e55-9adb-4280-bba6-958ebf5fc3c0 318179822 0 2023-05-09 16:52:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 c9af7725-ad84-4595-bbf1-8ed77eca7728 0xc00590d0b0 0xc00590d0b1}] [] [{kube-controller-manager Update v1 2023-05-09 16:52:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9af7725-ad84-4595-bbf1-8ed77eca7728\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 16:52:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pr9ff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pr9ff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:52:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:52:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:52:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 16:52:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:,StartTime:2023-05-09 16:52:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May  9 16:52:44.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-505" for this suite. 05/09/23 16:52:44.923
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:52:44.934
May  9 16:52:44.934: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename pod-network-test 05/09/23 16:52:44.935
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:52:44.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:52:44.964
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-4348 05/09/23 16:52:44.969
STEP: creating a selector 05/09/23 16:52:44.969
STEP: Creating the service pods in kubernetes 05/09/23 16:52:44.969
May  9 16:52:44.969: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  9 16:52:45.020: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4348" to be "running and ready"
May  9 16:52:45.027: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.408986ms
May  9 16:52:45.027: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:52:47.035: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.014865218s
May  9 16:52:47.035: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:52:49.034: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013171978s
May  9 16:52:49.034: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:52:51.036: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015901676s
May  9 16:52:51.036: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:52:53.040: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.019276671s
May  9 16:52:53.040: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:52:55.040: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.020086955s
May  9 16:52:55.041: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  9 16:52:57.036: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.015442422s
May  9 16:52:57.036: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  9 16:52:57.036: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  9 16:52:57.040: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4348" to be "running and ready"
May  9 16:52:57.046: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.853961ms
May  9 16:52:57.046: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  9 16:52:57.046: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  9 16:52:57.051: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4348" to be "running and ready"
May  9 16:52:57.057: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.383625ms
May  9 16:52:57.057: INFO: The phase of Pod netserver-2 is Running (Ready = false)
May  9 16:52:59.064: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 2.013151977s
May  9 16:52:59.064: INFO: The phase of Pod netserver-2 is Running (Ready = false)
May  9 16:53:01.065: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.01371352s
May  9 16:53:01.065: INFO: The phase of Pod netserver-2 is Running (Ready = false)
May  9 16:53:03.065: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.014315596s
May  9 16:53:03.065: INFO: The phase of Pod netserver-2 is Running (Ready = false)
May  9 16:53:05.065: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 8.01409706s
May  9 16:53:05.065: INFO: The phase of Pod netserver-2 is Running (Ready = false)
May  9 16:53:07.064: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.013320789s
May  9 16:53:07.064: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  9 16:53:07.065: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/09/23 16:53:07.07
May  9 16:53:07.086: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4348" to be "running"
May  9 16:53:07.092: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.134408ms
May  9 16:53:09.101: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014934811s
May  9 16:53:09.101: INFO: Pod "test-container-pod" satisfied condition "running"
May  9 16:53:09.106: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4348" to be "running"
May  9 16:53:09.111: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.664587ms
May  9 16:53:09.111: INFO: Pod "host-test-container-pod" satisfied condition "running"
May  9 16:53:09.116: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May  9 16:53:09.116: INFO: Going to poll 10.2.1.158 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May  9 16:53:09.121: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.1.158 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4348 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:53:09.121: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:53:09.121: INFO: ExecWithOptions: Clientset creation
May  9 16:53:09.121: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-4348/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.2.1.158+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  9 16:53:10.263: INFO: Found all 1 expected endpoints: [netserver-0]
May  9 16:53:10.263: INFO: Going to poll 10.2.0.238 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May  9 16:53:10.270: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.0.238 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4348 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:53:10.270: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:53:10.270: INFO: ExecWithOptions: Clientset creation
May  9 16:53:10.270: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-4348/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.2.0.238+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  9 16:53:11.426: INFO: Found all 1 expected endpoints: [netserver-1]
May  9 16:53:11.426: INFO: Going to poll 10.2.2.238 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May  9 16:53:11.432: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.2.238 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4348 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:53:11.432: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:53:11.433: INFO: ExecWithOptions: Clientset creation
May  9 16:53:11.433: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-4348/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.2.2.238+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  9 16:53:12.556: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
May  9 16:53:12.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4348" for this suite. 05/09/23 16:53:12.564
------------------------------
â€¢ [SLOW TEST] [27.641 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:52:44.934
    May  9 16:52:44.934: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename pod-network-test 05/09/23 16:52:44.935
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:52:44.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:52:44.964
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-4348 05/09/23 16:52:44.969
    STEP: creating a selector 05/09/23 16:52:44.969
    STEP: Creating the service pods in kubernetes 05/09/23 16:52:44.969
    May  9 16:52:44.969: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  9 16:52:45.020: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4348" to be "running and ready"
    May  9 16:52:45.027: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.408986ms
    May  9 16:52:45.027: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:52:47.035: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.014865218s
    May  9 16:52:47.035: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:52:49.034: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013171978s
    May  9 16:52:49.034: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:52:51.036: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015901676s
    May  9 16:52:51.036: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:52:53.040: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.019276671s
    May  9 16:52:53.040: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:52:55.040: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.020086955s
    May  9 16:52:55.041: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  9 16:52:57.036: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.015442422s
    May  9 16:52:57.036: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  9 16:52:57.036: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  9 16:52:57.040: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4348" to be "running and ready"
    May  9 16:52:57.046: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.853961ms
    May  9 16:52:57.046: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  9 16:52:57.046: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  9 16:52:57.051: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4348" to be "running and ready"
    May  9 16:52:57.057: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.383625ms
    May  9 16:52:57.057: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    May  9 16:52:59.064: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 2.013151977s
    May  9 16:52:59.064: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    May  9 16:53:01.065: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.01371352s
    May  9 16:53:01.065: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    May  9 16:53:03.065: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.014315596s
    May  9 16:53:03.065: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    May  9 16:53:05.065: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 8.01409706s
    May  9 16:53:05.065: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    May  9 16:53:07.064: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.013320789s
    May  9 16:53:07.064: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  9 16:53:07.065: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/09/23 16:53:07.07
    May  9 16:53:07.086: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4348" to be "running"
    May  9 16:53:07.092: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.134408ms
    May  9 16:53:09.101: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014934811s
    May  9 16:53:09.101: INFO: Pod "test-container-pod" satisfied condition "running"
    May  9 16:53:09.106: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4348" to be "running"
    May  9 16:53:09.111: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.664587ms
    May  9 16:53:09.111: INFO: Pod "host-test-container-pod" satisfied condition "running"
    May  9 16:53:09.116: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May  9 16:53:09.116: INFO: Going to poll 10.2.1.158 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    May  9 16:53:09.121: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.1.158 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4348 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:53:09.121: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:53:09.121: INFO: ExecWithOptions: Clientset creation
    May  9 16:53:09.121: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-4348/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.2.1.158+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  9 16:53:10.263: INFO: Found all 1 expected endpoints: [netserver-0]
    May  9 16:53:10.263: INFO: Going to poll 10.2.0.238 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    May  9 16:53:10.270: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.0.238 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4348 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:53:10.270: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:53:10.270: INFO: ExecWithOptions: Clientset creation
    May  9 16:53:10.270: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-4348/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.2.0.238+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  9 16:53:11.426: INFO: Found all 1 expected endpoints: [netserver-1]
    May  9 16:53:11.426: INFO: Going to poll 10.2.2.238 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    May  9 16:53:11.432: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.2.238 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4348 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:53:11.432: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:53:11.433: INFO: ExecWithOptions: Clientset creation
    May  9 16:53:11.433: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-4348/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.2.2.238+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  9 16:53:12.556: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    May  9 16:53:12.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4348" for this suite. 05/09/23 16:53:12.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:53:12.578
May  9 16:53:12.578: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename services 05/09/23 16:53:12.579
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:12.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:12.608
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  9 16:53:12.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8038" for this suite. 05/09/23 16:53:12.63
------------------------------
â€¢ [0.065 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:53:12.578
    May  9 16:53:12.578: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename services 05/09/23 16:53:12.579
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:12.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:12.608
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  9 16:53:12.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8038" for this suite. 05/09/23 16:53:12.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:53:12.644
May  9 16:53:12.644: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 16:53:12.645
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:12.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:12.67
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-90552ea5-cb83-433a-80b1-e039ccd17c89 05/09/23 16:53:12.676
STEP: Creating a pod to test consume configMaps 05/09/23 16:53:12.683
May  9 16:53:12.695: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1f140e6c-fc8d-4ee7-99f8-ccea419108fa" in namespace "projected-2279" to be "Succeeded or Failed"
May  9 16:53:12.703: INFO: Pod "pod-projected-configmaps-1f140e6c-fc8d-4ee7-99f8-ccea419108fa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.152012ms
May  9 16:53:14.713: INFO: Pod "pod-projected-configmaps-1f140e6c-fc8d-4ee7-99f8-ccea419108fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018807853s
May  9 16:53:16.710: INFO: Pod "pod-projected-configmaps-1f140e6c-fc8d-4ee7-99f8-ccea419108fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015552124s
May  9 16:53:18.712: INFO: Pod "pod-projected-configmaps-1f140e6c-fc8d-4ee7-99f8-ccea419108fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016944494s
STEP: Saw pod success 05/09/23 16:53:18.712
May  9 16:53:18.712: INFO: Pod "pod-projected-configmaps-1f140e6c-fc8d-4ee7-99f8-ccea419108fa" satisfied condition "Succeeded or Failed"
May  9 16:53:18.720: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 pod pod-projected-configmaps-1f140e6c-fc8d-4ee7-99f8-ccea419108fa container agnhost-container: <nil>
STEP: delete the pod 05/09/23 16:53:18.772
May  9 16:53:18.788: INFO: Waiting for pod pod-projected-configmaps-1f140e6c-fc8d-4ee7-99f8-ccea419108fa to disappear
May  9 16:53:18.792: INFO: Pod pod-projected-configmaps-1f140e6c-fc8d-4ee7-99f8-ccea419108fa no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May  9 16:53:18.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2279" for this suite. 05/09/23 16:53:18.799
------------------------------
â€¢ [SLOW TEST] [6.163 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:53:12.644
    May  9 16:53:12.644: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 16:53:12.645
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:12.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:12.67
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-90552ea5-cb83-433a-80b1-e039ccd17c89 05/09/23 16:53:12.676
    STEP: Creating a pod to test consume configMaps 05/09/23 16:53:12.683
    May  9 16:53:12.695: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1f140e6c-fc8d-4ee7-99f8-ccea419108fa" in namespace "projected-2279" to be "Succeeded or Failed"
    May  9 16:53:12.703: INFO: Pod "pod-projected-configmaps-1f140e6c-fc8d-4ee7-99f8-ccea419108fa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.152012ms
    May  9 16:53:14.713: INFO: Pod "pod-projected-configmaps-1f140e6c-fc8d-4ee7-99f8-ccea419108fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018807853s
    May  9 16:53:16.710: INFO: Pod "pod-projected-configmaps-1f140e6c-fc8d-4ee7-99f8-ccea419108fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015552124s
    May  9 16:53:18.712: INFO: Pod "pod-projected-configmaps-1f140e6c-fc8d-4ee7-99f8-ccea419108fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016944494s
    STEP: Saw pod success 05/09/23 16:53:18.712
    May  9 16:53:18.712: INFO: Pod "pod-projected-configmaps-1f140e6c-fc8d-4ee7-99f8-ccea419108fa" satisfied condition "Succeeded or Failed"
    May  9 16:53:18.720: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 pod pod-projected-configmaps-1f140e6c-fc8d-4ee7-99f8-ccea419108fa container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 16:53:18.772
    May  9 16:53:18.788: INFO: Waiting for pod pod-projected-configmaps-1f140e6c-fc8d-4ee7-99f8-ccea419108fa to disappear
    May  9 16:53:18.792: INFO: Pod pod-projected-configmaps-1f140e6c-fc8d-4ee7-99f8-ccea419108fa no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May  9 16:53:18.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2279" for this suite. 05/09/23 16:53:18.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:53:18.811
May  9 16:53:18.812: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename replicaset 05/09/23 16:53:18.813
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:18.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:18.833
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
May  9 16:53:18.861: INFO: Pod name sample-pod: Found 0 pods out of 1
May  9 16:53:23.866: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/09/23 16:53:23.867
STEP: Scaling up "test-rs" replicaset  05/09/23 16:53:23.867
May  9 16:53:23.877: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 05/09/23 16:53:23.877
W0509 16:53:23.888887      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May  9 16:53:23.896: INFO: observed ReplicaSet test-rs in namespace replicaset-1760 with ReadyReplicas 1, AvailableReplicas 1
May  9 16:53:23.905: INFO: observed ReplicaSet test-rs in namespace replicaset-1760 with ReadyReplicas 1, AvailableReplicas 1
May  9 16:53:23.920: INFO: observed ReplicaSet test-rs in namespace replicaset-1760 with ReadyReplicas 1, AvailableReplicas 1
May  9 16:53:23.926: INFO: observed ReplicaSet test-rs in namespace replicaset-1760 with ReadyReplicas 1, AvailableReplicas 1
May  9 16:53:25.278: INFO: observed ReplicaSet test-rs in namespace replicaset-1760 with ReadyReplicas 2, AvailableReplicas 2
May  9 16:53:25.885: INFO: observed Replicaset test-rs in namespace replicaset-1760 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May  9 16:53:25.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1760" for this suite. 05/09/23 16:53:25.892
------------------------------
â€¢ [SLOW TEST] [7.088 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:53:18.811
    May  9 16:53:18.812: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename replicaset 05/09/23 16:53:18.813
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:18.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:18.833
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    May  9 16:53:18.861: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  9 16:53:23.866: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/09/23 16:53:23.867
    STEP: Scaling up "test-rs" replicaset  05/09/23 16:53:23.867
    May  9 16:53:23.877: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 05/09/23 16:53:23.877
    W0509 16:53:23.888887      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May  9 16:53:23.896: INFO: observed ReplicaSet test-rs in namespace replicaset-1760 with ReadyReplicas 1, AvailableReplicas 1
    May  9 16:53:23.905: INFO: observed ReplicaSet test-rs in namespace replicaset-1760 with ReadyReplicas 1, AvailableReplicas 1
    May  9 16:53:23.920: INFO: observed ReplicaSet test-rs in namespace replicaset-1760 with ReadyReplicas 1, AvailableReplicas 1
    May  9 16:53:23.926: INFO: observed ReplicaSet test-rs in namespace replicaset-1760 with ReadyReplicas 1, AvailableReplicas 1
    May  9 16:53:25.278: INFO: observed ReplicaSet test-rs in namespace replicaset-1760 with ReadyReplicas 2, AvailableReplicas 2
    May  9 16:53:25.885: INFO: observed Replicaset test-rs in namespace replicaset-1760 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May  9 16:53:25.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1760" for this suite. 05/09/23 16:53:25.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:53:25.901
May  9 16:53:25.901: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename configmap 05/09/23 16:53:25.902
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:25.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:25.927
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-313222b8-535d-423a-90d8-f2208a256f8f 05/09/23 16:53:25.936
STEP: Creating the pod 05/09/23 16:53:25.947
May  9 16:53:25.967: INFO: Waiting up to 5m0s for pod "pod-configmaps-a632d6af-13d0-4811-8ecc-6492a7ae7564" in namespace "configmap-4924" to be "running"
May  9 16:53:25.974: INFO: Pod "pod-configmaps-a632d6af-13d0-4811-8ecc-6492a7ae7564": Phase="Pending", Reason="", readiness=false. Elapsed: 6.906502ms
May  9 16:53:27.982: INFO: Pod "pod-configmaps-a632d6af-13d0-4811-8ecc-6492a7ae7564": Phase="Running", Reason="", readiness=false. Elapsed: 2.01457361s
May  9 16:53:27.982: INFO: Pod "pod-configmaps-a632d6af-13d0-4811-8ecc-6492a7ae7564" satisfied condition "running"
STEP: Waiting for pod with text data 05/09/23 16:53:27.982
STEP: Waiting for pod with binary data 05/09/23 16:53:28.036
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  9 16:53:28.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4924" for this suite. 05/09/23 16:53:28.055
------------------------------
â€¢ [2.162 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:53:25.901
    May  9 16:53:25.901: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename configmap 05/09/23 16:53:25.902
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:25.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:25.927
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-313222b8-535d-423a-90d8-f2208a256f8f 05/09/23 16:53:25.936
    STEP: Creating the pod 05/09/23 16:53:25.947
    May  9 16:53:25.967: INFO: Waiting up to 5m0s for pod "pod-configmaps-a632d6af-13d0-4811-8ecc-6492a7ae7564" in namespace "configmap-4924" to be "running"
    May  9 16:53:25.974: INFO: Pod "pod-configmaps-a632d6af-13d0-4811-8ecc-6492a7ae7564": Phase="Pending", Reason="", readiness=false. Elapsed: 6.906502ms
    May  9 16:53:27.982: INFO: Pod "pod-configmaps-a632d6af-13d0-4811-8ecc-6492a7ae7564": Phase="Running", Reason="", readiness=false. Elapsed: 2.01457361s
    May  9 16:53:27.982: INFO: Pod "pod-configmaps-a632d6af-13d0-4811-8ecc-6492a7ae7564" satisfied condition "running"
    STEP: Waiting for pod with text data 05/09/23 16:53:27.982
    STEP: Waiting for pod with binary data 05/09/23 16:53:28.036
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  9 16:53:28.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4924" for this suite. 05/09/23 16:53:28.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:53:28.067
May  9 16:53:28.067: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 16:53:28.068
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:28.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:28.088
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
May  9 16:53:28.093: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:53:31.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6099" for this suite. 05/09/23 16:53:31.647
------------------------------
â€¢ [3.595 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:53:28.067
    May  9 16:53:28.067: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename custom-resource-definition 05/09/23 16:53:28.068
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:28.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:28.088
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    May  9 16:53:28.093: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:53:31.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6099" for this suite. 05/09/23 16:53:31.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:53:31.663
May  9 16:53:31.663: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename emptydir 05/09/23 16:53:31.666
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:31.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:31.686
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 05/09/23 16:53:31.69
May  9 16:53:31.702: INFO: Waiting up to 5m0s for pod "pod-323206eb-bc2e-45be-b9cf-d417e68d6461" in namespace "emptydir-9908" to be "Succeeded or Failed"
May  9 16:53:31.709: INFO: Pod "pod-323206eb-bc2e-45be-b9cf-d417e68d6461": Phase="Pending", Reason="", readiness=false. Elapsed: 7.026984ms
May  9 16:53:33.718: INFO: Pod "pod-323206eb-bc2e-45be-b9cf-d417e68d6461": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015515187s
May  9 16:53:35.717: INFO: Pod "pod-323206eb-bc2e-45be-b9cf-d417e68d6461": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014988565s
STEP: Saw pod success 05/09/23 16:53:35.717
May  9 16:53:35.717: INFO: Pod "pod-323206eb-bc2e-45be-b9cf-d417e68d6461" satisfied condition "Succeeded or Failed"
May  9 16:53:35.723: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod pod-323206eb-bc2e-45be-b9cf-d417e68d6461 container test-container: <nil>
STEP: delete the pod 05/09/23 16:53:35.778
May  9 16:53:35.799: INFO: Waiting for pod pod-323206eb-bc2e-45be-b9cf-d417e68d6461 to disappear
May  9 16:53:35.804: INFO: Pod pod-323206eb-bc2e-45be-b9cf-d417e68d6461 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  9 16:53:35.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9908" for this suite. 05/09/23 16:53:35.81
------------------------------
â€¢ [4.155 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:53:31.663
    May  9 16:53:31.663: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename emptydir 05/09/23 16:53:31.666
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:31.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:31.686
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 05/09/23 16:53:31.69
    May  9 16:53:31.702: INFO: Waiting up to 5m0s for pod "pod-323206eb-bc2e-45be-b9cf-d417e68d6461" in namespace "emptydir-9908" to be "Succeeded or Failed"
    May  9 16:53:31.709: INFO: Pod "pod-323206eb-bc2e-45be-b9cf-d417e68d6461": Phase="Pending", Reason="", readiness=false. Elapsed: 7.026984ms
    May  9 16:53:33.718: INFO: Pod "pod-323206eb-bc2e-45be-b9cf-d417e68d6461": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015515187s
    May  9 16:53:35.717: INFO: Pod "pod-323206eb-bc2e-45be-b9cf-d417e68d6461": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014988565s
    STEP: Saw pod success 05/09/23 16:53:35.717
    May  9 16:53:35.717: INFO: Pod "pod-323206eb-bc2e-45be-b9cf-d417e68d6461" satisfied condition "Succeeded or Failed"
    May  9 16:53:35.723: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod pod-323206eb-bc2e-45be-b9cf-d417e68d6461 container test-container: <nil>
    STEP: delete the pod 05/09/23 16:53:35.778
    May  9 16:53:35.799: INFO: Waiting for pod pod-323206eb-bc2e-45be-b9cf-d417e68d6461 to disappear
    May  9 16:53:35.804: INFO: Pod pod-323206eb-bc2e-45be-b9cf-d417e68d6461 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  9 16:53:35.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9908" for this suite. 05/09/23 16:53:35.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:53:35.819
May  9 16:53:35.819: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename discovery 05/09/23 16:53:35.82
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:35.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:35.839
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 05/09/23 16:53:35.85
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
May  9 16:53:36.278: INFO: Checking APIGroup: apiregistration.k8s.io
May  9 16:53:36.283: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May  9 16:53:36.283: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
May  9 16:53:36.283: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May  9 16:53:36.283: INFO: Checking APIGroup: apps
May  9 16:53:36.287: INFO: PreferredVersion.GroupVersion: apps/v1
May  9 16:53:36.287: INFO: Versions found [{apps/v1 v1}]
May  9 16:53:36.287: INFO: apps/v1 matches apps/v1
May  9 16:53:36.287: INFO: Checking APIGroup: events.k8s.io
May  9 16:53:36.289: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May  9 16:53:36.289: INFO: Versions found [{events.k8s.io/v1 v1}]
May  9 16:53:36.289: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May  9 16:53:36.289: INFO: Checking APIGroup: authentication.k8s.io
May  9 16:53:36.291: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May  9 16:53:36.291: INFO: Versions found [{authentication.k8s.io/v1 v1}]
May  9 16:53:36.291: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May  9 16:53:36.291: INFO: Checking APIGroup: authorization.k8s.io
May  9 16:53:36.293: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May  9 16:53:36.293: INFO: Versions found [{authorization.k8s.io/v1 v1}]
May  9 16:53:36.293: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May  9 16:53:36.293: INFO: Checking APIGroup: autoscaling
May  9 16:53:36.295: INFO: PreferredVersion.GroupVersion: autoscaling/v2
May  9 16:53:36.295: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
May  9 16:53:36.295: INFO: autoscaling/v2 matches autoscaling/v2
May  9 16:53:36.295: INFO: Checking APIGroup: batch
May  9 16:53:36.297: INFO: PreferredVersion.GroupVersion: batch/v1
May  9 16:53:36.297: INFO: Versions found [{batch/v1 v1}]
May  9 16:53:36.297: INFO: batch/v1 matches batch/v1
May  9 16:53:36.297: INFO: Checking APIGroup: certificates.k8s.io
May  9 16:53:36.299: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May  9 16:53:36.299: INFO: Versions found [{certificates.k8s.io/v1 v1}]
May  9 16:53:36.299: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May  9 16:53:36.299: INFO: Checking APIGroup: networking.k8s.io
May  9 16:53:36.301: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May  9 16:53:36.301: INFO: Versions found [{networking.k8s.io/v1 v1}]
May  9 16:53:36.301: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May  9 16:53:36.301: INFO: Checking APIGroup: policy
May  9 16:53:36.303: INFO: PreferredVersion.GroupVersion: policy/v1
May  9 16:53:36.303: INFO: Versions found [{policy/v1 v1}]
May  9 16:53:36.303: INFO: policy/v1 matches policy/v1
May  9 16:53:36.303: INFO: Checking APIGroup: rbac.authorization.k8s.io
May  9 16:53:36.304: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May  9 16:53:36.304: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
May  9 16:53:36.304: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May  9 16:53:36.304: INFO: Checking APIGroup: storage.k8s.io
May  9 16:53:36.311: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May  9 16:53:36.311: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May  9 16:53:36.311: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May  9 16:53:36.311: INFO: Checking APIGroup: admissionregistration.k8s.io
May  9 16:53:36.313: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May  9 16:53:36.313: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
May  9 16:53:36.313: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May  9 16:53:36.313: INFO: Checking APIGroup: apiextensions.k8s.io
May  9 16:53:36.315: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May  9 16:53:36.315: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
May  9 16:53:36.315: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May  9 16:53:36.315: INFO: Checking APIGroup: scheduling.k8s.io
May  9 16:53:36.317: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May  9 16:53:36.317: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
May  9 16:53:36.317: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May  9 16:53:36.317: INFO: Checking APIGroup: coordination.k8s.io
May  9 16:53:36.319: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May  9 16:53:36.319: INFO: Versions found [{coordination.k8s.io/v1 v1}]
May  9 16:53:36.319: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May  9 16:53:36.319: INFO: Checking APIGroup: node.k8s.io
May  9 16:53:36.321: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May  9 16:53:36.321: INFO: Versions found [{node.k8s.io/v1 v1}]
May  9 16:53:36.321: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May  9 16:53:36.321: INFO: Checking APIGroup: discovery.k8s.io
May  9 16:53:36.323: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
May  9 16:53:36.323: INFO: Versions found [{discovery.k8s.io/v1 v1}]
May  9 16:53:36.323: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
May  9 16:53:36.323: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May  9 16:53:36.324: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
May  9 16:53:36.324: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
May  9 16:53:36.324: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
May  9 16:53:36.324: INFO: Checking APIGroup: crd.projectcalico.org
May  9 16:53:36.327: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
May  9 16:53:36.327: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
May  9 16:53:36.327: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
May  9 16:53:36.327: INFO: Checking APIGroup: snapshot.storage.k8s.io
May  9 16:53:36.329: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
May  9 16:53:36.329: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
May  9 16:53:36.329: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
May  9 16:53:36.329: INFO: Checking APIGroup: kube.cloud.ovh.com
May  9 16:53:36.331: INFO: PreferredVersion.GroupVersion: kube.cloud.ovh.com/v1alpha1
May  9 16:53:36.331: INFO: Versions found [{kube.cloud.ovh.com/v1alpha1 v1alpha1}]
May  9 16:53:36.331: INFO: kube.cloud.ovh.com/v1alpha1 matches kube.cloud.ovh.com/v1alpha1
May  9 16:53:36.331: INFO: Checking APIGroup: metrics.k8s.io
May  9 16:53:36.338: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
May  9 16:53:36.339: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
May  9 16:53:36.339: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
May  9 16:53:36.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-5629" for this suite. 05/09/23 16:53:36.35
------------------------------
â€¢ [0.541 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:53:35.819
    May  9 16:53:35.819: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename discovery 05/09/23 16:53:35.82
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:35.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:35.839
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 05/09/23 16:53:35.85
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    May  9 16:53:36.278: INFO: Checking APIGroup: apiregistration.k8s.io
    May  9 16:53:36.283: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    May  9 16:53:36.283: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    May  9 16:53:36.283: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    May  9 16:53:36.283: INFO: Checking APIGroup: apps
    May  9 16:53:36.287: INFO: PreferredVersion.GroupVersion: apps/v1
    May  9 16:53:36.287: INFO: Versions found [{apps/v1 v1}]
    May  9 16:53:36.287: INFO: apps/v1 matches apps/v1
    May  9 16:53:36.287: INFO: Checking APIGroup: events.k8s.io
    May  9 16:53:36.289: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    May  9 16:53:36.289: INFO: Versions found [{events.k8s.io/v1 v1}]
    May  9 16:53:36.289: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    May  9 16:53:36.289: INFO: Checking APIGroup: authentication.k8s.io
    May  9 16:53:36.291: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    May  9 16:53:36.291: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    May  9 16:53:36.291: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    May  9 16:53:36.291: INFO: Checking APIGroup: authorization.k8s.io
    May  9 16:53:36.293: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    May  9 16:53:36.293: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    May  9 16:53:36.293: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    May  9 16:53:36.293: INFO: Checking APIGroup: autoscaling
    May  9 16:53:36.295: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    May  9 16:53:36.295: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    May  9 16:53:36.295: INFO: autoscaling/v2 matches autoscaling/v2
    May  9 16:53:36.295: INFO: Checking APIGroup: batch
    May  9 16:53:36.297: INFO: PreferredVersion.GroupVersion: batch/v1
    May  9 16:53:36.297: INFO: Versions found [{batch/v1 v1}]
    May  9 16:53:36.297: INFO: batch/v1 matches batch/v1
    May  9 16:53:36.297: INFO: Checking APIGroup: certificates.k8s.io
    May  9 16:53:36.299: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    May  9 16:53:36.299: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    May  9 16:53:36.299: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    May  9 16:53:36.299: INFO: Checking APIGroup: networking.k8s.io
    May  9 16:53:36.301: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    May  9 16:53:36.301: INFO: Versions found [{networking.k8s.io/v1 v1}]
    May  9 16:53:36.301: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    May  9 16:53:36.301: INFO: Checking APIGroup: policy
    May  9 16:53:36.303: INFO: PreferredVersion.GroupVersion: policy/v1
    May  9 16:53:36.303: INFO: Versions found [{policy/v1 v1}]
    May  9 16:53:36.303: INFO: policy/v1 matches policy/v1
    May  9 16:53:36.303: INFO: Checking APIGroup: rbac.authorization.k8s.io
    May  9 16:53:36.304: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    May  9 16:53:36.304: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    May  9 16:53:36.304: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    May  9 16:53:36.304: INFO: Checking APIGroup: storage.k8s.io
    May  9 16:53:36.311: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    May  9 16:53:36.311: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    May  9 16:53:36.311: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    May  9 16:53:36.311: INFO: Checking APIGroup: admissionregistration.k8s.io
    May  9 16:53:36.313: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    May  9 16:53:36.313: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    May  9 16:53:36.313: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    May  9 16:53:36.313: INFO: Checking APIGroup: apiextensions.k8s.io
    May  9 16:53:36.315: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    May  9 16:53:36.315: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    May  9 16:53:36.315: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    May  9 16:53:36.315: INFO: Checking APIGroup: scheduling.k8s.io
    May  9 16:53:36.317: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    May  9 16:53:36.317: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    May  9 16:53:36.317: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    May  9 16:53:36.317: INFO: Checking APIGroup: coordination.k8s.io
    May  9 16:53:36.319: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    May  9 16:53:36.319: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    May  9 16:53:36.319: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    May  9 16:53:36.319: INFO: Checking APIGroup: node.k8s.io
    May  9 16:53:36.321: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    May  9 16:53:36.321: INFO: Versions found [{node.k8s.io/v1 v1}]
    May  9 16:53:36.321: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    May  9 16:53:36.321: INFO: Checking APIGroup: discovery.k8s.io
    May  9 16:53:36.323: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    May  9 16:53:36.323: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    May  9 16:53:36.323: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    May  9 16:53:36.323: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    May  9 16:53:36.324: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    May  9 16:53:36.324: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    May  9 16:53:36.324: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    May  9 16:53:36.324: INFO: Checking APIGroup: crd.projectcalico.org
    May  9 16:53:36.327: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    May  9 16:53:36.327: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    May  9 16:53:36.327: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    May  9 16:53:36.327: INFO: Checking APIGroup: snapshot.storage.k8s.io
    May  9 16:53:36.329: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    May  9 16:53:36.329: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
    May  9 16:53:36.329: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    May  9 16:53:36.329: INFO: Checking APIGroup: kube.cloud.ovh.com
    May  9 16:53:36.331: INFO: PreferredVersion.GroupVersion: kube.cloud.ovh.com/v1alpha1
    May  9 16:53:36.331: INFO: Versions found [{kube.cloud.ovh.com/v1alpha1 v1alpha1}]
    May  9 16:53:36.331: INFO: kube.cloud.ovh.com/v1alpha1 matches kube.cloud.ovh.com/v1alpha1
    May  9 16:53:36.331: INFO: Checking APIGroup: metrics.k8s.io
    May  9 16:53:36.338: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    May  9 16:53:36.339: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    May  9 16:53:36.339: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    May  9 16:53:36.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-5629" for this suite. 05/09/23 16:53:36.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:53:36.361
May  9 16:53:36.361: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename var-expansion 05/09/23 16:53:36.362
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:36.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:36.384
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 05/09/23 16:53:36.388
May  9 16:53:36.400: INFO: Waiting up to 5m0s for pod "var-expansion-96c15828-a112-4daf-9360-ebaeadb7968c" in namespace "var-expansion-4544" to be "Succeeded or Failed"
May  9 16:53:36.406: INFO: Pod "var-expansion-96c15828-a112-4daf-9360-ebaeadb7968c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.089021ms
May  9 16:53:38.413: INFO: Pod "var-expansion-96c15828-a112-4daf-9360-ebaeadb7968c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013800094s
May  9 16:53:40.412: INFO: Pod "var-expansion-96c15828-a112-4daf-9360-ebaeadb7968c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012454525s
STEP: Saw pod success 05/09/23 16:53:40.412
May  9 16:53:40.412: INFO: Pod "var-expansion-96c15828-a112-4daf-9360-ebaeadb7968c" satisfied condition "Succeeded or Failed"
May  9 16:53:40.417: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod var-expansion-96c15828-a112-4daf-9360-ebaeadb7968c container dapi-container: <nil>
STEP: delete the pod 05/09/23 16:53:40.432
May  9 16:53:40.448: INFO: Waiting for pod var-expansion-96c15828-a112-4daf-9360-ebaeadb7968c to disappear
May  9 16:53:40.452: INFO: Pod var-expansion-96c15828-a112-4daf-9360-ebaeadb7968c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May  9 16:53:40.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4544" for this suite. 05/09/23 16:53:40.459
------------------------------
â€¢ [4.107 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:53:36.361
    May  9 16:53:36.361: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename var-expansion 05/09/23 16:53:36.362
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:36.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:36.384
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 05/09/23 16:53:36.388
    May  9 16:53:36.400: INFO: Waiting up to 5m0s for pod "var-expansion-96c15828-a112-4daf-9360-ebaeadb7968c" in namespace "var-expansion-4544" to be "Succeeded or Failed"
    May  9 16:53:36.406: INFO: Pod "var-expansion-96c15828-a112-4daf-9360-ebaeadb7968c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.089021ms
    May  9 16:53:38.413: INFO: Pod "var-expansion-96c15828-a112-4daf-9360-ebaeadb7968c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013800094s
    May  9 16:53:40.412: INFO: Pod "var-expansion-96c15828-a112-4daf-9360-ebaeadb7968c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012454525s
    STEP: Saw pod success 05/09/23 16:53:40.412
    May  9 16:53:40.412: INFO: Pod "var-expansion-96c15828-a112-4daf-9360-ebaeadb7968c" satisfied condition "Succeeded or Failed"
    May  9 16:53:40.417: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod var-expansion-96c15828-a112-4daf-9360-ebaeadb7968c container dapi-container: <nil>
    STEP: delete the pod 05/09/23 16:53:40.432
    May  9 16:53:40.448: INFO: Waiting for pod var-expansion-96c15828-a112-4daf-9360-ebaeadb7968c to disappear
    May  9 16:53:40.452: INFO: Pod var-expansion-96c15828-a112-4daf-9360-ebaeadb7968c no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May  9 16:53:40.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4544" for this suite. 05/09/23 16:53:40.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:53:40.469
May  9 16:53:40.469: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename dns 05/09/23 16:53:40.47
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:40.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:40.503
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 05/09/23 16:53:40.507
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 05/09/23 16:53:40.507
STEP: creating a pod to probe DNS 05/09/23 16:53:40.508
STEP: submitting the pod to kubernetes 05/09/23 16:53:40.508
May  9 16:53:40.519: INFO: Waiting up to 15m0s for pod "dns-test-744b8f09-6bf1-4c8a-a93d-61af32556f8d" in namespace "dns-4052" to be "running"
May  9 16:53:40.526: INFO: Pod "dns-test-744b8f09-6bf1-4c8a-a93d-61af32556f8d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.07143ms
May  9 16:53:42.532: INFO: Pod "dns-test-744b8f09-6bf1-4c8a-a93d-61af32556f8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.012994607s
May  9 16:53:42.533: INFO: Pod "dns-test-744b8f09-6bf1-4c8a-a93d-61af32556f8d" satisfied condition "running"
STEP: retrieving the pod 05/09/23 16:53:42.533
STEP: looking for the results for each expected name from probers 05/09/23 16:53:42.537
May  9 16:53:42.612: INFO: DNS probes using dns-4052/dns-test-744b8f09-6bf1-4c8a-a93d-61af32556f8d succeeded

STEP: deleting the pod 05/09/23 16:53:42.612
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May  9 16:53:42.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4052" for this suite. 05/09/23 16:53:42.637
------------------------------
â€¢ [2.185 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:53:40.469
    May  9 16:53:40.469: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename dns 05/09/23 16:53:40.47
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:40.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:40.503
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     05/09/23 16:53:40.507
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     05/09/23 16:53:40.507
    STEP: creating a pod to probe DNS 05/09/23 16:53:40.508
    STEP: submitting the pod to kubernetes 05/09/23 16:53:40.508
    May  9 16:53:40.519: INFO: Waiting up to 15m0s for pod "dns-test-744b8f09-6bf1-4c8a-a93d-61af32556f8d" in namespace "dns-4052" to be "running"
    May  9 16:53:40.526: INFO: Pod "dns-test-744b8f09-6bf1-4c8a-a93d-61af32556f8d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.07143ms
    May  9 16:53:42.532: INFO: Pod "dns-test-744b8f09-6bf1-4c8a-a93d-61af32556f8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.012994607s
    May  9 16:53:42.533: INFO: Pod "dns-test-744b8f09-6bf1-4c8a-a93d-61af32556f8d" satisfied condition "running"
    STEP: retrieving the pod 05/09/23 16:53:42.533
    STEP: looking for the results for each expected name from probers 05/09/23 16:53:42.537
    May  9 16:53:42.612: INFO: DNS probes using dns-4052/dns-test-744b8f09-6bf1-4c8a-a93d-61af32556f8d succeeded

    STEP: deleting the pod 05/09/23 16:53:42.612
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May  9 16:53:42.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4052" for this suite. 05/09/23 16:53:42.637
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:53:42.655
May  9 16:53:42.655: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename sysctl 05/09/23 16:53:42.656
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:42.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:42.678
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 05/09/23 16:53:42.683
STEP: Watching for error events or started pod 05/09/23 16:53:42.691
STEP: Waiting for pod completion 05/09/23 16:53:44.698
May  9 16:53:44.699: INFO: Waiting up to 3m0s for pod "sysctl-b77ac52a-67f6-41b7-861f-091a74879eb7" in namespace "sysctl-8916" to be "completed"
May  9 16:53:44.704: INFO: Pod "sysctl-b77ac52a-67f6-41b7-861f-091a74879eb7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.348627ms
May  9 16:53:46.710: INFO: Pod "sysctl-b77ac52a-67f6-41b7-861f-091a74879eb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011349314s
May  9 16:53:46.710: INFO: Pod "sysctl-b77ac52a-67f6-41b7-861f-091a74879eb7" satisfied condition "completed"
STEP: Checking that the pod succeeded 05/09/23 16:53:46.714
STEP: Getting logs from the pod 05/09/23 16:53:46.715
STEP: Checking that the sysctl is actually updated 05/09/23 16:53:46.73
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May  9 16:53:46.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-8916" for this suite. 05/09/23 16:53:46.736
------------------------------
â€¢ [4.094 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:53:42.655
    May  9 16:53:42.655: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename sysctl 05/09/23 16:53:42.656
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:42.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:42.678
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 05/09/23 16:53:42.683
    STEP: Watching for error events or started pod 05/09/23 16:53:42.691
    STEP: Waiting for pod completion 05/09/23 16:53:44.698
    May  9 16:53:44.699: INFO: Waiting up to 3m0s for pod "sysctl-b77ac52a-67f6-41b7-861f-091a74879eb7" in namespace "sysctl-8916" to be "completed"
    May  9 16:53:44.704: INFO: Pod "sysctl-b77ac52a-67f6-41b7-861f-091a74879eb7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.348627ms
    May  9 16:53:46.710: INFO: Pod "sysctl-b77ac52a-67f6-41b7-861f-091a74879eb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011349314s
    May  9 16:53:46.710: INFO: Pod "sysctl-b77ac52a-67f6-41b7-861f-091a74879eb7" satisfied condition "completed"
    STEP: Checking that the pod succeeded 05/09/23 16:53:46.714
    STEP: Getting logs from the pod 05/09/23 16:53:46.715
    STEP: Checking that the sysctl is actually updated 05/09/23 16:53:46.73
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May  9 16:53:46.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-8916" for this suite. 05/09/23 16:53:46.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:53:46.75
May  9 16:53:46.750: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 16:53:46.751
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:46.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:46.774
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ad8e2133-433b-48d6-9201-12a7abe8e074 05/09/23 16:53:46.785
STEP: Creating the pod 05/09/23 16:53:46.791
May  9 16:53:46.803: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0ddc7106-b62c-4ebe-ba30-25ac26983101" in namespace "projected-3394" to be "running and ready"
May  9 16:53:46.810: INFO: Pod "pod-projected-configmaps-0ddc7106-b62c-4ebe-ba30-25ac26983101": Phase="Pending", Reason="", readiness=false. Elapsed: 7.075424ms
May  9 16:53:46.810: INFO: The phase of Pod pod-projected-configmaps-0ddc7106-b62c-4ebe-ba30-25ac26983101 is Pending, waiting for it to be Running (with Ready = true)
May  9 16:53:48.819: INFO: Pod "pod-projected-configmaps-0ddc7106-b62c-4ebe-ba30-25ac26983101": Phase="Running", Reason="", readiness=true. Elapsed: 2.015544584s
May  9 16:53:48.819: INFO: The phase of Pod pod-projected-configmaps-0ddc7106-b62c-4ebe-ba30-25ac26983101 is Running (Ready = true)
May  9 16:53:48.819: INFO: Pod "pod-projected-configmaps-0ddc7106-b62c-4ebe-ba30-25ac26983101" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-ad8e2133-433b-48d6-9201-12a7abe8e074 05/09/23 16:53:48.833
STEP: waiting to observe update in volume 05/09/23 16:53:48.839
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May  9 16:53:50.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3394" for this suite. 05/09/23 16:53:50.871
------------------------------
â€¢ [4.132 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:53:46.75
    May  9 16:53:46.750: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 16:53:46.751
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:46.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:46.774
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-ad8e2133-433b-48d6-9201-12a7abe8e074 05/09/23 16:53:46.785
    STEP: Creating the pod 05/09/23 16:53:46.791
    May  9 16:53:46.803: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0ddc7106-b62c-4ebe-ba30-25ac26983101" in namespace "projected-3394" to be "running and ready"
    May  9 16:53:46.810: INFO: Pod "pod-projected-configmaps-0ddc7106-b62c-4ebe-ba30-25ac26983101": Phase="Pending", Reason="", readiness=false. Elapsed: 7.075424ms
    May  9 16:53:46.810: INFO: The phase of Pod pod-projected-configmaps-0ddc7106-b62c-4ebe-ba30-25ac26983101 is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:53:48.819: INFO: Pod "pod-projected-configmaps-0ddc7106-b62c-4ebe-ba30-25ac26983101": Phase="Running", Reason="", readiness=true. Elapsed: 2.015544584s
    May  9 16:53:48.819: INFO: The phase of Pod pod-projected-configmaps-0ddc7106-b62c-4ebe-ba30-25ac26983101 is Running (Ready = true)
    May  9 16:53:48.819: INFO: Pod "pod-projected-configmaps-0ddc7106-b62c-4ebe-ba30-25ac26983101" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-ad8e2133-433b-48d6-9201-12a7abe8e074 05/09/23 16:53:48.833
    STEP: waiting to observe update in volume 05/09/23 16:53:48.839
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May  9 16:53:50.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3394" for this suite. 05/09/23 16:53:50.871
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:53:50.882
May  9 16:53:50.882: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 05/09/23 16:53:50.883
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:50.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:50.912
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 05/09/23 16:53:50.919
STEP: Creating hostNetwork=false pod 05/09/23 16:53:50.919
May  9 16:53:50.930: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-7611" to be "running and ready"
May  9 16:53:50.935: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.931894ms
May  9 16:53:50.935: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May  9 16:53:52.942: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012189198s
May  9 16:53:52.942: INFO: The phase of Pod test-pod is Running (Ready = true)
May  9 16:53:52.942: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 05/09/23 16:53:52.947
May  9 16:53:52.956: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-7611" to be "running and ready"
May  9 16:53:52.962: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.554119ms
May  9 16:53:52.962: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
May  9 16:53:54.968: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012466725s
May  9 16:53:54.968: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
May  9 16:53:54.968: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 05/09/23 16:53:54.973
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 05/09/23 16:53:54.973
May  9 16:53:54.973: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:53:54.973: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:53:54.974: INFO: ExecWithOptions: Clientset creation
May  9 16:53:54.974: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  9 16:53:55.098: INFO: Exec stderr: ""
May  9 16:53:55.098: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:53:55.098: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:53:55.098: INFO: ExecWithOptions: Clientset creation
May  9 16:53:55.099: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  9 16:53:55.242: INFO: Exec stderr: ""
May  9 16:53:55.242: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:53:55.242: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:53:55.242: INFO: ExecWithOptions: Clientset creation
May  9 16:53:55.242: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  9 16:53:55.392: INFO: Exec stderr: ""
May  9 16:53:55.392: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:53:55.392: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:53:55.392: INFO: ExecWithOptions: Clientset creation
May  9 16:53:55.392: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  9 16:53:55.532: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 05/09/23 16:53:55.532
May  9 16:53:55.533: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:53:55.533: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:53:55.533: INFO: ExecWithOptions: Clientset creation
May  9 16:53:55.533: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
May  9 16:53:55.658: INFO: Exec stderr: ""
May  9 16:53:55.658: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:53:55.658: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:53:55.658: INFO: ExecWithOptions: Clientset creation
May  9 16:53:55.658: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
May  9 16:53:55.798: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 05/09/23 16:53:55.798
May  9 16:53:55.798: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:53:55.798: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:53:55.798: INFO: ExecWithOptions: Clientset creation
May  9 16:53:55.798: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  9 16:53:55.919: INFO: Exec stderr: ""
May  9 16:53:55.919: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:53:55.919: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:53:55.920: INFO: ExecWithOptions: Clientset creation
May  9 16:53:55.920: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  9 16:53:56.039: INFO: Exec stderr: ""
May  9 16:53:56.039: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:53:56.039: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:53:56.039: INFO: ExecWithOptions: Clientset creation
May  9 16:53:56.039: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  9 16:53:56.166: INFO: Exec stderr: ""
May  9 16:53:56.166: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  9 16:53:56.166: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
May  9 16:53:56.166: INFO: ExecWithOptions: Clientset creation
May  9 16:53:56.167: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  9 16:53:56.272: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
May  9 16:53:56.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7611" for this suite. 05/09/23 16:53:56.278
------------------------------
â€¢ [SLOW TEST] [5.404 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:53:50.882
    May  9 16:53:50.882: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 05/09/23 16:53:50.883
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:50.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:50.912
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 05/09/23 16:53:50.919
    STEP: Creating hostNetwork=false pod 05/09/23 16:53:50.919
    May  9 16:53:50.930: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-7611" to be "running and ready"
    May  9 16:53:50.935: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.931894ms
    May  9 16:53:50.935: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:53:52.942: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012189198s
    May  9 16:53:52.942: INFO: The phase of Pod test-pod is Running (Ready = true)
    May  9 16:53:52.942: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 05/09/23 16:53:52.947
    May  9 16:53:52.956: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-7611" to be "running and ready"
    May  9 16:53:52.962: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.554119ms
    May  9 16:53:52.962: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:53:54.968: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012466725s
    May  9 16:53:54.968: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    May  9 16:53:54.968: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 05/09/23 16:53:54.973
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 05/09/23 16:53:54.973
    May  9 16:53:54.973: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:53:54.973: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:53:54.974: INFO: ExecWithOptions: Clientset creation
    May  9 16:53:54.974: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  9 16:53:55.098: INFO: Exec stderr: ""
    May  9 16:53:55.098: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:53:55.098: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:53:55.098: INFO: ExecWithOptions: Clientset creation
    May  9 16:53:55.099: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  9 16:53:55.242: INFO: Exec stderr: ""
    May  9 16:53:55.242: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:53:55.242: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:53:55.242: INFO: ExecWithOptions: Clientset creation
    May  9 16:53:55.242: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  9 16:53:55.392: INFO: Exec stderr: ""
    May  9 16:53:55.392: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:53:55.392: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:53:55.392: INFO: ExecWithOptions: Clientset creation
    May  9 16:53:55.392: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  9 16:53:55.532: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 05/09/23 16:53:55.532
    May  9 16:53:55.533: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:53:55.533: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:53:55.533: INFO: ExecWithOptions: Clientset creation
    May  9 16:53:55.533: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    May  9 16:53:55.658: INFO: Exec stderr: ""
    May  9 16:53:55.658: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:53:55.658: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:53:55.658: INFO: ExecWithOptions: Clientset creation
    May  9 16:53:55.658: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    May  9 16:53:55.798: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 05/09/23 16:53:55.798
    May  9 16:53:55.798: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:53:55.798: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:53:55.798: INFO: ExecWithOptions: Clientset creation
    May  9 16:53:55.798: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  9 16:53:55.919: INFO: Exec stderr: ""
    May  9 16:53:55.919: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:53:55.919: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:53:55.920: INFO: ExecWithOptions: Clientset creation
    May  9 16:53:55.920: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  9 16:53:56.039: INFO: Exec stderr: ""
    May  9 16:53:56.039: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:53:56.039: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:53:56.039: INFO: ExecWithOptions: Clientset creation
    May  9 16:53:56.039: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  9 16:53:56.166: INFO: Exec stderr: ""
    May  9 16:53:56.166: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7611 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  9 16:53:56.166: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    May  9 16:53:56.166: INFO: ExecWithOptions: Clientset creation
    May  9 16:53:56.167: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7611/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  9 16:53:56.272: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    May  9 16:53:56.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-7611" for this suite. 05/09/23 16:53:56.278
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:53:56.287
May  9 16:53:56.287: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename downward-api 05/09/23 16:53:56.288
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:56.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:56.322
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 05/09/23 16:53:56.328
May  9 16:53:56.339: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4041f3b-cee5-4562-8e91-61f8106dc251" in namespace "downward-api-1433" to be "Succeeded or Failed"
May  9 16:53:56.344: INFO: Pod "downwardapi-volume-e4041f3b-cee5-4562-8e91-61f8106dc251": Phase="Pending", Reason="", readiness=false. Elapsed: 5.126846ms
May  9 16:53:58.350: INFO: Pod "downwardapi-volume-e4041f3b-cee5-4562-8e91-61f8106dc251": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011379328s
May  9 16:54:00.351: INFO: Pod "downwardapi-volume-e4041f3b-cee5-4562-8e91-61f8106dc251": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011757918s
STEP: Saw pod success 05/09/23 16:54:00.351
May  9 16:54:00.351: INFO: Pod "downwardapi-volume-e4041f3b-cee5-4562-8e91-61f8106dc251" satisfied condition "Succeeded or Failed"
May  9 16:54:00.356: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod downwardapi-volume-e4041f3b-cee5-4562-8e91-61f8106dc251 container client-container: <nil>
STEP: delete the pod 05/09/23 16:54:00.372
May  9 16:54:00.399: INFO: Waiting for pod downwardapi-volume-e4041f3b-cee5-4562-8e91-61f8106dc251 to disappear
May  9 16:54:00.408: INFO: Pod downwardapi-volume-e4041f3b-cee5-4562-8e91-61f8106dc251 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  9 16:54:00.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1433" for this suite. 05/09/23 16:54:00.417
------------------------------
â€¢ [4.139 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:53:56.287
    May  9 16:53:56.287: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename downward-api 05/09/23 16:53:56.288
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:53:56.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:53:56.322
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 05/09/23 16:53:56.328
    May  9 16:53:56.339: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4041f3b-cee5-4562-8e91-61f8106dc251" in namespace "downward-api-1433" to be "Succeeded or Failed"
    May  9 16:53:56.344: INFO: Pod "downwardapi-volume-e4041f3b-cee5-4562-8e91-61f8106dc251": Phase="Pending", Reason="", readiness=false. Elapsed: 5.126846ms
    May  9 16:53:58.350: INFO: Pod "downwardapi-volume-e4041f3b-cee5-4562-8e91-61f8106dc251": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011379328s
    May  9 16:54:00.351: INFO: Pod "downwardapi-volume-e4041f3b-cee5-4562-8e91-61f8106dc251": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011757918s
    STEP: Saw pod success 05/09/23 16:54:00.351
    May  9 16:54:00.351: INFO: Pod "downwardapi-volume-e4041f3b-cee5-4562-8e91-61f8106dc251" satisfied condition "Succeeded or Failed"
    May  9 16:54:00.356: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod downwardapi-volume-e4041f3b-cee5-4562-8e91-61f8106dc251 container client-container: <nil>
    STEP: delete the pod 05/09/23 16:54:00.372
    May  9 16:54:00.399: INFO: Waiting for pod downwardapi-volume-e4041f3b-cee5-4562-8e91-61f8106dc251 to disappear
    May  9 16:54:00.408: INFO: Pod downwardapi-volume-e4041f3b-cee5-4562-8e91-61f8106dc251 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  9 16:54:00.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1433" for this suite. 05/09/23 16:54:00.417
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:54:00.426
May  9 16:54:00.426: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename downward-api 05/09/23 16:54:00.427
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:54:00.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:54:00.45
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 05/09/23 16:54:00.454
May  9 16:54:00.464: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b22b92df-7274-4d6d-90e4-2fc222876878" in namespace "downward-api-695" to be "Succeeded or Failed"
May  9 16:54:00.468: INFO: Pod "downwardapi-volume-b22b92df-7274-4d6d-90e4-2fc222876878": Phase="Pending", Reason="", readiness=false. Elapsed: 3.89311ms
May  9 16:54:02.475: INFO: Pod "downwardapi-volume-b22b92df-7274-4d6d-90e4-2fc222876878": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010906649s
May  9 16:54:04.476: INFO: Pod "downwardapi-volume-b22b92df-7274-4d6d-90e4-2fc222876878": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012434878s
STEP: Saw pod success 05/09/23 16:54:04.476
May  9 16:54:04.477: INFO: Pod "downwardapi-volume-b22b92df-7274-4d6d-90e4-2fc222876878" satisfied condition "Succeeded or Failed"
May  9 16:54:04.482: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod downwardapi-volume-b22b92df-7274-4d6d-90e4-2fc222876878 container client-container: <nil>
STEP: delete the pod 05/09/23 16:54:04.493
May  9 16:54:04.510: INFO: Waiting for pod downwardapi-volume-b22b92df-7274-4d6d-90e4-2fc222876878 to disappear
May  9 16:54:04.515: INFO: Pod downwardapi-volume-b22b92df-7274-4d6d-90e4-2fc222876878 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  9 16:54:04.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-695" for this suite. 05/09/23 16:54:04.522
------------------------------
â€¢ [4.107 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:54:00.426
    May  9 16:54:00.426: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename downward-api 05/09/23 16:54:00.427
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:54:00.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:54:00.45
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 05/09/23 16:54:00.454
    May  9 16:54:00.464: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b22b92df-7274-4d6d-90e4-2fc222876878" in namespace "downward-api-695" to be "Succeeded or Failed"
    May  9 16:54:00.468: INFO: Pod "downwardapi-volume-b22b92df-7274-4d6d-90e4-2fc222876878": Phase="Pending", Reason="", readiness=false. Elapsed: 3.89311ms
    May  9 16:54:02.475: INFO: Pod "downwardapi-volume-b22b92df-7274-4d6d-90e4-2fc222876878": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010906649s
    May  9 16:54:04.476: INFO: Pod "downwardapi-volume-b22b92df-7274-4d6d-90e4-2fc222876878": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012434878s
    STEP: Saw pod success 05/09/23 16:54:04.476
    May  9 16:54:04.477: INFO: Pod "downwardapi-volume-b22b92df-7274-4d6d-90e4-2fc222876878" satisfied condition "Succeeded or Failed"
    May  9 16:54:04.482: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 pod downwardapi-volume-b22b92df-7274-4d6d-90e4-2fc222876878 container client-container: <nil>
    STEP: delete the pod 05/09/23 16:54:04.493
    May  9 16:54:04.510: INFO: Waiting for pod downwardapi-volume-b22b92df-7274-4d6d-90e4-2fc222876878 to disappear
    May  9 16:54:04.515: INFO: Pod downwardapi-volume-b22b92df-7274-4d6d-90e4-2fc222876878 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  9 16:54:04.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-695" for this suite. 05/09/23 16:54:04.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:54:04.534
May  9 16:54:04.534: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename emptydir 05/09/23 16:54:04.535
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:54:04.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:54:04.562
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 05/09/23 16:54:04.567
May  9 16:54:04.577: INFO: Waiting up to 5m0s for pod "pod-d9cedf21-e8f6-4207-8248-b084af014023" in namespace "emptydir-6591" to be "Succeeded or Failed"
May  9 16:54:04.583: INFO: Pod "pod-d9cedf21-e8f6-4207-8248-b084af014023": Phase="Pending", Reason="", readiness=false. Elapsed: 5.870043ms
May  9 16:54:06.591: INFO: Pod "pod-d9cedf21-e8f6-4207-8248-b084af014023": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013421076s
May  9 16:54:08.589: INFO: Pod "pod-d9cedf21-e8f6-4207-8248-b084af014023": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011771324s
STEP: Saw pod success 05/09/23 16:54:08.589
May  9 16:54:08.589: INFO: Pod "pod-d9cedf21-e8f6-4207-8248-b084af014023" satisfied condition "Succeeded or Failed"
May  9 16:54:08.596: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-d9cedf21-e8f6-4207-8248-b084af014023 container test-container: <nil>
STEP: delete the pod 05/09/23 16:54:08.612
May  9 16:54:08.627: INFO: Waiting for pod pod-d9cedf21-e8f6-4207-8248-b084af014023 to disappear
May  9 16:54:08.632: INFO: Pod pod-d9cedf21-e8f6-4207-8248-b084af014023 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  9 16:54:08.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6591" for this suite. 05/09/23 16:54:08.641
------------------------------
â€¢ [4.118 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:54:04.534
    May  9 16:54:04.534: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename emptydir 05/09/23 16:54:04.535
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:54:04.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:54:04.562
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 05/09/23 16:54:04.567
    May  9 16:54:04.577: INFO: Waiting up to 5m0s for pod "pod-d9cedf21-e8f6-4207-8248-b084af014023" in namespace "emptydir-6591" to be "Succeeded or Failed"
    May  9 16:54:04.583: INFO: Pod "pod-d9cedf21-e8f6-4207-8248-b084af014023": Phase="Pending", Reason="", readiness=false. Elapsed: 5.870043ms
    May  9 16:54:06.591: INFO: Pod "pod-d9cedf21-e8f6-4207-8248-b084af014023": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013421076s
    May  9 16:54:08.589: INFO: Pod "pod-d9cedf21-e8f6-4207-8248-b084af014023": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011771324s
    STEP: Saw pod success 05/09/23 16:54:08.589
    May  9 16:54:08.589: INFO: Pod "pod-d9cedf21-e8f6-4207-8248-b084af014023" satisfied condition "Succeeded or Failed"
    May  9 16:54:08.596: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-d9cedf21-e8f6-4207-8248-b084af014023 container test-container: <nil>
    STEP: delete the pod 05/09/23 16:54:08.612
    May  9 16:54:08.627: INFO: Waiting for pod pod-d9cedf21-e8f6-4207-8248-b084af014023 to disappear
    May  9 16:54:08.632: INFO: Pod pod-d9cedf21-e8f6-4207-8248-b084af014023 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  9 16:54:08.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6591" for this suite. 05/09/23 16:54:08.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:54:08.652
May  9 16:54:08.652: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename emptydir 05/09/23 16:54:08.653
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:54:08.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:54:08.673
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 05/09/23 16:54:08.679
May  9 16:54:08.688: INFO: Waiting up to 5m0s for pod "pod-7718dc6a-7986-4245-ac46-fb7a932dc03f" in namespace "emptydir-5401" to be "Succeeded or Failed"
May  9 16:54:08.693: INFO: Pod "pod-7718dc6a-7986-4245-ac46-fb7a932dc03f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.681012ms
May  9 16:54:10.703: INFO: Pod "pod-7718dc6a-7986-4245-ac46-fb7a932dc03f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015008334s
May  9 16:54:12.700: INFO: Pod "pod-7718dc6a-7986-4245-ac46-fb7a932dc03f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01285949s
STEP: Saw pod success 05/09/23 16:54:12.701
May  9 16:54:12.701: INFO: Pod "pod-7718dc6a-7986-4245-ac46-fb7a932dc03f" satisfied condition "Succeeded or Failed"
May  9 16:54:12.706: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-7718dc6a-7986-4245-ac46-fb7a932dc03f container test-container: <nil>
STEP: delete the pod 05/09/23 16:54:12.716
May  9 16:54:12.732: INFO: Waiting for pod pod-7718dc6a-7986-4245-ac46-fb7a932dc03f to disappear
May  9 16:54:12.736: INFO: Pod pod-7718dc6a-7986-4245-ac46-fb7a932dc03f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  9 16:54:12.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5401" for this suite. 05/09/23 16:54:12.743
------------------------------
â€¢ [4.099 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:54:08.652
    May  9 16:54:08.652: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename emptydir 05/09/23 16:54:08.653
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:54:08.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:54:08.673
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 05/09/23 16:54:08.679
    May  9 16:54:08.688: INFO: Waiting up to 5m0s for pod "pod-7718dc6a-7986-4245-ac46-fb7a932dc03f" in namespace "emptydir-5401" to be "Succeeded or Failed"
    May  9 16:54:08.693: INFO: Pod "pod-7718dc6a-7986-4245-ac46-fb7a932dc03f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.681012ms
    May  9 16:54:10.703: INFO: Pod "pod-7718dc6a-7986-4245-ac46-fb7a932dc03f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015008334s
    May  9 16:54:12.700: INFO: Pod "pod-7718dc6a-7986-4245-ac46-fb7a932dc03f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01285949s
    STEP: Saw pod success 05/09/23 16:54:12.701
    May  9 16:54:12.701: INFO: Pod "pod-7718dc6a-7986-4245-ac46-fb7a932dc03f" satisfied condition "Succeeded or Failed"
    May  9 16:54:12.706: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-7718dc6a-7986-4245-ac46-fb7a932dc03f container test-container: <nil>
    STEP: delete the pod 05/09/23 16:54:12.716
    May  9 16:54:12.732: INFO: Waiting for pod pod-7718dc6a-7986-4245-ac46-fb7a932dc03f to disappear
    May  9 16:54:12.736: INFO: Pod pod-7718dc6a-7986-4245-ac46-fb7a932dc03f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  9 16:54:12.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5401" for this suite. 05/09/23 16:54:12.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:54:12.753
May  9 16:54:12.753: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 16:54:12.754
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:54:12.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:54:12.774
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
May  9 16:54:12.778: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/09/23 16:54:16.083
May  9 16:54:16.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5248 --namespace=crd-publish-openapi-5248 create -f -'
May  9 16:54:17.330: INFO: stderr: ""
May  9 16:54:17.330: INFO: stdout: "e2e-test-crd-publish-openapi-243-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May  9 16:54:17.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5248 --namespace=crd-publish-openapi-5248 delete e2e-test-crd-publish-openapi-243-crds test-cr'
May  9 16:54:17.425: INFO: stderr: ""
May  9 16:54:17.425: INFO: stdout: "e2e-test-crd-publish-openapi-243-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May  9 16:54:17.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5248 --namespace=crd-publish-openapi-5248 apply -f -'
May  9 16:54:17.645: INFO: stderr: ""
May  9 16:54:17.645: INFO: stdout: "e2e-test-crd-publish-openapi-243-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May  9 16:54:17.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5248 --namespace=crd-publish-openapi-5248 delete e2e-test-crd-publish-openapi-243-crds test-cr'
May  9 16:54:17.743: INFO: stderr: ""
May  9 16:54:17.743: INFO: stdout: "e2e-test-crd-publish-openapi-243-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 05/09/23 16:54:17.743
May  9 16:54:17.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5248 explain e2e-test-crd-publish-openapi-243-crds'
May  9 16:54:17.934: INFO: stderr: ""
May  9 16:54:17.934: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-243-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:54:19.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5248" for this suite. 05/09/23 16:54:19.732
------------------------------
â€¢ [SLOW TEST] [6.987 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:54:12.753
    May  9 16:54:12.753: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 16:54:12.754
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:54:12.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:54:12.774
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    May  9 16:54:12.778: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/09/23 16:54:16.083
    May  9 16:54:16.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5248 --namespace=crd-publish-openapi-5248 create -f -'
    May  9 16:54:17.330: INFO: stderr: ""
    May  9 16:54:17.330: INFO: stdout: "e2e-test-crd-publish-openapi-243-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    May  9 16:54:17.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5248 --namespace=crd-publish-openapi-5248 delete e2e-test-crd-publish-openapi-243-crds test-cr'
    May  9 16:54:17.425: INFO: stderr: ""
    May  9 16:54:17.425: INFO: stdout: "e2e-test-crd-publish-openapi-243-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    May  9 16:54:17.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5248 --namespace=crd-publish-openapi-5248 apply -f -'
    May  9 16:54:17.645: INFO: stderr: ""
    May  9 16:54:17.645: INFO: stdout: "e2e-test-crd-publish-openapi-243-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    May  9 16:54:17.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5248 --namespace=crd-publish-openapi-5248 delete e2e-test-crd-publish-openapi-243-crds test-cr'
    May  9 16:54:17.743: INFO: stderr: ""
    May  9 16:54:17.743: INFO: stdout: "e2e-test-crd-publish-openapi-243-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 05/09/23 16:54:17.743
    May  9 16:54:17.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=crd-publish-openapi-5248 explain e2e-test-crd-publish-openapi-243-crds'
    May  9 16:54:17.934: INFO: stderr: ""
    May  9 16:54:17.934: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-243-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:54:19.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5248" for this suite. 05/09/23 16:54:19.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:54:19.742
May  9 16:54:19.742: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename taint-multiple-pods 05/09/23 16:54:19.743
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:54:19.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:54:19.764
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
May  9 16:54:19.768: INFO: Waiting up to 1m0s for all nodes to be ready
May  9 16:55:19.816: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
May  9 16:55:19.832: INFO: Starting informer...
STEP: Starting pods... 05/09/23 16:55:19.832
May  9 16:55:20.060: INFO: Pod1 is running on nodepool-8cc7f47e-9b0c-4801-88-node-7ad816. Tainting Node
May  9 16:55:20.273: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8736" to be "running"
May  9 16:55:20.279: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.559394ms
May  9 16:55:22.286: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012432794s
May  9 16:55:22.286: INFO: Pod "taint-eviction-b1" satisfied condition "running"
May  9 16:55:22.286: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8736" to be "running"
May  9 16:55:22.290: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.360022ms
May  9 16:55:22.290: INFO: Pod "taint-eviction-b2" satisfied condition "running"
May  9 16:55:22.290: INFO: Pod2 is running on nodepool-8cc7f47e-9b0c-4801-88-node-7ad816. Tainting Node
STEP: Trying to apply a taint on the Node 05/09/23 16:55:22.29
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/09/23 16:55:22.306
STEP: Waiting for Pod1 and Pod2 to be deleted 05/09/23 16:55:22.311
May  9 16:55:28.667: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May  9 16:55:47.678: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/09/23 16:55:47.694
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 16:55:47.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-8736" for this suite. 05/09/23 16:55:47.705
------------------------------
â€¢ [SLOW TEST] [87.972 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:54:19.742
    May  9 16:54:19.742: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename taint-multiple-pods 05/09/23 16:54:19.743
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:54:19.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:54:19.764
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    May  9 16:54:19.768: INFO: Waiting up to 1m0s for all nodes to be ready
    May  9 16:55:19.816: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    May  9 16:55:19.832: INFO: Starting informer...
    STEP: Starting pods... 05/09/23 16:55:19.832
    May  9 16:55:20.060: INFO: Pod1 is running on nodepool-8cc7f47e-9b0c-4801-88-node-7ad816. Tainting Node
    May  9 16:55:20.273: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8736" to be "running"
    May  9 16:55:20.279: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.559394ms
    May  9 16:55:22.286: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012432794s
    May  9 16:55:22.286: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    May  9 16:55:22.286: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8736" to be "running"
    May  9 16:55:22.290: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.360022ms
    May  9 16:55:22.290: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    May  9 16:55:22.290: INFO: Pod2 is running on nodepool-8cc7f47e-9b0c-4801-88-node-7ad816. Tainting Node
    STEP: Trying to apply a taint on the Node 05/09/23 16:55:22.29
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/09/23 16:55:22.306
    STEP: Waiting for Pod1 and Pod2 to be deleted 05/09/23 16:55:22.311
    May  9 16:55:28.667: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    May  9 16:55:47.678: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/09/23 16:55:47.694
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 16:55:47.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-8736" for this suite. 05/09/23 16:55:47.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:55:47.714
May  9 16:55:47.714: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename container-lifecycle-hook 05/09/23 16:55:47.715
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:55:47.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:55:47.737
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 05/09/23 16:55:47.747
May  9 16:55:47.759: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1465" to be "running and ready"
May  9 16:55:47.766: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.085355ms
May  9 16:55:47.766: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  9 16:55:49.772: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013105001s
May  9 16:55:49.772: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  9 16:55:49.772: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 05/09/23 16:55:49.778
May  9 16:55:49.784: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1465" to be "running and ready"
May  9 16:55:49.788: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.696716ms
May  9 16:55:49.788: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May  9 16:55:51.795: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010132119s
May  9 16:55:51.795: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
May  9 16:55:51.795: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 05/09/23 16:55:51.8
STEP: delete the pod with lifecycle hook 05/09/23 16:55:51.854
May  9 16:55:51.864: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  9 16:55:51.870: INFO: Pod pod-with-poststart-exec-hook still exists
May  9 16:55:53.871: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  9 16:55:53.877: INFO: Pod pod-with-poststart-exec-hook still exists
May  9 16:55:55.870: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  9 16:55:55.898: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
May  9 16:55:55.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1465" for this suite. 05/09/23 16:55:55.904
------------------------------
â€¢ [SLOW TEST] [8.201 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:55:47.714
    May  9 16:55:47.714: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/09/23 16:55:47.715
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:55:47.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:55:47.737
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 05/09/23 16:55:47.747
    May  9 16:55:47.759: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1465" to be "running and ready"
    May  9 16:55:47.766: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.085355ms
    May  9 16:55:47.766: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:55:49.772: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013105001s
    May  9 16:55:49.772: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  9 16:55:49.772: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 05/09/23 16:55:49.778
    May  9 16:55:49.784: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1465" to be "running and ready"
    May  9 16:55:49.788: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.696716ms
    May  9 16:55:49.788: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    May  9 16:55:51.795: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010132119s
    May  9 16:55:51.795: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    May  9 16:55:51.795: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 05/09/23 16:55:51.8
    STEP: delete the pod with lifecycle hook 05/09/23 16:55:51.854
    May  9 16:55:51.864: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May  9 16:55:51.870: INFO: Pod pod-with-poststart-exec-hook still exists
    May  9 16:55:53.871: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May  9 16:55:53.877: INFO: Pod pod-with-poststart-exec-hook still exists
    May  9 16:55:55.870: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May  9 16:55:55.898: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    May  9 16:55:55.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1465" for this suite. 05/09/23 16:55:55.904
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:55:55.915
May  9 16:55:55.915: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 16:55:55.916
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:55:55.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:55:55.938
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-343b7c6c-27d6-4c2c-b3e8-224836295322 05/09/23 16:55:55.943
STEP: Creating secret with name secret-projected-all-test-volume-7b203ef7-9075-43bb-9a04-833058b2ca7f 05/09/23 16:55:55.949
STEP: Creating a pod to test Check all projections for projected volume plugin 05/09/23 16:55:55.956
May  9 16:55:55.972: INFO: Waiting up to 5m0s for pod "projected-volume-54997597-3910-4ca5-aae8-c54c3c920391" in namespace "projected-4484" to be "Succeeded or Failed"
May  9 16:55:55.983: INFO: Pod "projected-volume-54997597-3910-4ca5-aae8-c54c3c920391": Phase="Pending", Reason="", readiness=false. Elapsed: 11.048597ms
May  9 16:55:57.992: INFO: Pod "projected-volume-54997597-3910-4ca5-aae8-c54c3c920391": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020100466s
May  9 16:55:59.992: INFO: Pod "projected-volume-54997597-3910-4ca5-aae8-c54c3c920391": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020071628s
STEP: Saw pod success 05/09/23 16:55:59.992
May  9 16:55:59.992: INFO: Pod "projected-volume-54997597-3910-4ca5-aae8-c54c3c920391" satisfied condition "Succeeded or Failed"
May  9 16:55:59.997: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod projected-volume-54997597-3910-4ca5-aae8-c54c3c920391 container projected-all-volume-test: <nil>
STEP: delete the pod 05/09/23 16:56:00.013
May  9 16:56:00.025: INFO: Waiting for pod projected-volume-54997597-3910-4ca5-aae8-c54c3c920391 to disappear
May  9 16:56:00.030: INFO: Pod projected-volume-54997597-3910-4ca5-aae8-c54c3c920391 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
May  9 16:56:00.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4484" for this suite. 05/09/23 16:56:00.037
------------------------------
â€¢ [4.131 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:55:55.915
    May  9 16:55:55.915: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 16:55:55.916
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:55:55.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:55:55.938
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-343b7c6c-27d6-4c2c-b3e8-224836295322 05/09/23 16:55:55.943
    STEP: Creating secret with name secret-projected-all-test-volume-7b203ef7-9075-43bb-9a04-833058b2ca7f 05/09/23 16:55:55.949
    STEP: Creating a pod to test Check all projections for projected volume plugin 05/09/23 16:55:55.956
    May  9 16:55:55.972: INFO: Waiting up to 5m0s for pod "projected-volume-54997597-3910-4ca5-aae8-c54c3c920391" in namespace "projected-4484" to be "Succeeded or Failed"
    May  9 16:55:55.983: INFO: Pod "projected-volume-54997597-3910-4ca5-aae8-c54c3c920391": Phase="Pending", Reason="", readiness=false. Elapsed: 11.048597ms
    May  9 16:55:57.992: INFO: Pod "projected-volume-54997597-3910-4ca5-aae8-c54c3c920391": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020100466s
    May  9 16:55:59.992: INFO: Pod "projected-volume-54997597-3910-4ca5-aae8-c54c3c920391": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020071628s
    STEP: Saw pod success 05/09/23 16:55:59.992
    May  9 16:55:59.992: INFO: Pod "projected-volume-54997597-3910-4ca5-aae8-c54c3c920391" satisfied condition "Succeeded or Failed"
    May  9 16:55:59.997: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod projected-volume-54997597-3910-4ca5-aae8-c54c3c920391 container projected-all-volume-test: <nil>
    STEP: delete the pod 05/09/23 16:56:00.013
    May  9 16:56:00.025: INFO: Waiting for pod projected-volume-54997597-3910-4ca5-aae8-c54c3c920391 to disappear
    May  9 16:56:00.030: INFO: Pod projected-volume-54997597-3910-4ca5-aae8-c54c3c920391 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    May  9 16:56:00.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4484" for this suite. 05/09/23 16:56:00.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:56:00.048
May  9 16:56:00.048: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename dns 05/09/23 16:56:00.048
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:56:00.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:56:00.07
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 05/09/23 16:56:00.074
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6928.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6928.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local;sleep 1; done
 05/09/23 16:56:00.08
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6928.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6928.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local;sleep 1; done
 05/09/23 16:56:00.08
STEP: creating a pod to probe DNS 05/09/23 16:56:00.081
STEP: submitting the pod to kubernetes 05/09/23 16:56:00.081
May  9 16:56:00.092: INFO: Waiting up to 15m0s for pod "dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9" in namespace "dns-6928" to be "running"
May  9 16:56:00.110: INFO: Pod "dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007073ms
May  9 16:56:02.117: INFO: Pod "dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024183996s
May  9 16:56:04.118: INFO: Pod "dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9": Phase="Running", Reason="", readiness=true. Elapsed: 4.025370032s
May  9 16:56:04.118: INFO: Pod "dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9" satisfied condition "running"
STEP: retrieving the pod 05/09/23 16:56:04.118
STEP: looking for the results for each expected name from probers 05/09/23 16:56:04.125
May  9 16:56:04.137: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:04.145: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:04.152: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:04.162: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:04.172: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:04.183: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:04.192: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:04.198: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:04.198: INFO: Lookups using dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local]

May  9 16:56:09.208: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:09.215: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:09.278: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:09.285: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:09.292: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:09.326: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:09.335: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:09.344: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:09.344: INFO: Lookups using dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local]

May  9 16:56:14.207: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:14.212: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:14.222: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:14.233: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:14.242: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:14.248: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:14.255: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:14.263: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:14.263: INFO: Lookups using dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local]

May  9 16:56:19.212: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:19.223: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:19.233: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:19.242: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:19.248: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:19.254: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:19.264: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:19.272: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:19.272: INFO: Lookups using dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local]

May  9 16:56:24.209: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:24.216: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:24.223: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:24.232: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:24.242: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:24.255: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:24.264: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:24.273: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:24.273: INFO: Lookups using dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local]

May  9 16:56:29.206: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:29.212: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:29.219: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:29.227: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:29.233: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:29.244: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:29.252: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:29.258: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
May  9 16:56:29.258: INFO: Lookups using dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local]

May  9 16:56:34.253: INFO: DNS probes using dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9 succeeded

STEP: deleting the pod 05/09/23 16:56:34.253
STEP: deleting the test headless service 05/09/23 16:56:34.274
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May  9 16:56:34.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6928" for this suite. 05/09/23 16:56:34.307
------------------------------
â€¢ [SLOW TEST] [34.271 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:56:00.048
    May  9 16:56:00.048: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename dns 05/09/23 16:56:00.048
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:56:00.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:56:00.07
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 05/09/23 16:56:00.074
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6928.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6928.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local;sleep 1; done
     05/09/23 16:56:00.08
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6928.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6928.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local;sleep 1; done
     05/09/23 16:56:00.08
    STEP: creating a pod to probe DNS 05/09/23 16:56:00.081
    STEP: submitting the pod to kubernetes 05/09/23 16:56:00.081
    May  9 16:56:00.092: INFO: Waiting up to 15m0s for pod "dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9" in namespace "dns-6928" to be "running"
    May  9 16:56:00.110: INFO: Pod "dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007073ms
    May  9 16:56:02.117: INFO: Pod "dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024183996s
    May  9 16:56:04.118: INFO: Pod "dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9": Phase="Running", Reason="", readiness=true. Elapsed: 4.025370032s
    May  9 16:56:04.118: INFO: Pod "dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9" satisfied condition "running"
    STEP: retrieving the pod 05/09/23 16:56:04.118
    STEP: looking for the results for each expected name from probers 05/09/23 16:56:04.125
    May  9 16:56:04.137: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:04.145: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:04.152: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:04.162: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:04.172: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:04.183: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:04.192: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:04.198: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:04.198: INFO: Lookups using dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local]

    May  9 16:56:09.208: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:09.215: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:09.278: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:09.285: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:09.292: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:09.326: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:09.335: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:09.344: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:09.344: INFO: Lookups using dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local]

    May  9 16:56:14.207: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:14.212: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:14.222: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:14.233: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:14.242: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:14.248: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:14.255: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:14.263: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:14.263: INFO: Lookups using dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local]

    May  9 16:56:19.212: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:19.223: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:19.233: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:19.242: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:19.248: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:19.254: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:19.264: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:19.272: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:19.272: INFO: Lookups using dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local]

    May  9 16:56:24.209: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:24.216: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:24.223: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:24.232: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:24.242: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:24.255: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:24.264: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:24.273: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:24.273: INFO: Lookups using dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local]

    May  9 16:56:29.206: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:29.212: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:29.219: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:29.227: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:29.233: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:29.244: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:29.252: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:29.258: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local from pod dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9: the server could not find the requested resource (get pods dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9)
    May  9 16:56:29.258: INFO: Lookups using dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6928.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6928.svc.cluster.local jessie_udp@dns-test-service-2.dns-6928.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6928.svc.cluster.local]

    May  9 16:56:34.253: INFO: DNS probes using dns-6928/dns-test-52c01873-5f2b-4b24-a702-995a87b1bdd9 succeeded

    STEP: deleting the pod 05/09/23 16:56:34.253
    STEP: deleting the test headless service 05/09/23 16:56:34.274
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May  9 16:56:34.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6928" for this suite. 05/09/23 16:56:34.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:56:34.319
May  9 16:56:34.319: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename daemonsets 05/09/23 16:56:34.32
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:56:34.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:56:34.34
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 05/09/23 16:56:34.377
STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 16:56:34.383
May  9 16:56:34.393: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:56:34.393: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 16:56:35.407: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:56:35.407: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
May  9 16:56:36.405: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  9 16:56:36.405: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 05/09/23 16:56:36.411
May  9 16:56:36.417: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 05/09/23 16:56:36.417
May  9 16:56:36.431: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 05/09/23 16:56:36.431
May  9 16:56:36.433: INFO: Observed &DaemonSet event: ADDED
May  9 16:56:36.433: INFO: Observed &DaemonSet event: MODIFIED
May  9 16:56:36.434: INFO: Observed &DaemonSet event: MODIFIED
May  9 16:56:36.434: INFO: Observed &DaemonSet event: MODIFIED
May  9 16:56:36.434: INFO: Observed &DaemonSet event: MODIFIED
May  9 16:56:36.434: INFO: Found daemon set daemon-set in namespace daemonsets-2854 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  9 16:56:36.434: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 05/09/23 16:56:36.434
STEP: watching for the daemon set status to be patched 05/09/23 16:56:36.442
May  9 16:56:36.445: INFO: Observed &DaemonSet event: ADDED
May  9 16:56:36.445: INFO: Observed &DaemonSet event: MODIFIED
May  9 16:56:36.445: INFO: Observed &DaemonSet event: MODIFIED
May  9 16:56:36.446: INFO: Observed &DaemonSet event: MODIFIED
May  9 16:56:36.446: INFO: Observed &DaemonSet event: MODIFIED
May  9 16:56:36.446: INFO: Observed daemon set daemon-set in namespace daemonsets-2854 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  9 16:56:36.446: INFO: Observed &DaemonSet event: MODIFIED
May  9 16:56:36.446: INFO: Found daemon set daemon-set in namespace daemonsets-2854 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
May  9 16:56:36.446: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/09/23 16:56:36.453
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2854, will wait for the garbage collector to delete the pods 05/09/23 16:56:36.453
May  9 16:56:36.518: INFO: Deleting DaemonSet.extensions daemon-set took: 8.546681ms
May  9 16:56:36.618: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.084242ms
May  9 16:56:39.324: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  9 16:56:39.324: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  9 16:56:39.328: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"318202908"},"items":null}

May  9 16:56:39.336: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"318202908"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 16:56:39.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2854" for this suite. 05/09/23 16:56:39.367
------------------------------
â€¢ [SLOW TEST] [5.057 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:56:34.319
    May  9 16:56:34.319: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename daemonsets 05/09/23 16:56:34.32
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:56:34.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:56:34.34
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 05/09/23 16:56:34.377
    STEP: Check that daemon pods launch on every node of the cluster. 05/09/23 16:56:34.383
    May  9 16:56:34.393: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:56:34.393: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 16:56:35.407: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:56:35.407: INFO: Node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 is running 0 daemon pod, expected 1
    May  9 16:56:36.405: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  9 16:56:36.405: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 05/09/23 16:56:36.411
    May  9 16:56:36.417: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 05/09/23 16:56:36.417
    May  9 16:56:36.431: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 05/09/23 16:56:36.431
    May  9 16:56:36.433: INFO: Observed &DaemonSet event: ADDED
    May  9 16:56:36.433: INFO: Observed &DaemonSet event: MODIFIED
    May  9 16:56:36.434: INFO: Observed &DaemonSet event: MODIFIED
    May  9 16:56:36.434: INFO: Observed &DaemonSet event: MODIFIED
    May  9 16:56:36.434: INFO: Observed &DaemonSet event: MODIFIED
    May  9 16:56:36.434: INFO: Found daemon set daemon-set in namespace daemonsets-2854 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  9 16:56:36.434: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 05/09/23 16:56:36.434
    STEP: watching for the daemon set status to be patched 05/09/23 16:56:36.442
    May  9 16:56:36.445: INFO: Observed &DaemonSet event: ADDED
    May  9 16:56:36.445: INFO: Observed &DaemonSet event: MODIFIED
    May  9 16:56:36.445: INFO: Observed &DaemonSet event: MODIFIED
    May  9 16:56:36.446: INFO: Observed &DaemonSet event: MODIFIED
    May  9 16:56:36.446: INFO: Observed &DaemonSet event: MODIFIED
    May  9 16:56:36.446: INFO: Observed daemon set daemon-set in namespace daemonsets-2854 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  9 16:56:36.446: INFO: Observed &DaemonSet event: MODIFIED
    May  9 16:56:36.446: INFO: Found daemon set daemon-set in namespace daemonsets-2854 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    May  9 16:56:36.446: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/09/23 16:56:36.453
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2854, will wait for the garbage collector to delete the pods 05/09/23 16:56:36.453
    May  9 16:56:36.518: INFO: Deleting DaemonSet.extensions daemon-set took: 8.546681ms
    May  9 16:56:36.618: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.084242ms
    May  9 16:56:39.324: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  9 16:56:39.324: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  9 16:56:39.328: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"318202908"},"items":null}

    May  9 16:56:39.336: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"318202908"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 16:56:39.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2854" for this suite. 05/09/23 16:56:39.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:56:39.376
May  9 16:56:39.376: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename job 05/09/23 16:56:39.377
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:56:39.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:56:39.406
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 05/09/23 16:56:39.41
STEP: Ensure pods equal to parallelism count is attached to the job 05/09/23 16:56:39.417
STEP: patching /status 05/09/23 16:56:41.424
STEP: updating /status 05/09/23 16:56:41.436
STEP: get /status 05/09/23 16:56:41.48
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May  9 16:56:41.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1108" for this suite. 05/09/23 16:56:41.494
------------------------------
â€¢ [2.131 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:56:39.376
    May  9 16:56:39.376: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename job 05/09/23 16:56:39.377
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:56:39.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:56:39.406
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 05/09/23 16:56:39.41
    STEP: Ensure pods equal to parallelism count is attached to the job 05/09/23 16:56:39.417
    STEP: patching /status 05/09/23 16:56:41.424
    STEP: updating /status 05/09/23 16:56:41.436
    STEP: get /status 05/09/23 16:56:41.48
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May  9 16:56:41.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1108" for this suite. 05/09/23 16:56:41.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:56:41.508
May  9 16:56:41.508: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename emptydir 05/09/23 16:56:41.509
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:56:41.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:56:41.539
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 05/09/23 16:56:41.544
May  9 16:56:41.555: INFO: Waiting up to 5m0s for pod "pod-d31f1948-5654-4c68-946d-859cba2dba2d" in namespace "emptydir-3825" to be "Succeeded or Failed"
May  9 16:56:41.560: INFO: Pod "pod-d31f1948-5654-4c68-946d-859cba2dba2d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066639ms
May  9 16:56:43.573: INFO: Pod "pod-d31f1948-5654-4c68-946d-859cba2dba2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017127312s
May  9 16:56:45.568: INFO: Pod "pod-d31f1948-5654-4c68-946d-859cba2dba2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012611101s
STEP: Saw pod success 05/09/23 16:56:45.568
May  9 16:56:45.568: INFO: Pod "pod-d31f1948-5654-4c68-946d-859cba2dba2d" satisfied condition "Succeeded or Failed"
May  9 16:56:45.573: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-d31f1948-5654-4c68-946d-859cba2dba2d container test-container: <nil>
STEP: delete the pod 05/09/23 16:56:45.585
May  9 16:56:45.601: INFO: Waiting for pod pod-d31f1948-5654-4c68-946d-859cba2dba2d to disappear
May  9 16:56:45.607: INFO: Pod pod-d31f1948-5654-4c68-946d-859cba2dba2d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  9 16:56:45.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3825" for this suite. 05/09/23 16:56:45.615
------------------------------
â€¢ [4.116 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:56:41.508
    May  9 16:56:41.508: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename emptydir 05/09/23 16:56:41.509
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:56:41.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:56:41.539
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 05/09/23 16:56:41.544
    May  9 16:56:41.555: INFO: Waiting up to 5m0s for pod "pod-d31f1948-5654-4c68-946d-859cba2dba2d" in namespace "emptydir-3825" to be "Succeeded or Failed"
    May  9 16:56:41.560: INFO: Pod "pod-d31f1948-5654-4c68-946d-859cba2dba2d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066639ms
    May  9 16:56:43.573: INFO: Pod "pod-d31f1948-5654-4c68-946d-859cba2dba2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017127312s
    May  9 16:56:45.568: INFO: Pod "pod-d31f1948-5654-4c68-946d-859cba2dba2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012611101s
    STEP: Saw pod success 05/09/23 16:56:45.568
    May  9 16:56:45.568: INFO: Pod "pod-d31f1948-5654-4c68-946d-859cba2dba2d" satisfied condition "Succeeded or Failed"
    May  9 16:56:45.573: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-d31f1948-5654-4c68-946d-859cba2dba2d container test-container: <nil>
    STEP: delete the pod 05/09/23 16:56:45.585
    May  9 16:56:45.601: INFO: Waiting for pod pod-d31f1948-5654-4c68-946d-859cba2dba2d to disappear
    May  9 16:56:45.607: INFO: Pod pod-d31f1948-5654-4c68-946d-859cba2dba2d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  9 16:56:45.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3825" for this suite. 05/09/23 16:56:45.615
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:56:45.625
May  9 16:56:45.625: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename services 05/09/23 16:56:45.626
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:56:45.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:56:45.651
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-942 05/09/23 16:56:45.656
STEP: creating service affinity-nodeport in namespace services-942 05/09/23 16:56:45.656
STEP: creating replication controller affinity-nodeport in namespace services-942 05/09/23 16:56:45.68
I0509 16:56:45.688733      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-942, replica count: 3
I0509 16:56:48.739947      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 16:56:48.755: INFO: Creating new exec pod
May  9 16:56:48.761: INFO: Waiting up to 5m0s for pod "execpod-affinitygvq72" in namespace "services-942" to be "running"
May  9 16:56:48.767: INFO: Pod "execpod-affinitygvq72": Phase="Pending", Reason="", readiness=false. Elapsed: 5.612705ms
May  9 16:56:50.772: INFO: Pod "execpod-affinitygvq72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010701948s
May  9 16:56:52.775: INFO: Pod "execpod-affinitygvq72": Phase="Running", Reason="", readiness=true. Elapsed: 4.013628701s
May  9 16:56:52.775: INFO: Pod "execpod-affinitygvq72" satisfied condition "running"
May  9 16:56:53.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-942 exec execpod-affinitygvq72 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
May  9 16:56:53.984: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May  9 16:56:53.984: INFO: stdout: ""
May  9 16:56:53.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-942 exec execpod-affinitygvq72 -- /bin/sh -x -c nc -v -z -w 2 10.3.159.186 80'
May  9 16:56:54.219: INFO: stderr: "+ nc -v -z -w 2 10.3.159.186 80\nConnection to 10.3.159.186 80 port [tcp/http] succeeded!\n"
May  9 16:56:54.219: INFO: stdout: ""
May  9 16:56:54.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-942 exec execpod-affinitygvq72 -- /bin/sh -x -c nc -v -z -w 2 51.68.93.170 30712'
May  9 16:56:54.435: INFO: stderr: "+ nc -v -z -w 2 51.68.93.170 30712\nConnection to 51.68.93.170 30712 port [tcp/*] succeeded!\n"
May  9 16:56:54.435: INFO: stdout: ""
May  9 16:56:54.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-942 exec execpod-affinitygvq72 -- /bin/sh -x -c nc -v -z -w 2 51.68.91.222 30712'
May  9 16:56:54.667: INFO: stderr: "+ nc -v -z -w 2 51.68.91.222 30712\nConnection to 51.68.91.222 30712 port [tcp/*] succeeded!\n"
May  9 16:56:54.667: INFO: stdout: ""
May  9 16:56:54.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-942 exec execpod-affinitygvq72 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://51.68.91.222:30712/ ; done'
May  9 16:56:54.960: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n"
May  9 16:56:54.960: INFO: stdout: "\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp"
May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
May  9 16:56:54.960: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-942, will wait for the garbage collector to delete the pods 05/09/23 16:56:54.976
May  9 16:56:55.043: INFO: Deleting ReplicationController affinity-nodeport took: 9.655253ms
May  9 16:56:55.143: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.362813ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  9 16:56:57.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-942" for this suite. 05/09/23 16:56:57.377
------------------------------
â€¢ [SLOW TEST] [11.761 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:56:45.625
    May  9 16:56:45.625: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename services 05/09/23 16:56:45.626
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:56:45.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:56:45.651
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-942 05/09/23 16:56:45.656
    STEP: creating service affinity-nodeport in namespace services-942 05/09/23 16:56:45.656
    STEP: creating replication controller affinity-nodeport in namespace services-942 05/09/23 16:56:45.68
    I0509 16:56:45.688733      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-942, replica count: 3
    I0509 16:56:48.739947      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 16:56:48.755: INFO: Creating new exec pod
    May  9 16:56:48.761: INFO: Waiting up to 5m0s for pod "execpod-affinitygvq72" in namespace "services-942" to be "running"
    May  9 16:56:48.767: INFO: Pod "execpod-affinitygvq72": Phase="Pending", Reason="", readiness=false. Elapsed: 5.612705ms
    May  9 16:56:50.772: INFO: Pod "execpod-affinitygvq72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010701948s
    May  9 16:56:52.775: INFO: Pod "execpod-affinitygvq72": Phase="Running", Reason="", readiness=true. Elapsed: 4.013628701s
    May  9 16:56:52.775: INFO: Pod "execpod-affinitygvq72" satisfied condition "running"
    May  9 16:56:53.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-942 exec execpod-affinitygvq72 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    May  9 16:56:53.984: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    May  9 16:56:53.984: INFO: stdout: ""
    May  9 16:56:53.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-942 exec execpod-affinitygvq72 -- /bin/sh -x -c nc -v -z -w 2 10.3.159.186 80'
    May  9 16:56:54.219: INFO: stderr: "+ nc -v -z -w 2 10.3.159.186 80\nConnection to 10.3.159.186 80 port [tcp/http] succeeded!\n"
    May  9 16:56:54.219: INFO: stdout: ""
    May  9 16:56:54.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-942 exec execpod-affinitygvq72 -- /bin/sh -x -c nc -v -z -w 2 51.68.93.170 30712'
    May  9 16:56:54.435: INFO: stderr: "+ nc -v -z -w 2 51.68.93.170 30712\nConnection to 51.68.93.170 30712 port [tcp/*] succeeded!\n"
    May  9 16:56:54.435: INFO: stdout: ""
    May  9 16:56:54.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-942 exec execpod-affinitygvq72 -- /bin/sh -x -c nc -v -z -w 2 51.68.91.222 30712'
    May  9 16:56:54.667: INFO: stderr: "+ nc -v -z -w 2 51.68.91.222 30712\nConnection to 51.68.91.222 30712 port [tcp/*] succeeded!\n"
    May  9 16:56:54.667: INFO: stdout: ""
    May  9 16:56:54.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-942 exec execpod-affinitygvq72 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://51.68.91.222:30712/ ; done'
    May  9 16:56:54.960: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n+ echo\n+ curl -q -s --connect-timeout 2 http://51.68.91.222:30712/\n"
    May  9 16:56:54.960: INFO: stdout: "\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp\naffinity-nodeport-pcnwp"
    May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
    May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
    May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
    May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
    May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
    May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
    May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
    May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
    May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
    May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
    May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
    May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
    May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
    May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
    May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
    May  9 16:56:54.960: INFO: Received response from host: affinity-nodeport-pcnwp
    May  9 16:56:54.960: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-942, will wait for the garbage collector to delete the pods 05/09/23 16:56:54.976
    May  9 16:56:55.043: INFO: Deleting ReplicationController affinity-nodeport took: 9.655253ms
    May  9 16:56:55.143: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.362813ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  9 16:56:57.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-942" for this suite. 05/09/23 16:56:57.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:56:57.386
May  9 16:56:57.386: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 16:56:57.387
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:56:57.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:56:57.415
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 05/09/23 16:56:57.419
May  9 16:56:57.420: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: rename a version 05/09/23 16:57:02.012
STEP: check the new version name is served 05/09/23 16:57:02.131
STEP: check the old version name is removed 05/09/23 16:57:04.005
STEP: check the other version is not changed 05/09/23 16:57:04.878
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 16:57:08.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1542" for this suite. 05/09/23 16:57:08.892
------------------------------
â€¢ [SLOW TEST] [11.515 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:56:57.386
    May  9 16:56:57.386: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename crd-publish-openapi 05/09/23 16:56:57.387
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:56:57.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:56:57.415
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 05/09/23 16:56:57.419
    May  9 16:56:57.420: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: rename a version 05/09/23 16:57:02.012
    STEP: check the new version name is served 05/09/23 16:57:02.131
    STEP: check the old version name is removed 05/09/23 16:57:04.005
    STEP: check the other version is not changed 05/09/23 16:57:04.878
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 16:57:08.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1542" for this suite. 05/09/23 16:57:08.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:57:08.902
May  9 16:57:08.902: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename statefulset 05/09/23 16:57:08.904
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:57:08.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:57:08.924
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9846 05/09/23 16:57:08.929
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 05/09/23 16:57:08.936
May  9 16:57:08.949: INFO: Found 0 stateful pods, waiting for 3
May  9 16:57:18.957: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  9 16:57:18.957: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  9 16:57:18.957: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May  9 16:57:19.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-9846 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 16:57:19.230: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 16:57:19.230: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 16:57:19.230: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 05/09/23 16:57:29.417
May  9 16:57:29.439: INFO: Updating stateful set ss2
STEP: Creating a new revision 05/09/23 16:57:29.44
STEP: Updating Pods in reverse ordinal order 05/09/23 16:57:39.462
May  9 16:57:39.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-9846 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  9 16:57:39.723: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  9 16:57:39.723: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  9 16:57:39.723: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 05/09/23 16:57:49.891
May  9 16:57:49.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-9846 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  9 16:57:50.123: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  9 16:57:50.123: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  9 16:57:50.123: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  9 16:58:00.200: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 05/09/23 16:58:10.231
May  9 16:58:10.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-9846 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  9 16:58:10.441: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  9 16:58:10.441: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  9 16:58:10.441: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May  9 16:58:20.480: INFO: Deleting all statefulset in ns statefulset-9846
May  9 16:58:20.484: INFO: Scaling statefulset ss2 to 0
May  9 16:58:30.511: INFO: Waiting for statefulset status.replicas updated to 0
May  9 16:58:30.516: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May  9 16:58:30.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9846" for this suite. 05/09/23 16:58:30.577
------------------------------
â€¢ [SLOW TEST] [81.684 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:57:08.902
    May  9 16:57:08.902: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename statefulset 05/09/23 16:57:08.904
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:57:08.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:57:08.924
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9846 05/09/23 16:57:08.929
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 05/09/23 16:57:08.936
    May  9 16:57:08.949: INFO: Found 0 stateful pods, waiting for 3
    May  9 16:57:18.957: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May  9 16:57:18.957: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May  9 16:57:18.957: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    May  9 16:57:19.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-9846 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 16:57:19.230: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 16:57:19.230: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 16:57:19.230: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 05/09/23 16:57:29.417
    May  9 16:57:29.439: INFO: Updating stateful set ss2
    STEP: Creating a new revision 05/09/23 16:57:29.44
    STEP: Updating Pods in reverse ordinal order 05/09/23 16:57:39.462
    May  9 16:57:39.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-9846 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  9 16:57:39.723: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  9 16:57:39.723: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  9 16:57:39.723: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 05/09/23 16:57:49.891
    May  9 16:57:49.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-9846 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  9 16:57:50.123: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  9 16:57:50.123: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  9 16:57:50.123: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  9 16:58:00.200: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 05/09/23 16:58:10.231
    May  9 16:58:10.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=statefulset-9846 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  9 16:58:10.441: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  9 16:58:10.441: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  9 16:58:10.441: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May  9 16:58:20.480: INFO: Deleting all statefulset in ns statefulset-9846
    May  9 16:58:20.484: INFO: Scaling statefulset ss2 to 0
    May  9 16:58:30.511: INFO: Waiting for statefulset status.replicas updated to 0
    May  9 16:58:30.516: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May  9 16:58:30.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9846" for this suite. 05/09/23 16:58:30.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:58:30.587
May  9 16:58:30.589: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename gc 05/09/23 16:58:30.589
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:58:30.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:58:30.615
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 05/09/23 16:58:30.619
STEP: Wait for the Deployment to create new ReplicaSet 05/09/23 16:58:30.626
STEP: delete the deployment 05/09/23 16:58:30.751
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 05/09/23 16:58:30.762
STEP: Gathering metrics 05/09/23 16:58:31.297
W0509 16:58:31.310241      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  9 16:58:31.310: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May  9 16:58:31.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6526" for this suite. 05/09/23 16:58:31.316
------------------------------
â€¢ [0.739 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:58:30.587
    May  9 16:58:30.589: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename gc 05/09/23 16:58:30.589
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:58:30.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:58:30.615
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 05/09/23 16:58:30.619
    STEP: Wait for the Deployment to create new ReplicaSet 05/09/23 16:58:30.626
    STEP: delete the deployment 05/09/23 16:58:30.751
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 05/09/23 16:58:30.762
    STEP: Gathering metrics 05/09/23 16:58:31.297
    W0509 16:58:31.310241      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May  9 16:58:31.310: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May  9 16:58:31.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6526" for this suite. 05/09/23 16:58:31.316
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:58:31.327
May  9 16:58:31.327: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename gc 05/09/23 16:58:31.328
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:58:31.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:58:31.351
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 05/09/23 16:58:31.363
STEP: delete the rc 05/09/23 16:58:36.462
STEP: wait for the rc to be deleted 05/09/23 16:58:36.562
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 05/09/23 16:58:41.567
STEP: Gathering metrics 05/09/23 16:59:11.596
W0509 16:59:11.614316      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  9 16:59:11.614: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May  9 16:59:11.614: INFO: Deleting pod "simpletest.rc-2cd25" in namespace "gc-4937"
May  9 16:59:11.632: INFO: Deleting pod "simpletest.rc-2jwng" in namespace "gc-4937"
May  9 16:59:11.652: INFO: Deleting pod "simpletest.rc-2n2rh" in namespace "gc-4937"
May  9 16:59:11.665: INFO: Deleting pod "simpletest.rc-2sw7q" in namespace "gc-4937"
May  9 16:59:11.682: INFO: Deleting pod "simpletest.rc-2v9l6" in namespace "gc-4937"
May  9 16:59:11.702: INFO: Deleting pod "simpletest.rc-2zlrp" in namespace "gc-4937"
May  9 16:59:11.719: INFO: Deleting pod "simpletest.rc-45fxd" in namespace "gc-4937"
May  9 16:59:11.735: INFO: Deleting pod "simpletest.rc-4nzkg" in namespace "gc-4937"
May  9 16:59:11.752: INFO: Deleting pod "simpletest.rc-4svhj" in namespace "gc-4937"
May  9 16:59:11.775: INFO: Deleting pod "simpletest.rc-4tbz2" in namespace "gc-4937"
May  9 16:59:11.795: INFO: Deleting pod "simpletest.rc-4z7z8" in namespace "gc-4937"
May  9 16:59:11.817: INFO: Deleting pod "simpletest.rc-59hj8" in namespace "gc-4937"
May  9 16:59:11.832: INFO: Deleting pod "simpletest.rc-5fmht" in namespace "gc-4937"
May  9 16:59:11.852: INFO: Deleting pod "simpletest.rc-5frvw" in namespace "gc-4937"
May  9 16:59:11.871: INFO: Deleting pod "simpletest.rc-5m455" in namespace "gc-4937"
May  9 16:59:11.886: INFO: Deleting pod "simpletest.rc-5rt5d" in namespace "gc-4937"
May  9 16:59:11.902: INFO: Deleting pod "simpletest.rc-5tvjt" in namespace "gc-4937"
May  9 16:59:11.916: INFO: Deleting pod "simpletest.rc-66vtg" in namespace "gc-4937"
May  9 16:59:11.930: INFO: Deleting pod "simpletest.rc-67h2t" in namespace "gc-4937"
May  9 16:59:11.949: INFO: Deleting pod "simpletest.rc-6b8nv" in namespace "gc-4937"
May  9 16:59:11.981: INFO: Deleting pod "simpletest.rc-6l2m8" in namespace "gc-4937"
May  9 16:59:12.005: INFO: Deleting pod "simpletest.rc-6rmlf" in namespace "gc-4937"
May  9 16:59:12.047: INFO: Deleting pod "simpletest.rc-7fz7p" in namespace "gc-4937"
May  9 16:59:12.091: INFO: Deleting pod "simpletest.rc-7q45f" in namespace "gc-4937"
May  9 16:59:12.110: INFO: Deleting pod "simpletest.rc-7xrtd" in namespace "gc-4937"
May  9 16:59:12.129: INFO: Deleting pod "simpletest.rc-8bn5x" in namespace "gc-4937"
May  9 16:59:12.156: INFO: Deleting pod "simpletest.rc-8crhf" in namespace "gc-4937"
May  9 16:59:12.181: INFO: Deleting pod "simpletest.rc-8cscw" in namespace "gc-4937"
May  9 16:59:12.195: INFO: Deleting pod "simpletest.rc-8vwgt" in namespace "gc-4937"
May  9 16:59:12.214: INFO: Deleting pod "simpletest.rc-927sx" in namespace "gc-4937"
May  9 16:59:12.231: INFO: Deleting pod "simpletest.rc-9j4h2" in namespace "gc-4937"
May  9 16:59:12.247: INFO: Deleting pod "simpletest.rc-9mn9k" in namespace "gc-4937"
May  9 16:59:12.274: INFO: Deleting pod "simpletest.rc-9vjgz" in namespace "gc-4937"
May  9 16:59:12.289: INFO: Deleting pod "simpletest.rc-b2zkf" in namespace "gc-4937"
May  9 16:59:12.318: INFO: Deleting pod "simpletest.rc-b66ll" in namespace "gc-4937"
May  9 16:59:12.367: INFO: Deleting pod "simpletest.rc-b7sqb" in namespace "gc-4937"
May  9 16:59:12.384: INFO: Deleting pod "simpletest.rc-b8zdh" in namespace "gc-4937"
May  9 16:59:12.400: INFO: Deleting pod "simpletest.rc-bh29k" in namespace "gc-4937"
May  9 16:59:12.468: INFO: Deleting pod "simpletest.rc-c7jnb" in namespace "gc-4937"
May  9 16:59:12.572: INFO: Deleting pod "simpletest.rc-cvwvf" in namespace "gc-4937"
May  9 16:59:12.682: INFO: Deleting pod "simpletest.rc-cxlrk" in namespace "gc-4937"
May  9 16:59:12.774: INFO: Deleting pod "simpletest.rc-d48ll" in namespace "gc-4937"
May  9 16:59:12.866: INFO: Deleting pod "simpletest.rc-f2wrs" in namespace "gc-4937"
May  9 16:59:12.884: INFO: Deleting pod "simpletest.rc-f7cxg" in namespace "gc-4937"
May  9 16:59:12.899: INFO: Deleting pod "simpletest.rc-fbzl9" in namespace "gc-4937"
May  9 16:59:12.913: INFO: Deleting pod "simpletest.rc-fdfsv" in namespace "gc-4937"
May  9 16:59:12.979: INFO: Deleting pod "simpletest.rc-gtrxh" in namespace "gc-4937"
May  9 16:59:12.994: INFO: Deleting pod "simpletest.rc-gv79g" in namespace "gc-4937"
May  9 16:59:13.017: INFO: Deleting pod "simpletest.rc-h5x47" in namespace "gc-4937"
May  9 16:59:13.035: INFO: Deleting pod "simpletest.rc-hc4fx" in namespace "gc-4937"
May  9 16:59:13.049: INFO: Deleting pod "simpletest.rc-hthr6" in namespace "gc-4937"
May  9 16:59:13.073: INFO: Deleting pod "simpletest.rc-k85m5" in namespace "gc-4937"
May  9 16:59:13.266: INFO: Deleting pod "simpletest.rc-kfm6m" in namespace "gc-4937"
May  9 16:59:13.866: INFO: Deleting pod "simpletest.rc-klvhl" in namespace "gc-4937"
May  9 16:59:13.883: INFO: Deleting pod "simpletest.rc-l7gjf" in namespace "gc-4937"
May  9 16:59:13.962: INFO: Deleting pod "simpletest.rc-l8xtc" in namespace "gc-4937"
May  9 16:59:13.975: INFO: Deleting pod "simpletest.rc-lvpjm" in namespace "gc-4937"
May  9 16:59:13.988: INFO: Deleting pod "simpletest.rc-m8lhf" in namespace "gc-4937"
May  9 16:59:14.000: INFO: Deleting pod "simpletest.rc-md8p8" in namespace "gc-4937"
May  9 16:59:14.016: INFO: Deleting pod "simpletest.rc-mfn49" in namespace "gc-4937"
May  9 16:59:14.043: INFO: Deleting pod "simpletest.rc-mntct" in namespace "gc-4937"
May  9 16:59:14.059: INFO: Deleting pod "simpletest.rc-mrf7t" in namespace "gc-4937"
May  9 16:59:14.074: INFO: Deleting pod "simpletest.rc-mzkhr" in namespace "gc-4937"
May  9 16:59:14.103: INFO: Deleting pod "simpletest.rc-ngjb8" in namespace "gc-4937"
May  9 16:59:14.117: INFO: Deleting pod "simpletest.rc-nwr5q" in namespace "gc-4937"
May  9 16:59:14.131: INFO: Deleting pod "simpletest.rc-p56zh" in namespace "gc-4937"
May  9 16:59:14.153: INFO: Deleting pod "simpletest.rc-p6kpn" in namespace "gc-4937"
May  9 16:59:14.170: INFO: Deleting pod "simpletest.rc-p8nsg" in namespace "gc-4937"
May  9 16:59:14.182: INFO: Deleting pod "simpletest.rc-p8x9f" in namespace "gc-4937"
May  9 16:59:14.199: INFO: Deleting pod "simpletest.rc-p9sxt" in namespace "gc-4937"
May  9 16:59:14.214: INFO: Deleting pod "simpletest.rc-pcd2l" in namespace "gc-4937"
May  9 16:59:14.233: INFO: Deleting pod "simpletest.rc-pczz6" in namespace "gc-4937"
May  9 16:59:14.254: INFO: Deleting pod "simpletest.rc-pj2ws" in namespace "gc-4937"
May  9 16:59:14.272: INFO: Deleting pod "simpletest.rc-q4sk5" in namespace "gc-4937"
May  9 16:59:14.290: INFO: Deleting pod "simpletest.rc-qbbtp" in namespace "gc-4937"
May  9 16:59:14.310: INFO: Deleting pod "simpletest.rc-qfdhv" in namespace "gc-4937"
May  9 16:59:14.326: INFO: Deleting pod "simpletest.rc-qnl2c" in namespace "gc-4937"
May  9 16:59:14.342: INFO: Deleting pod "simpletest.rc-qtddp" in namespace "gc-4937"
May  9 16:59:14.370: INFO: Deleting pod "simpletest.rc-rrrnw" in namespace "gc-4937"
May  9 16:59:14.396: INFO: Deleting pod "simpletest.rc-rz6x6" in namespace "gc-4937"
May  9 16:59:14.411: INFO: Deleting pod "simpletest.rc-spppw" in namespace "gc-4937"
May  9 16:59:14.432: INFO: Deleting pod "simpletest.rc-sx5qq" in namespace "gc-4937"
May  9 16:59:14.452: INFO: Deleting pod "simpletest.rc-szrzl" in namespace "gc-4937"
May  9 16:59:14.475: INFO: Deleting pod "simpletest.rc-th5gx" in namespace "gc-4937"
May  9 16:59:14.493: INFO: Deleting pod "simpletest.rc-tmsgk" in namespace "gc-4937"
May  9 16:59:14.511: INFO: Deleting pod "simpletest.rc-tr7ss" in namespace "gc-4937"
May  9 16:59:14.531: INFO: Deleting pod "simpletest.rc-ttjxf" in namespace "gc-4937"
May  9 16:59:14.544: INFO: Deleting pod "simpletest.rc-tvplp" in namespace "gc-4937"
May  9 16:59:14.558: INFO: Deleting pod "simpletest.rc-txd79" in namespace "gc-4937"
May  9 16:59:14.578: INFO: Deleting pod "simpletest.rc-v8bbm" in namespace "gc-4937"
May  9 16:59:14.600: INFO: Deleting pod "simpletest.rc-vlsl5" in namespace "gc-4937"
May  9 16:59:14.620: INFO: Deleting pod "simpletest.rc-vpsh6" in namespace "gc-4937"
May  9 16:59:14.642: INFO: Deleting pod "simpletest.rc-w6sbz" in namespace "gc-4937"
May  9 16:59:14.665: INFO: Deleting pod "simpletest.rc-wp57g" in namespace "gc-4937"
May  9 16:59:14.700: INFO: Deleting pod "simpletest.rc-x5gj7" in namespace "gc-4937"
May  9 16:59:14.720: INFO: Deleting pod "simpletest.rc-xl86x" in namespace "gc-4937"
May  9 16:59:14.743: INFO: Deleting pod "simpletest.rc-zfkfl" in namespace "gc-4937"
May  9 16:59:14.763: INFO: Deleting pod "simpletest.rc-zh6tc" in namespace "gc-4937"
May  9 16:59:14.806: INFO: Deleting pod "simpletest.rc-zlm8p" in namespace "gc-4937"
May  9 16:59:14.822: INFO: Deleting pod "simpletest.rc-zqvk6" in namespace "gc-4937"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May  9 16:59:14.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4937" for this suite. 05/09/23 16:59:14.882
------------------------------
â€¢ [SLOW TEST] [43.568 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:58:31.327
    May  9 16:58:31.327: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename gc 05/09/23 16:58:31.328
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:58:31.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:58:31.351
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 05/09/23 16:58:31.363
    STEP: delete the rc 05/09/23 16:58:36.462
    STEP: wait for the rc to be deleted 05/09/23 16:58:36.562
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 05/09/23 16:58:41.567
    STEP: Gathering metrics 05/09/23 16:59:11.596
    W0509 16:59:11.614316      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May  9 16:59:11.614: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    May  9 16:59:11.614: INFO: Deleting pod "simpletest.rc-2cd25" in namespace "gc-4937"
    May  9 16:59:11.632: INFO: Deleting pod "simpletest.rc-2jwng" in namespace "gc-4937"
    May  9 16:59:11.652: INFO: Deleting pod "simpletest.rc-2n2rh" in namespace "gc-4937"
    May  9 16:59:11.665: INFO: Deleting pod "simpletest.rc-2sw7q" in namespace "gc-4937"
    May  9 16:59:11.682: INFO: Deleting pod "simpletest.rc-2v9l6" in namespace "gc-4937"
    May  9 16:59:11.702: INFO: Deleting pod "simpletest.rc-2zlrp" in namespace "gc-4937"
    May  9 16:59:11.719: INFO: Deleting pod "simpletest.rc-45fxd" in namespace "gc-4937"
    May  9 16:59:11.735: INFO: Deleting pod "simpletest.rc-4nzkg" in namespace "gc-4937"
    May  9 16:59:11.752: INFO: Deleting pod "simpletest.rc-4svhj" in namespace "gc-4937"
    May  9 16:59:11.775: INFO: Deleting pod "simpletest.rc-4tbz2" in namespace "gc-4937"
    May  9 16:59:11.795: INFO: Deleting pod "simpletest.rc-4z7z8" in namespace "gc-4937"
    May  9 16:59:11.817: INFO: Deleting pod "simpletest.rc-59hj8" in namespace "gc-4937"
    May  9 16:59:11.832: INFO: Deleting pod "simpletest.rc-5fmht" in namespace "gc-4937"
    May  9 16:59:11.852: INFO: Deleting pod "simpletest.rc-5frvw" in namespace "gc-4937"
    May  9 16:59:11.871: INFO: Deleting pod "simpletest.rc-5m455" in namespace "gc-4937"
    May  9 16:59:11.886: INFO: Deleting pod "simpletest.rc-5rt5d" in namespace "gc-4937"
    May  9 16:59:11.902: INFO: Deleting pod "simpletest.rc-5tvjt" in namespace "gc-4937"
    May  9 16:59:11.916: INFO: Deleting pod "simpletest.rc-66vtg" in namespace "gc-4937"
    May  9 16:59:11.930: INFO: Deleting pod "simpletest.rc-67h2t" in namespace "gc-4937"
    May  9 16:59:11.949: INFO: Deleting pod "simpletest.rc-6b8nv" in namespace "gc-4937"
    May  9 16:59:11.981: INFO: Deleting pod "simpletest.rc-6l2m8" in namespace "gc-4937"
    May  9 16:59:12.005: INFO: Deleting pod "simpletest.rc-6rmlf" in namespace "gc-4937"
    May  9 16:59:12.047: INFO: Deleting pod "simpletest.rc-7fz7p" in namespace "gc-4937"
    May  9 16:59:12.091: INFO: Deleting pod "simpletest.rc-7q45f" in namespace "gc-4937"
    May  9 16:59:12.110: INFO: Deleting pod "simpletest.rc-7xrtd" in namespace "gc-4937"
    May  9 16:59:12.129: INFO: Deleting pod "simpletest.rc-8bn5x" in namespace "gc-4937"
    May  9 16:59:12.156: INFO: Deleting pod "simpletest.rc-8crhf" in namespace "gc-4937"
    May  9 16:59:12.181: INFO: Deleting pod "simpletest.rc-8cscw" in namespace "gc-4937"
    May  9 16:59:12.195: INFO: Deleting pod "simpletest.rc-8vwgt" in namespace "gc-4937"
    May  9 16:59:12.214: INFO: Deleting pod "simpletest.rc-927sx" in namespace "gc-4937"
    May  9 16:59:12.231: INFO: Deleting pod "simpletest.rc-9j4h2" in namespace "gc-4937"
    May  9 16:59:12.247: INFO: Deleting pod "simpletest.rc-9mn9k" in namespace "gc-4937"
    May  9 16:59:12.274: INFO: Deleting pod "simpletest.rc-9vjgz" in namespace "gc-4937"
    May  9 16:59:12.289: INFO: Deleting pod "simpletest.rc-b2zkf" in namespace "gc-4937"
    May  9 16:59:12.318: INFO: Deleting pod "simpletest.rc-b66ll" in namespace "gc-4937"
    May  9 16:59:12.367: INFO: Deleting pod "simpletest.rc-b7sqb" in namespace "gc-4937"
    May  9 16:59:12.384: INFO: Deleting pod "simpletest.rc-b8zdh" in namespace "gc-4937"
    May  9 16:59:12.400: INFO: Deleting pod "simpletest.rc-bh29k" in namespace "gc-4937"
    May  9 16:59:12.468: INFO: Deleting pod "simpletest.rc-c7jnb" in namespace "gc-4937"
    May  9 16:59:12.572: INFO: Deleting pod "simpletest.rc-cvwvf" in namespace "gc-4937"
    May  9 16:59:12.682: INFO: Deleting pod "simpletest.rc-cxlrk" in namespace "gc-4937"
    May  9 16:59:12.774: INFO: Deleting pod "simpletest.rc-d48ll" in namespace "gc-4937"
    May  9 16:59:12.866: INFO: Deleting pod "simpletest.rc-f2wrs" in namespace "gc-4937"
    May  9 16:59:12.884: INFO: Deleting pod "simpletest.rc-f7cxg" in namespace "gc-4937"
    May  9 16:59:12.899: INFO: Deleting pod "simpletest.rc-fbzl9" in namespace "gc-4937"
    May  9 16:59:12.913: INFO: Deleting pod "simpletest.rc-fdfsv" in namespace "gc-4937"
    May  9 16:59:12.979: INFO: Deleting pod "simpletest.rc-gtrxh" in namespace "gc-4937"
    May  9 16:59:12.994: INFO: Deleting pod "simpletest.rc-gv79g" in namespace "gc-4937"
    May  9 16:59:13.017: INFO: Deleting pod "simpletest.rc-h5x47" in namespace "gc-4937"
    May  9 16:59:13.035: INFO: Deleting pod "simpletest.rc-hc4fx" in namespace "gc-4937"
    May  9 16:59:13.049: INFO: Deleting pod "simpletest.rc-hthr6" in namespace "gc-4937"
    May  9 16:59:13.073: INFO: Deleting pod "simpletest.rc-k85m5" in namespace "gc-4937"
    May  9 16:59:13.266: INFO: Deleting pod "simpletest.rc-kfm6m" in namespace "gc-4937"
    May  9 16:59:13.866: INFO: Deleting pod "simpletest.rc-klvhl" in namespace "gc-4937"
    May  9 16:59:13.883: INFO: Deleting pod "simpletest.rc-l7gjf" in namespace "gc-4937"
    May  9 16:59:13.962: INFO: Deleting pod "simpletest.rc-l8xtc" in namespace "gc-4937"
    May  9 16:59:13.975: INFO: Deleting pod "simpletest.rc-lvpjm" in namespace "gc-4937"
    May  9 16:59:13.988: INFO: Deleting pod "simpletest.rc-m8lhf" in namespace "gc-4937"
    May  9 16:59:14.000: INFO: Deleting pod "simpletest.rc-md8p8" in namespace "gc-4937"
    May  9 16:59:14.016: INFO: Deleting pod "simpletest.rc-mfn49" in namespace "gc-4937"
    May  9 16:59:14.043: INFO: Deleting pod "simpletest.rc-mntct" in namespace "gc-4937"
    May  9 16:59:14.059: INFO: Deleting pod "simpletest.rc-mrf7t" in namespace "gc-4937"
    May  9 16:59:14.074: INFO: Deleting pod "simpletest.rc-mzkhr" in namespace "gc-4937"
    May  9 16:59:14.103: INFO: Deleting pod "simpletest.rc-ngjb8" in namespace "gc-4937"
    May  9 16:59:14.117: INFO: Deleting pod "simpletest.rc-nwr5q" in namespace "gc-4937"
    May  9 16:59:14.131: INFO: Deleting pod "simpletest.rc-p56zh" in namespace "gc-4937"
    May  9 16:59:14.153: INFO: Deleting pod "simpletest.rc-p6kpn" in namespace "gc-4937"
    May  9 16:59:14.170: INFO: Deleting pod "simpletest.rc-p8nsg" in namespace "gc-4937"
    May  9 16:59:14.182: INFO: Deleting pod "simpletest.rc-p8x9f" in namespace "gc-4937"
    May  9 16:59:14.199: INFO: Deleting pod "simpletest.rc-p9sxt" in namespace "gc-4937"
    May  9 16:59:14.214: INFO: Deleting pod "simpletest.rc-pcd2l" in namespace "gc-4937"
    May  9 16:59:14.233: INFO: Deleting pod "simpletest.rc-pczz6" in namespace "gc-4937"
    May  9 16:59:14.254: INFO: Deleting pod "simpletest.rc-pj2ws" in namespace "gc-4937"
    May  9 16:59:14.272: INFO: Deleting pod "simpletest.rc-q4sk5" in namespace "gc-4937"
    May  9 16:59:14.290: INFO: Deleting pod "simpletest.rc-qbbtp" in namespace "gc-4937"
    May  9 16:59:14.310: INFO: Deleting pod "simpletest.rc-qfdhv" in namespace "gc-4937"
    May  9 16:59:14.326: INFO: Deleting pod "simpletest.rc-qnl2c" in namespace "gc-4937"
    May  9 16:59:14.342: INFO: Deleting pod "simpletest.rc-qtddp" in namespace "gc-4937"
    May  9 16:59:14.370: INFO: Deleting pod "simpletest.rc-rrrnw" in namespace "gc-4937"
    May  9 16:59:14.396: INFO: Deleting pod "simpletest.rc-rz6x6" in namespace "gc-4937"
    May  9 16:59:14.411: INFO: Deleting pod "simpletest.rc-spppw" in namespace "gc-4937"
    May  9 16:59:14.432: INFO: Deleting pod "simpletest.rc-sx5qq" in namespace "gc-4937"
    May  9 16:59:14.452: INFO: Deleting pod "simpletest.rc-szrzl" in namespace "gc-4937"
    May  9 16:59:14.475: INFO: Deleting pod "simpletest.rc-th5gx" in namespace "gc-4937"
    May  9 16:59:14.493: INFO: Deleting pod "simpletest.rc-tmsgk" in namespace "gc-4937"
    May  9 16:59:14.511: INFO: Deleting pod "simpletest.rc-tr7ss" in namespace "gc-4937"
    May  9 16:59:14.531: INFO: Deleting pod "simpletest.rc-ttjxf" in namespace "gc-4937"
    May  9 16:59:14.544: INFO: Deleting pod "simpletest.rc-tvplp" in namespace "gc-4937"
    May  9 16:59:14.558: INFO: Deleting pod "simpletest.rc-txd79" in namespace "gc-4937"
    May  9 16:59:14.578: INFO: Deleting pod "simpletest.rc-v8bbm" in namespace "gc-4937"
    May  9 16:59:14.600: INFO: Deleting pod "simpletest.rc-vlsl5" in namespace "gc-4937"
    May  9 16:59:14.620: INFO: Deleting pod "simpletest.rc-vpsh6" in namespace "gc-4937"
    May  9 16:59:14.642: INFO: Deleting pod "simpletest.rc-w6sbz" in namespace "gc-4937"
    May  9 16:59:14.665: INFO: Deleting pod "simpletest.rc-wp57g" in namespace "gc-4937"
    May  9 16:59:14.700: INFO: Deleting pod "simpletest.rc-x5gj7" in namespace "gc-4937"
    May  9 16:59:14.720: INFO: Deleting pod "simpletest.rc-xl86x" in namespace "gc-4937"
    May  9 16:59:14.743: INFO: Deleting pod "simpletest.rc-zfkfl" in namespace "gc-4937"
    May  9 16:59:14.763: INFO: Deleting pod "simpletest.rc-zh6tc" in namespace "gc-4937"
    May  9 16:59:14.806: INFO: Deleting pod "simpletest.rc-zlm8p" in namespace "gc-4937"
    May  9 16:59:14.822: INFO: Deleting pod "simpletest.rc-zqvk6" in namespace "gc-4937"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May  9 16:59:14.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4937" for this suite. 05/09/23 16:59:14.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 16:59:14.896
May  9 16:59:14.896: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename crd-watch 05/09/23 16:59:14.897
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:59:14.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:59:14.929
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
May  9 16:59:14.937: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Creating first CR  05/09/23 16:59:16.295
May  9 16:59:16.301: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T16:59:16Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T16:59:16Z]] name:name1 resourceVersion:318219949 uid:3bef1d72-6c65-4276-9ef6-02c8cfe44e71] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 05/09/23 16:59:26.303
May  9 16:59:26.310: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T16:59:26Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T16:59:26Z]] name:name2 resourceVersion:318221445 uid:05db85a5-d0df-4dce-abdd-bddf1ace02a7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 05/09/23 16:59:36.311
May  9 16:59:36.322: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T16:59:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T16:59:36Z]] name:name1 resourceVersion:318222334 uid:3bef1d72-6c65-4276-9ef6-02c8cfe44e71] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 05/09/23 16:59:46.323
May  9 16:59:46.360: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T16:59:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T16:59:46Z]] name:name2 resourceVersion:318223194 uid:05db85a5-d0df-4dce-abdd-bddf1ace02a7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 05/09/23 16:59:56.362
May  9 16:59:56.371: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T16:59:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T16:59:36Z]] name:name1 resourceVersion:318224038 uid:3bef1d72-6c65-4276-9ef6-02c8cfe44e71] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 05/09/23 17:00:06.371
May  9 17:00:06.383: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T16:59:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T16:59:46Z]] name:name2 resourceVersion:318225668 uid:05db85a5-d0df-4dce-abdd-bddf1ace02a7] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 17:00:16.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-1172" for this suite. 05/09/23 17:00:16.925
------------------------------
â€¢ [SLOW TEST] [62.040 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 16:59:14.896
    May  9 16:59:14.896: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename crd-watch 05/09/23 16:59:14.897
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 16:59:14.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 16:59:14.929
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    May  9 16:59:14.937: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Creating first CR  05/09/23 16:59:16.295
    May  9 16:59:16.301: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T16:59:16Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T16:59:16Z]] name:name1 resourceVersion:318219949 uid:3bef1d72-6c65-4276-9ef6-02c8cfe44e71] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 05/09/23 16:59:26.303
    May  9 16:59:26.310: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T16:59:26Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T16:59:26Z]] name:name2 resourceVersion:318221445 uid:05db85a5-d0df-4dce-abdd-bddf1ace02a7] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 05/09/23 16:59:36.311
    May  9 16:59:36.322: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T16:59:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T16:59:36Z]] name:name1 resourceVersion:318222334 uid:3bef1d72-6c65-4276-9ef6-02c8cfe44e71] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 05/09/23 16:59:46.323
    May  9 16:59:46.360: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T16:59:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T16:59:46Z]] name:name2 resourceVersion:318223194 uid:05db85a5-d0df-4dce-abdd-bddf1ace02a7] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 05/09/23 16:59:56.362
    May  9 16:59:56.371: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T16:59:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T16:59:36Z]] name:name1 resourceVersion:318224038 uid:3bef1d72-6c65-4276-9ef6-02c8cfe44e71] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 05/09/23 17:00:06.371
    May  9 17:00:06.383: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-09T16:59:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-09T16:59:46Z]] name:name2 resourceVersion:318225668 uid:05db85a5-d0df-4dce-abdd-bddf1ace02a7] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 17:00:16.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-1172" for this suite. 05/09/23 17:00:16.925
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:00:16.936
May  9 17:00:16.936: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename pods 05/09/23 17:00:16.937
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:00:16.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:00:16.959
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 05/09/23 17:00:16.966
STEP: submitting the pod to kubernetes 05/09/23 17:00:16.967
May  9 17:00:16.977: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3" in namespace "pods-8362" to be "running and ready"
May  9 17:00:16.986: INFO: Pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.822773ms
May  9 17:00:16.986: INFO: The phase of Pod pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3 is Pending, waiting for it to be Running (with Ready = true)
May  9 17:00:18.992: INFO: Pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3": Phase="Running", Reason="", readiness=true. Elapsed: 2.014918153s
May  9 17:00:18.992: INFO: The phase of Pod pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3 is Running (Ready = true)
May  9 17:00:18.992: INFO: Pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 05/09/23 17:00:18.997
STEP: updating the pod 05/09/23 17:00:19.002
May  9 17:00:19.518: INFO: Successfully updated pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3"
May  9 17:00:19.518: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3" in namespace "pods-8362" to be "terminated with reason DeadlineExceeded"
May  9 17:00:19.522: INFO: Pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3": Phase="Running", Reason="", readiness=true. Elapsed: 4.322538ms
May  9 17:00:21.530: INFO: Pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3": Phase="Running", Reason="", readiness=true. Elapsed: 2.012008785s
May  9 17:00:23.528: INFO: Pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3": Phase="Running", Reason="", readiness=false. Elapsed: 4.009771464s
May  9 17:00:25.529: INFO: Pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.011440953s
May  9 17:00:25.529: INFO: Pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  9 17:00:25.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8362" for this suite. 05/09/23 17:00:25.539
------------------------------
â€¢ [SLOW TEST] [8.613 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:00:16.936
    May  9 17:00:16.936: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename pods 05/09/23 17:00:16.937
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:00:16.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:00:16.959
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 05/09/23 17:00:16.966
    STEP: submitting the pod to kubernetes 05/09/23 17:00:16.967
    May  9 17:00:16.977: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3" in namespace "pods-8362" to be "running and ready"
    May  9 17:00:16.986: INFO: Pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.822773ms
    May  9 17:00:16.986: INFO: The phase of Pod pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3 is Pending, waiting for it to be Running (with Ready = true)
    May  9 17:00:18.992: INFO: Pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3": Phase="Running", Reason="", readiness=true. Elapsed: 2.014918153s
    May  9 17:00:18.992: INFO: The phase of Pod pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3 is Running (Ready = true)
    May  9 17:00:18.992: INFO: Pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 05/09/23 17:00:18.997
    STEP: updating the pod 05/09/23 17:00:19.002
    May  9 17:00:19.518: INFO: Successfully updated pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3"
    May  9 17:00:19.518: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3" in namespace "pods-8362" to be "terminated with reason DeadlineExceeded"
    May  9 17:00:19.522: INFO: Pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3": Phase="Running", Reason="", readiness=true. Elapsed: 4.322538ms
    May  9 17:00:21.530: INFO: Pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3": Phase="Running", Reason="", readiness=true. Elapsed: 2.012008785s
    May  9 17:00:23.528: INFO: Pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3": Phase="Running", Reason="", readiness=false. Elapsed: 4.009771464s
    May  9 17:00:25.529: INFO: Pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.011440953s
    May  9 17:00:25.529: INFO: Pod "pod-update-activedeadlineseconds-65519eaa-30fd-4547-acb8-9abe762f0bf3" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  9 17:00:25.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8362" for this suite. 05/09/23 17:00:25.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:00:25.55
May  9 17:00:25.550: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 17:00:25.551
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:00:25.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:00:25.574
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 05/09/23 17:00:25.579
May  9 17:00:25.592: INFO: Waiting up to 5m0s for pod "annotationupdate8d66e405-6e76-422a-9617-64e791ef4b36" in namespace "projected-8457" to be "running and ready"
May  9 17:00:25.597: INFO: Pod "annotationupdate8d66e405-6e76-422a-9617-64e791ef4b36": Phase="Pending", Reason="", readiness=false. Elapsed: 5.23495ms
May  9 17:00:25.597: INFO: The phase of Pod annotationupdate8d66e405-6e76-422a-9617-64e791ef4b36 is Pending, waiting for it to be Running (with Ready = true)
May  9 17:00:27.603: INFO: Pod "annotationupdate8d66e405-6e76-422a-9617-64e791ef4b36": Phase="Running", Reason="", readiness=true. Elapsed: 2.011693341s
May  9 17:00:27.603: INFO: The phase of Pod annotationupdate8d66e405-6e76-422a-9617-64e791ef4b36 is Running (Ready = true)
May  9 17:00:27.603: INFO: Pod "annotationupdate8d66e405-6e76-422a-9617-64e791ef4b36" satisfied condition "running and ready"
May  9 17:00:28.186: INFO: Successfully updated pod "annotationupdate8d66e405-6e76-422a-9617-64e791ef4b36"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  9 17:00:32.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8457" for this suite. 05/09/23 17:00:32.235
------------------------------
â€¢ [SLOW TEST] [6.693 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:00:25.55
    May  9 17:00:25.550: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 17:00:25.551
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:00:25.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:00:25.574
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 05/09/23 17:00:25.579
    May  9 17:00:25.592: INFO: Waiting up to 5m0s for pod "annotationupdate8d66e405-6e76-422a-9617-64e791ef4b36" in namespace "projected-8457" to be "running and ready"
    May  9 17:00:25.597: INFO: Pod "annotationupdate8d66e405-6e76-422a-9617-64e791ef4b36": Phase="Pending", Reason="", readiness=false. Elapsed: 5.23495ms
    May  9 17:00:25.597: INFO: The phase of Pod annotationupdate8d66e405-6e76-422a-9617-64e791ef4b36 is Pending, waiting for it to be Running (with Ready = true)
    May  9 17:00:27.603: INFO: Pod "annotationupdate8d66e405-6e76-422a-9617-64e791ef4b36": Phase="Running", Reason="", readiness=true. Elapsed: 2.011693341s
    May  9 17:00:27.603: INFO: The phase of Pod annotationupdate8d66e405-6e76-422a-9617-64e791ef4b36 is Running (Ready = true)
    May  9 17:00:27.603: INFO: Pod "annotationupdate8d66e405-6e76-422a-9617-64e791ef4b36" satisfied condition "running and ready"
    May  9 17:00:28.186: INFO: Successfully updated pod "annotationupdate8d66e405-6e76-422a-9617-64e791ef4b36"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  9 17:00:32.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8457" for this suite. 05/09/23 17:00:32.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:00:32.245
May  9 17:00:32.245: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename watch 05/09/23 17:00:32.246
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:00:32.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:00:32.264
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 05/09/23 17:00:32.267
STEP: creating a watch on configmaps with label B 05/09/23 17:00:32.269
STEP: creating a watch on configmaps with label A or B 05/09/23 17:00:32.271
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 05/09/23 17:00:32.273
May  9 17:00:32.279: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-604  f1c405f1-1f25-45a6-a7bc-af199144095d 318228390 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 17:00:32.279: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-604  f1c405f1-1f25-45a6-a7bc-af199144095d 318228390 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 05/09/23 17:00:32.279
May  9 17:00:32.290: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-604  f1c405f1-1f25-45a6-a7bc-af199144095d 318228391 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 17:00:32.290: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-604  f1c405f1-1f25-45a6-a7bc-af199144095d 318228391 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 05/09/23 17:00:32.29
May  9 17:00:32.302: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-604  f1c405f1-1f25-45a6-a7bc-af199144095d 318228393 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 17:00:32.302: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-604  f1c405f1-1f25-45a6-a7bc-af199144095d 318228393 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 05/09/23 17:00:32.302
May  9 17:00:32.309: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-604  f1c405f1-1f25-45a6-a7bc-af199144095d 318228394 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 17:00:32.309: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-604  f1c405f1-1f25-45a6-a7bc-af199144095d 318228394 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 05/09/23 17:00:32.309
May  9 17:00:32.314: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-604  98cb3735-4c1b-47b9-ba30-efff625f720d 318228395 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 17:00:32.315: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-604  98cb3735-4c1b-47b9-ba30-efff625f720d 318228395 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 05/09/23 17:00:42.316
May  9 17:00:42.326: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-604  98cb3735-4c1b-47b9-ba30-efff625f720d 318229267 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  9 17:00:42.326: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-604  98cb3735-4c1b-47b9-ba30-efff625f720d 318229267 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May  9 17:00:52.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-604" for this suite. 05/09/23 17:00:52.335
------------------------------
â€¢ [SLOW TEST] [20.098 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:00:32.245
    May  9 17:00:32.245: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename watch 05/09/23 17:00:32.246
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:00:32.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:00:32.264
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 05/09/23 17:00:32.267
    STEP: creating a watch on configmaps with label B 05/09/23 17:00:32.269
    STEP: creating a watch on configmaps with label A or B 05/09/23 17:00:32.271
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 05/09/23 17:00:32.273
    May  9 17:00:32.279: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-604  f1c405f1-1f25-45a6-a7bc-af199144095d 318228390 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 17:00:32.279: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-604  f1c405f1-1f25-45a6-a7bc-af199144095d 318228390 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 05/09/23 17:00:32.279
    May  9 17:00:32.290: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-604  f1c405f1-1f25-45a6-a7bc-af199144095d 318228391 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 17:00:32.290: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-604  f1c405f1-1f25-45a6-a7bc-af199144095d 318228391 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 05/09/23 17:00:32.29
    May  9 17:00:32.302: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-604  f1c405f1-1f25-45a6-a7bc-af199144095d 318228393 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 17:00:32.302: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-604  f1c405f1-1f25-45a6-a7bc-af199144095d 318228393 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 05/09/23 17:00:32.302
    May  9 17:00:32.309: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-604  f1c405f1-1f25-45a6-a7bc-af199144095d 318228394 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 17:00:32.309: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-604  f1c405f1-1f25-45a6-a7bc-af199144095d 318228394 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 05/09/23 17:00:32.309
    May  9 17:00:32.314: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-604  98cb3735-4c1b-47b9-ba30-efff625f720d 318228395 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 17:00:32.315: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-604  98cb3735-4c1b-47b9-ba30-efff625f720d 318228395 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 05/09/23 17:00:42.316
    May  9 17:00:42.326: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-604  98cb3735-4c1b-47b9-ba30-efff625f720d 318229267 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  9 17:00:42.326: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-604  98cb3735-4c1b-47b9-ba30-efff625f720d 318229267 0 2023-05-09 17:00:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-09 17:00:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May  9 17:00:52.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-604" for this suite. 05/09/23 17:00:52.335
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:00:52.343
May  9 17:00:52.343: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename webhook 05/09/23 17:00:52.345
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:00:52.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:00:52.367
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/09/23 17:00:52.395
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 17:00:52.914
STEP: Deploying the webhook pod 05/09/23 17:00:52.922
STEP: Wait for the deployment to be ready 05/09/23 17:00:52.938
May  9 17:00:52.956: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 17:00:54.971
STEP: Verifying the service has paired with the endpoint 05/09/23 17:00:54.985
May  9 17:00:55.985: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
May  9 17:00:55.991: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1107-crds.webhook.example.com via the AdmissionRegistration API 05/09/23 17:00:56.504
STEP: Creating a custom resource that should be mutated by the webhook 05/09/23 17:00:56.526
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 17:00:59.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3271" for this suite. 05/09/23 17:00:59.273
STEP: Destroying namespace "webhook-3271-markers" for this suite. 05/09/23 17:00:59.286
------------------------------
â€¢ [SLOW TEST] [6.951 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:00:52.343
    May  9 17:00:52.343: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename webhook 05/09/23 17:00:52.345
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:00:52.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:00:52.367
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/09/23 17:00:52.395
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 17:00:52.914
    STEP: Deploying the webhook pod 05/09/23 17:00:52.922
    STEP: Wait for the deployment to be ready 05/09/23 17:00:52.938
    May  9 17:00:52.956: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 17:00:54.971
    STEP: Verifying the service has paired with the endpoint 05/09/23 17:00:54.985
    May  9 17:00:55.985: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    May  9 17:00:55.991: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1107-crds.webhook.example.com via the AdmissionRegistration API 05/09/23 17:00:56.504
    STEP: Creating a custom resource that should be mutated by the webhook 05/09/23 17:00:56.526
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 17:00:59.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3271" for this suite. 05/09/23 17:00:59.273
    STEP: Destroying namespace "webhook-3271-markers" for this suite. 05/09/23 17:00:59.286
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:00:59.296
May  9 17:00:59.296: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 17:00:59.297
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:00:59.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:00:59.322
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-8f4344ff-9916-44f7-a69a-ac34a32ab500 05/09/23 17:00:59.327
STEP: Creating a pod to test consume configMaps 05/09/23 17:00:59.335
May  9 17:00:59.359: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9bb3f30b-6434-41dc-9ffd-c83430d9d148" in namespace "projected-1008" to be "Succeeded or Failed"
May  9 17:00:59.366: INFO: Pod "pod-projected-configmaps-9bb3f30b-6434-41dc-9ffd-c83430d9d148": Phase="Pending", Reason="", readiness=false. Elapsed: 7.139059ms
May  9 17:01:01.372: INFO: Pod "pod-projected-configmaps-9bb3f30b-6434-41dc-9ffd-c83430d9d148": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013590227s
May  9 17:01:03.374: INFO: Pod "pod-projected-configmaps-9bb3f30b-6434-41dc-9ffd-c83430d9d148": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015007468s
STEP: Saw pod success 05/09/23 17:01:03.374
May  9 17:01:03.374: INFO: Pod "pod-projected-configmaps-9bb3f30b-6434-41dc-9ffd-c83430d9d148" satisfied condition "Succeeded or Failed"
May  9 17:01:03.381: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-configmaps-9bb3f30b-6434-41dc-9ffd-c83430d9d148 container agnhost-container: <nil>
STEP: delete the pod 05/09/23 17:01:03.392
May  9 17:01:03.410: INFO: Waiting for pod pod-projected-configmaps-9bb3f30b-6434-41dc-9ffd-c83430d9d148 to disappear
May  9 17:01:03.415: INFO: Pod pod-projected-configmaps-9bb3f30b-6434-41dc-9ffd-c83430d9d148 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May  9 17:01:03.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1008" for this suite. 05/09/23 17:01:03.421
------------------------------
â€¢ [4.138 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:00:59.296
    May  9 17:00:59.296: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 17:00:59.297
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:00:59.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:00:59.322
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-8f4344ff-9916-44f7-a69a-ac34a32ab500 05/09/23 17:00:59.327
    STEP: Creating a pod to test consume configMaps 05/09/23 17:00:59.335
    May  9 17:00:59.359: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9bb3f30b-6434-41dc-9ffd-c83430d9d148" in namespace "projected-1008" to be "Succeeded or Failed"
    May  9 17:00:59.366: INFO: Pod "pod-projected-configmaps-9bb3f30b-6434-41dc-9ffd-c83430d9d148": Phase="Pending", Reason="", readiness=false. Elapsed: 7.139059ms
    May  9 17:01:01.372: INFO: Pod "pod-projected-configmaps-9bb3f30b-6434-41dc-9ffd-c83430d9d148": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013590227s
    May  9 17:01:03.374: INFO: Pod "pod-projected-configmaps-9bb3f30b-6434-41dc-9ffd-c83430d9d148": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015007468s
    STEP: Saw pod success 05/09/23 17:01:03.374
    May  9 17:01:03.374: INFO: Pod "pod-projected-configmaps-9bb3f30b-6434-41dc-9ffd-c83430d9d148" satisfied condition "Succeeded or Failed"
    May  9 17:01:03.381: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-configmaps-9bb3f30b-6434-41dc-9ffd-c83430d9d148 container agnhost-container: <nil>
    STEP: delete the pod 05/09/23 17:01:03.392
    May  9 17:01:03.410: INFO: Waiting for pod pod-projected-configmaps-9bb3f30b-6434-41dc-9ffd-c83430d9d148 to disappear
    May  9 17:01:03.415: INFO: Pod pod-projected-configmaps-9bb3f30b-6434-41dc-9ffd-c83430d9d148 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May  9 17:01:03.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1008" for this suite. 05/09/23 17:01:03.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:01:03.436
May  9 17:01:03.436: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename sched-pred 05/09/23 17:01:03.436
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:03.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:03.458
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
May  9 17:01:03.463: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  9 17:01:03.473: INFO: Waiting for terminating namespaces to be deleted...
May  9 17:01:03.480: INFO: 
Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 before test
May  9 17:01:03.490: INFO: canal-zcmck from kube-system started at 2023-05-09 08:48:05 +0000 UTC (2 container statuses recorded)
May  9 17:01:03.490: INFO: 	Container calico-node ready: true, restart count 2
May  9 17:01:03.490: INFO: 	Container kube-flannel ready: true, restart count 0
May  9 17:01:03.490: INFO: kube-proxy-q8nhz from kube-system started at 2023-05-09 08:48:05 +0000 UTC (1 container statuses recorded)
May  9 17:01:03.490: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 17:01:03.490: INFO: wormhole-xj4hj from kube-system started at 2023-05-09 08:48:05 +0000 UTC (1 container statuses recorded)
May  9 17:01:03.490: INFO: 	Container wormhole ready: true, restart count 0
May  9 17:01:03.490: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-69c6h from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
May  9 17:01:03.490: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 17:01:03.490: INFO: 	Container systemd-logs ready: true, restart count 0
May  9 17:01:03.490: INFO: 
Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 before test
May  9 17:01:03.500: INFO: calico-kube-controllers-6cffbf7894-zrprz from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
May  9 17:01:03.500: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  9 17:01:03.500: INFO: canal-22bj2 from kube-system started at 2023-05-09 08:47:59 +0000 UTC (2 container statuses recorded)
May  9 17:01:03.500: INFO: 	Container calico-node ready: true, restart count 2
May  9 17:01:03.500: INFO: 	Container kube-flannel ready: true, restart count 0
May  9 17:01:03.500: INFO: coredns-996c5dbc5-24wst from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
May  9 17:01:03.500: INFO: 	Container coredns ready: true, restart count 0
May  9 17:01:03.500: INFO: kube-dns-autoscaler-789d47d664-hhx85 from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
May  9 17:01:03.500: INFO: 	Container autoscaler ready: true, restart count 0
May  9 17:01:03.500: INFO: kube-proxy-r9g6q from kube-system started at 2023-05-09 08:47:59 +0000 UTC (1 container statuses recorded)
May  9 17:01:03.500: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 17:01:03.500: INFO: metrics-server-5f9c95d78-hk5zz from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
May  9 17:01:03.500: INFO: 	Container metrics-server ready: true, restart count 0
May  9 17:01:03.500: INFO: wormhole-hnkjk from kube-system started at 2023-05-09 08:47:59 +0000 UTC (1 container statuses recorded)
May  9 17:01:03.500: INFO: 	Container wormhole ready: true, restart count 0
May  9 17:01:03.500: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-2x6mg from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
May  9 17:01:03.500: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 17:01:03.500: INFO: 	Container systemd-logs ready: true, restart count 0
May  9 17:01:03.500: INFO: 
Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 before test
May  9 17:01:03.511: INFO: canal-jl6h7 from kube-system started at 2023-05-09 08:48:03 +0000 UTC (2 container statuses recorded)
May  9 17:01:03.511: INFO: 	Container calico-node ready: true, restart count 2
May  9 17:01:03.511: INFO: 	Container kube-flannel ready: true, restart count 0
May  9 17:01:03.511: INFO: coredns-996c5dbc5-f5psh from kube-system started at 2023-05-09 16:55:22 +0000 UTC (1 container statuses recorded)
May  9 17:01:03.511: INFO: 	Container coredns ready: true, restart count 0
May  9 17:01:03.511: INFO: kube-proxy-qqc8l from kube-system started at 2023-05-09 08:48:03 +0000 UTC (1 container statuses recorded)
May  9 17:01:03.511: INFO: 	Container kube-proxy ready: true, restart count 0
May  9 17:01:03.511: INFO: wormhole-825jz from kube-system started at 2023-05-09 08:48:03 +0000 UTC (1 container statuses recorded)
May  9 17:01:03.511: INFO: 	Container wormhole ready: true, restart count 0
May  9 17:01:03.511: INFO: sonobuoy from sonobuoy started at 2023-05-09 15:24:58 +0000 UTC (1 container statuses recorded)
May  9 17:01:03.511: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  9 17:01:03.511: INFO: sonobuoy-e2e-job-d96e230b174b4cad from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
May  9 17:01:03.511: INFO: 	Container e2e ready: true, restart count 0
May  9 17:01:03.511: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 17:01:03.511: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-zvq2k from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
May  9 17:01:03.511: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  9 17:01:03.511: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 05/09/23 17:01:03.511
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.175d884484f34bfa], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 05/09/23 17:01:03.662
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
May  9 17:01:04.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8934" for this suite. 05/09/23 17:01:04.587
------------------------------
â€¢ [1.159 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:01:03.436
    May  9 17:01:03.436: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename sched-pred 05/09/23 17:01:03.436
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:03.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:03.458
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    May  9 17:01:03.463: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  9 17:01:03.473: INFO: Waiting for terminating namespaces to be deleted...
    May  9 17:01:03.480: INFO: 
    Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 before test
    May  9 17:01:03.490: INFO: canal-zcmck from kube-system started at 2023-05-09 08:48:05 +0000 UTC (2 container statuses recorded)
    May  9 17:01:03.490: INFO: 	Container calico-node ready: true, restart count 2
    May  9 17:01:03.490: INFO: 	Container kube-flannel ready: true, restart count 0
    May  9 17:01:03.490: INFO: kube-proxy-q8nhz from kube-system started at 2023-05-09 08:48:05 +0000 UTC (1 container statuses recorded)
    May  9 17:01:03.490: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 17:01:03.490: INFO: wormhole-xj4hj from kube-system started at 2023-05-09 08:48:05 +0000 UTC (1 container statuses recorded)
    May  9 17:01:03.490: INFO: 	Container wormhole ready: true, restart count 0
    May  9 17:01:03.490: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-69c6h from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
    May  9 17:01:03.490: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 17:01:03.490: INFO: 	Container systemd-logs ready: true, restart count 0
    May  9 17:01:03.490: INFO: 
    Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-bbade7 before test
    May  9 17:01:03.500: INFO: calico-kube-controllers-6cffbf7894-zrprz from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
    May  9 17:01:03.500: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  9 17:01:03.500: INFO: canal-22bj2 from kube-system started at 2023-05-09 08:47:59 +0000 UTC (2 container statuses recorded)
    May  9 17:01:03.500: INFO: 	Container calico-node ready: true, restart count 2
    May  9 17:01:03.500: INFO: 	Container kube-flannel ready: true, restart count 0
    May  9 17:01:03.500: INFO: coredns-996c5dbc5-24wst from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
    May  9 17:01:03.500: INFO: 	Container coredns ready: true, restart count 0
    May  9 17:01:03.500: INFO: kube-dns-autoscaler-789d47d664-hhx85 from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
    May  9 17:01:03.500: INFO: 	Container autoscaler ready: true, restart count 0
    May  9 17:01:03.500: INFO: kube-proxy-r9g6q from kube-system started at 2023-05-09 08:47:59 +0000 UTC (1 container statuses recorded)
    May  9 17:01:03.500: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 17:01:03.500: INFO: metrics-server-5f9c95d78-hk5zz from kube-system started at 2023-05-09 08:48:17 +0000 UTC (1 container statuses recorded)
    May  9 17:01:03.500: INFO: 	Container metrics-server ready: true, restart count 0
    May  9 17:01:03.500: INFO: wormhole-hnkjk from kube-system started at 2023-05-09 08:47:59 +0000 UTC (1 container statuses recorded)
    May  9 17:01:03.500: INFO: 	Container wormhole ready: true, restart count 0
    May  9 17:01:03.500: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-2x6mg from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
    May  9 17:01:03.500: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 17:01:03.500: INFO: 	Container systemd-logs ready: true, restart count 0
    May  9 17:01:03.500: INFO: 
    Logging pods the apiserver thinks is on node nodepool-8cc7f47e-9b0c-4801-88-node-f76f62 before test
    May  9 17:01:03.511: INFO: canal-jl6h7 from kube-system started at 2023-05-09 08:48:03 +0000 UTC (2 container statuses recorded)
    May  9 17:01:03.511: INFO: 	Container calico-node ready: true, restart count 2
    May  9 17:01:03.511: INFO: 	Container kube-flannel ready: true, restart count 0
    May  9 17:01:03.511: INFO: coredns-996c5dbc5-f5psh from kube-system started at 2023-05-09 16:55:22 +0000 UTC (1 container statuses recorded)
    May  9 17:01:03.511: INFO: 	Container coredns ready: true, restart count 0
    May  9 17:01:03.511: INFO: kube-proxy-qqc8l from kube-system started at 2023-05-09 08:48:03 +0000 UTC (1 container statuses recorded)
    May  9 17:01:03.511: INFO: 	Container kube-proxy ready: true, restart count 0
    May  9 17:01:03.511: INFO: wormhole-825jz from kube-system started at 2023-05-09 08:48:03 +0000 UTC (1 container statuses recorded)
    May  9 17:01:03.511: INFO: 	Container wormhole ready: true, restart count 0
    May  9 17:01:03.511: INFO: sonobuoy from sonobuoy started at 2023-05-09 15:24:58 +0000 UTC (1 container statuses recorded)
    May  9 17:01:03.511: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  9 17:01:03.511: INFO: sonobuoy-e2e-job-d96e230b174b4cad from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
    May  9 17:01:03.511: INFO: 	Container e2e ready: true, restart count 0
    May  9 17:01:03.511: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 17:01:03.511: INFO: sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-zvq2k from sonobuoy started at 2023-05-09 15:25:00 +0000 UTC (2 container statuses recorded)
    May  9 17:01:03.511: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  9 17:01:03.511: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 05/09/23 17:01:03.511
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.175d884484f34bfa], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 05/09/23 17:01:03.662
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    May  9 17:01:04.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8934" for this suite. 05/09/23 17:01:04.587
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:01:04.594
May  9 17:01:04.594: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename runtimeclass 05/09/23 17:01:04.595
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:04.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:04.619
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
May  9 17:01:04.640: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8232 to be scheduled
May  9 17:01:04.643: INFO: 1 pods are not scheduled: [runtimeclass-8232/test-runtimeclass-runtimeclass-8232-preconfigured-handler-9qnc9(a6d3c02a-deb6-486f-8fb8-6c94fb93fdaf)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May  9 17:01:06.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8232" for this suite. 05/09/23 17:01:06.663
------------------------------
â€¢ [2.080 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:01:04.594
    May  9 17:01:04.594: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename runtimeclass 05/09/23 17:01:04.595
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:04.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:04.619
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    May  9 17:01:04.640: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8232 to be scheduled
    May  9 17:01:04.643: INFO: 1 pods are not scheduled: [runtimeclass-8232/test-runtimeclass-runtimeclass-8232-preconfigured-handler-9qnc9(a6d3c02a-deb6-486f-8fb8-6c94fb93fdaf)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May  9 17:01:06.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8232" for this suite. 05/09/23 17:01:06.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:01:06.676
May  9 17:01:06.676: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename ingressclass 05/09/23 17:01:06.676
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:06.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:06.697
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 05/09/23 17:01:06.701
STEP: getting /apis/networking.k8s.io 05/09/23 17:01:06.705
STEP: getting /apis/networking.k8s.iov1 05/09/23 17:01:06.707
STEP: creating 05/09/23 17:01:06.709
STEP: getting 05/09/23 17:01:06.735
STEP: listing 05/09/23 17:01:06.744
STEP: watching 05/09/23 17:01:06.748
May  9 17:01:06.748: INFO: starting watch
STEP: patching 05/09/23 17:01:06.75
STEP: updating 05/09/23 17:01:06.759
May  9 17:01:06.767: INFO: waiting for watch events with expected annotations
May  9 17:01:06.767: INFO: saw patched and updated annotations
STEP: deleting 05/09/23 17:01:06.767
STEP: deleting a collection 05/09/23 17:01:06.788
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
May  9 17:01:06.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-4600" for this suite. 05/09/23 17:01:06.814
------------------------------
â€¢ [0.152 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:01:06.676
    May  9 17:01:06.676: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename ingressclass 05/09/23 17:01:06.676
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:06.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:06.697
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 05/09/23 17:01:06.701
    STEP: getting /apis/networking.k8s.io 05/09/23 17:01:06.705
    STEP: getting /apis/networking.k8s.iov1 05/09/23 17:01:06.707
    STEP: creating 05/09/23 17:01:06.709
    STEP: getting 05/09/23 17:01:06.735
    STEP: listing 05/09/23 17:01:06.744
    STEP: watching 05/09/23 17:01:06.748
    May  9 17:01:06.748: INFO: starting watch
    STEP: patching 05/09/23 17:01:06.75
    STEP: updating 05/09/23 17:01:06.759
    May  9 17:01:06.767: INFO: waiting for watch events with expected annotations
    May  9 17:01:06.767: INFO: saw patched and updated annotations
    STEP: deleting 05/09/23 17:01:06.767
    STEP: deleting a collection 05/09/23 17:01:06.788
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    May  9 17:01:06.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-4600" for this suite. 05/09/23 17:01:06.814
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:01:06.828
May  9 17:01:06.828: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename webhook 05/09/23 17:01:06.829
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:06.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:06.85
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/09/23 17:01:06.876
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 17:01:07.198
STEP: Deploying the webhook pod 05/09/23 17:01:07.205
STEP: Wait for the deployment to be ready 05/09/23 17:01:07.222
May  9 17:01:07.232: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/09/23 17:01:09.248
STEP: Verifying the service has paired with the endpoint 05/09/23 17:01:09.26
May  9 17:01:10.261: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 05/09/23 17:01:10.269
STEP: create a pod that should be denied by the webhook 05/09/23 17:01:10.292
STEP: create a pod that causes the webhook to hang 05/09/23 17:01:10.337
STEP: create a configmap that should be denied by the webhook 05/09/23 17:01:20.349
STEP: create a configmap that should be admitted by the webhook 05/09/23 17:01:20.365
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 05/09/23 17:01:20.382
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 05/09/23 17:01:20.397
STEP: create a namespace that bypass the webhook 05/09/23 17:01:20.408
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 05/09/23 17:01:20.417
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  9 17:01:20.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1357" for this suite. 05/09/23 17:01:20.505
STEP: Destroying namespace "webhook-1357-markers" for this suite. 05/09/23 17:01:20.514
------------------------------
â€¢ [SLOW TEST] [13.696 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:01:06.828
    May  9 17:01:06.828: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename webhook 05/09/23 17:01:06.829
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:06.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:06.85
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/09/23 17:01:06.876
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/09/23 17:01:07.198
    STEP: Deploying the webhook pod 05/09/23 17:01:07.205
    STEP: Wait for the deployment to be ready 05/09/23 17:01:07.222
    May  9 17:01:07.232: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/09/23 17:01:09.248
    STEP: Verifying the service has paired with the endpoint 05/09/23 17:01:09.26
    May  9 17:01:10.261: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 05/09/23 17:01:10.269
    STEP: create a pod that should be denied by the webhook 05/09/23 17:01:10.292
    STEP: create a pod that causes the webhook to hang 05/09/23 17:01:10.337
    STEP: create a configmap that should be denied by the webhook 05/09/23 17:01:20.349
    STEP: create a configmap that should be admitted by the webhook 05/09/23 17:01:20.365
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 05/09/23 17:01:20.382
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 05/09/23 17:01:20.397
    STEP: create a namespace that bypass the webhook 05/09/23 17:01:20.408
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 05/09/23 17:01:20.417
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  9 17:01:20.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1357" for this suite. 05/09/23 17:01:20.505
    STEP: Destroying namespace "webhook-1357-markers" for this suite. 05/09/23 17:01:20.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:01:20.524
May  9 17:01:20.524: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename pods 05/09/23 17:01:20.524
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:20.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:20.549
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 05/09/23 17:01:20.554
May  9 17:01:20.564: INFO: Waiting up to 5m0s for pod "pod-hostip-58e98b09-5921-4372-9f7f-7b0a1c3ff4c8" in namespace "pods-6980" to be "running and ready"
May  9 17:01:20.569: INFO: Pod "pod-hostip-58e98b09-5921-4372-9f7f-7b0a1c3ff4c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.325782ms
May  9 17:01:20.569: INFO: The phase of Pod pod-hostip-58e98b09-5921-4372-9f7f-7b0a1c3ff4c8 is Pending, waiting for it to be Running (with Ready = true)
May  9 17:01:22.582: INFO: Pod "pod-hostip-58e98b09-5921-4372-9f7f-7b0a1c3ff4c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.017469384s
May  9 17:01:22.582: INFO: The phase of Pod pod-hostip-58e98b09-5921-4372-9f7f-7b0a1c3ff4c8 is Running (Ready = true)
May  9 17:01:22.582: INFO: Pod "pod-hostip-58e98b09-5921-4372-9f7f-7b0a1c3ff4c8" satisfied condition "running and ready"
May  9 17:01:22.592: INFO: Pod pod-hostip-58e98b09-5921-4372-9f7f-7b0a1c3ff4c8 has hostIP: 51.68.91.222
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  9 17:01:22.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6980" for this suite. 05/09/23 17:01:22.598
------------------------------
â€¢ [2.084 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:01:20.524
    May  9 17:01:20.524: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename pods 05/09/23 17:01:20.524
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:20.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:20.549
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 05/09/23 17:01:20.554
    May  9 17:01:20.564: INFO: Waiting up to 5m0s for pod "pod-hostip-58e98b09-5921-4372-9f7f-7b0a1c3ff4c8" in namespace "pods-6980" to be "running and ready"
    May  9 17:01:20.569: INFO: Pod "pod-hostip-58e98b09-5921-4372-9f7f-7b0a1c3ff4c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.325782ms
    May  9 17:01:20.569: INFO: The phase of Pod pod-hostip-58e98b09-5921-4372-9f7f-7b0a1c3ff4c8 is Pending, waiting for it to be Running (with Ready = true)
    May  9 17:01:22.582: INFO: Pod "pod-hostip-58e98b09-5921-4372-9f7f-7b0a1c3ff4c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.017469384s
    May  9 17:01:22.582: INFO: The phase of Pod pod-hostip-58e98b09-5921-4372-9f7f-7b0a1c3ff4c8 is Running (Ready = true)
    May  9 17:01:22.582: INFO: Pod "pod-hostip-58e98b09-5921-4372-9f7f-7b0a1c3ff4c8" satisfied condition "running and ready"
    May  9 17:01:22.592: INFO: Pod pod-hostip-58e98b09-5921-4372-9f7f-7b0a1c3ff4c8 has hostIP: 51.68.91.222
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  9 17:01:22.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6980" for this suite. 05/09/23 17:01:22.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:01:22.608
May  9 17:01:22.608: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubectl 05/09/23 17:01:22.609
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:22.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:22.632
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
May  9 17:01:22.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4090 create -f -'
May  9 17:01:24.125: INFO: stderr: ""
May  9 17:01:24.125: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May  9 17:01:24.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4090 create -f -'
May  9 17:01:24.358: INFO: stderr: ""
May  9 17:01:24.358: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/09/23 17:01:24.358
May  9 17:01:25.365: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 17:01:25.365: INFO: Found 0 / 1
May  9 17:01:26.365: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 17:01:26.365: INFO: Found 1 / 1
May  9 17:01:26.365: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  9 17:01:26.370: INFO: Selector matched 1 pods for map[app:agnhost]
May  9 17:01:26.370: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  9 17:01:26.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4090 describe pod agnhost-primary-hr6mg'
May  9 17:01:26.462: INFO: stderr: ""
May  9 17:01:26.462: INFO: stdout: "Name:             agnhost-primary-hr6mg\nNamespace:        kubectl-4090\nPriority:         0\nService Account:  default\nNode:             nodepool-8cc7f47e-9b0c-4801-88-node-7ad816/51.68.91.222\nStart Time:       Tue, 09 May 2023 17:01:24 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: a3d1d95fad5bd54cd09beaab05148eca9d0218bc6d6fd1792676daa52d231569\n                  cni.projectcalico.org/podIP: 10.2.1.225/32\n                  cni.projectcalico.org/podIPs: 10.2.1.225/32\nStatus:           Running\nIP:               10.2.1.225\nIPs:\n  IP:           10.2.1.225\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://97e0e6083ba6b8c960a65b93c3ae0b309490d5f93d9c6c72257b9bbf0441c249\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 09 May 2023 17:01:25 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2z9bv (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-2z9bv:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-4090/agnhost-primary-hr6mg to nodepool-8cc7f47e-9b0c-4801-88-node-7ad816\n  Normal  Pulling    2s    kubelet            Pulling image \"registry.k8s.io/e2e-test-images/agnhost:2.43\"\n  Normal  Pulled     2s    kubelet            Successfully pulled image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" in 199.866089ms (199.883518ms including waiting)\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
May  9 17:01:26.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4090 describe rc agnhost-primary'
May  9 17:01:26.559: INFO: stderr: ""
May  9 17:01:26.559: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4090\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-hr6mg\n"
May  9 17:01:26.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4090 describe service agnhost-primary'
May  9 17:01:26.661: INFO: stderr: ""
May  9 17:01:26.661: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4090\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.3.167.114\nIPs:               10.3.167.114\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.2.1.225:6379\nSession Affinity:  None\nEvents:            <none>\n"
May  9 17:01:26.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4090 describe node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816'
May  9 17:01:26.785: INFO: stderr: ""
May  9 17:01:26.785: INFO: stdout: "Name:               nodepool-8cc7f47e-9b0c-4801-88-node-7ad816\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=nodepool-8cc7f47e-9b0c-4801-88-node-7ad816\n                    kubernetes.io/os=linux\n                    node.k8s.ovh/type=standard\n                    nodepool=nodepool-8cc7f47e-9b0c-4801-8849-7d6a02a9b582\n                    topology.cinder.csi.openstack.org/zone=nova\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"b3c9f8f0-3473-480e-8a50-c9f49a58eacc\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"9a:aa:47:16:80:64\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 51.68.91.222\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.2.1.0/32\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.2.1.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 09 May 2023 08:47:59 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  nodepool-8cc7f47e-9b0c-4801-88-node-7ad816\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 09 May 2023 17:01:20 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 09 May 2023 08:48:28 +0000   Tue, 09 May 2023 08:48:28 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Tue, 09 May 2023 16:58:17 +0000   Tue, 09 May 2023 08:47:59 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 09 May 2023 16:58:17 +0000   Tue, 09 May 2023 08:47:59 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 09 May 2023 16:58:17 +0000   Tue, 09 May 2023 08:47:59 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 09 May 2023 16:58:17 +0000   Tue, 09 May 2023 08:48:24 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  51.68.91.222\n  Hostname:    nodepool-8cc7f47e-9b0c-4801-88-node-7ad816\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      50620216Ki\n  hugepages-2Mi:          0\n  memory:                 7945704Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    3830m\n  ephemeral-storage:      33387320Ki\n  hugepages-2Mi:          0\n  memory:                 6243816Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 b3c9f8f03473480e8a50c9f49a58eacc\n  System UUID:                b3c9f8f0-3473-480e-8a50-c9f49a58eacc\n  Boot ID:                    7aef3adb-4f6c-4c04-afc5-4dfdb866d786\n  Kernel Version:             5.15.0-71-generic\n  OS Image:                   Ubuntu 22.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.20\n  Kubelet Version:            v1.26.4\n  Kube-Proxy Version:         v1.26.4\nPodCIDR:                      10.2.1.0/24\nPodCIDRs:                     10.2.1.0/24\nNon-terminated Pods:          (6 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 canal-zcmck                                                250m (6%)     0 (0%)      0 (0%)           0 (0%)         8h\n  kube-system                 kube-proxy-q8nhz                                           100m (2%)     0 (0%)      200Mi (3%)       200Mi (3%)     8h\n  kube-system                 wormhole-xj4hj                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         8h\n  kubectl-4090                agnhost-primary-hr6mg                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  pods-6980                   pod-hostip-58e98b09-5921-4372-9f7f-7b0a1c3ff4c8            0 (0%)        0 (0%)      0 (0%)           0 (0%)         6s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-69c6h    0 (0%)        0 (0%)      0 (0%)           0 (0%)         96m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    350m (9%)   0 (0%)\n  memory                 200Mi (3%)  200Mi (3%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  scheduling.k8s.io/foo  0           0\nEvents:                  <none>\n"
May  9 17:01:26.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4090 describe namespace kubectl-4090'
May  9 17:01:26.884: INFO: stderr: ""
May  9 17:01:26.884: INFO: stdout: "Name:         kubectl-4090\nLabels:       e2e-framework=kubectl\n              e2e-run=f3f539e6-417f-4a68-8004-5f74a75285d0\n              kubernetes.io/metadata.name=kubectl-4090\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  9 17:01:26.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4090" for this suite. 05/09/23 17:01:26.89
------------------------------
â€¢ [4.290 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:01:22.608
    May  9 17:01:22.608: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubectl 05/09/23 17:01:22.609
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:22.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:22.632
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    May  9 17:01:22.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4090 create -f -'
    May  9 17:01:24.125: INFO: stderr: ""
    May  9 17:01:24.125: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    May  9 17:01:24.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4090 create -f -'
    May  9 17:01:24.358: INFO: stderr: ""
    May  9 17:01:24.358: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/09/23 17:01:24.358
    May  9 17:01:25.365: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 17:01:25.365: INFO: Found 0 / 1
    May  9 17:01:26.365: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 17:01:26.365: INFO: Found 1 / 1
    May  9 17:01:26.365: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    May  9 17:01:26.370: INFO: Selector matched 1 pods for map[app:agnhost]
    May  9 17:01:26.370: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May  9 17:01:26.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4090 describe pod agnhost-primary-hr6mg'
    May  9 17:01:26.462: INFO: stderr: ""
    May  9 17:01:26.462: INFO: stdout: "Name:             agnhost-primary-hr6mg\nNamespace:        kubectl-4090\nPriority:         0\nService Account:  default\nNode:             nodepool-8cc7f47e-9b0c-4801-88-node-7ad816/51.68.91.222\nStart Time:       Tue, 09 May 2023 17:01:24 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: a3d1d95fad5bd54cd09beaab05148eca9d0218bc6d6fd1792676daa52d231569\n                  cni.projectcalico.org/podIP: 10.2.1.225/32\n                  cni.projectcalico.org/podIPs: 10.2.1.225/32\nStatus:           Running\nIP:               10.2.1.225\nIPs:\n  IP:           10.2.1.225\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://97e0e6083ba6b8c960a65b93c3ae0b309490d5f93d9c6c72257b9bbf0441c249\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 09 May 2023 17:01:25 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2z9bv (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-2z9bv:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-4090/agnhost-primary-hr6mg to nodepool-8cc7f47e-9b0c-4801-88-node-7ad816\n  Normal  Pulling    2s    kubelet            Pulling image \"registry.k8s.io/e2e-test-images/agnhost:2.43\"\n  Normal  Pulled     2s    kubelet            Successfully pulled image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" in 199.866089ms (199.883518ms including waiting)\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    May  9 17:01:26.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4090 describe rc agnhost-primary'
    May  9 17:01:26.559: INFO: stderr: ""
    May  9 17:01:26.559: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4090\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-hr6mg\n"
    May  9 17:01:26.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4090 describe service agnhost-primary'
    May  9 17:01:26.661: INFO: stderr: ""
    May  9 17:01:26.661: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4090\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.3.167.114\nIPs:               10.3.167.114\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.2.1.225:6379\nSession Affinity:  None\nEvents:            <none>\n"
    May  9 17:01:26.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4090 describe node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816'
    May  9 17:01:26.785: INFO: stderr: ""
    May  9 17:01:26.785: INFO: stdout: "Name:               nodepool-8cc7f47e-9b0c-4801-88-node-7ad816\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=nodepool-8cc7f47e-9b0c-4801-88-node-7ad816\n                    kubernetes.io/os=linux\n                    node.k8s.ovh/type=standard\n                    nodepool=nodepool-8cc7f47e-9b0c-4801-8849-7d6a02a9b582\n                    topology.cinder.csi.openstack.org/zone=nova\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"b3c9f8f0-3473-480e-8a50-c9f49a58eacc\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"9a:aa:47:16:80:64\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 51.68.91.222\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.2.1.0/32\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.2.1.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 09 May 2023 08:47:59 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  nodepool-8cc7f47e-9b0c-4801-88-node-7ad816\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 09 May 2023 17:01:20 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 09 May 2023 08:48:28 +0000   Tue, 09 May 2023 08:48:28 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Tue, 09 May 2023 16:58:17 +0000   Tue, 09 May 2023 08:47:59 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 09 May 2023 16:58:17 +0000   Tue, 09 May 2023 08:47:59 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 09 May 2023 16:58:17 +0000   Tue, 09 May 2023 08:47:59 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 09 May 2023 16:58:17 +0000   Tue, 09 May 2023 08:48:24 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  51.68.91.222\n  Hostname:    nodepool-8cc7f47e-9b0c-4801-88-node-7ad816\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      50620216Ki\n  hugepages-2Mi:          0\n  memory:                 7945704Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    3830m\n  ephemeral-storage:      33387320Ki\n  hugepages-2Mi:          0\n  memory:                 6243816Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 b3c9f8f03473480e8a50c9f49a58eacc\n  System UUID:                b3c9f8f0-3473-480e-8a50-c9f49a58eacc\n  Boot ID:                    7aef3adb-4f6c-4c04-afc5-4dfdb866d786\n  Kernel Version:             5.15.0-71-generic\n  OS Image:                   Ubuntu 22.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.20\n  Kubelet Version:            v1.26.4\n  Kube-Proxy Version:         v1.26.4\nPodCIDR:                      10.2.1.0/24\nPodCIDRs:                     10.2.1.0/24\nNon-terminated Pods:          (6 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 canal-zcmck                                                250m (6%)     0 (0%)      0 (0%)           0 (0%)         8h\n  kube-system                 kube-proxy-q8nhz                                           100m (2%)     0 (0%)      200Mi (3%)       200Mi (3%)     8h\n  kube-system                 wormhole-xj4hj                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         8h\n  kubectl-4090                agnhost-primary-hr6mg                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  pods-6980                   pod-hostip-58e98b09-5921-4372-9f7f-7b0a1c3ff4c8            0 (0%)        0 (0%)      0 (0%)           0 (0%)         6s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-c62f326c03934fe5-69c6h    0 (0%)        0 (0%)      0 (0%)           0 (0%)         96m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    350m (9%)   0 (0%)\n  memory                 200Mi (3%)  200Mi (3%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  scheduling.k8s.io/foo  0           0\nEvents:                  <none>\n"
    May  9 17:01:26.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-4090 describe namespace kubectl-4090'
    May  9 17:01:26.884: INFO: stderr: ""
    May  9 17:01:26.884: INFO: stdout: "Name:         kubectl-4090\nLabels:       e2e-framework=kubectl\n              e2e-run=f3f539e6-417f-4a68-8004-5f74a75285d0\n              kubernetes.io/metadata.name=kubectl-4090\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  9 17:01:26.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4090" for this suite. 05/09/23 17:01:26.89
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:01:26.898
May  9 17:01:26.898: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename resourcequota 05/09/23 17:01:26.899
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:26.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:26.921
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 05/09/23 17:01:26.928
STEP: Ensuring ResourceQuota status is calculated 05/09/23 17:01:26.933
STEP: Creating a ResourceQuota with not terminating scope 05/09/23 17:01:28.94
STEP: Ensuring ResourceQuota status is calculated 05/09/23 17:01:28.946
STEP: Creating a long running pod 05/09/23 17:01:30.953
STEP: Ensuring resource quota with not terminating scope captures the pod usage 05/09/23 17:01:30.97
STEP: Ensuring resource quota with terminating scope ignored the pod usage 05/09/23 17:01:32.976
STEP: Deleting the pod 05/09/23 17:01:34.983
STEP: Ensuring resource quota status released the pod usage 05/09/23 17:01:35.009
STEP: Creating a terminating pod 05/09/23 17:01:37.015
STEP: Ensuring resource quota with terminating scope captures the pod usage 05/09/23 17:01:37.027
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 05/09/23 17:01:39.035
STEP: Deleting the pod 05/09/23 17:01:41.042
STEP: Ensuring resource quota status released the pod usage 05/09/23 17:01:41.062
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  9 17:01:43.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5917" for this suite. 05/09/23 17:01:43.076
------------------------------
â€¢ [SLOW TEST] [16.186 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:01:26.898
    May  9 17:01:26.898: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename resourcequota 05/09/23 17:01:26.899
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:26.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:26.921
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 05/09/23 17:01:26.928
    STEP: Ensuring ResourceQuota status is calculated 05/09/23 17:01:26.933
    STEP: Creating a ResourceQuota with not terminating scope 05/09/23 17:01:28.94
    STEP: Ensuring ResourceQuota status is calculated 05/09/23 17:01:28.946
    STEP: Creating a long running pod 05/09/23 17:01:30.953
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 05/09/23 17:01:30.97
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 05/09/23 17:01:32.976
    STEP: Deleting the pod 05/09/23 17:01:34.983
    STEP: Ensuring resource quota status released the pod usage 05/09/23 17:01:35.009
    STEP: Creating a terminating pod 05/09/23 17:01:37.015
    STEP: Ensuring resource quota with terminating scope captures the pod usage 05/09/23 17:01:37.027
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 05/09/23 17:01:39.035
    STEP: Deleting the pod 05/09/23 17:01:41.042
    STEP: Ensuring resource quota status released the pod usage 05/09/23 17:01:41.062
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  9 17:01:43.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5917" for this suite. 05/09/23 17:01:43.076
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:01:43.085
May  9 17:01:43.085: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename secrets 05/09/23 17:01:43.086
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:43.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:43.114
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-3043/secret-test-d56b169d-4a37-4e7e-b746-818037bdeac0 05/09/23 17:01:43.118
STEP: Creating a pod to test consume secrets 05/09/23 17:01:43.123
May  9 17:01:43.134: INFO: Waiting up to 5m0s for pod "pod-configmaps-8b06000f-5e7c-4dad-b103-473c526fb116" in namespace "secrets-3043" to be "Succeeded or Failed"
May  9 17:01:43.137: INFO: Pod "pod-configmaps-8b06000f-5e7c-4dad-b103-473c526fb116": Phase="Pending", Reason="", readiness=false. Elapsed: 3.771581ms
May  9 17:01:45.145: INFO: Pod "pod-configmaps-8b06000f-5e7c-4dad-b103-473c526fb116": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011766448s
May  9 17:01:47.146: INFO: Pod "pod-configmaps-8b06000f-5e7c-4dad-b103-473c526fb116": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012038774s
STEP: Saw pod success 05/09/23 17:01:47.146
May  9 17:01:47.146: INFO: Pod "pod-configmaps-8b06000f-5e7c-4dad-b103-473c526fb116" satisfied condition "Succeeded or Failed"
May  9 17:01:47.151: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-configmaps-8b06000f-5e7c-4dad-b103-473c526fb116 container env-test: <nil>
STEP: delete the pod 05/09/23 17:01:47.162
May  9 17:01:47.180: INFO: Waiting for pod pod-configmaps-8b06000f-5e7c-4dad-b103-473c526fb116 to disappear
May  9 17:01:47.184: INFO: Pod pod-configmaps-8b06000f-5e7c-4dad-b103-473c526fb116 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
May  9 17:01:47.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3043" for this suite. 05/09/23 17:01:47.193
------------------------------
â€¢ [4.115 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:01:43.085
    May  9 17:01:43.085: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename secrets 05/09/23 17:01:43.086
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:43.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:43.114
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-3043/secret-test-d56b169d-4a37-4e7e-b746-818037bdeac0 05/09/23 17:01:43.118
    STEP: Creating a pod to test consume secrets 05/09/23 17:01:43.123
    May  9 17:01:43.134: INFO: Waiting up to 5m0s for pod "pod-configmaps-8b06000f-5e7c-4dad-b103-473c526fb116" in namespace "secrets-3043" to be "Succeeded or Failed"
    May  9 17:01:43.137: INFO: Pod "pod-configmaps-8b06000f-5e7c-4dad-b103-473c526fb116": Phase="Pending", Reason="", readiness=false. Elapsed: 3.771581ms
    May  9 17:01:45.145: INFO: Pod "pod-configmaps-8b06000f-5e7c-4dad-b103-473c526fb116": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011766448s
    May  9 17:01:47.146: INFO: Pod "pod-configmaps-8b06000f-5e7c-4dad-b103-473c526fb116": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012038774s
    STEP: Saw pod success 05/09/23 17:01:47.146
    May  9 17:01:47.146: INFO: Pod "pod-configmaps-8b06000f-5e7c-4dad-b103-473c526fb116" satisfied condition "Succeeded or Failed"
    May  9 17:01:47.151: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-configmaps-8b06000f-5e7c-4dad-b103-473c526fb116 container env-test: <nil>
    STEP: delete the pod 05/09/23 17:01:47.162
    May  9 17:01:47.180: INFO: Waiting for pod pod-configmaps-8b06000f-5e7c-4dad-b103-473c526fb116 to disappear
    May  9 17:01:47.184: INFO: Pod pod-configmaps-8b06000f-5e7c-4dad-b103-473c526fb116 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    May  9 17:01:47.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3043" for this suite. 05/09/23 17:01:47.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:01:47.201
May  9 17:01:47.201: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename csiinlinevolumes 05/09/23 17:01:47.202
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:47.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:47.223
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 05/09/23 17:01:47.23
STEP: getting 05/09/23 17:01:47.258
STEP: listing 05/09/23 17:01:47.268
STEP: deleting 05/09/23 17:01:47.274
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
May  9 17:01:47.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-6721" for this suite. 05/09/23 17:01:47.305
------------------------------
â€¢ [0.115 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:01:47.201
    May  9 17:01:47.201: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename csiinlinevolumes 05/09/23 17:01:47.202
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:47.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:47.223
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 05/09/23 17:01:47.23
    STEP: getting 05/09/23 17:01:47.258
    STEP: listing 05/09/23 17:01:47.268
    STEP: deleting 05/09/23 17:01:47.274
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    May  9 17:01:47.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-6721" for this suite. 05/09/23 17:01:47.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:01:47.32
May  9 17:01:47.320: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename disruption 05/09/23 17:01:47.321
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:47.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:47.342
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:01:47.347
May  9 17:01:47.347: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename disruption-2 05/09/23 17:01:47.348
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:47.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:47.369
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 05/09/23 17:01:47.378
STEP: Waiting for the pdb to be processed 05/09/23 17:01:49.397
STEP: Waiting for the pdb to be processed 05/09/23 17:01:51.422
STEP: listing a collection of PDBs across all namespaces 05/09/23 17:01:53.434
STEP: listing a collection of PDBs in namespace disruption-5997 05/09/23 17:01:53.44
STEP: deleting a collection of PDBs 05/09/23 17:01:53.445
STEP: Waiting for the PDB collection to be deleted 05/09/23 17:01:53.463
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
May  9 17:01:53.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May  9 17:01:53.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-991" for this suite. 05/09/23 17:01:53.481
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5997" for this suite. 05/09/23 17:01:53.489
------------------------------
â€¢ [SLOW TEST] [6.179 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:01:47.32
    May  9 17:01:47.320: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename disruption 05/09/23 17:01:47.321
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:47.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:47.342
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:01:47.347
    May  9 17:01:47.347: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename disruption-2 05/09/23 17:01:47.348
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:47.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:47.369
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 05/09/23 17:01:47.378
    STEP: Waiting for the pdb to be processed 05/09/23 17:01:49.397
    STEP: Waiting for the pdb to be processed 05/09/23 17:01:51.422
    STEP: listing a collection of PDBs across all namespaces 05/09/23 17:01:53.434
    STEP: listing a collection of PDBs in namespace disruption-5997 05/09/23 17:01:53.44
    STEP: deleting a collection of PDBs 05/09/23 17:01:53.445
    STEP: Waiting for the PDB collection to be deleted 05/09/23 17:01:53.463
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    May  9 17:01:53.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May  9 17:01:53.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-991" for this suite. 05/09/23 17:01:53.481
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5997" for this suite. 05/09/23 17:01:53.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:01:53.502
May  9 17:01:53.502: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename container-runtime 05/09/23 17:01:53.503
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:53.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:53.526
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 05/09/23 17:01:53.531
STEP: wait for the container to reach Succeeded 05/09/23 17:01:53.54
STEP: get the container status 05/09/23 17:01:57.574
STEP: the container should be terminated 05/09/23 17:01:57.58
STEP: the termination message should be set 05/09/23 17:01:57.58
May  9 17:01:57.580: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 05/09/23 17:01:57.58
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May  9 17:01:57.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8848" for this suite. 05/09/23 17:01:57.61
------------------------------
â€¢ [4.116 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:01:53.502
    May  9 17:01:53.502: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename container-runtime 05/09/23 17:01:53.503
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:53.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:53.526
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 05/09/23 17:01:53.531
    STEP: wait for the container to reach Succeeded 05/09/23 17:01:53.54
    STEP: get the container status 05/09/23 17:01:57.574
    STEP: the container should be terminated 05/09/23 17:01:57.58
    STEP: the termination message should be set 05/09/23 17:01:57.58
    May  9 17:01:57.580: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 05/09/23 17:01:57.58
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May  9 17:01:57.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8848" for this suite. 05/09/23 17:01:57.61
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:01:57.628
May  9 17:01:57.628: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 17:01:57.629
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:57.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:57.65
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-051d3e3a-fef9-4d81-84e5-40066a15cc43 05/09/23 17:01:57.655
STEP: Creating a pod to test consume secrets 05/09/23 17:01:57.662
May  9 17:01:57.674: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d1f61a97-0188-4e4e-b003-328240a57f24" in namespace "projected-7508" to be "Succeeded or Failed"
May  9 17:01:57.680: INFO: Pod "pod-projected-secrets-d1f61a97-0188-4e4e-b003-328240a57f24": Phase="Pending", Reason="", readiness=false. Elapsed: 5.514828ms
May  9 17:01:59.688: INFO: Pod "pod-projected-secrets-d1f61a97-0188-4e4e-b003-328240a57f24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013710936s
May  9 17:02:01.688: INFO: Pod "pod-projected-secrets-d1f61a97-0188-4e4e-b003-328240a57f24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013646229s
STEP: Saw pod success 05/09/23 17:02:01.688
May  9 17:02:01.688: INFO: Pod "pod-projected-secrets-d1f61a97-0188-4e4e-b003-328240a57f24" satisfied condition "Succeeded or Failed"
May  9 17:02:01.694: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-secrets-d1f61a97-0188-4e4e-b003-328240a57f24 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/09/23 17:02:01.706
May  9 17:02:01.730: INFO: Waiting for pod pod-projected-secrets-d1f61a97-0188-4e4e-b003-328240a57f24 to disappear
May  9 17:02:01.735: INFO: Pod pod-projected-secrets-d1f61a97-0188-4e4e-b003-328240a57f24 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May  9 17:02:01.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7508" for this suite. 05/09/23 17:02:01.742
------------------------------
â€¢ [4.122 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:01:57.628
    May  9 17:01:57.628: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 17:01:57.629
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:01:57.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:01:57.65
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-051d3e3a-fef9-4d81-84e5-40066a15cc43 05/09/23 17:01:57.655
    STEP: Creating a pod to test consume secrets 05/09/23 17:01:57.662
    May  9 17:01:57.674: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d1f61a97-0188-4e4e-b003-328240a57f24" in namespace "projected-7508" to be "Succeeded or Failed"
    May  9 17:01:57.680: INFO: Pod "pod-projected-secrets-d1f61a97-0188-4e4e-b003-328240a57f24": Phase="Pending", Reason="", readiness=false. Elapsed: 5.514828ms
    May  9 17:01:59.688: INFO: Pod "pod-projected-secrets-d1f61a97-0188-4e4e-b003-328240a57f24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013710936s
    May  9 17:02:01.688: INFO: Pod "pod-projected-secrets-d1f61a97-0188-4e4e-b003-328240a57f24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013646229s
    STEP: Saw pod success 05/09/23 17:02:01.688
    May  9 17:02:01.688: INFO: Pod "pod-projected-secrets-d1f61a97-0188-4e4e-b003-328240a57f24" satisfied condition "Succeeded or Failed"
    May  9 17:02:01.694: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-secrets-d1f61a97-0188-4e4e-b003-328240a57f24 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 17:02:01.706
    May  9 17:02:01.730: INFO: Waiting for pod pod-projected-secrets-d1f61a97-0188-4e4e-b003-328240a57f24 to disappear
    May  9 17:02:01.735: INFO: Pod pod-projected-secrets-d1f61a97-0188-4e4e-b003-328240a57f24 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May  9 17:02:01.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7508" for this suite. 05/09/23 17:02:01.742
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:02:01.751
May  9 17:02:01.751: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 17:02:01.752
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:01.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:01.777
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-3039d576-68ad-4e76-8fba-0a053a6e9567 05/09/23 17:02:01.783
STEP: Creating a pod to test consume secrets 05/09/23 17:02:01.789
May  9 17:02:01.800: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-47d1c4d9-437b-4314-bce0-b0f963200019" in namespace "projected-1236" to be "Succeeded or Failed"
May  9 17:02:01.806: INFO: Pod "pod-projected-secrets-47d1c4d9-437b-4314-bce0-b0f963200019": Phase="Pending", Reason="", readiness=false. Elapsed: 5.073573ms
May  9 17:02:03.812: INFO: Pod "pod-projected-secrets-47d1c4d9-437b-4314-bce0-b0f963200019": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011274258s
May  9 17:02:05.813: INFO: Pod "pod-projected-secrets-47d1c4d9-437b-4314-bce0-b0f963200019": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012378603s
STEP: Saw pod success 05/09/23 17:02:05.813
May  9 17:02:05.813: INFO: Pod "pod-projected-secrets-47d1c4d9-437b-4314-bce0-b0f963200019" satisfied condition "Succeeded or Failed"
May  9 17:02:05.824: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-secrets-47d1c4d9-437b-4314-bce0-b0f963200019 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/09/23 17:02:05.835
May  9 17:02:05.852: INFO: Waiting for pod pod-projected-secrets-47d1c4d9-437b-4314-bce0-b0f963200019 to disappear
May  9 17:02:05.857: INFO: Pod pod-projected-secrets-47d1c4d9-437b-4314-bce0-b0f963200019 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May  9 17:02:05.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1236" for this suite. 05/09/23 17:02:05.863
------------------------------
â€¢ [4.120 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:02:01.751
    May  9 17:02:01.751: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 17:02:01.752
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:01.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:01.777
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-3039d576-68ad-4e76-8fba-0a053a6e9567 05/09/23 17:02:01.783
    STEP: Creating a pod to test consume secrets 05/09/23 17:02:01.789
    May  9 17:02:01.800: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-47d1c4d9-437b-4314-bce0-b0f963200019" in namespace "projected-1236" to be "Succeeded or Failed"
    May  9 17:02:01.806: INFO: Pod "pod-projected-secrets-47d1c4d9-437b-4314-bce0-b0f963200019": Phase="Pending", Reason="", readiness=false. Elapsed: 5.073573ms
    May  9 17:02:03.812: INFO: Pod "pod-projected-secrets-47d1c4d9-437b-4314-bce0-b0f963200019": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011274258s
    May  9 17:02:05.813: INFO: Pod "pod-projected-secrets-47d1c4d9-437b-4314-bce0-b0f963200019": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012378603s
    STEP: Saw pod success 05/09/23 17:02:05.813
    May  9 17:02:05.813: INFO: Pod "pod-projected-secrets-47d1c4d9-437b-4314-bce0-b0f963200019" satisfied condition "Succeeded or Failed"
    May  9 17:02:05.824: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod pod-projected-secrets-47d1c4d9-437b-4314-bce0-b0f963200019 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/09/23 17:02:05.835
    May  9 17:02:05.852: INFO: Waiting for pod pod-projected-secrets-47d1c4d9-437b-4314-bce0-b0f963200019 to disappear
    May  9 17:02:05.857: INFO: Pod pod-projected-secrets-47d1c4d9-437b-4314-bce0-b0f963200019 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May  9 17:02:05.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1236" for this suite. 05/09/23 17:02:05.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:02:05.871
May  9 17:02:05.871: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename svcaccounts 05/09/23 17:02:05.872
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:05.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:05.893
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 05/09/23 17:02:05.898
STEP: watching for the ServiceAccount to be added 05/09/23 17:02:05.91
STEP: patching the ServiceAccount 05/09/23 17:02:05.912
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 05/09/23 17:02:05.919
STEP: deleting the ServiceAccount 05/09/23 17:02:05.924
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May  9 17:02:05.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8011" for this suite. 05/09/23 17:02:05.95
------------------------------
â€¢ [0.088 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:02:05.871
    May  9 17:02:05.871: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename svcaccounts 05/09/23 17:02:05.872
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:05.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:05.893
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 05/09/23 17:02:05.898
    STEP: watching for the ServiceAccount to be added 05/09/23 17:02:05.91
    STEP: patching the ServiceAccount 05/09/23 17:02:05.912
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 05/09/23 17:02:05.919
    STEP: deleting the ServiceAccount 05/09/23 17:02:05.924
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May  9 17:02:05.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8011" for this suite. 05/09/23 17:02:05.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:02:05.963
May  9 17:02:05.963: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename events 05/09/23 17:02:05.964
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:05.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:05.983
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 05/09/23 17:02:05.987
STEP: listing all events in all namespaces 05/09/23 17:02:05.994
STEP: patching the test event 05/09/23 17:02:06
STEP: fetching the test event 05/09/23 17:02:06.01
STEP: updating the test event 05/09/23 17:02:06.018
STEP: getting the test event 05/09/23 17:02:06.034
STEP: deleting the test event 05/09/23 17:02:06.04
STEP: listing all events in all namespaces 05/09/23 17:02:06.051
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
May  9 17:02:06.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-2508" for this suite. 05/09/23 17:02:06.066
------------------------------
â€¢ [0.112 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:02:05.963
    May  9 17:02:05.963: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename events 05/09/23 17:02:05.964
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:05.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:05.983
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 05/09/23 17:02:05.987
    STEP: listing all events in all namespaces 05/09/23 17:02:05.994
    STEP: patching the test event 05/09/23 17:02:06
    STEP: fetching the test event 05/09/23 17:02:06.01
    STEP: updating the test event 05/09/23 17:02:06.018
    STEP: getting the test event 05/09/23 17:02:06.034
    STEP: deleting the test event 05/09/23 17:02:06.04
    STEP: listing all events in all namespaces 05/09/23 17:02:06.051
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    May  9 17:02:06.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-2508" for this suite. 05/09/23 17:02:06.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:02:06.076
May  9 17:02:06.077: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename var-expansion 05/09/23 17:02:06.077
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:06.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:06.099
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 05/09/23 17:02:06.104
May  9 17:02:06.118: INFO: Waiting up to 5m0s for pod "var-expansion-c24030c7-daf6-4e4c-8fa9-9b308608b01d" in namespace "var-expansion-5973" to be "Succeeded or Failed"
May  9 17:02:06.125: INFO: Pod "var-expansion-c24030c7-daf6-4e4c-8fa9-9b308608b01d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.854611ms
May  9 17:02:08.132: INFO: Pod "var-expansion-c24030c7-daf6-4e4c-8fa9-9b308608b01d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013355481s
May  9 17:02:10.133: INFO: Pod "var-expansion-c24030c7-daf6-4e4c-8fa9-9b308608b01d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014649247s
STEP: Saw pod success 05/09/23 17:02:10.133
May  9 17:02:10.133: INFO: Pod "var-expansion-c24030c7-daf6-4e4c-8fa9-9b308608b01d" satisfied condition "Succeeded or Failed"
May  9 17:02:10.138: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod var-expansion-c24030c7-daf6-4e4c-8fa9-9b308608b01d container dapi-container: <nil>
STEP: delete the pod 05/09/23 17:02:10.149
May  9 17:02:10.165: INFO: Waiting for pod var-expansion-c24030c7-daf6-4e4c-8fa9-9b308608b01d to disappear
May  9 17:02:10.169: INFO: Pod var-expansion-c24030c7-daf6-4e4c-8fa9-9b308608b01d no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May  9 17:02:10.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5973" for this suite. 05/09/23 17:02:10.175
------------------------------
â€¢ [4.108 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:02:06.076
    May  9 17:02:06.077: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename var-expansion 05/09/23 17:02:06.077
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:06.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:06.099
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 05/09/23 17:02:06.104
    May  9 17:02:06.118: INFO: Waiting up to 5m0s for pod "var-expansion-c24030c7-daf6-4e4c-8fa9-9b308608b01d" in namespace "var-expansion-5973" to be "Succeeded or Failed"
    May  9 17:02:06.125: INFO: Pod "var-expansion-c24030c7-daf6-4e4c-8fa9-9b308608b01d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.854611ms
    May  9 17:02:08.132: INFO: Pod "var-expansion-c24030c7-daf6-4e4c-8fa9-9b308608b01d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013355481s
    May  9 17:02:10.133: INFO: Pod "var-expansion-c24030c7-daf6-4e4c-8fa9-9b308608b01d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014649247s
    STEP: Saw pod success 05/09/23 17:02:10.133
    May  9 17:02:10.133: INFO: Pod "var-expansion-c24030c7-daf6-4e4c-8fa9-9b308608b01d" satisfied condition "Succeeded or Failed"
    May  9 17:02:10.138: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod var-expansion-c24030c7-daf6-4e4c-8fa9-9b308608b01d container dapi-container: <nil>
    STEP: delete the pod 05/09/23 17:02:10.149
    May  9 17:02:10.165: INFO: Waiting for pod var-expansion-c24030c7-daf6-4e4c-8fa9-9b308608b01d to disappear
    May  9 17:02:10.169: INFO: Pod var-expansion-c24030c7-daf6-4e4c-8fa9-9b308608b01d no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May  9 17:02:10.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5973" for this suite. 05/09/23 17:02:10.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:02:10.186
May  9 17:02:10.186: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename limitrange 05/09/23 17:02:10.187
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:10.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:10.209
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 05/09/23 17:02:10.213
STEP: Setting up watch 05/09/23 17:02:10.213
STEP: Submitting a LimitRange 05/09/23 17:02:10.327
STEP: Verifying LimitRange creation was observed 05/09/23 17:02:10.335
STEP: Fetching the LimitRange to ensure it has proper values 05/09/23 17:02:10.336
May  9 17:02:10.343: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May  9 17:02:10.343: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 05/09/23 17:02:10.343
STEP: Ensuring Pod has resource requirements applied from LimitRange 05/09/23 17:02:10.352
May  9 17:02:10.356: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May  9 17:02:10.356: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 05/09/23 17:02:10.357
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 05/09/23 17:02:10.37
May  9 17:02:10.375: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May  9 17:02:10.375: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 05/09/23 17:02:10.375
STEP: Failing to create a Pod with more than max resources 05/09/23 17:02:10.379
STEP: Updating a LimitRange 05/09/23 17:02:10.382
STEP: Verifying LimitRange updating is effective 05/09/23 17:02:10.397
STEP: Creating a Pod with less than former min resources 05/09/23 17:02:12.404
STEP: Failing to create a Pod with more than max resources 05/09/23 17:02:12.413
STEP: Deleting a LimitRange 05/09/23 17:02:12.417
STEP: Verifying the LimitRange was deleted 05/09/23 17:02:12.426
May  9 17:02:17.471: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 05/09/23 17:02:17.471
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
May  9 17:02:17.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-882" for this suite. 05/09/23 17:02:17.504
------------------------------
â€¢ [SLOW TEST] [7.328 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:02:10.186
    May  9 17:02:10.186: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename limitrange 05/09/23 17:02:10.187
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:10.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:10.209
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 05/09/23 17:02:10.213
    STEP: Setting up watch 05/09/23 17:02:10.213
    STEP: Submitting a LimitRange 05/09/23 17:02:10.327
    STEP: Verifying LimitRange creation was observed 05/09/23 17:02:10.335
    STEP: Fetching the LimitRange to ensure it has proper values 05/09/23 17:02:10.336
    May  9 17:02:10.343: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    May  9 17:02:10.343: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 05/09/23 17:02:10.343
    STEP: Ensuring Pod has resource requirements applied from LimitRange 05/09/23 17:02:10.352
    May  9 17:02:10.356: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    May  9 17:02:10.356: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 05/09/23 17:02:10.357
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 05/09/23 17:02:10.37
    May  9 17:02:10.375: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    May  9 17:02:10.375: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 05/09/23 17:02:10.375
    STEP: Failing to create a Pod with more than max resources 05/09/23 17:02:10.379
    STEP: Updating a LimitRange 05/09/23 17:02:10.382
    STEP: Verifying LimitRange updating is effective 05/09/23 17:02:10.397
    STEP: Creating a Pod with less than former min resources 05/09/23 17:02:12.404
    STEP: Failing to create a Pod with more than max resources 05/09/23 17:02:12.413
    STEP: Deleting a LimitRange 05/09/23 17:02:12.417
    STEP: Verifying the LimitRange was deleted 05/09/23 17:02:12.426
    May  9 17:02:17.471: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 05/09/23 17:02:17.471
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    May  9 17:02:17.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-882" for this suite. 05/09/23 17:02:17.504
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:02:17.514
May  9 17:02:17.514: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename kubectl 05/09/23 17:02:17.515
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:17.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:17.538
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 05/09/23 17:02:17.543
May  9 17:02:17.543: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6409 proxy --unix-socket=/tmp/kubectl-proxy-unix3944212440/test'
STEP: retrieving proxy /api/ output 05/09/23 17:02:17.588
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  9 17:02:17.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6409" for this suite. 05/09/23 17:02:17.595
------------------------------
â€¢ [0.092 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:02:17.514
    May  9 17:02:17.514: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename kubectl 05/09/23 17:02:17.515
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:17.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:17.538
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 05/09/23 17:02:17.543
    May  9 17:02:17.543: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=kubectl-6409 proxy --unix-socket=/tmp/kubectl-proxy-unix3944212440/test'
    STEP: retrieving proxy /api/ output 05/09/23 17:02:17.588
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  9 17:02:17.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6409" for this suite. 05/09/23 17:02:17.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:02:17.608
May  9 17:02:17.608: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename watch 05/09/23 17:02:17.609
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:17.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:17.63
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 05/09/23 17:02:17.634
STEP: starting a background goroutine to produce watch events 05/09/23 17:02:17.638
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 05/09/23 17:02:17.638
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May  9 17:02:20.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1488" for this suite. 05/09/23 17:02:20.466
------------------------------
â€¢ [2.910 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:02:17.608
    May  9 17:02:17.608: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename watch 05/09/23 17:02:17.609
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:17.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:17.63
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 05/09/23 17:02:17.634
    STEP: starting a background goroutine to produce watch events 05/09/23 17:02:17.638
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 05/09/23 17:02:17.638
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May  9 17:02:20.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1488" for this suite. 05/09/23 17:02:20.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:02:20.518
May  9 17:02:20.518: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 17:02:20.519
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:20.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:20.538
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 05/09/23 17:02:20.542
May  9 17:02:20.552: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab6560ac-9f72-42b3-aeaa-452449f2da9a" in namespace "projected-357" to be "Succeeded or Failed"
May  9 17:02:20.558: INFO: Pod "downwardapi-volume-ab6560ac-9f72-42b3-aeaa-452449f2da9a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.855122ms
May  9 17:02:22.564: INFO: Pod "downwardapi-volume-ab6560ac-9f72-42b3-aeaa-452449f2da9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011596412s
May  9 17:02:24.568: INFO: Pod "downwardapi-volume-ab6560ac-9f72-42b3-aeaa-452449f2da9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01594051s
STEP: Saw pod success 05/09/23 17:02:24.568
May  9 17:02:24.568: INFO: Pod "downwardapi-volume-ab6560ac-9f72-42b3-aeaa-452449f2da9a" satisfied condition "Succeeded or Failed"
May  9 17:02:24.574: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-ab6560ac-9f72-42b3-aeaa-452449f2da9a container client-container: <nil>
STEP: delete the pod 05/09/23 17:02:24.585
May  9 17:02:24.607: INFO: Waiting for pod downwardapi-volume-ab6560ac-9f72-42b3-aeaa-452449f2da9a to disappear
May  9 17:02:24.614: INFO: Pod downwardapi-volume-ab6560ac-9f72-42b3-aeaa-452449f2da9a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  9 17:02:24.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-357" for this suite. 05/09/23 17:02:24.62
------------------------------
â€¢ [4.112 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:02:20.518
    May  9 17:02:20.518: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 17:02:20.519
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:20.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:20.538
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 05/09/23 17:02:20.542
    May  9 17:02:20.552: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab6560ac-9f72-42b3-aeaa-452449f2da9a" in namespace "projected-357" to be "Succeeded or Failed"
    May  9 17:02:20.558: INFO: Pod "downwardapi-volume-ab6560ac-9f72-42b3-aeaa-452449f2da9a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.855122ms
    May  9 17:02:22.564: INFO: Pod "downwardapi-volume-ab6560ac-9f72-42b3-aeaa-452449f2da9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011596412s
    May  9 17:02:24.568: INFO: Pod "downwardapi-volume-ab6560ac-9f72-42b3-aeaa-452449f2da9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01594051s
    STEP: Saw pod success 05/09/23 17:02:24.568
    May  9 17:02:24.568: INFO: Pod "downwardapi-volume-ab6560ac-9f72-42b3-aeaa-452449f2da9a" satisfied condition "Succeeded or Failed"
    May  9 17:02:24.574: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-ab6560ac-9f72-42b3-aeaa-452449f2da9a container client-container: <nil>
    STEP: delete the pod 05/09/23 17:02:24.585
    May  9 17:02:24.607: INFO: Waiting for pod downwardapi-volume-ab6560ac-9f72-42b3-aeaa-452449f2da9a to disappear
    May  9 17:02:24.614: INFO: Pod downwardapi-volume-ab6560ac-9f72-42b3-aeaa-452449f2da9a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  9 17:02:24.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-357" for this suite. 05/09/23 17:02:24.62
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:02:24.63
May  9 17:02:24.630: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename services 05/09/23 17:02:24.631
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:24.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:24.665
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3118 05/09/23 17:02:24.67
STEP: changing the ExternalName service to type=NodePort 05/09/23 17:02:24.677
STEP: creating replication controller externalname-service in namespace services-3118 05/09/23 17:02:24.704
I0509 17:02:24.716084      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3118, replica count: 2
I0509 17:02:27.767628      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  9 17:02:27.767: INFO: Creating new exec pod
May  9 17:02:27.776: INFO: Waiting up to 5m0s for pod "execpodxrf26" in namespace "services-3118" to be "running"
May  9 17:02:27.782: INFO: Pod "execpodxrf26": Phase="Pending", Reason="", readiness=false. Elapsed: 6.721032ms
May  9 17:02:29.788: INFO: Pod "execpodxrf26": Phase="Running", Reason="", readiness=true. Elapsed: 2.011957284s
May  9 17:02:29.788: INFO: Pod "execpodxrf26" satisfied condition "running"
May  9 17:02:30.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-3118 exec execpodxrf26 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
May  9 17:02:31.006: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  9 17:02:31.006: INFO: stdout: ""
May  9 17:02:31.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-3118 exec execpodxrf26 -- /bin/sh -x -c nc -v -z -w 2 10.3.84.143 80'
May  9 17:02:31.248: INFO: stderr: "+ nc -v -z -w 2 10.3.84.143 80\nConnection to 10.3.84.143 80 port [tcp/http] succeeded!\n"
May  9 17:02:31.248: INFO: stdout: ""
May  9 17:02:31.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-3118 exec execpodxrf26 -- /bin/sh -x -c nc -v -z -w 2 51.68.91.222 30968'
May  9 17:02:31.485: INFO: stderr: "+ nc -v -z -w 2 51.68.91.222 30968\nConnection to 51.68.91.222 30968 port [tcp/*] succeeded!\n"
May  9 17:02:31.485: INFO: stdout: ""
May  9 17:02:31.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-3118 exec execpodxrf26 -- /bin/sh -x -c nc -v -z -w 2 51.68.93.170 30968'
May  9 17:02:31.726: INFO: stderr: "+ nc -v -z -w 2 51.68.93.170 30968\nConnection to 51.68.93.170 30968 port [tcp/*] succeeded!\n"
May  9 17:02:31.726: INFO: stdout: ""
May  9 17:02:31.726: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  9 17:02:31.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3118" for this suite. 05/09/23 17:02:31.781
------------------------------
â€¢ [SLOW TEST] [7.163 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:02:24.63
    May  9 17:02:24.630: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename services 05/09/23 17:02:24.631
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:24.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:24.665
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-3118 05/09/23 17:02:24.67
    STEP: changing the ExternalName service to type=NodePort 05/09/23 17:02:24.677
    STEP: creating replication controller externalname-service in namespace services-3118 05/09/23 17:02:24.704
    I0509 17:02:24.716084      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3118, replica count: 2
    I0509 17:02:27.767628      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  9 17:02:27.767: INFO: Creating new exec pod
    May  9 17:02:27.776: INFO: Waiting up to 5m0s for pod "execpodxrf26" in namespace "services-3118" to be "running"
    May  9 17:02:27.782: INFO: Pod "execpodxrf26": Phase="Pending", Reason="", readiness=false. Elapsed: 6.721032ms
    May  9 17:02:29.788: INFO: Pod "execpodxrf26": Phase="Running", Reason="", readiness=true. Elapsed: 2.011957284s
    May  9 17:02:29.788: INFO: Pod "execpodxrf26" satisfied condition "running"
    May  9 17:02:30.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-3118 exec execpodxrf26 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    May  9 17:02:31.006: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    May  9 17:02:31.006: INFO: stdout: ""
    May  9 17:02:31.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-3118 exec execpodxrf26 -- /bin/sh -x -c nc -v -z -w 2 10.3.84.143 80'
    May  9 17:02:31.248: INFO: stderr: "+ nc -v -z -w 2 10.3.84.143 80\nConnection to 10.3.84.143 80 port [tcp/http] succeeded!\n"
    May  9 17:02:31.248: INFO: stdout: ""
    May  9 17:02:31.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-3118 exec execpodxrf26 -- /bin/sh -x -c nc -v -z -w 2 51.68.91.222 30968'
    May  9 17:02:31.485: INFO: stderr: "+ nc -v -z -w 2 51.68.91.222 30968\nConnection to 51.68.91.222 30968 port [tcp/*] succeeded!\n"
    May  9 17:02:31.485: INFO: stdout: ""
    May  9 17:02:31.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-818321586 --namespace=services-3118 exec execpodxrf26 -- /bin/sh -x -c nc -v -z -w 2 51.68.93.170 30968'
    May  9 17:02:31.726: INFO: stderr: "+ nc -v -z -w 2 51.68.93.170 30968\nConnection to 51.68.93.170 30968 port [tcp/*] succeeded!\n"
    May  9 17:02:31.726: INFO: stdout: ""
    May  9 17:02:31.726: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  9 17:02:31.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3118" for this suite. 05/09/23 17:02:31.781
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:02:31.793
May  9 17:02:31.793: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename runtimeclass 05/09/23 17:02:31.795
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:31.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:31.813
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 05/09/23 17:02:31.818
STEP: getting /apis/node.k8s.io 05/09/23 17:02:31.823
STEP: getting /apis/node.k8s.io/v1 05/09/23 17:02:31.825
STEP: creating 05/09/23 17:02:31.828
STEP: watching 05/09/23 17:02:31.853
May  9 17:02:31.853: INFO: starting watch
STEP: getting 05/09/23 17:02:31.861
STEP: listing 05/09/23 17:02:31.866
STEP: patching 05/09/23 17:02:31.87
STEP: updating 05/09/23 17:02:31.876
May  9 17:02:31.882: INFO: waiting for watch events with expected annotations
STEP: deleting 05/09/23 17:02:31.882
STEP: deleting a collection 05/09/23 17:02:31.897
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May  9 17:02:31.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3414" for this suite. 05/09/23 17:02:31.929
------------------------------
â€¢ [0.146 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:02:31.793
    May  9 17:02:31.793: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename runtimeclass 05/09/23 17:02:31.795
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:31.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:31.813
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 05/09/23 17:02:31.818
    STEP: getting /apis/node.k8s.io 05/09/23 17:02:31.823
    STEP: getting /apis/node.k8s.io/v1 05/09/23 17:02:31.825
    STEP: creating 05/09/23 17:02:31.828
    STEP: watching 05/09/23 17:02:31.853
    May  9 17:02:31.853: INFO: starting watch
    STEP: getting 05/09/23 17:02:31.861
    STEP: listing 05/09/23 17:02:31.866
    STEP: patching 05/09/23 17:02:31.87
    STEP: updating 05/09/23 17:02:31.876
    May  9 17:02:31.882: INFO: waiting for watch events with expected annotations
    STEP: deleting 05/09/23 17:02:31.882
    STEP: deleting a collection 05/09/23 17:02:31.897
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May  9 17:02:31.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3414" for this suite. 05/09/23 17:02:31.929
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:02:31.94
May  9 17:02:31.940: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename projected 05/09/23 17:02:31.941
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:31.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:31.958
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 05/09/23 17:02:31.963
May  9 17:02:31.981: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3bd06398-7724-4732-8f3b-52d03f426391" in namespace "projected-2494" to be "Succeeded or Failed"
May  9 17:02:31.990: INFO: Pod "downwardapi-volume-3bd06398-7724-4732-8f3b-52d03f426391": Phase="Pending", Reason="", readiness=false. Elapsed: 8.930346ms
May  9 17:02:33.997: INFO: Pod "downwardapi-volume-3bd06398-7724-4732-8f3b-52d03f426391": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016182359s
May  9 17:02:35.997: INFO: Pod "downwardapi-volume-3bd06398-7724-4732-8f3b-52d03f426391": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015938643s
STEP: Saw pod success 05/09/23 17:02:35.997
May  9 17:02:35.997: INFO: Pod "downwardapi-volume-3bd06398-7724-4732-8f3b-52d03f426391" satisfied condition "Succeeded or Failed"
May  9 17:02:36.002: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-3bd06398-7724-4732-8f3b-52d03f426391 container client-container: <nil>
STEP: delete the pod 05/09/23 17:02:36.016
May  9 17:02:36.035: INFO: Waiting for pod downwardapi-volume-3bd06398-7724-4732-8f3b-52d03f426391 to disappear
May  9 17:02:36.039: INFO: Pod downwardapi-volume-3bd06398-7724-4732-8f3b-52d03f426391 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  9 17:02:36.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2494" for this suite. 05/09/23 17:02:36.045
------------------------------
â€¢ [4.116 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:02:31.94
    May  9 17:02:31.940: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename projected 05/09/23 17:02:31.941
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:31.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:31.958
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 05/09/23 17:02:31.963
    May  9 17:02:31.981: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3bd06398-7724-4732-8f3b-52d03f426391" in namespace "projected-2494" to be "Succeeded or Failed"
    May  9 17:02:31.990: INFO: Pod "downwardapi-volume-3bd06398-7724-4732-8f3b-52d03f426391": Phase="Pending", Reason="", readiness=false. Elapsed: 8.930346ms
    May  9 17:02:33.997: INFO: Pod "downwardapi-volume-3bd06398-7724-4732-8f3b-52d03f426391": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016182359s
    May  9 17:02:35.997: INFO: Pod "downwardapi-volume-3bd06398-7724-4732-8f3b-52d03f426391": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015938643s
    STEP: Saw pod success 05/09/23 17:02:35.997
    May  9 17:02:35.997: INFO: Pod "downwardapi-volume-3bd06398-7724-4732-8f3b-52d03f426391" satisfied condition "Succeeded or Failed"
    May  9 17:02:36.002: INFO: Trying to get logs from node nodepool-8cc7f47e-9b0c-4801-88-node-7ad816 pod downwardapi-volume-3bd06398-7724-4732-8f3b-52d03f426391 container client-container: <nil>
    STEP: delete the pod 05/09/23 17:02:36.016
    May  9 17:02:36.035: INFO: Waiting for pod downwardapi-volume-3bd06398-7724-4732-8f3b-52d03f426391 to disappear
    May  9 17:02:36.039: INFO: Pod downwardapi-volume-3bd06398-7724-4732-8f3b-52d03f426391 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  9 17:02:36.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2494" for this suite. 05/09/23 17:02:36.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/09/23 17:02:36.066
May  9 17:02:36.066: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
STEP: Building a namespace api object, basename deployment 05/09/23 17:02:36.067
STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:36.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:36.089
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
May  9 17:02:36.093: INFO: Creating simple deployment test-new-deployment
May  9 17:02:36.161: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 17, 2, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 17, 2, 36, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 17, 2, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 17, 2, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 05/09/23 17:02:38.171
STEP: updating a scale subresource 05/09/23 17:02:38.175
STEP: verifying the deployment Spec.Replicas was modified 05/09/23 17:02:38.182
STEP: Patch a scale subresource 05/09/23 17:02:38.187
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  9 17:02:38.207: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-1343  4ed24513-e6e7-466f-b32f-84cc8a8b012b 318241052 3 2023-05-09 17:02:36 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-09 17:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 17:02:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0058f0f38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-09 17:02:37 +0000 UTC,LastTransitionTime:2023-05-09 17:02:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-05-09 17:02:37 +0000 UTC,LastTransitionTime:2023-05-09 17:02:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  9 17:02:38.212: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-1343  07f150a5-2770-4a37-9c61-8aaa4a549093 318241057 2 2023-05-09 17:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 4ed24513-e6e7-466f-b32f-84cc8a8b012b 0xc00577e397 0xc00577e398}] [] [{kube-controller-manager Update apps/v1 2023-05-09 17:02:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ed24513-e6e7-466f-b32f-84cc8a8b012b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 17:02:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00577e428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  9 17:02:38.259: INFO: Pod "test-new-deployment-7f5969cbc7-64m8n" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-64m8n test-new-deployment-7f5969cbc7- deployment-1343  5584925a-89af-4f1a-bfe4-dd55640dfd92 318240956 0 2023-05-09 17:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a8770bd295ee96a2f40597f77dd2ee55fbb0d2225156c9743e56390c36d16f95 cni.projectcalico.org/podIP:10.2.1.235/32 cni.projectcalico.org/podIPs:10.2.1.235/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 07f150a5-2770-4a37-9c61-8aaa4a549093 0xc00577e827 0xc00577e828}] [] [{calico Update v1 2023-05-09 17:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 17:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07f150a5-2770-4a37-9c61-8aaa4a549093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 17:02:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-crvh4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-crvh4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 17:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 17:02:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 17:02:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 17:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:10.2.1.235,StartTime:2023-05-09 17:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 17:02:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6092d782b09aa75c1a0e49497741f391beb596493a29094f8772c258ba38e038,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  9 17:02:38.269: INFO: Pod "test-new-deployment-7f5969cbc7-xwhbq" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-xwhbq test-new-deployment-7f5969cbc7- deployment-1343  37156b1d-c336-4a43-aa69-b4ac73831bbd 318241056 0 2023-05-09 17:02:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 07f150a5-2770-4a37-9c61-8aaa4a549093 0xc00577ea17 0xc00577ea18}] [] [{kube-controller-manager Update v1 2023-05-09 17:02:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07f150a5-2770-4a37-9c61-8aaa4a549093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j4kbg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j4kbg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 17:02:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May  9 17:02:38.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1343" for this suite. 05/09/23 17:02:38.276
------------------------------
â€¢ [2.221 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/09/23 17:02:36.066
    May  9 17:02:36.066: INFO: >>> kubeConfig: /tmp/kubeconfig-818321586
    STEP: Building a namespace api object, basename deployment 05/09/23 17:02:36.067
    STEP: Waiting for a default service account to be provisioned in namespace 05/09/23 17:02:36.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/09/23 17:02:36.089
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    May  9 17:02:36.093: INFO: Creating simple deployment test-new-deployment
    May  9 17:02:36.161: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 9, 17, 2, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 17, 2, 36, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 9, 17, 2, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 9, 17, 2, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 05/09/23 17:02:38.171
    STEP: updating a scale subresource 05/09/23 17:02:38.175
    STEP: verifying the deployment Spec.Replicas was modified 05/09/23 17:02:38.182
    STEP: Patch a scale subresource 05/09/23 17:02:38.187
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  9 17:02:38.207: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-1343  4ed24513-e6e7-466f-b32f-84cc8a8b012b 318241052 3 2023-05-09 17:02:36 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-09 17:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 17:02:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0058f0f38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-09 17:02:37 +0000 UTC,LastTransitionTime:2023-05-09 17:02:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-05-09 17:02:37 +0000 UTC,LastTransitionTime:2023-05-09 17:02:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  9 17:02:38.212: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-1343  07f150a5-2770-4a37-9c61-8aaa4a549093 318241057 2 2023-05-09 17:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 4ed24513-e6e7-466f-b32f-84cc8a8b012b 0xc00577e397 0xc00577e398}] [] [{kube-controller-manager Update apps/v1 2023-05-09 17:02:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ed24513-e6e7-466f-b32f-84cc8a8b012b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-09 17:02:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00577e428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  9 17:02:38.259: INFO: Pod "test-new-deployment-7f5969cbc7-64m8n" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-64m8n test-new-deployment-7f5969cbc7- deployment-1343  5584925a-89af-4f1a-bfe4-dd55640dfd92 318240956 0 2023-05-09 17:02:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a8770bd295ee96a2f40597f77dd2ee55fbb0d2225156c9743e56390c36d16f95 cni.projectcalico.org/podIP:10.2.1.235/32 cni.projectcalico.org/podIPs:10.2.1.235/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 07f150a5-2770-4a37-9c61-8aaa4a549093 0xc00577e827 0xc00577e828}] [] [{calico Update v1 2023-05-09 17:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-09 17:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07f150a5-2770-4a37-9c61-8aaa4a549093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-09 17:02:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.1.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-crvh4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-crvh4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-7ad816,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 17:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 17:02:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 17:02:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 17:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:51.68.91.222,PodIP:10.2.1.235,StartTime:2023-05-09 17:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-09 17:02:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6092d782b09aa75c1a0e49497741f391beb596493a29094f8772c258ba38e038,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.1.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  9 17:02:38.269: INFO: Pod "test-new-deployment-7f5969cbc7-xwhbq" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-xwhbq test-new-deployment-7f5969cbc7- deployment-1343  37156b1d-c336-4a43-aa69-b4ac73831bbd 318241056 0 2023-05-09 17:02:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 07f150a5-2770-4a37-9c61-8aaa4a549093 0xc00577ea17 0xc00577ea18}] [] [{kube-controller-manager Update v1 2023-05-09 17:02:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07f150a5-2770-4a37-9c61-8aaa4a549093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j4kbg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j4kbg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodepool-8cc7f47e-9b0c-4801-88-node-f76f62,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-09 17:02:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May  9 17:02:38.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1343" for this suite. 05/09/23 17:02:38.276
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
May  9 17:02:38.289: INFO: Running AfterSuite actions on node 1
May  9 17:02:38.289: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    May  9 17:02:38.289: INFO: Running AfterSuite actions on node 1
    May  9 17:02:38.289: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.088 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5856.135 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h37m36.551656868s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

