I0506 06:59:14.293854      21 e2e.go:126] Starting e2e run "a676f9dd-4d35-458a-89a0-5f79d2a64d54" on Ginkgo node 1
May  6 06:59:14.308: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1683356354 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
May  6 06:59:14.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 06:59:14.407: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May  6 06:59:14.419: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May  6 06:59:14.438: INFO: 17 / 17 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May  6 06:59:14.438: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
May  6 06:59:14.438: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May  6 06:59:14.441: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May  6 06:59:14.441: INFO: e2e test version: v1.26.3
May  6 06:59:14.441: INFO: kube-apiserver version: v1.26.3
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
May  6 06:59:14.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 06:59:14.445: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.039 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    May  6 06:59:14.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 06:59:14.407: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    May  6 06:59:14.419: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    May  6 06:59:14.438: INFO: 17 / 17 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    May  6 06:59:14.438: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    May  6 06:59:14.438: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    May  6 06:59:14.441: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    May  6 06:59:14.441: INFO: e2e test version: v1.26.3
    May  6 06:59:14.441: INFO: kube-apiserver version: v1.26.3
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    May  6 06:59:14.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 06:59:14.445: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 06:59:14.46
May  6 06:59:14.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename security-context-test 05/06/23 06:59:14.461
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:15.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:15.478
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
May  6 06:59:15.485: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-08d3ef46-d82a-4ada-aa34-ee4624f00735" in namespace "security-context-test-8324" to be "Succeeded or Failed"
May  6 06:59:15.488: INFO: Pod "busybox-readonly-false-08d3ef46-d82a-4ada-aa34-ee4624f00735": Phase="Pending", Reason="", readiness=false. Elapsed: 2.671828ms
May  6 06:59:17.491: INFO: Pod "busybox-readonly-false-08d3ef46-d82a-4ada-aa34-ee4624f00735": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005668539s
May  6 06:59:19.492: INFO: Pod "busybox-readonly-false-08d3ef46-d82a-4ada-aa34-ee4624f00735": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006808094s
May  6 06:59:19.492: INFO: Pod "busybox-readonly-false-08d3ef46-d82a-4ada-aa34-ee4624f00735" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May  6 06:59:19.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8324" for this suite. 05/06/23 06:59:19.495
------------------------------
â€¢ [SLOW TEST] [5.040 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 06:59:14.46
    May  6 06:59:14.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename security-context-test 05/06/23 06:59:14.461
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:15.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:15.478
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    May  6 06:59:15.485: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-08d3ef46-d82a-4ada-aa34-ee4624f00735" in namespace "security-context-test-8324" to be "Succeeded or Failed"
    May  6 06:59:15.488: INFO: Pod "busybox-readonly-false-08d3ef46-d82a-4ada-aa34-ee4624f00735": Phase="Pending", Reason="", readiness=false. Elapsed: 2.671828ms
    May  6 06:59:17.491: INFO: Pod "busybox-readonly-false-08d3ef46-d82a-4ada-aa34-ee4624f00735": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005668539s
    May  6 06:59:19.492: INFO: Pod "busybox-readonly-false-08d3ef46-d82a-4ada-aa34-ee4624f00735": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006808094s
    May  6 06:59:19.492: INFO: Pod "busybox-readonly-false-08d3ef46-d82a-4ada-aa34-ee4624f00735" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May  6 06:59:19.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8324" for this suite. 05/06/23 06:59:19.495
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 06:59:19.5
May  6 06:59:19.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename security-context-test 05/06/23 06:59:19.501
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:20.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:20.516
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
May  6 06:59:20.525: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-71ba34ab-11de-46b6-a4b2-bb011c8e4349" in namespace "security-context-test-1012" to be "Succeeded or Failed"
May  6 06:59:20.528: INFO: Pod "busybox-privileged-false-71ba34ab-11de-46b6-a4b2-bb011c8e4349": Phase="Pending", Reason="", readiness=false. Elapsed: 2.52038ms
May  6 06:59:22.532: INFO: Pod "busybox-privileged-false-71ba34ab-11de-46b6-a4b2-bb011c8e4349": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006568598s
May  6 06:59:24.532: INFO: Pod "busybox-privileged-false-71ba34ab-11de-46b6-a4b2-bb011c8e4349": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006241098s
May  6 06:59:24.532: INFO: Pod "busybox-privileged-false-71ba34ab-11de-46b6-a4b2-bb011c8e4349" satisfied condition "Succeeded or Failed"
May  6 06:59:24.542: INFO: Got logs for pod "busybox-privileged-false-71ba34ab-11de-46b6-a4b2-bb011c8e4349": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May  6 06:59:24.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-1012" for this suite. 05/06/23 06:59:24.545
------------------------------
â€¢ [SLOW TEST] [5.050 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 06:59:19.5
    May  6 06:59:19.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename security-context-test 05/06/23 06:59:19.501
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:20.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:20.516
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    May  6 06:59:20.525: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-71ba34ab-11de-46b6-a4b2-bb011c8e4349" in namespace "security-context-test-1012" to be "Succeeded or Failed"
    May  6 06:59:20.528: INFO: Pod "busybox-privileged-false-71ba34ab-11de-46b6-a4b2-bb011c8e4349": Phase="Pending", Reason="", readiness=false. Elapsed: 2.52038ms
    May  6 06:59:22.532: INFO: Pod "busybox-privileged-false-71ba34ab-11de-46b6-a4b2-bb011c8e4349": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006568598s
    May  6 06:59:24.532: INFO: Pod "busybox-privileged-false-71ba34ab-11de-46b6-a4b2-bb011c8e4349": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006241098s
    May  6 06:59:24.532: INFO: Pod "busybox-privileged-false-71ba34ab-11de-46b6-a4b2-bb011c8e4349" satisfied condition "Succeeded or Failed"
    May  6 06:59:24.542: INFO: Got logs for pod "busybox-privileged-false-71ba34ab-11de-46b6-a4b2-bb011c8e4349": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May  6 06:59:24.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-1012" for this suite. 05/06/23 06:59:24.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 06:59:24.551
May  6 06:59:24.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename container-lifecycle-hook 05/06/23 06:59:24.551
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:25.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:25.566
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 05/06/23 06:59:25.571
May  6 06:59:25.579: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-114" to be "running and ready"
May  6 06:59:25.581: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.465236ms
May  6 06:59:25.581: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  6 06:59:27.586: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006772319s
May  6 06:59:27.586: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  6 06:59:27.586: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 05/06/23 06:59:27.589
May  6 06:59:27.594: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-114" to be "running and ready"
May  6 06:59:27.597: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.966268ms
May  6 06:59:27.597: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
May  6 06:59:29.601: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007405242s
May  6 06:59:29.601: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
May  6 06:59:29.601: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 05/06/23 06:59:29.603
STEP: delete the pod with lifecycle hook 05/06/23 06:59:29.615
May  6 06:59:29.622: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  6 06:59:29.625: INFO: Pod pod-with-poststart-http-hook still exists
May  6 06:59:31.626: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  6 06:59:31.629: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
May  6 06:59:31.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-114" for this suite. 05/06/23 06:59:31.633
------------------------------
â€¢ [SLOW TEST] [7.087 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 06:59:24.551
    May  6 06:59:24.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/06/23 06:59:24.551
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:25.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:25.566
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 05/06/23 06:59:25.571
    May  6 06:59:25.579: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-114" to be "running and ready"
    May  6 06:59:25.581: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.465236ms
    May  6 06:59:25.581: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  6 06:59:27.586: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006772319s
    May  6 06:59:27.586: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  6 06:59:27.586: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 05/06/23 06:59:27.589
    May  6 06:59:27.594: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-114" to be "running and ready"
    May  6 06:59:27.597: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.966268ms
    May  6 06:59:27.597: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    May  6 06:59:29.601: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007405242s
    May  6 06:59:29.601: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    May  6 06:59:29.601: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 05/06/23 06:59:29.603
    STEP: delete the pod with lifecycle hook 05/06/23 06:59:29.615
    May  6 06:59:29.622: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May  6 06:59:29.625: INFO: Pod pod-with-poststart-http-hook still exists
    May  6 06:59:31.626: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May  6 06:59:31.629: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    May  6 06:59:31.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-114" for this suite. 05/06/23 06:59:31.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 06:59:31.639
May  6 06:59:31.639: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename downward-api 05/06/23 06:59:31.64
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:32.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:32.655
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 05/06/23 06:59:32.657
May  6 06:59:32.669: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f399454-049e-43be-bfaf-a7497dbd76e6" in namespace "downward-api-1105" to be "Succeeded or Failed"
May  6 06:59:32.673: INFO: Pod "downwardapi-volume-7f399454-049e-43be-bfaf-a7497dbd76e6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.820642ms
May  6 06:59:34.677: INFO: Pod "downwardapi-volume-7f399454-049e-43be-bfaf-a7497dbd76e6": Phase="Running", Reason="", readiness=false. Elapsed: 2.00823184s
May  6 06:59:36.677: INFO: Pod "downwardapi-volume-7f399454-049e-43be-bfaf-a7497dbd76e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008337299s
STEP: Saw pod success 05/06/23 06:59:36.677
May  6 06:59:36.677: INFO: Pod "downwardapi-volume-7f399454-049e-43be-bfaf-a7497dbd76e6" satisfied condition "Succeeded or Failed"
May  6 06:59:36.679: INFO: Trying to get logs from node cncf-3 pod downwardapi-volume-7f399454-049e-43be-bfaf-a7497dbd76e6 container client-container: <nil>
STEP: delete the pod 05/06/23 06:59:36.691
May  6 06:59:36.703: INFO: Waiting for pod downwardapi-volume-7f399454-049e-43be-bfaf-a7497dbd76e6 to disappear
May  6 06:59:36.705: INFO: Pod downwardapi-volume-7f399454-049e-43be-bfaf-a7497dbd76e6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  6 06:59:36.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1105" for this suite. 05/06/23 06:59:36.709
------------------------------
â€¢ [SLOW TEST] [5.077 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 06:59:31.639
    May  6 06:59:31.639: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename downward-api 05/06/23 06:59:31.64
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:32.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:32.655
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 05/06/23 06:59:32.657
    May  6 06:59:32.669: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f399454-049e-43be-bfaf-a7497dbd76e6" in namespace "downward-api-1105" to be "Succeeded or Failed"
    May  6 06:59:32.673: INFO: Pod "downwardapi-volume-7f399454-049e-43be-bfaf-a7497dbd76e6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.820642ms
    May  6 06:59:34.677: INFO: Pod "downwardapi-volume-7f399454-049e-43be-bfaf-a7497dbd76e6": Phase="Running", Reason="", readiness=false. Elapsed: 2.00823184s
    May  6 06:59:36.677: INFO: Pod "downwardapi-volume-7f399454-049e-43be-bfaf-a7497dbd76e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008337299s
    STEP: Saw pod success 05/06/23 06:59:36.677
    May  6 06:59:36.677: INFO: Pod "downwardapi-volume-7f399454-049e-43be-bfaf-a7497dbd76e6" satisfied condition "Succeeded or Failed"
    May  6 06:59:36.679: INFO: Trying to get logs from node cncf-3 pod downwardapi-volume-7f399454-049e-43be-bfaf-a7497dbd76e6 container client-container: <nil>
    STEP: delete the pod 05/06/23 06:59:36.691
    May  6 06:59:36.703: INFO: Waiting for pod downwardapi-volume-7f399454-049e-43be-bfaf-a7497dbd76e6 to disappear
    May  6 06:59:36.705: INFO: Pod downwardapi-volume-7f399454-049e-43be-bfaf-a7497dbd76e6 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  6 06:59:36.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1105" for this suite. 05/06/23 06:59:36.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 06:59:36.716
May  6 06:59:36.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename events 05/06/23 06:59:36.717
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:37.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:37.733
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 05/06/23 06:59:37.735
May  6 06:59:37.739: INFO: created test-event-1
May  6 06:59:37.744: INFO: created test-event-2
May  6 06:59:37.748: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 05/06/23 06:59:37.748
STEP: delete collection of events 05/06/23 06:59:37.75
May  6 06:59:37.750: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 05/06/23 06:59:37.768
May  6 06:59:37.768: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
May  6 06:59:37.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-8162" for this suite. 05/06/23 06:59:37.773
------------------------------
â€¢ [1.062 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 06:59:36.716
    May  6 06:59:36.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename events 05/06/23 06:59:36.717
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:37.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:37.733
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 05/06/23 06:59:37.735
    May  6 06:59:37.739: INFO: created test-event-1
    May  6 06:59:37.744: INFO: created test-event-2
    May  6 06:59:37.748: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 05/06/23 06:59:37.748
    STEP: delete collection of events 05/06/23 06:59:37.75
    May  6 06:59:37.750: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 05/06/23 06:59:37.768
    May  6 06:59:37.768: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    May  6 06:59:37.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-8162" for this suite. 05/06/23 06:59:37.773
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 06:59:37.778
May  6 06:59:37.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename emptydir 05/06/23 06:59:37.779
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:38.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:38.796
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 05/06/23 06:59:38.798
May  6 06:59:38.804: INFO: Waiting up to 5m0s for pod "pod-b84c7431-b21c-4e6f-911e-fef1fb12a887" in namespace "emptydir-6227" to be "Succeeded or Failed"
May  6 06:59:38.807: INFO: Pod "pod-b84c7431-b21c-4e6f-911e-fef1fb12a887": Phase="Pending", Reason="", readiness=false. Elapsed: 2.466998ms
May  6 06:59:40.810: INFO: Pod "pod-b84c7431-b21c-4e6f-911e-fef1fb12a887": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005814447s
May  6 06:59:42.810: INFO: Pod "pod-b84c7431-b21c-4e6f-911e-fef1fb12a887": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006012139s
STEP: Saw pod success 05/06/23 06:59:42.81
May  6 06:59:42.811: INFO: Pod "pod-b84c7431-b21c-4e6f-911e-fef1fb12a887" satisfied condition "Succeeded or Failed"
May  6 06:59:42.813: INFO: Trying to get logs from node cncf-0 pod pod-b84c7431-b21c-4e6f-911e-fef1fb12a887 container test-container: <nil>
STEP: delete the pod 05/06/23 06:59:42.821
May  6 06:59:42.832: INFO: Waiting for pod pod-b84c7431-b21c-4e6f-911e-fef1fb12a887 to disappear
May  6 06:59:42.834: INFO: Pod pod-b84c7431-b21c-4e6f-911e-fef1fb12a887 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  6 06:59:42.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6227" for this suite. 05/06/23 06:59:42.837
------------------------------
â€¢ [SLOW TEST] [5.066 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 06:59:37.778
    May  6 06:59:37.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename emptydir 05/06/23 06:59:37.779
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:38.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:38.796
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 05/06/23 06:59:38.798
    May  6 06:59:38.804: INFO: Waiting up to 5m0s for pod "pod-b84c7431-b21c-4e6f-911e-fef1fb12a887" in namespace "emptydir-6227" to be "Succeeded or Failed"
    May  6 06:59:38.807: INFO: Pod "pod-b84c7431-b21c-4e6f-911e-fef1fb12a887": Phase="Pending", Reason="", readiness=false. Elapsed: 2.466998ms
    May  6 06:59:40.810: INFO: Pod "pod-b84c7431-b21c-4e6f-911e-fef1fb12a887": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005814447s
    May  6 06:59:42.810: INFO: Pod "pod-b84c7431-b21c-4e6f-911e-fef1fb12a887": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006012139s
    STEP: Saw pod success 05/06/23 06:59:42.81
    May  6 06:59:42.811: INFO: Pod "pod-b84c7431-b21c-4e6f-911e-fef1fb12a887" satisfied condition "Succeeded or Failed"
    May  6 06:59:42.813: INFO: Trying to get logs from node cncf-0 pod pod-b84c7431-b21c-4e6f-911e-fef1fb12a887 container test-container: <nil>
    STEP: delete the pod 05/06/23 06:59:42.821
    May  6 06:59:42.832: INFO: Waiting for pod pod-b84c7431-b21c-4e6f-911e-fef1fb12a887 to disappear
    May  6 06:59:42.834: INFO: Pod pod-b84c7431-b21c-4e6f-911e-fef1fb12a887 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  6 06:59:42.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6227" for this suite. 05/06/23 06:59:42.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 06:59:42.846
May  6 06:59:42.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename secrets 05/06/23 06:59:42.846
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:43.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:43.86
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-8159/secret-test-9db84771-cdfe-4ecf-ae6d-a8e53a10dcea 05/06/23 06:59:43.862
STEP: Creating a pod to test consume secrets 05/06/23 06:59:43.868
May  6 06:59:43.876: INFO: Waiting up to 5m0s for pod "pod-configmaps-af8a4614-e188-4fee-a833-0b82fbf2faf3" in namespace "secrets-8159" to be "Succeeded or Failed"
May  6 06:59:43.883: INFO: Pod "pod-configmaps-af8a4614-e188-4fee-a833-0b82fbf2faf3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.89325ms
May  6 06:59:45.887: INFO: Pod "pod-configmaps-af8a4614-e188-4fee-a833-0b82fbf2faf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010184089s
May  6 06:59:47.888: INFO: Pod "pod-configmaps-af8a4614-e188-4fee-a833-0b82fbf2faf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011572643s
STEP: Saw pod success 05/06/23 06:59:47.888
May  6 06:59:47.888: INFO: Pod "pod-configmaps-af8a4614-e188-4fee-a833-0b82fbf2faf3" satisfied condition "Succeeded or Failed"
May  6 06:59:47.891: INFO: Trying to get logs from node cncf-0 pod pod-configmaps-af8a4614-e188-4fee-a833-0b82fbf2faf3 container env-test: <nil>
STEP: delete the pod 05/06/23 06:59:47.897
May  6 06:59:47.908: INFO: Waiting for pod pod-configmaps-af8a4614-e188-4fee-a833-0b82fbf2faf3 to disappear
May  6 06:59:47.910: INFO: Pod pod-configmaps-af8a4614-e188-4fee-a833-0b82fbf2faf3 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
May  6 06:59:47.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8159" for this suite. 05/06/23 06:59:47.913
------------------------------
â€¢ [SLOW TEST] [5.073 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 06:59:42.846
    May  6 06:59:42.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename secrets 05/06/23 06:59:42.846
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:43.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:43.86
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-8159/secret-test-9db84771-cdfe-4ecf-ae6d-a8e53a10dcea 05/06/23 06:59:43.862
    STEP: Creating a pod to test consume secrets 05/06/23 06:59:43.868
    May  6 06:59:43.876: INFO: Waiting up to 5m0s for pod "pod-configmaps-af8a4614-e188-4fee-a833-0b82fbf2faf3" in namespace "secrets-8159" to be "Succeeded or Failed"
    May  6 06:59:43.883: INFO: Pod "pod-configmaps-af8a4614-e188-4fee-a833-0b82fbf2faf3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.89325ms
    May  6 06:59:45.887: INFO: Pod "pod-configmaps-af8a4614-e188-4fee-a833-0b82fbf2faf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010184089s
    May  6 06:59:47.888: INFO: Pod "pod-configmaps-af8a4614-e188-4fee-a833-0b82fbf2faf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011572643s
    STEP: Saw pod success 05/06/23 06:59:47.888
    May  6 06:59:47.888: INFO: Pod "pod-configmaps-af8a4614-e188-4fee-a833-0b82fbf2faf3" satisfied condition "Succeeded or Failed"
    May  6 06:59:47.891: INFO: Trying to get logs from node cncf-0 pod pod-configmaps-af8a4614-e188-4fee-a833-0b82fbf2faf3 container env-test: <nil>
    STEP: delete the pod 05/06/23 06:59:47.897
    May  6 06:59:47.908: INFO: Waiting for pod pod-configmaps-af8a4614-e188-4fee-a833-0b82fbf2faf3 to disappear
    May  6 06:59:47.910: INFO: Pod pod-configmaps-af8a4614-e188-4fee-a833-0b82fbf2faf3 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    May  6 06:59:47.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8159" for this suite. 05/06/23 06:59:47.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 06:59:47.92
May  6 06:59:47.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename runtimeclass 05/06/23 06:59:47.92
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:48.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:48.935
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
May  6 06:59:48.950: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8121 to be scheduled
May  6 06:59:48.959: INFO: 1 pods are not scheduled: [runtimeclass-8121/test-runtimeclass-runtimeclass-8121-preconfigured-handler-9b6gp(b83c3c00-b787-449e-86d3-474ab58e43be)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May  6 06:59:50.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8121" for this suite. 05/06/23 06:59:50.971
------------------------------
â€¢ [3.057 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 06:59:47.92
    May  6 06:59:47.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename runtimeclass 05/06/23 06:59:47.92
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:48.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:48.935
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    May  6 06:59:48.950: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8121 to be scheduled
    May  6 06:59:48.959: INFO: 1 pods are not scheduled: [runtimeclass-8121/test-runtimeclass-runtimeclass-8121-preconfigured-handler-9b6gp(b83c3c00-b787-449e-86d3-474ab58e43be)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May  6 06:59:50.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8121" for this suite. 05/06/23 06:59:50.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 06:59:50.977
May  6 06:59:50.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename replicaset 05/06/23 06:59:50.977
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:51.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:51.995
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 05/06/23 06:59:51.997
May  6 06:59:52.004: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9743" to be "running and ready"
May  6 06:59:52.007: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.746348ms
May  6 06:59:52.007: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May  6 06:59:54.010: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.006205437s
May  6 06:59:54.010: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
May  6 06:59:54.010: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 05/06/23 06:59:54.012
STEP: Then the orphan pod is adopted 05/06/23 06:59:54.017
STEP: When the matched label of one of its pods change 05/06/23 06:59:55.022
May  6 06:59:55.025: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 05/06/23 06:59:55.034
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May  6 06:59:56.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9743" for this suite. 05/06/23 06:59:56.044
------------------------------
â€¢ [SLOW TEST] [5.074 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 06:59:50.977
    May  6 06:59:50.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename replicaset 05/06/23 06:59:50.977
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:51.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:51.995
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 05/06/23 06:59:51.997
    May  6 06:59:52.004: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9743" to be "running and ready"
    May  6 06:59:52.007: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.746348ms
    May  6 06:59:52.007: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    May  6 06:59:54.010: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.006205437s
    May  6 06:59:54.010: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    May  6 06:59:54.010: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 05/06/23 06:59:54.012
    STEP: Then the orphan pod is adopted 05/06/23 06:59:54.017
    STEP: When the matched label of one of its pods change 05/06/23 06:59:55.022
    May  6 06:59:55.025: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 05/06/23 06:59:55.034
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May  6 06:59:56.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9743" for this suite. 05/06/23 06:59:56.044
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 06:59:56.05
May  6 06:59:56.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename subpath 05/06/23 06:59:56.051
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:57.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:57.073
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/06/23 06:59:57.075
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-d8xx 05/06/23 06:59:57.083
STEP: Creating a pod to test atomic-volume-subpath 05/06/23 06:59:57.083
May  6 06:59:57.090: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-d8xx" in namespace "subpath-4835" to be "Succeeded or Failed"
May  6 06:59:57.093: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.338614ms
May  6 06:59:59.095: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 2.005076591s
May  6 07:00:01.096: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 4.005462861s
May  6 07:00:03.097: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 6.006525163s
May  6 07:00:05.096: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 8.005358839s
May  6 07:00:07.096: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 10.005514821s
May  6 07:00:09.096: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 12.005517208s
May  6 07:00:11.097: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 14.006553452s
May  6 07:00:13.097: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 16.006354365s
May  6 07:00:15.096: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 18.005709251s
May  6 07:00:17.097: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 20.007049521s
May  6 07:00:19.096: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=false. Elapsed: 22.005477459s
May  6 07:00:21.096: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005296258s
STEP: Saw pod success 05/06/23 07:00:21.096
May  6 07:00:21.096: INFO: Pod "pod-subpath-test-secret-d8xx" satisfied condition "Succeeded or Failed"
May  6 07:00:21.099: INFO: Trying to get logs from node cncf-0 pod pod-subpath-test-secret-d8xx container test-container-subpath-secret-d8xx: <nil>
STEP: delete the pod 05/06/23 07:00:21.103
May  6 07:00:21.114: INFO: Waiting for pod pod-subpath-test-secret-d8xx to disappear
May  6 07:00:21.116: INFO: Pod pod-subpath-test-secret-d8xx no longer exists
STEP: Deleting pod pod-subpath-test-secret-d8xx 05/06/23 07:00:21.116
May  6 07:00:21.116: INFO: Deleting pod "pod-subpath-test-secret-d8xx" in namespace "subpath-4835"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May  6 07:00:21.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4835" for this suite. 05/06/23 07:00:21.122
------------------------------
â€¢ [SLOW TEST] [25.077 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 06:59:56.05
    May  6 06:59:56.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename subpath 05/06/23 06:59:56.051
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 06:59:57.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 06:59:57.073
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/06/23 06:59:57.075
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-d8xx 05/06/23 06:59:57.083
    STEP: Creating a pod to test atomic-volume-subpath 05/06/23 06:59:57.083
    May  6 06:59:57.090: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-d8xx" in namespace "subpath-4835" to be "Succeeded or Failed"
    May  6 06:59:57.093: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.338614ms
    May  6 06:59:59.095: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 2.005076591s
    May  6 07:00:01.096: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 4.005462861s
    May  6 07:00:03.097: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 6.006525163s
    May  6 07:00:05.096: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 8.005358839s
    May  6 07:00:07.096: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 10.005514821s
    May  6 07:00:09.096: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 12.005517208s
    May  6 07:00:11.097: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 14.006553452s
    May  6 07:00:13.097: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 16.006354365s
    May  6 07:00:15.096: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 18.005709251s
    May  6 07:00:17.097: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=true. Elapsed: 20.007049521s
    May  6 07:00:19.096: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Running", Reason="", readiness=false. Elapsed: 22.005477459s
    May  6 07:00:21.096: INFO: Pod "pod-subpath-test-secret-d8xx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005296258s
    STEP: Saw pod success 05/06/23 07:00:21.096
    May  6 07:00:21.096: INFO: Pod "pod-subpath-test-secret-d8xx" satisfied condition "Succeeded or Failed"
    May  6 07:00:21.099: INFO: Trying to get logs from node cncf-0 pod pod-subpath-test-secret-d8xx container test-container-subpath-secret-d8xx: <nil>
    STEP: delete the pod 05/06/23 07:00:21.103
    May  6 07:00:21.114: INFO: Waiting for pod pod-subpath-test-secret-d8xx to disappear
    May  6 07:00:21.116: INFO: Pod pod-subpath-test-secret-d8xx no longer exists
    STEP: Deleting pod pod-subpath-test-secret-d8xx 05/06/23 07:00:21.116
    May  6 07:00:21.116: INFO: Deleting pod "pod-subpath-test-secret-d8xx" in namespace "subpath-4835"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May  6 07:00:21.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4835" for this suite. 05/06/23 07:00:21.122
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:00:21.127
May  6 07:00:21.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename csistoragecapacity 05/06/23 07:00:21.128
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:00:22.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:00:22.145
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 05/06/23 07:00:22.147
STEP: getting /apis/storage.k8s.io 05/06/23 07:00:22.149
STEP: getting /apis/storage.k8s.io/v1 05/06/23 07:00:22.149
STEP: creating 05/06/23 07:00:22.15
STEP: watching 05/06/23 07:00:22.163
May  6 07:00:22.163: INFO: starting watch
STEP: getting 05/06/23 07:00:22.168
STEP: listing in namespace 05/06/23 07:00:22.17
STEP: listing across namespaces 05/06/23 07:00:22.172
STEP: patching 05/06/23 07:00:22.174
STEP: updating 05/06/23 07:00:22.179
May  6 07:00:22.183: INFO: waiting for watch events with expected annotations in namespace
May  6 07:00:22.183: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 05/06/23 07:00:22.183
STEP: deleting a collection 05/06/23 07:00:22.192
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
May  6 07:00:22.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-709" for this suite. 05/06/23 07:00:22.207
------------------------------
â€¢ [1.087 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:00:21.127
    May  6 07:00:21.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename csistoragecapacity 05/06/23 07:00:21.128
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:00:22.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:00:22.145
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 05/06/23 07:00:22.147
    STEP: getting /apis/storage.k8s.io 05/06/23 07:00:22.149
    STEP: getting /apis/storage.k8s.io/v1 05/06/23 07:00:22.149
    STEP: creating 05/06/23 07:00:22.15
    STEP: watching 05/06/23 07:00:22.163
    May  6 07:00:22.163: INFO: starting watch
    STEP: getting 05/06/23 07:00:22.168
    STEP: listing in namespace 05/06/23 07:00:22.17
    STEP: listing across namespaces 05/06/23 07:00:22.172
    STEP: patching 05/06/23 07:00:22.174
    STEP: updating 05/06/23 07:00:22.179
    May  6 07:00:22.183: INFO: waiting for watch events with expected annotations in namespace
    May  6 07:00:22.183: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 05/06/23 07:00:22.183
    STEP: deleting a collection 05/06/23 07:00:22.192
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    May  6 07:00:22.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-709" for this suite. 05/06/23 07:00:22.207
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:00:22.214
May  6 07:00:22.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename pods 05/06/23 07:00:22.215
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:00:23.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:00:23.231
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 05/06/23 07:00:23.232
May  6 07:00:23.239: INFO: Waiting up to 5m0s for pod "pod-hostip-66fe09a3-b59b-4935-b12d-7f7b5d3afe26" in namespace "pods-1830" to be "running and ready"
May  6 07:00:23.241: INFO: Pod "pod-hostip-66fe09a3-b59b-4935-b12d-7f7b5d3afe26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.196605ms
May  6 07:00:23.241: INFO: The phase of Pod pod-hostip-66fe09a3-b59b-4935-b12d-7f7b5d3afe26 is Pending, waiting for it to be Running (with Ready = true)
May  6 07:00:25.244: INFO: Pod "pod-hostip-66fe09a3-b59b-4935-b12d-7f7b5d3afe26": Phase="Running", Reason="", readiness=true. Elapsed: 2.005723432s
May  6 07:00:25.245: INFO: The phase of Pod pod-hostip-66fe09a3-b59b-4935-b12d-7f7b5d3afe26 is Running (Ready = true)
May  6 07:00:25.245: INFO: Pod "pod-hostip-66fe09a3-b59b-4935-b12d-7f7b5d3afe26" satisfied condition "running and ready"
May  6 07:00:25.249: INFO: Pod pod-hostip-66fe09a3-b59b-4935-b12d-7f7b5d3afe26 has hostIP: 10.0.0.134
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  6 07:00:25.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1830" for this suite. 05/06/23 07:00:25.252
------------------------------
â€¢ [3.044 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:00:22.214
    May  6 07:00:22.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename pods 05/06/23 07:00:22.215
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:00:23.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:00:23.231
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 05/06/23 07:00:23.232
    May  6 07:00:23.239: INFO: Waiting up to 5m0s for pod "pod-hostip-66fe09a3-b59b-4935-b12d-7f7b5d3afe26" in namespace "pods-1830" to be "running and ready"
    May  6 07:00:23.241: INFO: Pod "pod-hostip-66fe09a3-b59b-4935-b12d-7f7b5d3afe26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.196605ms
    May  6 07:00:23.241: INFO: The phase of Pod pod-hostip-66fe09a3-b59b-4935-b12d-7f7b5d3afe26 is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:00:25.244: INFO: Pod "pod-hostip-66fe09a3-b59b-4935-b12d-7f7b5d3afe26": Phase="Running", Reason="", readiness=true. Elapsed: 2.005723432s
    May  6 07:00:25.245: INFO: The phase of Pod pod-hostip-66fe09a3-b59b-4935-b12d-7f7b5d3afe26 is Running (Ready = true)
    May  6 07:00:25.245: INFO: Pod "pod-hostip-66fe09a3-b59b-4935-b12d-7f7b5d3afe26" satisfied condition "running and ready"
    May  6 07:00:25.249: INFO: Pod pod-hostip-66fe09a3-b59b-4935-b12d-7f7b5d3afe26 has hostIP: 10.0.0.134
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  6 07:00:25.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1830" for this suite. 05/06/23 07:00:25.252
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:00:25.258
May  6 07:00:25.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename subpath 05/06/23 07:00:25.259
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:00:26.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:00:26.277
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/06/23 07:00:26.279
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-2m9s 05/06/23 07:00:26.288
STEP: Creating a pod to test atomic-volume-subpath 05/06/23 07:00:26.288
May  6 07:00:26.296: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-2m9s" in namespace "subpath-855" to be "Succeeded or Failed"
May  6 07:00:26.299: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.915131ms
May  6 07:00:28.302: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 2.006321718s
May  6 07:00:30.302: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 4.005911482s
May  6 07:00:32.302: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 6.005843206s
May  6 07:00:34.301: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 8.005272454s
May  6 07:00:36.302: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 10.00628992s
May  6 07:00:38.302: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 12.005897046s
May  6 07:00:40.302: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 14.005917169s
May  6 07:00:42.302: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 16.006206471s
May  6 07:00:44.302: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 18.00574623s
May  6 07:00:46.302: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 20.006071401s
May  6 07:00:48.303: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=false. Elapsed: 22.006583315s
May  6 07:00:50.303: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007153815s
STEP: Saw pod success 05/06/23 07:00:50.303
May  6 07:00:50.303: INFO: Pod "pod-subpath-test-projected-2m9s" satisfied condition "Succeeded or Failed"
May  6 07:00:50.306: INFO: Trying to get logs from node cncf-2 pod pod-subpath-test-projected-2m9s container test-container-subpath-projected-2m9s: <nil>
STEP: delete the pod 05/06/23 07:00:50.311
May  6 07:00:50.322: INFO: Waiting for pod pod-subpath-test-projected-2m9s to disappear
May  6 07:00:50.324: INFO: Pod pod-subpath-test-projected-2m9s no longer exists
STEP: Deleting pod pod-subpath-test-projected-2m9s 05/06/23 07:00:50.324
May  6 07:00:50.324: INFO: Deleting pod "pod-subpath-test-projected-2m9s" in namespace "subpath-855"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May  6 07:00:50.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-855" for this suite. 05/06/23 07:00:50.33
------------------------------
â€¢ [SLOW TEST] [25.079 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:00:25.258
    May  6 07:00:25.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename subpath 05/06/23 07:00:25.259
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:00:26.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:00:26.277
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/06/23 07:00:26.279
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-2m9s 05/06/23 07:00:26.288
    STEP: Creating a pod to test atomic-volume-subpath 05/06/23 07:00:26.288
    May  6 07:00:26.296: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-2m9s" in namespace "subpath-855" to be "Succeeded or Failed"
    May  6 07:00:26.299: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.915131ms
    May  6 07:00:28.302: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 2.006321718s
    May  6 07:00:30.302: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 4.005911482s
    May  6 07:00:32.302: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 6.005843206s
    May  6 07:00:34.301: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 8.005272454s
    May  6 07:00:36.302: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 10.00628992s
    May  6 07:00:38.302: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 12.005897046s
    May  6 07:00:40.302: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 14.005917169s
    May  6 07:00:42.302: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 16.006206471s
    May  6 07:00:44.302: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 18.00574623s
    May  6 07:00:46.302: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=true. Elapsed: 20.006071401s
    May  6 07:00:48.303: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Running", Reason="", readiness=false. Elapsed: 22.006583315s
    May  6 07:00:50.303: INFO: Pod "pod-subpath-test-projected-2m9s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007153815s
    STEP: Saw pod success 05/06/23 07:00:50.303
    May  6 07:00:50.303: INFO: Pod "pod-subpath-test-projected-2m9s" satisfied condition "Succeeded or Failed"
    May  6 07:00:50.306: INFO: Trying to get logs from node cncf-2 pod pod-subpath-test-projected-2m9s container test-container-subpath-projected-2m9s: <nil>
    STEP: delete the pod 05/06/23 07:00:50.311
    May  6 07:00:50.322: INFO: Waiting for pod pod-subpath-test-projected-2m9s to disappear
    May  6 07:00:50.324: INFO: Pod pod-subpath-test-projected-2m9s no longer exists
    STEP: Deleting pod pod-subpath-test-projected-2m9s 05/06/23 07:00:50.324
    May  6 07:00:50.324: INFO: Deleting pod "pod-subpath-test-projected-2m9s" in namespace "subpath-855"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May  6 07:00:50.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-855" for this suite. 05/06/23 07:00:50.33
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:00:50.339
May  6 07:00:50.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:00:50.34
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:00:51.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:00:51.359
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 05/06/23 07:00:51.361
May  6 07:00:51.369: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bb147021-33f3-46ed-859d-89087c501802" in namespace "projected-7867" to be "Succeeded or Failed"
May  6 07:00:51.371: INFO: Pod "downwardapi-volume-bb147021-33f3-46ed-859d-89087c501802": Phase="Pending", Reason="", readiness=false. Elapsed: 2.354435ms
May  6 07:00:53.375: INFO: Pod "downwardapi-volume-bb147021-33f3-46ed-859d-89087c501802": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006385239s
May  6 07:00:55.374: INFO: Pod "downwardapi-volume-bb147021-33f3-46ed-859d-89087c501802": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005429888s
STEP: Saw pod success 05/06/23 07:00:55.374
May  6 07:00:55.374: INFO: Pod "downwardapi-volume-bb147021-33f3-46ed-859d-89087c501802" satisfied condition "Succeeded or Failed"
May  6 07:00:55.377: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-bb147021-33f3-46ed-859d-89087c501802 container client-container: <nil>
STEP: delete the pod 05/06/23 07:00:55.383
May  6 07:00:55.394: INFO: Waiting for pod downwardapi-volume-bb147021-33f3-46ed-859d-89087c501802 to disappear
May  6 07:00:55.397: INFO: Pod downwardapi-volume-bb147021-33f3-46ed-859d-89087c501802 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  6 07:00:55.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7867" for this suite. 05/06/23 07:00:55.4
------------------------------
â€¢ [SLOW TEST] [5.069 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:00:50.339
    May  6 07:00:50.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:00:50.34
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:00:51.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:00:51.359
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 05/06/23 07:00:51.361
    May  6 07:00:51.369: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bb147021-33f3-46ed-859d-89087c501802" in namespace "projected-7867" to be "Succeeded or Failed"
    May  6 07:00:51.371: INFO: Pod "downwardapi-volume-bb147021-33f3-46ed-859d-89087c501802": Phase="Pending", Reason="", readiness=false. Elapsed: 2.354435ms
    May  6 07:00:53.375: INFO: Pod "downwardapi-volume-bb147021-33f3-46ed-859d-89087c501802": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006385239s
    May  6 07:00:55.374: INFO: Pod "downwardapi-volume-bb147021-33f3-46ed-859d-89087c501802": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005429888s
    STEP: Saw pod success 05/06/23 07:00:55.374
    May  6 07:00:55.374: INFO: Pod "downwardapi-volume-bb147021-33f3-46ed-859d-89087c501802" satisfied condition "Succeeded or Failed"
    May  6 07:00:55.377: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-bb147021-33f3-46ed-859d-89087c501802 container client-container: <nil>
    STEP: delete the pod 05/06/23 07:00:55.383
    May  6 07:00:55.394: INFO: Waiting for pod downwardapi-volume-bb147021-33f3-46ed-859d-89087c501802 to disappear
    May  6 07:00:55.397: INFO: Pod downwardapi-volume-bb147021-33f3-46ed-859d-89087c501802 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  6 07:00:55.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7867" for this suite. 05/06/23 07:00:55.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:00:55.408
May  6 07:00:55.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename tables 05/06/23 07:00:55.408
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:00:56.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:00:56.424
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
May  6 07:00:56.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-1026" for this suite. 05/06/23 07:00:56.431
------------------------------
â€¢ [1.029 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:00:55.408
    May  6 07:00:55.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename tables 05/06/23 07:00:55.408
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:00:56.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:00:56.424
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    May  6 07:00:56.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-1026" for this suite. 05/06/23 07:00:56.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:00:56.441
May  6 07:00:56.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename cronjob 05/06/23 07:00:56.442
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:00:57.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:00:57.458
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 05/06/23 07:00:57.46
STEP: creating 05/06/23 07:00:57.46
STEP: getting 05/06/23 07:00:57.464
STEP: listing 05/06/23 07:00:57.467
STEP: watching 05/06/23 07:00:57.469
May  6 07:00:57.469: INFO: starting watch
STEP: cluster-wide listing 05/06/23 07:00:57.47
STEP: cluster-wide watching 05/06/23 07:00:57.472
May  6 07:00:57.472: INFO: starting watch
STEP: patching 05/06/23 07:00:57.473
STEP: updating 05/06/23 07:00:57.479
May  6 07:00:57.486: INFO: waiting for watch events with expected annotations
May  6 07:00:57.486: INFO: saw patched and updated annotations
STEP: patching /status 05/06/23 07:00:57.486
STEP: updating /status 05/06/23 07:00:57.491
STEP: get /status 05/06/23 07:00:57.497
STEP: deleting 05/06/23 07:00:57.499
STEP: deleting a collection 05/06/23 07:00:57.511
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May  6 07:00:57.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-764" for this suite. 05/06/23 07:00:57.523
------------------------------
â€¢ [1.088 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:00:56.441
    May  6 07:00:56.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename cronjob 05/06/23 07:00:56.442
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:00:57.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:00:57.458
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 05/06/23 07:00:57.46
    STEP: creating 05/06/23 07:00:57.46
    STEP: getting 05/06/23 07:00:57.464
    STEP: listing 05/06/23 07:00:57.467
    STEP: watching 05/06/23 07:00:57.469
    May  6 07:00:57.469: INFO: starting watch
    STEP: cluster-wide listing 05/06/23 07:00:57.47
    STEP: cluster-wide watching 05/06/23 07:00:57.472
    May  6 07:00:57.472: INFO: starting watch
    STEP: patching 05/06/23 07:00:57.473
    STEP: updating 05/06/23 07:00:57.479
    May  6 07:00:57.486: INFO: waiting for watch events with expected annotations
    May  6 07:00:57.486: INFO: saw patched and updated annotations
    STEP: patching /status 05/06/23 07:00:57.486
    STEP: updating /status 05/06/23 07:00:57.491
    STEP: get /status 05/06/23 07:00:57.497
    STEP: deleting 05/06/23 07:00:57.499
    STEP: deleting a collection 05/06/23 07:00:57.511
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May  6 07:00:57.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-764" for this suite. 05/06/23 07:00:57.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:00:57.53
May  6 07:00:57.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename pods 05/06/23 07:00:57.531
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:00:58.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:00:58.545
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
May  6 07:00:58.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: creating the pod 05/06/23 07:00:58.547
STEP: submitting the pod to kubernetes 05/06/23 07:00:58.547
May  6 07:00:58.555: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-cc09a448-c6d7-4dbe-9626-ccc6294b6c90" in namespace "pods-2169" to be "running and ready"
May  6 07:00:58.558: INFO: Pod "pod-logs-websocket-cc09a448-c6d7-4dbe-9626-ccc6294b6c90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.979622ms
May  6 07:00:58.558: INFO: The phase of Pod pod-logs-websocket-cc09a448-c6d7-4dbe-9626-ccc6294b6c90 is Pending, waiting for it to be Running (with Ready = true)
May  6 07:01:00.562: INFO: Pod "pod-logs-websocket-cc09a448-c6d7-4dbe-9626-ccc6294b6c90": Phase="Running", Reason="", readiness=true. Elapsed: 2.006638044s
May  6 07:01:00.562: INFO: The phase of Pod pod-logs-websocket-cc09a448-c6d7-4dbe-9626-ccc6294b6c90 is Running (Ready = true)
May  6 07:01:00.562: INFO: Pod "pod-logs-websocket-cc09a448-c6d7-4dbe-9626-ccc6294b6c90" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  6 07:01:00.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2169" for this suite. 05/06/23 07:01:00.576
------------------------------
â€¢ [3.053 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:00:57.53
    May  6 07:00:57.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename pods 05/06/23 07:00:57.531
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:00:58.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:00:58.545
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    May  6 07:00:58.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: creating the pod 05/06/23 07:00:58.547
    STEP: submitting the pod to kubernetes 05/06/23 07:00:58.547
    May  6 07:00:58.555: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-cc09a448-c6d7-4dbe-9626-ccc6294b6c90" in namespace "pods-2169" to be "running and ready"
    May  6 07:00:58.558: INFO: Pod "pod-logs-websocket-cc09a448-c6d7-4dbe-9626-ccc6294b6c90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.979622ms
    May  6 07:00:58.558: INFO: The phase of Pod pod-logs-websocket-cc09a448-c6d7-4dbe-9626-ccc6294b6c90 is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:01:00.562: INFO: Pod "pod-logs-websocket-cc09a448-c6d7-4dbe-9626-ccc6294b6c90": Phase="Running", Reason="", readiness=true. Elapsed: 2.006638044s
    May  6 07:01:00.562: INFO: The phase of Pod pod-logs-websocket-cc09a448-c6d7-4dbe-9626-ccc6294b6c90 is Running (Ready = true)
    May  6 07:01:00.562: INFO: Pod "pod-logs-websocket-cc09a448-c6d7-4dbe-9626-ccc6294b6c90" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  6 07:01:00.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2169" for this suite. 05/06/23 07:01:00.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:01:00.585
May  6 07:01:00.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename subpath 05/06/23 07:01:00.585
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:01:01.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:01:01.6
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/06/23 07:01:01.602
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-fg9h 05/06/23 07:01:01.611
STEP: Creating a pod to test atomic-volume-subpath 05/06/23 07:01:01.611
May  6 07:01:01.628: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fg9h" in namespace "subpath-541" to be "Succeeded or Failed"
May  6 07:01:01.631: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.492857ms
May  6 07:01:03.635: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 2.006566801s
May  6 07:01:05.635: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 4.006448104s
May  6 07:01:07.634: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 6.006057874s
May  6 07:01:09.634: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 8.005710277s
May  6 07:01:11.635: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 10.006126941s
May  6 07:01:13.635: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 12.00697671s
May  6 07:01:15.634: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 14.005144952s
May  6 07:01:17.636: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 16.007280305s
May  6 07:01:19.635: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 18.006433279s
May  6 07:01:21.636: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 20.00756266s
May  6 07:01:23.635: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=false. Elapsed: 22.007039542s
May  6 07:01:25.634: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005786215s
STEP: Saw pod success 05/06/23 07:01:25.634
May  6 07:01:25.634: INFO: Pod "pod-subpath-test-configmap-fg9h" satisfied condition "Succeeded or Failed"
May  6 07:01:25.637: INFO: Trying to get logs from node cncf-2 pod pod-subpath-test-configmap-fg9h container test-container-subpath-configmap-fg9h: <nil>
STEP: delete the pod 05/06/23 07:01:25.642
May  6 07:01:25.653: INFO: Waiting for pod pod-subpath-test-configmap-fg9h to disappear
May  6 07:01:25.655: INFO: Pod pod-subpath-test-configmap-fg9h no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fg9h 05/06/23 07:01:25.655
May  6 07:01:25.655: INFO: Deleting pod "pod-subpath-test-configmap-fg9h" in namespace "subpath-541"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May  6 07:01:25.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-541" for this suite. 05/06/23 07:01:25.661
------------------------------
â€¢ [SLOW TEST] [25.081 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:01:00.585
    May  6 07:01:00.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename subpath 05/06/23 07:01:00.585
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:01:01.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:01:01.6
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/06/23 07:01:01.602
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-fg9h 05/06/23 07:01:01.611
    STEP: Creating a pod to test atomic-volume-subpath 05/06/23 07:01:01.611
    May  6 07:01:01.628: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fg9h" in namespace "subpath-541" to be "Succeeded or Failed"
    May  6 07:01:01.631: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.492857ms
    May  6 07:01:03.635: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 2.006566801s
    May  6 07:01:05.635: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 4.006448104s
    May  6 07:01:07.634: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 6.006057874s
    May  6 07:01:09.634: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 8.005710277s
    May  6 07:01:11.635: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 10.006126941s
    May  6 07:01:13.635: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 12.00697671s
    May  6 07:01:15.634: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 14.005144952s
    May  6 07:01:17.636: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 16.007280305s
    May  6 07:01:19.635: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 18.006433279s
    May  6 07:01:21.636: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=true. Elapsed: 20.00756266s
    May  6 07:01:23.635: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Running", Reason="", readiness=false. Elapsed: 22.007039542s
    May  6 07:01:25.634: INFO: Pod "pod-subpath-test-configmap-fg9h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005786215s
    STEP: Saw pod success 05/06/23 07:01:25.634
    May  6 07:01:25.634: INFO: Pod "pod-subpath-test-configmap-fg9h" satisfied condition "Succeeded or Failed"
    May  6 07:01:25.637: INFO: Trying to get logs from node cncf-2 pod pod-subpath-test-configmap-fg9h container test-container-subpath-configmap-fg9h: <nil>
    STEP: delete the pod 05/06/23 07:01:25.642
    May  6 07:01:25.653: INFO: Waiting for pod pod-subpath-test-configmap-fg9h to disappear
    May  6 07:01:25.655: INFO: Pod pod-subpath-test-configmap-fg9h no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-fg9h 05/06/23 07:01:25.655
    May  6 07:01:25.655: INFO: Deleting pod "pod-subpath-test-configmap-fg9h" in namespace "subpath-541"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May  6 07:01:25.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-541" for this suite. 05/06/23 07:01:25.661
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:01:25.666
May  6 07:01:25.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename deployment 05/06/23 07:01:25.667
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:01:26.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:01:26.684
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
May  6 07:01:26.686: INFO: Creating simple deployment test-new-deployment
May  6 07:01:26.696: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 05/06/23 07:01:28.706
STEP: updating a scale subresource 05/06/23 07:01:28.709
STEP: verifying the deployment Spec.Replicas was modified 05/06/23 07:01:28.719
STEP: Patch a scale subresource 05/06/23 07:01:28.721
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  6 07:01:28.740: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-1754  ceb7db68-c6c6-4875-94ce-dbaa494b69a4 141534 3 2023-05-06 07:01:26 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-06 07:01:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:01:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005019788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-06 07:01:27 +0000 UTC,LastTransitionTime:2023-05-06 07:01:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-05-06 07:01:27 +0000 UTC,LastTransitionTime:2023-05-06 07:01:26 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  6 07:01:28.743: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-1754  5dc285c1-19cb-4711-916f-77d47c99f30f 141539 3 2023-05-06 07:01:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment ceb7db68-c6c6-4875-94ce-dbaa494b69a4 0xc0019fb997 0xc0019fb998}] [] [{kube-controller-manager Update apps/v1 2023-05-06 07:01:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-06 07:01:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceb7db68-c6c6-4875-94ce-dbaa494b69a4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0019fba28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  6 07:01:28.749: INFO: Pod "test-new-deployment-7f5969cbc7-kr9gl" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-kr9gl test-new-deployment-7f5969cbc7- deployment-1754  432e9fe3-bd61-46ab-a295-d9177304cac6 141537 0 2023-05-06 07:01:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 5dc285c1-19cb-4711-916f-77d47c99f30f 0xc0019fbdb7 0xc0019fbdb8}] [] [{kube-controller-manager Update v1 2023-05-06 07:01:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5dc285c1-19cb-4711-916f-77d47c99f30f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k566v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k566v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:01:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:01:28.750: INFO: Pod "test-new-deployment-7f5969cbc7-vn4bw" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-vn4bw test-new-deployment-7f5969cbc7- deployment-1754  a8cda234-edc1-4e23-8e2a-f861b6d9eae1 141527 0 2023-05-06 07:01:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7561cf4a0bee4108d55d7097c5e0bae8a283e73c7207f61fc9589cf9e1d454ee cni.projectcalico.org/podIP:10.244.20.168/32 cni.projectcalico.org/podIPs:10.244.20.168/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 5dc285c1-19cb-4711-916f-77d47c99f30f 0xc0019fbf40 0xc0019fbf41}] [] [{kube-controller-manager Update v1 2023-05-06 07:01:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5dc285c1-19cb-4711-916f-77d47c99f30f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:01:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:01:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.20.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m7xrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m7xrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:01:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:01:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:01:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:01:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:10.244.20.168,StartTime:2023-05-06 07:01:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:01:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fcb177aeeec64129f2b10c2ad9b92614552b9ac916a098b9203b60d52fb60086,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.20.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May  6 07:01:28.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1754" for this suite. 05/06/23 07:01:28.755
------------------------------
â€¢ [3.099 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:01:25.666
    May  6 07:01:25.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename deployment 05/06/23 07:01:25.667
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:01:26.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:01:26.684
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    May  6 07:01:26.686: INFO: Creating simple deployment test-new-deployment
    May  6 07:01:26.696: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 05/06/23 07:01:28.706
    STEP: updating a scale subresource 05/06/23 07:01:28.709
    STEP: verifying the deployment Spec.Replicas was modified 05/06/23 07:01:28.719
    STEP: Patch a scale subresource 05/06/23 07:01:28.721
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  6 07:01:28.740: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-1754  ceb7db68-c6c6-4875-94ce-dbaa494b69a4 141534 3 2023-05-06 07:01:26 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-06 07:01:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:01:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005019788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-06 07:01:27 +0000 UTC,LastTransitionTime:2023-05-06 07:01:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-05-06 07:01:27 +0000 UTC,LastTransitionTime:2023-05-06 07:01:26 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  6 07:01:28.743: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-1754  5dc285c1-19cb-4711-916f-77d47c99f30f 141539 3 2023-05-06 07:01:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment ceb7db68-c6c6-4875-94ce-dbaa494b69a4 0xc0019fb997 0xc0019fb998}] [] [{kube-controller-manager Update apps/v1 2023-05-06 07:01:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-06 07:01:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ceb7db68-c6c6-4875-94ce-dbaa494b69a4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0019fba28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  6 07:01:28.749: INFO: Pod "test-new-deployment-7f5969cbc7-kr9gl" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-kr9gl test-new-deployment-7f5969cbc7- deployment-1754  432e9fe3-bd61-46ab-a295-d9177304cac6 141537 0 2023-05-06 07:01:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 5dc285c1-19cb-4711-916f-77d47c99f30f 0xc0019fbdb7 0xc0019fbdb8}] [] [{kube-controller-manager Update v1 2023-05-06 07:01:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5dc285c1-19cb-4711-916f-77d47c99f30f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k566v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k566v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:01:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:01:28.750: INFO: Pod "test-new-deployment-7f5969cbc7-vn4bw" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-vn4bw test-new-deployment-7f5969cbc7- deployment-1754  a8cda234-edc1-4e23-8e2a-f861b6d9eae1 141527 0 2023-05-06 07:01:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7561cf4a0bee4108d55d7097c5e0bae8a283e73c7207f61fc9589cf9e1d454ee cni.projectcalico.org/podIP:10.244.20.168/32 cni.projectcalico.org/podIPs:10.244.20.168/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 5dc285c1-19cb-4711-916f-77d47c99f30f 0xc0019fbf40 0xc0019fbf41}] [] [{kube-controller-manager Update v1 2023-05-06 07:01:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5dc285c1-19cb-4711-916f-77d47c99f30f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:01:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:01:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.20.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m7xrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m7xrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:01:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:01:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:01:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:01:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:10.244.20.168,StartTime:2023-05-06 07:01:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:01:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fcb177aeeec64129f2b10c2ad9b92614552b9ac916a098b9203b60d52fb60086,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.20.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May  6 07:01:28.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1754" for this suite. 05/06/23 07:01:28.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:01:28.768
May  6 07:01:28.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:01:28.769
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:01:29.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:01:29.783
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-b26eb488-9294-4ef8-a9cf-75b1a0958cfa 05/06/23 07:01:29.785
STEP: Creating a pod to test consume configMaps 05/06/23 07:01:29.79
May  6 07:01:29.797: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4e13851b-0414-4ae9-9398-3186e47d42be" in namespace "projected-1662" to be "Succeeded or Failed"
May  6 07:01:29.799: INFO: Pod "pod-projected-configmaps-4e13851b-0414-4ae9-9398-3186e47d42be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.171386ms
May  6 07:01:31.802: INFO: Pod "pod-projected-configmaps-4e13851b-0414-4ae9-9398-3186e47d42be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005441355s
May  6 07:01:33.802: INFO: Pod "pod-projected-configmaps-4e13851b-0414-4ae9-9398-3186e47d42be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005365559s
STEP: Saw pod success 05/06/23 07:01:33.802
May  6 07:01:33.802: INFO: Pod "pod-projected-configmaps-4e13851b-0414-4ae9-9398-3186e47d42be" satisfied condition "Succeeded or Failed"
May  6 07:01:33.806: INFO: Trying to get logs from node cncf-2 pod pod-projected-configmaps-4e13851b-0414-4ae9-9398-3186e47d42be container projected-configmap-volume-test: <nil>
STEP: delete the pod 05/06/23 07:01:33.811
May  6 07:01:33.825: INFO: Waiting for pod pod-projected-configmaps-4e13851b-0414-4ae9-9398-3186e47d42be to disappear
May  6 07:01:33.828: INFO: Pod pod-projected-configmaps-4e13851b-0414-4ae9-9398-3186e47d42be no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May  6 07:01:33.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1662" for this suite. 05/06/23 07:01:33.831
------------------------------
â€¢ [SLOW TEST] [5.070 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:01:28.768
    May  6 07:01:28.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:01:28.769
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:01:29.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:01:29.783
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-b26eb488-9294-4ef8-a9cf-75b1a0958cfa 05/06/23 07:01:29.785
    STEP: Creating a pod to test consume configMaps 05/06/23 07:01:29.79
    May  6 07:01:29.797: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4e13851b-0414-4ae9-9398-3186e47d42be" in namespace "projected-1662" to be "Succeeded or Failed"
    May  6 07:01:29.799: INFO: Pod "pod-projected-configmaps-4e13851b-0414-4ae9-9398-3186e47d42be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.171386ms
    May  6 07:01:31.802: INFO: Pod "pod-projected-configmaps-4e13851b-0414-4ae9-9398-3186e47d42be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005441355s
    May  6 07:01:33.802: INFO: Pod "pod-projected-configmaps-4e13851b-0414-4ae9-9398-3186e47d42be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005365559s
    STEP: Saw pod success 05/06/23 07:01:33.802
    May  6 07:01:33.802: INFO: Pod "pod-projected-configmaps-4e13851b-0414-4ae9-9398-3186e47d42be" satisfied condition "Succeeded or Failed"
    May  6 07:01:33.806: INFO: Trying to get logs from node cncf-2 pod pod-projected-configmaps-4e13851b-0414-4ae9-9398-3186e47d42be container projected-configmap-volume-test: <nil>
    STEP: delete the pod 05/06/23 07:01:33.811
    May  6 07:01:33.825: INFO: Waiting for pod pod-projected-configmaps-4e13851b-0414-4ae9-9398-3186e47d42be to disappear
    May  6 07:01:33.828: INFO: Pod pod-projected-configmaps-4e13851b-0414-4ae9-9398-3186e47d42be no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May  6 07:01:33.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1662" for this suite. 05/06/23 07:01:33.831
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:01:33.838
May  6 07:01:33.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename init-container 05/06/23 07:01:33.839
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:01:34.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:01:34.856
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 05/06/23 07:01:34.858
May  6 07:01:34.858: INFO: PodSpec: initContainers in spec.initContainers
May  6 07:02:17.674: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-513aa613-a928-4e76-9b39-96b458d74828", GenerateName:"", Namespace:"init-container-8688", SelfLink:"", UID:"14ba99bd-4143-4f46-8ae7-d0e11d43d83a", ResourceVersion:"141836", Generation:0, CreationTimestamp:time.Date(2023, time.May, 6, 7, 1, 34, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"858164385"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"424a28ec1b32bdebf34385644c5a0672094626489e152354cfe4d68cbb82f3eb", "cni.projectcalico.org/podIP":"10.244.245.119/32", "cni.projectcalico.org/podIPs":"10.244.245.119/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 6, 7, 1, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0003218a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 6, 7, 1, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0003218d8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 6, 7, 2, 17, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000321908), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-74q5w", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0019b5fc0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-74q5w", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-74q5w", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-74q5w", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0028358f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cncf-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000704cb0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002835980)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0028359a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0028359a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0028359ac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0005b4a50), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 6, 7, 1, 34, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 6, 7, 1, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 6, 7, 1, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 6, 7, 1, 34, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.0.240", PodIP:"10.244.245.119", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.245.119"}}, StartTime:time.Date(2023, time.May, 6, 7, 1, 34, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000704d90)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000704e00)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://2a20283e46367b47ed9843862c6934cd899920d98e9a084b7c47d8dde09642c6", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001caa040), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001caa020), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc002835a2f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May  6 07:02:17.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-8688" for this suite. 05/06/23 07:02:17.677
------------------------------
â€¢ [SLOW TEST] [43.845 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:01:33.838
    May  6 07:01:33.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename init-container 05/06/23 07:01:33.839
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:01:34.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:01:34.856
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 05/06/23 07:01:34.858
    May  6 07:01:34.858: INFO: PodSpec: initContainers in spec.initContainers
    May  6 07:02:17.674: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-513aa613-a928-4e76-9b39-96b458d74828", GenerateName:"", Namespace:"init-container-8688", SelfLink:"", UID:"14ba99bd-4143-4f46-8ae7-d0e11d43d83a", ResourceVersion:"141836", Generation:0, CreationTimestamp:time.Date(2023, time.May, 6, 7, 1, 34, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"858164385"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"424a28ec1b32bdebf34385644c5a0672094626489e152354cfe4d68cbb82f3eb", "cni.projectcalico.org/podIP":"10.244.245.119/32", "cni.projectcalico.org/podIPs":"10.244.245.119/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 6, 7, 1, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0003218a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 6, 7, 1, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0003218d8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 6, 7, 2, 17, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000321908), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-74q5w", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0019b5fc0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-74q5w", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-74q5w", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-74q5w", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0028358f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cncf-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000704cb0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002835980)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0028359a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0028359a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0028359ac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0005b4a50), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 6, 7, 1, 34, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 6, 7, 1, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 6, 7, 1, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 6, 7, 1, 34, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.0.240", PodIP:"10.244.245.119", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.245.119"}}, StartTime:time.Date(2023, time.May, 6, 7, 1, 34, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000704d90)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000704e00)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://2a20283e46367b47ed9843862c6934cd899920d98e9a084b7c47d8dde09642c6", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001caa040), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001caa020), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc002835a2f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May  6 07:02:17.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-8688" for this suite. 05/06/23 07:02:17.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:02:17.684
May  6 07:02:17.684: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename webhook 05/06/23 07:02:17.685
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:02:18.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:02:18.702
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/06/23 07:02:20.72
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:02:21.178
STEP: Deploying the webhook pod 05/06/23 07:02:21.184
STEP: Wait for the deployment to be ready 05/06/23 07:02:21.195
May  6 07:02:21.201: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/06/23 07:02:23.21
STEP: Verifying the service has paired with the endpoint 05/06/23 07:02:23.222
May  6 07:02:24.222: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 05/06/23 07:02:24.225
STEP: create a pod that should be updated by the webhook 05/06/23 07:02:24.236
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:02:24.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7830" for this suite. 05/06/23 07:02:24.303
STEP: Destroying namespace "webhook-7830-markers" for this suite. 05/06/23 07:02:24.31
------------------------------
â€¢ [SLOW TEST] [6.633 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:02:17.684
    May  6 07:02:17.684: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename webhook 05/06/23 07:02:17.685
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:02:18.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:02:18.702
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/06/23 07:02:20.72
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:02:21.178
    STEP: Deploying the webhook pod 05/06/23 07:02:21.184
    STEP: Wait for the deployment to be ready 05/06/23 07:02:21.195
    May  6 07:02:21.201: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/06/23 07:02:23.21
    STEP: Verifying the service has paired with the endpoint 05/06/23 07:02:23.222
    May  6 07:02:24.222: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 05/06/23 07:02:24.225
    STEP: create a pod that should be updated by the webhook 05/06/23 07:02:24.236
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:02:24.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7830" for this suite. 05/06/23 07:02:24.303
    STEP: Destroying namespace "webhook-7830-markers" for this suite. 05/06/23 07:02:24.31
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:02:24.317
May  6 07:02:24.317: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename aggregator 05/06/23 07:02:24.318
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:02:25.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:02:25.336
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
May  6 07:02:25.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 05/06/23 07:02:25.339
May  6 07:02:25.689: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May  6 07:02:27.728: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 07:02:29.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 07:02:31.732: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 07:02:33.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 07:02:35.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 07:02:37.732: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 07:02:39.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 07:02:41.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 07:02:43.732: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 07:02:45.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 07:02:47.858: INFO: Waited 116.081548ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 05/06/23 07:02:47.906
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 05/06/23 07:02:47.908
STEP: List APIServices 05/06/23 07:02:47.914
May  6 07:02:47.918: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
May  6 07:02:48.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-7912" for this suite. 05/06/23 07:02:48.199
------------------------------
â€¢ [SLOW TEST] [23.935 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:02:24.317
    May  6 07:02:24.317: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename aggregator 05/06/23 07:02:24.318
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:02:25.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:02:25.336
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    May  6 07:02:25.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 05/06/23 07:02:25.339
    May  6 07:02:25.689: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    May  6 07:02:27.728: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  6 07:02:29.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  6 07:02:31.732: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  6 07:02:33.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  6 07:02:35.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  6 07:02:37.732: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  6 07:02:39.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  6 07:02:41.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  6 07:02:43.732: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  6 07:02:45.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 2, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  6 07:02:47.858: INFO: Waited 116.081548ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 05/06/23 07:02:47.906
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 05/06/23 07:02:47.908
    STEP: List APIServices 05/06/23 07:02:47.914
    May  6 07:02:47.918: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    May  6 07:02:48.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-7912" for this suite. 05/06/23 07:02:48.199
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:02:48.253
May  6 07:02:48.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename pods 05/06/23 07:02:48.253
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:02:49.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:02:49.271
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
May  6 07:02:49.283: INFO: Waiting up to 5m0s for pod "server-envvars-2dabb1e2-20c4-4b54-97d1-1f371c25b354" in namespace "pods-3272" to be "running and ready"
May  6 07:02:49.285: INFO: Pod "server-envvars-2dabb1e2-20c4-4b54-97d1-1f371c25b354": Phase="Pending", Reason="", readiness=false. Elapsed: 2.543834ms
May  6 07:02:49.285: INFO: The phase of Pod server-envvars-2dabb1e2-20c4-4b54-97d1-1f371c25b354 is Pending, waiting for it to be Running (with Ready = true)
May  6 07:02:51.289: INFO: Pod "server-envvars-2dabb1e2-20c4-4b54-97d1-1f371c25b354": Phase="Running", Reason="", readiness=true. Elapsed: 2.006384453s
May  6 07:02:51.289: INFO: The phase of Pod server-envvars-2dabb1e2-20c4-4b54-97d1-1f371c25b354 is Running (Ready = true)
May  6 07:02:51.289: INFO: Pod "server-envvars-2dabb1e2-20c4-4b54-97d1-1f371c25b354" satisfied condition "running and ready"
May  6 07:02:51.314: INFO: Waiting up to 5m0s for pod "client-envvars-62b537de-2f62-4121-8f9f-636409d74902" in namespace "pods-3272" to be "Succeeded or Failed"
May  6 07:02:51.317: INFO: Pod "client-envvars-62b537de-2f62-4121-8f9f-636409d74902": Phase="Pending", Reason="", readiness=false. Elapsed: 2.927012ms
May  6 07:02:53.321: INFO: Pod "client-envvars-62b537de-2f62-4121-8f9f-636409d74902": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00695118s
May  6 07:02:55.320: INFO: Pod "client-envvars-62b537de-2f62-4121-8f9f-636409d74902": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006070996s
STEP: Saw pod success 05/06/23 07:02:55.32
May  6 07:02:55.320: INFO: Pod "client-envvars-62b537de-2f62-4121-8f9f-636409d74902" satisfied condition "Succeeded or Failed"
May  6 07:02:55.323: INFO: Trying to get logs from node cncf-3 pod client-envvars-62b537de-2f62-4121-8f9f-636409d74902 container env3cont: <nil>
STEP: delete the pod 05/06/23 07:02:55.332
May  6 07:02:55.343: INFO: Waiting for pod client-envvars-62b537de-2f62-4121-8f9f-636409d74902 to disappear
May  6 07:02:55.345: INFO: Pod client-envvars-62b537de-2f62-4121-8f9f-636409d74902 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  6 07:02:55.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3272" for this suite. 05/06/23 07:02:55.348
------------------------------
â€¢ [SLOW TEST] [7.101 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:02:48.253
    May  6 07:02:48.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename pods 05/06/23 07:02:48.253
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:02:49.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:02:49.271
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    May  6 07:02:49.283: INFO: Waiting up to 5m0s for pod "server-envvars-2dabb1e2-20c4-4b54-97d1-1f371c25b354" in namespace "pods-3272" to be "running and ready"
    May  6 07:02:49.285: INFO: Pod "server-envvars-2dabb1e2-20c4-4b54-97d1-1f371c25b354": Phase="Pending", Reason="", readiness=false. Elapsed: 2.543834ms
    May  6 07:02:49.285: INFO: The phase of Pod server-envvars-2dabb1e2-20c4-4b54-97d1-1f371c25b354 is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:02:51.289: INFO: Pod "server-envvars-2dabb1e2-20c4-4b54-97d1-1f371c25b354": Phase="Running", Reason="", readiness=true. Elapsed: 2.006384453s
    May  6 07:02:51.289: INFO: The phase of Pod server-envvars-2dabb1e2-20c4-4b54-97d1-1f371c25b354 is Running (Ready = true)
    May  6 07:02:51.289: INFO: Pod "server-envvars-2dabb1e2-20c4-4b54-97d1-1f371c25b354" satisfied condition "running and ready"
    May  6 07:02:51.314: INFO: Waiting up to 5m0s for pod "client-envvars-62b537de-2f62-4121-8f9f-636409d74902" in namespace "pods-3272" to be "Succeeded or Failed"
    May  6 07:02:51.317: INFO: Pod "client-envvars-62b537de-2f62-4121-8f9f-636409d74902": Phase="Pending", Reason="", readiness=false. Elapsed: 2.927012ms
    May  6 07:02:53.321: INFO: Pod "client-envvars-62b537de-2f62-4121-8f9f-636409d74902": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00695118s
    May  6 07:02:55.320: INFO: Pod "client-envvars-62b537de-2f62-4121-8f9f-636409d74902": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006070996s
    STEP: Saw pod success 05/06/23 07:02:55.32
    May  6 07:02:55.320: INFO: Pod "client-envvars-62b537de-2f62-4121-8f9f-636409d74902" satisfied condition "Succeeded or Failed"
    May  6 07:02:55.323: INFO: Trying to get logs from node cncf-3 pod client-envvars-62b537de-2f62-4121-8f9f-636409d74902 container env3cont: <nil>
    STEP: delete the pod 05/06/23 07:02:55.332
    May  6 07:02:55.343: INFO: Waiting for pod client-envvars-62b537de-2f62-4121-8f9f-636409d74902 to disappear
    May  6 07:02:55.345: INFO: Pod client-envvars-62b537de-2f62-4121-8f9f-636409d74902 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  6 07:02:55.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3272" for this suite. 05/06/23 07:02:55.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:02:55.354
May  6 07:02:55.354: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename watch 05/06/23 07:02:55.355
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:02:56.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:02:56.372
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 05/06/23 07:02:56.374
STEP: creating a new configmap 05/06/23 07:02:56.375
STEP: modifying the configmap once 05/06/23 07:02:56.379
STEP: closing the watch once it receives two notifications 05/06/23 07:02:56.386
May  6 07:02:56.386: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8406  6218c733-1f7a-4855-8604-5679b0400e88 142189 0 2023-05-06 07:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-06 07:02:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  6 07:02:56.386: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8406  6218c733-1f7a-4855-8604-5679b0400e88 142190 0 2023-05-06 07:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-06 07:02:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 05/06/23 07:02:56.386
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 05/06/23 07:02:56.392
STEP: deleting the configmap 05/06/23 07:02:56.393
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 05/06/23 07:02:56.398
May  6 07:02:56.398: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8406  6218c733-1f7a-4855-8604-5679b0400e88 142191 0 2023-05-06 07:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-06 07:02:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  6 07:02:56.398: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8406  6218c733-1f7a-4855-8604-5679b0400e88 142192 0 2023-05-06 07:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-06 07:02:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May  6 07:02:56.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8406" for this suite. 05/06/23 07:02:56.401
------------------------------
â€¢ [1.053 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:02:55.354
    May  6 07:02:55.354: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename watch 05/06/23 07:02:55.355
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:02:56.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:02:56.372
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 05/06/23 07:02:56.374
    STEP: creating a new configmap 05/06/23 07:02:56.375
    STEP: modifying the configmap once 05/06/23 07:02:56.379
    STEP: closing the watch once it receives two notifications 05/06/23 07:02:56.386
    May  6 07:02:56.386: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8406  6218c733-1f7a-4855-8604-5679b0400e88 142189 0 2023-05-06 07:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-06 07:02:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  6 07:02:56.386: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8406  6218c733-1f7a-4855-8604-5679b0400e88 142190 0 2023-05-06 07:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-06 07:02:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 05/06/23 07:02:56.386
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 05/06/23 07:02:56.392
    STEP: deleting the configmap 05/06/23 07:02:56.393
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 05/06/23 07:02:56.398
    May  6 07:02:56.398: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8406  6218c733-1f7a-4855-8604-5679b0400e88 142191 0 2023-05-06 07:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-06 07:02:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  6 07:02:56.398: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8406  6218c733-1f7a-4855-8604-5679b0400e88 142192 0 2023-05-06 07:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-06 07:02:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May  6 07:02:56.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8406" for this suite. 05/06/23 07:02:56.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:02:56.408
May  6 07:02:56.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename crd-publish-openapi 05/06/23 07:02:56.409
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:02:57.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:02:57.426
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
May  6 07:02:57.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/06/23 07:02:59.36
May  6 07:02:59.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6120 --namespace=crd-publish-openapi-6120 create -f -'
May  6 07:02:59.929: INFO: stderr: ""
May  6 07:02:59.929: INFO: stdout: "e2e-test-crd-publish-openapi-4818-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May  6 07:02:59.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6120 --namespace=crd-publish-openapi-6120 delete e2e-test-crd-publish-openapi-4818-crds test-cr'
May  6 07:03:00.007: INFO: stderr: ""
May  6 07:03:00.007: INFO: stdout: "e2e-test-crd-publish-openapi-4818-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May  6 07:03:00.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6120 --namespace=crd-publish-openapi-6120 apply -f -'
May  6 07:03:00.436: INFO: stderr: ""
May  6 07:03:00.436: INFO: stdout: "e2e-test-crd-publish-openapi-4818-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May  6 07:03:00.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6120 --namespace=crd-publish-openapi-6120 delete e2e-test-crd-publish-openapi-4818-crds test-cr'
May  6 07:03:00.494: INFO: stderr: ""
May  6 07:03:00.494: INFO: stdout: "e2e-test-crd-publish-openapi-4818-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 05/06/23 07:03:00.494
May  6 07:03:00.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6120 explain e2e-test-crd-publish-openapi-4818-crds'
May  6 07:03:00.964: INFO: stderr: ""
May  6 07:03:00.964: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4818-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:03:03.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6120" for this suite. 05/06/23 07:03:03.421
------------------------------
â€¢ [SLOW TEST] [7.019 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:02:56.408
    May  6 07:02:56.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename crd-publish-openapi 05/06/23 07:02:56.409
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:02:57.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:02:57.426
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    May  6 07:02:57.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/06/23 07:02:59.36
    May  6 07:02:59.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6120 --namespace=crd-publish-openapi-6120 create -f -'
    May  6 07:02:59.929: INFO: stderr: ""
    May  6 07:02:59.929: INFO: stdout: "e2e-test-crd-publish-openapi-4818-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    May  6 07:02:59.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6120 --namespace=crd-publish-openapi-6120 delete e2e-test-crd-publish-openapi-4818-crds test-cr'
    May  6 07:03:00.007: INFO: stderr: ""
    May  6 07:03:00.007: INFO: stdout: "e2e-test-crd-publish-openapi-4818-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    May  6 07:03:00.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6120 --namespace=crd-publish-openapi-6120 apply -f -'
    May  6 07:03:00.436: INFO: stderr: ""
    May  6 07:03:00.436: INFO: stdout: "e2e-test-crd-publish-openapi-4818-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    May  6 07:03:00.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6120 --namespace=crd-publish-openapi-6120 delete e2e-test-crd-publish-openapi-4818-crds test-cr'
    May  6 07:03:00.494: INFO: stderr: ""
    May  6 07:03:00.494: INFO: stdout: "e2e-test-crd-publish-openapi-4818-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 05/06/23 07:03:00.494
    May  6 07:03:00.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6120 explain e2e-test-crd-publish-openapi-4818-crds'
    May  6 07:03:00.964: INFO: stderr: ""
    May  6 07:03:00.964: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4818-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:03:03.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6120" for this suite. 05/06/23 07:03:03.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:03:03.428
May  6 07:03:03.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename daemonsets 05/06/23 07:03:03.429
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:03:04.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:03:04.446
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
May  6 07:03:05.472: INFO: Create a RollingUpdate DaemonSet
May  6 07:03:05.478: INFO: Check that daemon pods launch on every node of the cluster
May  6 07:03:05.487: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 07:03:05.487: INFO: Node cncf-0 is running 0 daemon pod, expected 1
May  6 07:03:06.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  6 07:03:06.493: INFO: Node cncf-1 is running 0 daemon pod, expected 1
May  6 07:03:07.495: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
May  6 07:03:07.495: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
May  6 07:03:07.495: INFO: Update the DaemonSet to trigger a rollout
May  6 07:03:07.504: INFO: Updating DaemonSet daemon-set
May  6 07:03:10.518: INFO: Roll back the DaemonSet before rollout is complete
May  6 07:03:10.527: INFO: Updating DaemonSet daemon-set
May  6 07:03:10.527: INFO: Make sure DaemonSet rollback is complete
May  6 07:03:10.530: INFO: Wrong image for pod: daemon-set-7v567. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
May  6 07:03:10.530: INFO: Pod daemon-set-7v567 is not available
May  6 07:03:13.537: INFO: Pod daemon-set-9gtmj is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/06/23 07:03:13.544
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1815, will wait for the garbage collector to delete the pods 05/06/23 07:03:13.545
May  6 07:03:13.602: INFO: Deleting DaemonSet.extensions daemon-set took: 5.150188ms
May  6 07:03:13.703: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.761864ms
May  6 07:03:15.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 07:03:15.807: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  6 07:03:15.810: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"142462"},"items":null}

May  6 07:03:15.812: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"142462"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 07:03:15.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1815" for this suite. 05/06/23 07:03:15.827
------------------------------
â€¢ [SLOW TEST] [12.405 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:03:03.428
    May  6 07:03:03.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename daemonsets 05/06/23 07:03:03.429
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:03:04.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:03:04.446
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    May  6 07:03:05.472: INFO: Create a RollingUpdate DaemonSet
    May  6 07:03:05.478: INFO: Check that daemon pods launch on every node of the cluster
    May  6 07:03:05.487: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 07:03:05.487: INFO: Node cncf-0 is running 0 daemon pod, expected 1
    May  6 07:03:06.493: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  6 07:03:06.493: INFO: Node cncf-1 is running 0 daemon pod, expected 1
    May  6 07:03:07.495: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    May  6 07:03:07.495: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    May  6 07:03:07.495: INFO: Update the DaemonSet to trigger a rollout
    May  6 07:03:07.504: INFO: Updating DaemonSet daemon-set
    May  6 07:03:10.518: INFO: Roll back the DaemonSet before rollout is complete
    May  6 07:03:10.527: INFO: Updating DaemonSet daemon-set
    May  6 07:03:10.527: INFO: Make sure DaemonSet rollback is complete
    May  6 07:03:10.530: INFO: Wrong image for pod: daemon-set-7v567. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    May  6 07:03:10.530: INFO: Pod daemon-set-7v567 is not available
    May  6 07:03:13.537: INFO: Pod daemon-set-9gtmj is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/06/23 07:03:13.544
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1815, will wait for the garbage collector to delete the pods 05/06/23 07:03:13.545
    May  6 07:03:13.602: INFO: Deleting DaemonSet.extensions daemon-set took: 5.150188ms
    May  6 07:03:13.703: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.761864ms
    May  6 07:03:15.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 07:03:15.807: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  6 07:03:15.810: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"142462"},"items":null}

    May  6 07:03:15.812: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"142462"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 07:03:15.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1815" for this suite. 05/06/23 07:03:15.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:03:15.833
May  6 07:03:15.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename secrets 05/06/23 07:03:15.834
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:03:16.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:03:16.853
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-15bbdfa4-86c3-4434-9be1-7979b229bec6 05/06/23 07:03:16.854
STEP: Creating a pod to test consume secrets 05/06/23 07:03:16.859
May  6 07:03:16.867: INFO: Waiting up to 5m0s for pod "pod-secrets-b76fa6d8-e25d-4f5f-8b60-11f55a9edbdc" in namespace "secrets-3787" to be "Succeeded or Failed"
May  6 07:03:16.869: INFO: Pod "pod-secrets-b76fa6d8-e25d-4f5f-8b60-11f55a9edbdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.336049ms
May  6 07:03:18.873: INFO: Pod "pod-secrets-b76fa6d8-e25d-4f5f-8b60-11f55a9edbdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006122367s
May  6 07:03:20.873: INFO: Pod "pod-secrets-b76fa6d8-e25d-4f5f-8b60-11f55a9edbdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006580333s
STEP: Saw pod success 05/06/23 07:03:20.873
May  6 07:03:20.873: INFO: Pod "pod-secrets-b76fa6d8-e25d-4f5f-8b60-11f55a9edbdc" satisfied condition "Succeeded or Failed"
May  6 07:03:20.876: INFO: Trying to get logs from node cncf-0 pod pod-secrets-b76fa6d8-e25d-4f5f-8b60-11f55a9edbdc container secret-volume-test: <nil>
STEP: delete the pod 05/06/23 07:03:20.885
May  6 07:03:20.900: INFO: Waiting for pod pod-secrets-b76fa6d8-e25d-4f5f-8b60-11f55a9edbdc to disappear
May  6 07:03:20.903: INFO: Pod pod-secrets-b76fa6d8-e25d-4f5f-8b60-11f55a9edbdc no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May  6 07:03:20.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3787" for this suite. 05/06/23 07:03:20.906
------------------------------
â€¢ [SLOW TEST] [5.078 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:03:15.833
    May  6 07:03:15.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename secrets 05/06/23 07:03:15.834
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:03:16.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:03:16.853
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-15bbdfa4-86c3-4434-9be1-7979b229bec6 05/06/23 07:03:16.854
    STEP: Creating a pod to test consume secrets 05/06/23 07:03:16.859
    May  6 07:03:16.867: INFO: Waiting up to 5m0s for pod "pod-secrets-b76fa6d8-e25d-4f5f-8b60-11f55a9edbdc" in namespace "secrets-3787" to be "Succeeded or Failed"
    May  6 07:03:16.869: INFO: Pod "pod-secrets-b76fa6d8-e25d-4f5f-8b60-11f55a9edbdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.336049ms
    May  6 07:03:18.873: INFO: Pod "pod-secrets-b76fa6d8-e25d-4f5f-8b60-11f55a9edbdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006122367s
    May  6 07:03:20.873: INFO: Pod "pod-secrets-b76fa6d8-e25d-4f5f-8b60-11f55a9edbdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006580333s
    STEP: Saw pod success 05/06/23 07:03:20.873
    May  6 07:03:20.873: INFO: Pod "pod-secrets-b76fa6d8-e25d-4f5f-8b60-11f55a9edbdc" satisfied condition "Succeeded or Failed"
    May  6 07:03:20.876: INFO: Trying to get logs from node cncf-0 pod pod-secrets-b76fa6d8-e25d-4f5f-8b60-11f55a9edbdc container secret-volume-test: <nil>
    STEP: delete the pod 05/06/23 07:03:20.885
    May  6 07:03:20.900: INFO: Waiting for pod pod-secrets-b76fa6d8-e25d-4f5f-8b60-11f55a9edbdc to disappear
    May  6 07:03:20.903: INFO: Pod pod-secrets-b76fa6d8-e25d-4f5f-8b60-11f55a9edbdc no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May  6 07:03:20.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3787" for this suite. 05/06/23 07:03:20.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:03:20.913
May  6 07:03:20.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename resourcequota 05/06/23 07:03:20.913
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:03:21.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:03:21.932
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 05/06/23 07:03:21.934
STEP: Creating a ResourceQuota 05/06/23 07:03:26.938
STEP: Ensuring resource quota status is calculated 05/06/23 07:03:26.943
STEP: Creating a ReplicaSet 05/06/23 07:03:28.946
STEP: Ensuring resource quota status captures replicaset creation 05/06/23 07:03:28.958
STEP: Deleting a ReplicaSet 05/06/23 07:03:30.963
STEP: Ensuring resource quota status released usage 05/06/23 07:03:30.969
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  6 07:03:32.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7678" for this suite. 05/06/23 07:03:32.976
------------------------------
â€¢ [SLOW TEST] [12.069 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:03:20.913
    May  6 07:03:20.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename resourcequota 05/06/23 07:03:20.913
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:03:21.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:03:21.932
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 05/06/23 07:03:21.934
    STEP: Creating a ResourceQuota 05/06/23 07:03:26.938
    STEP: Ensuring resource quota status is calculated 05/06/23 07:03:26.943
    STEP: Creating a ReplicaSet 05/06/23 07:03:28.946
    STEP: Ensuring resource quota status captures replicaset creation 05/06/23 07:03:28.958
    STEP: Deleting a ReplicaSet 05/06/23 07:03:30.963
    STEP: Ensuring resource quota status released usage 05/06/23 07:03:30.969
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  6 07:03:32.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7678" for this suite. 05/06/23 07:03:32.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:03:32.982
May  6 07:03:32.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename downward-api 05/06/23 07:03:32.983
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:03:34.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:03:34.004
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 05/06/23 07:03:34.006
May  6 07:03:34.014: INFO: Waiting up to 5m0s for pod "downwardapi-volume-541eae63-3aa2-4d79-a9e4-22b1cd10cb56" in namespace "downward-api-5176" to be "Succeeded or Failed"
May  6 07:03:34.018: INFO: Pod "downwardapi-volume-541eae63-3aa2-4d79-a9e4-22b1cd10cb56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.23041ms
May  6 07:03:36.021: INFO: Pod "downwardapi-volume-541eae63-3aa2-4d79-a9e4-22b1cd10cb56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007615752s
May  6 07:03:38.022: INFO: Pod "downwardapi-volume-541eae63-3aa2-4d79-a9e4-22b1cd10cb56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008343295s
STEP: Saw pod success 05/06/23 07:03:38.022
May  6 07:03:38.022: INFO: Pod "downwardapi-volume-541eae63-3aa2-4d79-a9e4-22b1cd10cb56" satisfied condition "Succeeded or Failed"
May  6 07:03:38.025: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-541eae63-3aa2-4d79-a9e4-22b1cd10cb56 container client-container: <nil>
STEP: delete the pod 05/06/23 07:03:38.032
May  6 07:03:38.048: INFO: Waiting for pod downwardapi-volume-541eae63-3aa2-4d79-a9e4-22b1cd10cb56 to disappear
May  6 07:03:38.050: INFO: Pod downwardapi-volume-541eae63-3aa2-4d79-a9e4-22b1cd10cb56 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  6 07:03:38.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5176" for this suite. 05/06/23 07:03:38.054
------------------------------
â€¢ [SLOW TEST] [5.078 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:03:32.982
    May  6 07:03:32.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename downward-api 05/06/23 07:03:32.983
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:03:34.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:03:34.004
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 05/06/23 07:03:34.006
    May  6 07:03:34.014: INFO: Waiting up to 5m0s for pod "downwardapi-volume-541eae63-3aa2-4d79-a9e4-22b1cd10cb56" in namespace "downward-api-5176" to be "Succeeded or Failed"
    May  6 07:03:34.018: INFO: Pod "downwardapi-volume-541eae63-3aa2-4d79-a9e4-22b1cd10cb56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.23041ms
    May  6 07:03:36.021: INFO: Pod "downwardapi-volume-541eae63-3aa2-4d79-a9e4-22b1cd10cb56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007615752s
    May  6 07:03:38.022: INFO: Pod "downwardapi-volume-541eae63-3aa2-4d79-a9e4-22b1cd10cb56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008343295s
    STEP: Saw pod success 05/06/23 07:03:38.022
    May  6 07:03:38.022: INFO: Pod "downwardapi-volume-541eae63-3aa2-4d79-a9e4-22b1cd10cb56" satisfied condition "Succeeded or Failed"
    May  6 07:03:38.025: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-541eae63-3aa2-4d79-a9e4-22b1cd10cb56 container client-container: <nil>
    STEP: delete the pod 05/06/23 07:03:38.032
    May  6 07:03:38.048: INFO: Waiting for pod downwardapi-volume-541eae63-3aa2-4d79-a9e4-22b1cd10cb56 to disappear
    May  6 07:03:38.050: INFO: Pod downwardapi-volume-541eae63-3aa2-4d79-a9e4-22b1cd10cb56 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  6 07:03:38.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5176" for this suite. 05/06/23 07:03:38.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:03:38.062
May  6 07:03:38.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename container-runtime 05/06/23 07:03:38.063
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:03:39.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:03:39.08
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 05/06/23 07:03:39.082
STEP: wait for the container to reach Failed 05/06/23 07:03:39.089
STEP: get the container status 05/06/23 07:03:43.104
STEP: the container should be terminated 05/06/23 07:03:43.106
STEP: the termination message should be set 05/06/23 07:03:43.106
May  6 07:03:43.106: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 05/06/23 07:03:43.106
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May  6 07:03:43.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9134" for this suite. 05/06/23 07:03:43.125
------------------------------
â€¢ [SLOW TEST] [5.069 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:03:38.062
    May  6 07:03:38.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename container-runtime 05/06/23 07:03:38.063
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:03:39.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:03:39.08
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 05/06/23 07:03:39.082
    STEP: wait for the container to reach Failed 05/06/23 07:03:39.089
    STEP: get the container status 05/06/23 07:03:43.104
    STEP: the container should be terminated 05/06/23 07:03:43.106
    STEP: the termination message should be set 05/06/23 07:03:43.106
    May  6 07:03:43.106: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 05/06/23 07:03:43.106
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May  6 07:03:43.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9134" for this suite. 05/06/23 07:03:43.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:03:43.132
May  6 07:03:43.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:03:43.133
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:03:44.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:03:44.153
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 05/06/23 07:03:44.155
May  6 07:03:44.164: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d7e6189b-401d-4b8b-8106-eb97435607e0" in namespace "projected-15" to be "Succeeded or Failed"
May  6 07:03:44.167: INFO: Pod "downwardapi-volume-d7e6189b-401d-4b8b-8106-eb97435607e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.400271ms
May  6 07:03:46.169: INFO: Pod "downwardapi-volume-d7e6189b-401d-4b8b-8106-eb97435607e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005344235s
May  6 07:03:48.171: INFO: Pod "downwardapi-volume-d7e6189b-401d-4b8b-8106-eb97435607e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006590695s
STEP: Saw pod success 05/06/23 07:03:48.171
May  6 07:03:48.171: INFO: Pod "downwardapi-volume-d7e6189b-401d-4b8b-8106-eb97435607e0" satisfied condition "Succeeded or Failed"
May  6 07:03:48.175: INFO: Trying to get logs from node cncf-3 pod downwardapi-volume-d7e6189b-401d-4b8b-8106-eb97435607e0 container client-container: <nil>
STEP: delete the pod 05/06/23 07:03:48.183
May  6 07:03:48.195: INFO: Waiting for pod downwardapi-volume-d7e6189b-401d-4b8b-8106-eb97435607e0 to disappear
May  6 07:03:48.197: INFO: Pod downwardapi-volume-d7e6189b-401d-4b8b-8106-eb97435607e0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  6 07:03:48.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-15" for this suite. 05/06/23 07:03:48.2
------------------------------
â€¢ [SLOW TEST] [5.073 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:03:43.132
    May  6 07:03:43.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:03:43.133
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:03:44.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:03:44.153
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 05/06/23 07:03:44.155
    May  6 07:03:44.164: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d7e6189b-401d-4b8b-8106-eb97435607e0" in namespace "projected-15" to be "Succeeded or Failed"
    May  6 07:03:44.167: INFO: Pod "downwardapi-volume-d7e6189b-401d-4b8b-8106-eb97435607e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.400271ms
    May  6 07:03:46.169: INFO: Pod "downwardapi-volume-d7e6189b-401d-4b8b-8106-eb97435607e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005344235s
    May  6 07:03:48.171: INFO: Pod "downwardapi-volume-d7e6189b-401d-4b8b-8106-eb97435607e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006590695s
    STEP: Saw pod success 05/06/23 07:03:48.171
    May  6 07:03:48.171: INFO: Pod "downwardapi-volume-d7e6189b-401d-4b8b-8106-eb97435607e0" satisfied condition "Succeeded or Failed"
    May  6 07:03:48.175: INFO: Trying to get logs from node cncf-3 pod downwardapi-volume-d7e6189b-401d-4b8b-8106-eb97435607e0 container client-container: <nil>
    STEP: delete the pod 05/06/23 07:03:48.183
    May  6 07:03:48.195: INFO: Waiting for pod downwardapi-volume-d7e6189b-401d-4b8b-8106-eb97435607e0 to disappear
    May  6 07:03:48.197: INFO: Pod downwardapi-volume-d7e6189b-401d-4b8b-8106-eb97435607e0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  6 07:03:48.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-15" for this suite. 05/06/23 07:03:48.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:03:48.206
May  6 07:03:48.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename daemonsets 05/06/23 07:03:48.207
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:03:49.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:03:49.224
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 05/06/23 07:03:50.245
STEP: Check that daemon pods launch on every node of the cluster. 05/06/23 07:03:50.251
May  6 07:03:50.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 07:03:50.256: INFO: Node cncf-0 is running 0 daemon pod, expected 1
May  6 07:03:51.263: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  6 07:03:51.263: INFO: Node cncf-0 is running 0 daemon pod, expected 1
May  6 07:03:52.263: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
May  6 07:03:52.263: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: listing all DeamonSets 05/06/23 07:03:52.265
STEP: DeleteCollection of the DaemonSets 05/06/23 07:03:52.268
STEP: Verify that ReplicaSets have been deleted 05/06/23 07:03:52.274
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
May  6 07:03:52.281: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"142825"},"items":null}

May  6 07:03:52.283: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"142825"},"items":[{"metadata":{"name":"daemon-set-2n7d6","generateName":"daemon-set-","namespace":"daemonsets-1648","uid":"51b48ad3-a5a9-41e6-b8a0-bf86a9fdf3a1","resourceVersion":"142821","creationTimestamp":"2023-05-06T07:03:50Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"cb589c9f1fe5b27c2ccf1ee677f610848d970b4a17a833a6c8e40b820d1f5d6e","cni.projectcalico.org/podIP":"10.244.245.125/32","cni.projectcalico.org/podIPs":"10.244.245.125/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4ac2e1fa-07e6-449c-87e0-d9b08cb53e41","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ac2e1fa-07e6-449c-87e0-d9b08cb53e41\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.245.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-lsb2m","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-lsb2m","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:50Z"}],"hostIP":"10.0.0.240","podIP":"10.244.245.125","podIPs":[{"ip":"10.244.245.125"}],"startTime":"2023-05-06T07:03:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-06T07:03:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4ff29e1f7cda63e2b2f48d964aa8c995fab8855bc05466e3f38c2374886f9f4c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-6kvt7","generateName":"daemon-set-","namespace":"daemonsets-1648","uid":"cdd7de39-6fcb-4d23-b608-0033bed13829","resourceVersion":"142815","creationTimestamp":"2023-05-06T07:03:50Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6b8e1fd6e5c71b51c7f6909f2edbd7d80b7ff8043662a802234be464e36db5dc","cni.projectcalico.org/podIP":"10.244.174.169/32","cni.projectcalico.org/podIPs":"10.244.174.169/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4ac2e1fa-07e6-449c-87e0-d9b08cb53e41","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ac2e1fa-07e6-449c-87e0-d9b08cb53e41\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.174.169\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4p85r","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4p85r","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:50Z"}],"hostIP":"10.0.0.134","podIP":"10.244.174.169","podIPs":[{"ip":"10.244.174.169"}],"startTime":"2023-05-06T07:03:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-06T07:03:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e985b62eb884d136a47cf61ea35ad0ad25214a0b0031670bfffc365aa3681186","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-dtkqv","generateName":"daemon-set-","namespace":"daemonsets-1648","uid":"bb608f37-59fd-4417-bc9d-32ff40bc80f4","resourceVersion":"142812","creationTimestamp":"2023-05-06T07:03:50Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"b1e2275b9eb3ae5bf46bb73a396f00bc6d1ccca2691e1d06d8048635937c0810","cni.projectcalico.org/podIP":"10.244.20.171/32","cni.projectcalico.org/podIPs":"10.244.20.171/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4ac2e1fa-07e6-449c-87e0-d9b08cb53e41","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ac2e1fa-07e6-449c-87e0-d9b08cb53e41\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.20.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-lkq5c","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-lkq5c","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:50Z"}],"hostIP":"10.0.0.180","podIP":"10.244.20.171","podIPs":[{"ip":"10.244.20.171"}],"startTime":"2023-05-06T07:03:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-06T07:03:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://ce121af29d57a89d869f98754297dd9872960d37b7c0f975cab756ddc639f7d0","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rx5qz","generateName":"daemon-set-","namespace":"daemonsets-1648","uid":"a554cdd3-dc01-45a6-a1ac-7b7e9a91bbe6","resourceVersion":"142819","creationTimestamp":"2023-05-06T07:03:50Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"890539e062faf2b1079ce8f5a78f19429aed8e7a6dbfb12e140b9759f4d07cae","cni.projectcalico.org/podIP":"10.244.21.105/32","cni.projectcalico.org/podIPs":"10.244.21.105/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4ac2e1fa-07e6-449c-87e0-d9b08cb53e41","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ac2e1fa-07e6-449c-87e0-d9b08cb53e41\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.21.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-x6fvw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-x6fvw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:50Z"}],"hostIP":"10.0.0.107","podIP":"10.244.21.105","podIPs":[{"ip":"10.244.21.105"}],"startTime":"2023-05-06T07:03:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-06T07:03:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://3699d18391bc847fbd519d53cb5aea8840f9fe778e29f23bc3b4b61cef8fe574","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 07:03:52.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1648" for this suite. 05/06/23 07:03:52.305
------------------------------
â€¢ [4.104 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:03:48.206
    May  6 07:03:48.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename daemonsets 05/06/23 07:03:48.207
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:03:49.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:03:49.224
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 05/06/23 07:03:50.245
    STEP: Check that daemon pods launch on every node of the cluster. 05/06/23 07:03:50.251
    May  6 07:03:50.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 07:03:50.256: INFO: Node cncf-0 is running 0 daemon pod, expected 1
    May  6 07:03:51.263: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  6 07:03:51.263: INFO: Node cncf-0 is running 0 daemon pod, expected 1
    May  6 07:03:52.263: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    May  6 07:03:52.263: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: listing all DeamonSets 05/06/23 07:03:52.265
    STEP: DeleteCollection of the DaemonSets 05/06/23 07:03:52.268
    STEP: Verify that ReplicaSets have been deleted 05/06/23 07:03:52.274
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    May  6 07:03:52.281: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"142825"},"items":null}

    May  6 07:03:52.283: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"142825"},"items":[{"metadata":{"name":"daemon-set-2n7d6","generateName":"daemon-set-","namespace":"daemonsets-1648","uid":"51b48ad3-a5a9-41e6-b8a0-bf86a9fdf3a1","resourceVersion":"142821","creationTimestamp":"2023-05-06T07:03:50Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"cb589c9f1fe5b27c2ccf1ee677f610848d970b4a17a833a6c8e40b820d1f5d6e","cni.projectcalico.org/podIP":"10.244.245.125/32","cni.projectcalico.org/podIPs":"10.244.245.125/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4ac2e1fa-07e6-449c-87e0-d9b08cb53e41","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ac2e1fa-07e6-449c-87e0-d9b08cb53e41\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.245.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-lsb2m","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-lsb2m","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:50Z"}],"hostIP":"10.0.0.240","podIP":"10.244.245.125","podIPs":[{"ip":"10.244.245.125"}],"startTime":"2023-05-06T07:03:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-06T07:03:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4ff29e1f7cda63e2b2f48d964aa8c995fab8855bc05466e3f38c2374886f9f4c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-6kvt7","generateName":"daemon-set-","namespace":"daemonsets-1648","uid":"cdd7de39-6fcb-4d23-b608-0033bed13829","resourceVersion":"142815","creationTimestamp":"2023-05-06T07:03:50Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6b8e1fd6e5c71b51c7f6909f2edbd7d80b7ff8043662a802234be464e36db5dc","cni.projectcalico.org/podIP":"10.244.174.169/32","cni.projectcalico.org/podIPs":"10.244.174.169/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4ac2e1fa-07e6-449c-87e0-d9b08cb53e41","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ac2e1fa-07e6-449c-87e0-d9b08cb53e41\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.174.169\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4p85r","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4p85r","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:50Z"}],"hostIP":"10.0.0.134","podIP":"10.244.174.169","podIPs":[{"ip":"10.244.174.169"}],"startTime":"2023-05-06T07:03:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-06T07:03:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e985b62eb884d136a47cf61ea35ad0ad25214a0b0031670bfffc365aa3681186","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-dtkqv","generateName":"daemon-set-","namespace":"daemonsets-1648","uid":"bb608f37-59fd-4417-bc9d-32ff40bc80f4","resourceVersion":"142812","creationTimestamp":"2023-05-06T07:03:50Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"b1e2275b9eb3ae5bf46bb73a396f00bc6d1ccca2691e1d06d8048635937c0810","cni.projectcalico.org/podIP":"10.244.20.171/32","cni.projectcalico.org/podIPs":"10.244.20.171/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4ac2e1fa-07e6-449c-87e0-d9b08cb53e41","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ac2e1fa-07e6-449c-87e0-d9b08cb53e41\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.20.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-lkq5c","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-lkq5c","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:50Z"}],"hostIP":"10.0.0.180","podIP":"10.244.20.171","podIPs":[{"ip":"10.244.20.171"}],"startTime":"2023-05-06T07:03:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-06T07:03:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://ce121af29d57a89d869f98754297dd9872960d37b7c0f975cab756ddc639f7d0","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rx5qz","generateName":"daemon-set-","namespace":"daemonsets-1648","uid":"a554cdd3-dc01-45a6-a1ac-7b7e9a91bbe6","resourceVersion":"142819","creationTimestamp":"2023-05-06T07:03:50Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"890539e062faf2b1079ce8f5a78f19429aed8e7a6dbfb12e140b9759f4d07cae","cni.projectcalico.org/podIP":"10.244.21.105/32","cni.projectcalico.org/podIPs":"10.244.21.105/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4ac2e1fa-07e6-449c-87e0-d9b08cb53e41","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ac2e1fa-07e6-449c-87e0-d9b08cb53e41\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-06T07:03:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.21.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-x6fvw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-x6fvw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-06T07:03:50Z"}],"hostIP":"10.0.0.107","podIP":"10.244.21.105","podIPs":[{"ip":"10.244.21.105"}],"startTime":"2023-05-06T07:03:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-06T07:03:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://3699d18391bc847fbd519d53cb5aea8840f9fe778e29f23bc3b4b61cef8fe574","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 07:03:52.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1648" for this suite. 05/06/23 07:03:52.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:03:52.314
May  6 07:03:52.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename crd-watch 05/06/23 07:03:52.315
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:03:53.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:03:53.332
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
May  6 07:03:53.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Creating first CR  05/06/23 07:03:55.88
May  6 07:03:55.884: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-06T07:03:55Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-06T07:03:55Z]] name:name1 resourceVersion:142890 uid:b15418f2-1555-4be0-bbe3-c8d274a11771] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 05/06/23 07:04:05.884
May  6 07:04:05.892: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-06T07:04:05Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-06T07:04:05Z]] name:name2 resourceVersion:142951 uid:98f01467-944f-4da5-9ffb-3b185b0cbdfd] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 05/06/23 07:04:15.892
May  6 07:04:15.899: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-06T07:03:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-06T07:04:15Z]] name:name1 resourceVersion:142986 uid:b15418f2-1555-4be0-bbe3-c8d274a11771] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 05/06/23 07:04:25.899
May  6 07:04:25.905: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-06T07:04:05Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-06T07:04:25Z]] name:name2 resourceVersion:143021 uid:98f01467-944f-4da5-9ffb-3b185b0cbdfd] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 05/06/23 07:04:35.906
May  6 07:04:35.912: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-06T07:03:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-06T07:04:15Z]] name:name1 resourceVersion:143061 uid:b15418f2-1555-4be0-bbe3-c8d274a11771] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 05/06/23 07:04:45.913
May  6 07:04:45.919: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-06T07:04:05Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-06T07:04:25Z]] name:name2 resourceVersion:143096 uid:98f01467-944f-4da5-9ffb-3b185b0cbdfd] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:04:56.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-8045" for this suite. 05/06/23 07:04:56.434
------------------------------
â€¢ [SLOW TEST] [64.127 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:03:52.314
    May  6 07:03:52.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename crd-watch 05/06/23 07:03:52.315
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:03:53.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:03:53.332
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    May  6 07:03:53.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Creating first CR  05/06/23 07:03:55.88
    May  6 07:03:55.884: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-06T07:03:55Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-06T07:03:55Z]] name:name1 resourceVersion:142890 uid:b15418f2-1555-4be0-bbe3-c8d274a11771] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 05/06/23 07:04:05.884
    May  6 07:04:05.892: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-06T07:04:05Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-06T07:04:05Z]] name:name2 resourceVersion:142951 uid:98f01467-944f-4da5-9ffb-3b185b0cbdfd] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 05/06/23 07:04:15.892
    May  6 07:04:15.899: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-06T07:03:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-06T07:04:15Z]] name:name1 resourceVersion:142986 uid:b15418f2-1555-4be0-bbe3-c8d274a11771] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 05/06/23 07:04:25.899
    May  6 07:04:25.905: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-06T07:04:05Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-06T07:04:25Z]] name:name2 resourceVersion:143021 uid:98f01467-944f-4da5-9ffb-3b185b0cbdfd] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 05/06/23 07:04:35.906
    May  6 07:04:35.912: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-06T07:03:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-06T07:04:15Z]] name:name1 resourceVersion:143061 uid:b15418f2-1555-4be0-bbe3-c8d274a11771] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 05/06/23 07:04:45.913
    May  6 07:04:45.919: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-06T07:04:05Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-06T07:04:25Z]] name:name2 resourceVersion:143096 uid:98f01467-944f-4da5-9ffb-3b185b0cbdfd] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:04:56.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-8045" for this suite. 05/06/23 07:04:56.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:04:56.442
May  6 07:04:56.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename resourcequota 05/06/23 07:04:56.442
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:04:57.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:04:57.461
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 05/06/23 07:04:57.462
STEP: Getting a ResourceQuota 05/06/23 07:04:57.467
STEP: Listing all ResourceQuotas with LabelSelector 05/06/23 07:04:57.469
STEP: Patching the ResourceQuota 05/06/23 07:04:57.473
STEP: Deleting a Collection of ResourceQuotas 05/06/23 07:04:57.479
STEP: Verifying the deleted ResourceQuota 05/06/23 07:04:57.488
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  6 07:04:57.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-528" for this suite. 05/06/23 07:04:57.494
------------------------------
â€¢ [1.057 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:04:56.442
    May  6 07:04:56.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename resourcequota 05/06/23 07:04:56.442
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:04:57.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:04:57.461
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 05/06/23 07:04:57.462
    STEP: Getting a ResourceQuota 05/06/23 07:04:57.467
    STEP: Listing all ResourceQuotas with LabelSelector 05/06/23 07:04:57.469
    STEP: Patching the ResourceQuota 05/06/23 07:04:57.473
    STEP: Deleting a Collection of ResourceQuotas 05/06/23 07:04:57.479
    STEP: Verifying the deleted ResourceQuota 05/06/23 07:04:57.488
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  6 07:04:57.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-528" for this suite. 05/06/23 07:04:57.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:04:57.5
May  6 07:04:57.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename services 05/06/23 07:04:57.5
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:04:58.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:04:58.517
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 05/06/23 07:04:58.522
STEP: waiting for available Endpoint 05/06/23 07:04:58.526
STEP: listing all Endpoints 05/06/23 07:04:58.527
STEP: updating the Endpoint 05/06/23 07:04:58.529
STEP: fetching the Endpoint 05/06/23 07:04:58.534
STEP: patching the Endpoint 05/06/23 07:04:58.536
STEP: fetching the Endpoint 05/06/23 07:04:58.546
STEP: deleting the Endpoint by Collection 05/06/23 07:04:58.549
STEP: waiting for Endpoint deletion 05/06/23 07:04:58.555
STEP: fetching the Endpoint 05/06/23 07:04:58.556
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  6 07:04:58.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6660" for this suite. 05/06/23 07:04:58.562
------------------------------
â€¢ [1.067 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:04:57.5
    May  6 07:04:57.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename services 05/06/23 07:04:57.5
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:04:58.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:04:58.517
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 05/06/23 07:04:58.522
    STEP: waiting for available Endpoint 05/06/23 07:04:58.526
    STEP: listing all Endpoints 05/06/23 07:04:58.527
    STEP: updating the Endpoint 05/06/23 07:04:58.529
    STEP: fetching the Endpoint 05/06/23 07:04:58.534
    STEP: patching the Endpoint 05/06/23 07:04:58.536
    STEP: fetching the Endpoint 05/06/23 07:04:58.546
    STEP: deleting the Endpoint by Collection 05/06/23 07:04:58.549
    STEP: waiting for Endpoint deletion 05/06/23 07:04:58.555
    STEP: fetching the Endpoint 05/06/23 07:04:58.556
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  6 07:04:58.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6660" for this suite. 05/06/23 07:04:58.562
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:04:58.568
May  6 07:04:58.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename downward-api 05/06/23 07:04:58.569
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:04:59.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:04:59.589
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 05/06/23 07:04:59.59
May  6 07:04:59.596: INFO: Waiting up to 5m0s for pod "downward-api-3f59764f-0bcc-40bd-9c42-8533a63b8b0e" in namespace "downward-api-9249" to be "Succeeded or Failed"
May  6 07:04:59.599: INFO: Pod "downward-api-3f59764f-0bcc-40bd-9c42-8533a63b8b0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.362109ms
May  6 07:05:01.603: INFO: Pod "downward-api-3f59764f-0bcc-40bd-9c42-8533a63b8b0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006158603s
May  6 07:05:03.603: INFO: Pod "downward-api-3f59764f-0bcc-40bd-9c42-8533a63b8b0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006229089s
STEP: Saw pod success 05/06/23 07:05:03.603
May  6 07:05:03.603: INFO: Pod "downward-api-3f59764f-0bcc-40bd-9c42-8533a63b8b0e" satisfied condition "Succeeded or Failed"
May  6 07:05:03.606: INFO: Trying to get logs from node cncf-3 pod downward-api-3f59764f-0bcc-40bd-9c42-8533a63b8b0e container dapi-container: <nil>
STEP: delete the pod 05/06/23 07:05:03.614
May  6 07:05:03.625: INFO: Waiting for pod downward-api-3f59764f-0bcc-40bd-9c42-8533a63b8b0e to disappear
May  6 07:05:03.628: INFO: Pod downward-api-3f59764f-0bcc-40bd-9c42-8533a63b8b0e no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May  6 07:05:03.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9249" for this suite. 05/06/23 07:05:03.631
------------------------------
â€¢ [SLOW TEST] [5.068 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:04:58.568
    May  6 07:04:58.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename downward-api 05/06/23 07:04:58.569
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:04:59.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:04:59.589
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 05/06/23 07:04:59.59
    May  6 07:04:59.596: INFO: Waiting up to 5m0s for pod "downward-api-3f59764f-0bcc-40bd-9c42-8533a63b8b0e" in namespace "downward-api-9249" to be "Succeeded or Failed"
    May  6 07:04:59.599: INFO: Pod "downward-api-3f59764f-0bcc-40bd-9c42-8533a63b8b0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.362109ms
    May  6 07:05:01.603: INFO: Pod "downward-api-3f59764f-0bcc-40bd-9c42-8533a63b8b0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006158603s
    May  6 07:05:03.603: INFO: Pod "downward-api-3f59764f-0bcc-40bd-9c42-8533a63b8b0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006229089s
    STEP: Saw pod success 05/06/23 07:05:03.603
    May  6 07:05:03.603: INFO: Pod "downward-api-3f59764f-0bcc-40bd-9c42-8533a63b8b0e" satisfied condition "Succeeded or Failed"
    May  6 07:05:03.606: INFO: Trying to get logs from node cncf-3 pod downward-api-3f59764f-0bcc-40bd-9c42-8533a63b8b0e container dapi-container: <nil>
    STEP: delete the pod 05/06/23 07:05:03.614
    May  6 07:05:03.625: INFO: Waiting for pod downward-api-3f59764f-0bcc-40bd-9c42-8533a63b8b0e to disappear
    May  6 07:05:03.628: INFO: Pod downward-api-3f59764f-0bcc-40bd-9c42-8533a63b8b0e no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May  6 07:05:03.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9249" for this suite. 05/06/23 07:05:03.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:05:03.637
May  6 07:05:03.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename statefulset 05/06/23 07:05:03.638
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:05:04.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:05:04.657
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6682 05/06/23 07:05:04.659
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 05/06/23 07:05:04.663
STEP: Creating pod with conflicting port in namespace statefulset-6682 05/06/23 07:05:04.666
STEP: Waiting until pod test-pod will start running in namespace statefulset-6682 05/06/23 07:05:04.674
May  6 07:05:04.675: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-6682" to be "running"
May  6 07:05:04.677: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.397656ms
May  6 07:05:06.681: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00594493s
May  6 07:05:06.681: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-6682 05/06/23 07:05:06.681
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6682 05/06/23 07:05:06.686
May  6 07:05:06.699: INFO: Observed stateful pod in namespace: statefulset-6682, name: ss-0, uid: beb9c070-a8e7-4f35-b0ec-c162fc0dc423, status phase: Pending. Waiting for statefulset controller to delete.
May  6 07:05:06.712: INFO: Observed stateful pod in namespace: statefulset-6682, name: ss-0, uid: beb9c070-a8e7-4f35-b0ec-c162fc0dc423, status phase: Failed. Waiting for statefulset controller to delete.
May  6 07:05:06.731: INFO: Observed stateful pod in namespace: statefulset-6682, name: ss-0, uid: beb9c070-a8e7-4f35-b0ec-c162fc0dc423, status phase: Failed. Waiting for statefulset controller to delete.
May  6 07:05:06.734: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6682
STEP: Removing pod with conflicting port in namespace statefulset-6682 05/06/23 07:05:06.734
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6682 and will be in running state 05/06/23 07:05:06.746
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May  6 07:05:08.754: INFO: Deleting all statefulset in ns statefulset-6682
May  6 07:05:08.758: INFO: Scaling statefulset ss to 0
May  6 07:05:18.772: INFO: Waiting for statefulset status.replicas updated to 0
May  6 07:05:18.774: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May  6 07:05:18.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6682" for this suite. 05/06/23 07:05:18.787
------------------------------
â€¢ [SLOW TEST] [15.155 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:05:03.637
    May  6 07:05:03.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename statefulset 05/06/23 07:05:03.638
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:05:04.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:05:04.657
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6682 05/06/23 07:05:04.659
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 05/06/23 07:05:04.663
    STEP: Creating pod with conflicting port in namespace statefulset-6682 05/06/23 07:05:04.666
    STEP: Waiting until pod test-pod will start running in namespace statefulset-6682 05/06/23 07:05:04.674
    May  6 07:05:04.675: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-6682" to be "running"
    May  6 07:05:04.677: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.397656ms
    May  6 07:05:06.681: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00594493s
    May  6 07:05:06.681: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-6682 05/06/23 07:05:06.681
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6682 05/06/23 07:05:06.686
    May  6 07:05:06.699: INFO: Observed stateful pod in namespace: statefulset-6682, name: ss-0, uid: beb9c070-a8e7-4f35-b0ec-c162fc0dc423, status phase: Pending. Waiting for statefulset controller to delete.
    May  6 07:05:06.712: INFO: Observed stateful pod in namespace: statefulset-6682, name: ss-0, uid: beb9c070-a8e7-4f35-b0ec-c162fc0dc423, status phase: Failed. Waiting for statefulset controller to delete.
    May  6 07:05:06.731: INFO: Observed stateful pod in namespace: statefulset-6682, name: ss-0, uid: beb9c070-a8e7-4f35-b0ec-c162fc0dc423, status phase: Failed. Waiting for statefulset controller to delete.
    May  6 07:05:06.734: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6682
    STEP: Removing pod with conflicting port in namespace statefulset-6682 05/06/23 07:05:06.734
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6682 and will be in running state 05/06/23 07:05:06.746
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May  6 07:05:08.754: INFO: Deleting all statefulset in ns statefulset-6682
    May  6 07:05:08.758: INFO: Scaling statefulset ss to 0
    May  6 07:05:18.772: INFO: Waiting for statefulset status.replicas updated to 0
    May  6 07:05:18.774: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May  6 07:05:18.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6682" for this suite. 05/06/23 07:05:18.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:05:18.793
May  6 07:05:18.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename emptydir 05/06/23 07:05:18.794
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:05:19.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:05:19.815
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 05/06/23 07:05:19.817
May  6 07:05:19.826: INFO: Waiting up to 5m0s for pod "pod-b8b38774-5a88-4471-82e0-e1833965cb92" in namespace "emptydir-1060" to be "Succeeded or Failed"
May  6 07:05:19.828: INFO: Pod "pod-b8b38774-5a88-4471-82e0-e1833965cb92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.753983ms
May  6 07:05:21.832: INFO: Pod "pod-b8b38774-5a88-4471-82e0-e1833965cb92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006512699s
May  6 07:05:23.831: INFO: Pod "pod-b8b38774-5a88-4471-82e0-e1833965cb92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005402936s
STEP: Saw pod success 05/06/23 07:05:23.831
May  6 07:05:23.831: INFO: Pod "pod-b8b38774-5a88-4471-82e0-e1833965cb92" satisfied condition "Succeeded or Failed"
May  6 07:05:23.834: INFO: Trying to get logs from node cncf-0 pod pod-b8b38774-5a88-4471-82e0-e1833965cb92 container test-container: <nil>
STEP: delete the pod 05/06/23 07:05:23.846
May  6 07:05:23.859: INFO: Waiting for pod pod-b8b38774-5a88-4471-82e0-e1833965cb92 to disappear
May  6 07:05:23.862: INFO: Pod pod-b8b38774-5a88-4471-82e0-e1833965cb92 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  6 07:05:23.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1060" for this suite. 05/06/23 07:05:23.865
------------------------------
â€¢ [SLOW TEST] [5.077 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:05:18.793
    May  6 07:05:18.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename emptydir 05/06/23 07:05:18.794
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:05:19.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:05:19.815
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 05/06/23 07:05:19.817
    May  6 07:05:19.826: INFO: Waiting up to 5m0s for pod "pod-b8b38774-5a88-4471-82e0-e1833965cb92" in namespace "emptydir-1060" to be "Succeeded or Failed"
    May  6 07:05:19.828: INFO: Pod "pod-b8b38774-5a88-4471-82e0-e1833965cb92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.753983ms
    May  6 07:05:21.832: INFO: Pod "pod-b8b38774-5a88-4471-82e0-e1833965cb92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006512699s
    May  6 07:05:23.831: INFO: Pod "pod-b8b38774-5a88-4471-82e0-e1833965cb92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005402936s
    STEP: Saw pod success 05/06/23 07:05:23.831
    May  6 07:05:23.831: INFO: Pod "pod-b8b38774-5a88-4471-82e0-e1833965cb92" satisfied condition "Succeeded or Failed"
    May  6 07:05:23.834: INFO: Trying to get logs from node cncf-0 pod pod-b8b38774-5a88-4471-82e0-e1833965cb92 container test-container: <nil>
    STEP: delete the pod 05/06/23 07:05:23.846
    May  6 07:05:23.859: INFO: Waiting for pod pod-b8b38774-5a88-4471-82e0-e1833965cb92 to disappear
    May  6 07:05:23.862: INFO: Pod pod-b8b38774-5a88-4471-82e0-e1833965cb92 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  6 07:05:23.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1060" for this suite. 05/06/23 07:05:23.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:05:23.871
May  6 07:05:23.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename replication-controller 05/06/23 07:05:23.872
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:05:24.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:05:24.894
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 05/06/23 07:05:24.898
STEP: waiting for RC to be added 05/06/23 07:05:24.903
STEP: waiting for available Replicas 05/06/23 07:05:24.904
STEP: patching ReplicationController 05/06/23 07:05:25.645
STEP: waiting for RC to be modified 05/06/23 07:05:25.652
STEP: patching ReplicationController status 05/06/23 07:05:25.652
STEP: waiting for RC to be modified 05/06/23 07:05:25.659
STEP: waiting for available Replicas 05/06/23 07:05:25.659
STEP: fetching ReplicationController status 05/06/23 07:05:25.663
STEP: patching ReplicationController scale 05/06/23 07:05:25.665
STEP: waiting for RC to be modified 05/06/23 07:05:25.671
STEP: waiting for ReplicationController's scale to be the max amount 05/06/23 07:05:25.671
STEP: fetching ReplicationController; ensuring that it's patched 05/06/23 07:05:27.203
STEP: updating ReplicationController status 05/06/23 07:05:27.206
STEP: waiting for RC to be modified 05/06/23 07:05:27.212
STEP: listing all ReplicationControllers 05/06/23 07:05:27.212
STEP: checking that ReplicationController has expected values 05/06/23 07:05:27.214
STEP: deleting ReplicationControllers by collection 05/06/23 07:05:27.214
STEP: waiting for ReplicationController to have a DELETED watchEvent 05/06/23 07:05:27.221
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May  6 07:05:27.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2953" for this suite. 05/06/23 07:05:27.264
------------------------------
â€¢ [3.398 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:05:23.871
    May  6 07:05:23.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename replication-controller 05/06/23 07:05:23.872
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:05:24.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:05:24.894
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 05/06/23 07:05:24.898
    STEP: waiting for RC to be added 05/06/23 07:05:24.903
    STEP: waiting for available Replicas 05/06/23 07:05:24.904
    STEP: patching ReplicationController 05/06/23 07:05:25.645
    STEP: waiting for RC to be modified 05/06/23 07:05:25.652
    STEP: patching ReplicationController status 05/06/23 07:05:25.652
    STEP: waiting for RC to be modified 05/06/23 07:05:25.659
    STEP: waiting for available Replicas 05/06/23 07:05:25.659
    STEP: fetching ReplicationController status 05/06/23 07:05:25.663
    STEP: patching ReplicationController scale 05/06/23 07:05:25.665
    STEP: waiting for RC to be modified 05/06/23 07:05:25.671
    STEP: waiting for ReplicationController's scale to be the max amount 05/06/23 07:05:25.671
    STEP: fetching ReplicationController; ensuring that it's patched 05/06/23 07:05:27.203
    STEP: updating ReplicationController status 05/06/23 07:05:27.206
    STEP: waiting for RC to be modified 05/06/23 07:05:27.212
    STEP: listing all ReplicationControllers 05/06/23 07:05:27.212
    STEP: checking that ReplicationController has expected values 05/06/23 07:05:27.214
    STEP: deleting ReplicationControllers by collection 05/06/23 07:05:27.214
    STEP: waiting for ReplicationController to have a DELETED watchEvent 05/06/23 07:05:27.221
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May  6 07:05:27.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2953" for this suite. 05/06/23 07:05:27.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:05:27.27
May  6 07:05:27.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename crd-publish-openapi 05/06/23 07:05:27.27
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:05:28.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:05:28.288
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
May  6 07:05:28.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/06/23 07:05:30.284
May  6 07:05:30.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-195 --namespace=crd-publish-openapi-195 create -f -'
May  6 07:05:30.845: INFO: stderr: ""
May  6 07:05:30.845: INFO: stdout: "e2e-test-crd-publish-openapi-1430-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May  6 07:05:30.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-195 --namespace=crd-publish-openapi-195 delete e2e-test-crd-publish-openapi-1430-crds test-cr'
May  6 07:05:30.920: INFO: stderr: ""
May  6 07:05:30.920: INFO: stdout: "e2e-test-crd-publish-openapi-1430-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May  6 07:05:30.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-195 --namespace=crd-publish-openapi-195 apply -f -'
May  6 07:05:31.075: INFO: stderr: ""
May  6 07:05:31.075: INFO: stdout: "e2e-test-crd-publish-openapi-1430-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May  6 07:05:31.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-195 --namespace=crd-publish-openapi-195 delete e2e-test-crd-publish-openapi-1430-crds test-cr'
May  6 07:05:31.128: INFO: stderr: ""
May  6 07:05:31.128: INFO: stdout: "e2e-test-crd-publish-openapi-1430-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 05/06/23 07:05:31.128
May  6 07:05:31.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-195 explain e2e-test-crd-publish-openapi-1430-crds'
May  6 07:05:31.600: INFO: stderr: ""
May  6 07:05:31.600: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1430-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:05:33.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-195" for this suite. 05/06/23 07:05:33.542
------------------------------
â€¢ [SLOW TEST] [6.280 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:05:27.27
    May  6 07:05:27.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename crd-publish-openapi 05/06/23 07:05:27.27
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:05:28.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:05:28.288
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    May  6 07:05:28.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/06/23 07:05:30.284
    May  6 07:05:30.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-195 --namespace=crd-publish-openapi-195 create -f -'
    May  6 07:05:30.845: INFO: stderr: ""
    May  6 07:05:30.845: INFO: stdout: "e2e-test-crd-publish-openapi-1430-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    May  6 07:05:30.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-195 --namespace=crd-publish-openapi-195 delete e2e-test-crd-publish-openapi-1430-crds test-cr'
    May  6 07:05:30.920: INFO: stderr: ""
    May  6 07:05:30.920: INFO: stdout: "e2e-test-crd-publish-openapi-1430-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    May  6 07:05:30.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-195 --namespace=crd-publish-openapi-195 apply -f -'
    May  6 07:05:31.075: INFO: stderr: ""
    May  6 07:05:31.075: INFO: stdout: "e2e-test-crd-publish-openapi-1430-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    May  6 07:05:31.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-195 --namespace=crd-publish-openapi-195 delete e2e-test-crd-publish-openapi-1430-crds test-cr'
    May  6 07:05:31.128: INFO: stderr: ""
    May  6 07:05:31.128: INFO: stdout: "e2e-test-crd-publish-openapi-1430-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 05/06/23 07:05:31.128
    May  6 07:05:31.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-195 explain e2e-test-crd-publish-openapi-1430-crds'
    May  6 07:05:31.600: INFO: stderr: ""
    May  6 07:05:31.600: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1430-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:05:33.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-195" for this suite. 05/06/23 07:05:33.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:05:33.552
May  6 07:05:33.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename statefulset 05/06/23 07:05:33.552
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:05:34.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:05:34.57
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9147 05/06/23 07:05:34.572
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 05/06/23 07:05:34.576
May  6 07:05:34.585: INFO: Found 0 stateful pods, waiting for 3
May  6 07:05:44.589: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  6 07:05:44.589: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  6 07:05:44.589: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 05/06/23 07:05:44.597
May  6 07:05:44.615: INFO: Updating stateful set ss2
STEP: Creating a new revision 05/06/23 07:05:44.615
STEP: Not applying an update when the partition is greater than the number of replicas 05/06/23 07:05:54.628
STEP: Performing a canary update 05/06/23 07:05:54.628
May  6 07:05:54.648: INFO: Updating stateful set ss2
May  6 07:05:54.652: INFO: Waiting for Pod statefulset-9147/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 05/06/23 07:06:04.66
May  6 07:06:04.716: INFO: Found 2 stateful pods, waiting for 3
May  6 07:06:14.721: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  6 07:06:14.721: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  6 07:06:14.721: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 05/06/23 07:06:14.727
May  6 07:06:14.747: INFO: Updating stateful set ss2
May  6 07:06:14.751: INFO: Waiting for Pod statefulset-9147/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
May  6 07:06:24.776: INFO: Updating stateful set ss2
May  6 07:06:24.781: INFO: Waiting for StatefulSet statefulset-9147/ss2 to complete update
May  6 07:06:24.781: INFO: Waiting for Pod statefulset-9147/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May  6 07:06:34.787: INFO: Deleting all statefulset in ns statefulset-9147
May  6 07:06:34.790: INFO: Scaling statefulset ss2 to 0
May  6 07:06:44.804: INFO: Waiting for statefulset status.replicas updated to 0
May  6 07:06:44.806: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May  6 07:06:44.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9147" for this suite. 05/06/23 07:06:44.819
------------------------------
â€¢ [SLOW TEST] [71.273 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:05:33.552
    May  6 07:05:33.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename statefulset 05/06/23 07:05:33.552
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:05:34.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:05:34.57
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9147 05/06/23 07:05:34.572
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 05/06/23 07:05:34.576
    May  6 07:05:34.585: INFO: Found 0 stateful pods, waiting for 3
    May  6 07:05:44.589: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May  6 07:05:44.589: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May  6 07:05:44.589: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 05/06/23 07:05:44.597
    May  6 07:05:44.615: INFO: Updating stateful set ss2
    STEP: Creating a new revision 05/06/23 07:05:44.615
    STEP: Not applying an update when the partition is greater than the number of replicas 05/06/23 07:05:54.628
    STEP: Performing a canary update 05/06/23 07:05:54.628
    May  6 07:05:54.648: INFO: Updating stateful set ss2
    May  6 07:05:54.652: INFO: Waiting for Pod statefulset-9147/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 05/06/23 07:06:04.66
    May  6 07:06:04.716: INFO: Found 2 stateful pods, waiting for 3
    May  6 07:06:14.721: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May  6 07:06:14.721: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May  6 07:06:14.721: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 05/06/23 07:06:14.727
    May  6 07:06:14.747: INFO: Updating stateful set ss2
    May  6 07:06:14.751: INFO: Waiting for Pod statefulset-9147/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    May  6 07:06:24.776: INFO: Updating stateful set ss2
    May  6 07:06:24.781: INFO: Waiting for StatefulSet statefulset-9147/ss2 to complete update
    May  6 07:06:24.781: INFO: Waiting for Pod statefulset-9147/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May  6 07:06:34.787: INFO: Deleting all statefulset in ns statefulset-9147
    May  6 07:06:34.790: INFO: Scaling statefulset ss2 to 0
    May  6 07:06:44.804: INFO: Waiting for statefulset status.replicas updated to 0
    May  6 07:06:44.806: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May  6 07:06:44.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9147" for this suite. 05/06/23 07:06:44.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:06:44.827
May  6 07:06:44.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename daemonsets 05/06/23 07:06:44.828
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:06:45.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:06:45.846
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 05/06/23 07:06:46.865
STEP: Check that daemon pods launch on every node of the cluster. 05/06/23 07:06:46.871
May  6 07:06:46.876: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 07:06:46.876: INFO: Node cncf-0 is running 0 daemon pod, expected 1
May  6 07:06:47.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  6 07:06:47.883: INFO: Node cncf-1 is running 0 daemon pod, expected 1
May  6 07:06:48.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
May  6 07:06:48.883: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 05/06/23 07:06:48.885
May  6 07:06:48.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
May  6 07:06:48.905: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 05/06/23 07:06:48.905
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/06/23 07:06:49.916
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1549, will wait for the garbage collector to delete the pods 05/06/23 07:06:49.916
May  6 07:06:49.978: INFO: Deleting DaemonSet.extensions daemon-set took: 8.040109ms
May  6 07:06:50.079: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.795782ms
May  6 07:06:52.182: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 07:06:52.182: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  6 07:06:52.184: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"144323"},"items":null}

May  6 07:06:52.186: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"144323"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 07:06:52.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1549" for this suite. 05/06/23 07:06:52.201
------------------------------
â€¢ [SLOW TEST] [7.380 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:06:44.827
    May  6 07:06:44.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename daemonsets 05/06/23 07:06:44.828
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:06:45.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:06:45.846
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 05/06/23 07:06:46.865
    STEP: Check that daemon pods launch on every node of the cluster. 05/06/23 07:06:46.871
    May  6 07:06:46.876: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 07:06:46.876: INFO: Node cncf-0 is running 0 daemon pod, expected 1
    May  6 07:06:47.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  6 07:06:47.883: INFO: Node cncf-1 is running 0 daemon pod, expected 1
    May  6 07:06:48.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    May  6 07:06:48.883: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 05/06/23 07:06:48.885
    May  6 07:06:48.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    May  6 07:06:48.905: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 05/06/23 07:06:48.905
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/06/23 07:06:49.916
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1549, will wait for the garbage collector to delete the pods 05/06/23 07:06:49.916
    May  6 07:06:49.978: INFO: Deleting DaemonSet.extensions daemon-set took: 8.040109ms
    May  6 07:06:50.079: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.795782ms
    May  6 07:06:52.182: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 07:06:52.182: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  6 07:06:52.184: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"144323"},"items":null}

    May  6 07:06:52.186: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"144323"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 07:06:52.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1549" for this suite. 05/06/23 07:06:52.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:06:52.208
May  6 07:06:52.208: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename webhook 05/06/23 07:06:52.209
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:06:53.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:06:53.226
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/06/23 07:06:55.245
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:06:55.445
STEP: Deploying the webhook pod 05/06/23 07:06:55.453
STEP: Wait for the deployment to be ready 05/06/23 07:06:55.478
May  6 07:06:55.486: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/06/23 07:06:57.495
STEP: Verifying the service has paired with the endpoint 05/06/23 07:06:57.51
May  6 07:06:58.510: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
May  6 07:06:58.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4994-crds.webhook.example.com via the AdmissionRegistration API 05/06/23 07:06:59.025
STEP: Creating a custom resource that should be mutated by the webhook 05/06/23 07:07:00.041
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:07:01.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3057" for this suite. 05/06/23 07:07:01.658
STEP: Destroying namespace "webhook-3057-markers" for this suite. 05/06/23 07:07:01.665
------------------------------
â€¢ [SLOW TEST] [9.471 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:06:52.208
    May  6 07:06:52.208: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename webhook 05/06/23 07:06:52.209
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:06:53.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:06:53.226
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/06/23 07:06:55.245
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:06:55.445
    STEP: Deploying the webhook pod 05/06/23 07:06:55.453
    STEP: Wait for the deployment to be ready 05/06/23 07:06:55.478
    May  6 07:06:55.486: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/06/23 07:06:57.495
    STEP: Verifying the service has paired with the endpoint 05/06/23 07:06:57.51
    May  6 07:06:58.510: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    May  6 07:06:58.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4994-crds.webhook.example.com via the AdmissionRegistration API 05/06/23 07:06:59.025
    STEP: Creating a custom resource that should be mutated by the webhook 05/06/23 07:07:00.041
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:07:01.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3057" for this suite. 05/06/23 07:07:01.658
    STEP: Destroying namespace "webhook-3057-markers" for this suite. 05/06/23 07:07:01.665
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:07:01.68
May  6 07:07:01.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename pods 05/06/23 07:07:01.68
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:07:02.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:07:02.707
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 05/06/23 07:07:02.709
STEP: setting up watch 05/06/23 07:07:02.709
STEP: submitting the pod to kubernetes 05/06/23 07:07:02.812
STEP: verifying the pod is in kubernetes 05/06/23 07:07:02.821
STEP: verifying pod creation was observed 05/06/23 07:07:02.823
May  6 07:07:02.823: INFO: Waiting up to 5m0s for pod "pod-submit-remove-1eadc4c2-13db-42ee-ba42-4a08b736b8cb" in namespace "pods-652" to be "running"
May  6 07:07:02.827: INFO: Pod "pod-submit-remove-1eadc4c2-13db-42ee-ba42-4a08b736b8cb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.941931ms
May  6 07:07:04.831: INFO: Pod "pod-submit-remove-1eadc4c2-13db-42ee-ba42-4a08b736b8cb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007457218s
May  6 07:07:04.831: INFO: Pod "pod-submit-remove-1eadc4c2-13db-42ee-ba42-4a08b736b8cb" satisfied condition "running"
STEP: deleting the pod gracefully 05/06/23 07:07:04.833
STEP: verifying pod deletion was observed 05/06/23 07:07:04.841
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  6 07:07:06.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-652" for this suite. 05/06/23 07:07:06.864
------------------------------
â€¢ [SLOW TEST] [5.191 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:07:01.68
    May  6 07:07:01.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename pods 05/06/23 07:07:01.68
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:07:02.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:07:02.707
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 05/06/23 07:07:02.709
    STEP: setting up watch 05/06/23 07:07:02.709
    STEP: submitting the pod to kubernetes 05/06/23 07:07:02.812
    STEP: verifying the pod is in kubernetes 05/06/23 07:07:02.821
    STEP: verifying pod creation was observed 05/06/23 07:07:02.823
    May  6 07:07:02.823: INFO: Waiting up to 5m0s for pod "pod-submit-remove-1eadc4c2-13db-42ee-ba42-4a08b736b8cb" in namespace "pods-652" to be "running"
    May  6 07:07:02.827: INFO: Pod "pod-submit-remove-1eadc4c2-13db-42ee-ba42-4a08b736b8cb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.941931ms
    May  6 07:07:04.831: INFO: Pod "pod-submit-remove-1eadc4c2-13db-42ee-ba42-4a08b736b8cb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007457218s
    May  6 07:07:04.831: INFO: Pod "pod-submit-remove-1eadc4c2-13db-42ee-ba42-4a08b736b8cb" satisfied condition "running"
    STEP: deleting the pod gracefully 05/06/23 07:07:04.833
    STEP: verifying pod deletion was observed 05/06/23 07:07:04.841
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  6 07:07:06.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-652" for this suite. 05/06/23 07:07:06.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:07:06.871
May  6 07:07:06.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:07:06.872
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:07:07.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:07:07.893
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-a096aedd-21cc-455a-9306-14a220304b0b 05/06/23 07:07:07.895
STEP: Creating a pod to test consume configMaps 05/06/23 07:07:07.899
May  6 07:07:07.906: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b5ec3df0-061b-4634-a7a2-1bec8e659a70" in namespace "projected-7669" to be "Succeeded or Failed"
May  6 07:07:07.909: INFO: Pod "pod-projected-configmaps-b5ec3df0-061b-4634-a7a2-1bec8e659a70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.452961ms
May  6 07:07:09.912: INFO: Pod "pod-projected-configmaps-b5ec3df0-061b-4634-a7a2-1bec8e659a70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005689832s
May  6 07:07:11.912: INFO: Pod "pod-projected-configmaps-b5ec3df0-061b-4634-a7a2-1bec8e659a70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005833954s
STEP: Saw pod success 05/06/23 07:07:11.912
May  6 07:07:11.912: INFO: Pod "pod-projected-configmaps-b5ec3df0-061b-4634-a7a2-1bec8e659a70" satisfied condition "Succeeded or Failed"
May  6 07:07:11.915: INFO: Trying to get logs from node cncf-0 pod pod-projected-configmaps-b5ec3df0-061b-4634-a7a2-1bec8e659a70 container agnhost-container: <nil>
STEP: delete the pod 05/06/23 07:07:11.924
May  6 07:07:11.937: INFO: Waiting for pod pod-projected-configmaps-b5ec3df0-061b-4634-a7a2-1bec8e659a70 to disappear
May  6 07:07:11.939: INFO: Pod pod-projected-configmaps-b5ec3df0-061b-4634-a7a2-1bec8e659a70 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May  6 07:07:11.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7669" for this suite. 05/06/23 07:07:11.943
------------------------------
â€¢ [SLOW TEST] [5.077 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:07:06.871
    May  6 07:07:06.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:07:06.872
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:07:07.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:07:07.893
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-a096aedd-21cc-455a-9306-14a220304b0b 05/06/23 07:07:07.895
    STEP: Creating a pod to test consume configMaps 05/06/23 07:07:07.899
    May  6 07:07:07.906: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b5ec3df0-061b-4634-a7a2-1bec8e659a70" in namespace "projected-7669" to be "Succeeded or Failed"
    May  6 07:07:07.909: INFO: Pod "pod-projected-configmaps-b5ec3df0-061b-4634-a7a2-1bec8e659a70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.452961ms
    May  6 07:07:09.912: INFO: Pod "pod-projected-configmaps-b5ec3df0-061b-4634-a7a2-1bec8e659a70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005689832s
    May  6 07:07:11.912: INFO: Pod "pod-projected-configmaps-b5ec3df0-061b-4634-a7a2-1bec8e659a70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005833954s
    STEP: Saw pod success 05/06/23 07:07:11.912
    May  6 07:07:11.912: INFO: Pod "pod-projected-configmaps-b5ec3df0-061b-4634-a7a2-1bec8e659a70" satisfied condition "Succeeded or Failed"
    May  6 07:07:11.915: INFO: Trying to get logs from node cncf-0 pod pod-projected-configmaps-b5ec3df0-061b-4634-a7a2-1bec8e659a70 container agnhost-container: <nil>
    STEP: delete the pod 05/06/23 07:07:11.924
    May  6 07:07:11.937: INFO: Waiting for pod pod-projected-configmaps-b5ec3df0-061b-4634-a7a2-1bec8e659a70 to disappear
    May  6 07:07:11.939: INFO: Pod pod-projected-configmaps-b5ec3df0-061b-4634-a7a2-1bec8e659a70 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May  6 07:07:11.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7669" for this suite. 05/06/23 07:07:11.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:07:11.948
May  6 07:07:11.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:07:11.949
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:07:12.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:07:12.966
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 05/06/23 07:07:12.968
May  6 07:07:12.975: INFO: Waiting up to 5m0s for pod "downwardapi-volume-528e78e2-e7a6-4557-8165-1e501459d4c0" in namespace "projected-8147" to be "Succeeded or Failed"
May  6 07:07:12.978: INFO: Pod "downwardapi-volume-528e78e2-e7a6-4557-8165-1e501459d4c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.296935ms
May  6 07:07:14.981: INFO: Pod "downwardapi-volume-528e78e2-e7a6-4557-8165-1e501459d4c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005492227s
May  6 07:07:16.982: INFO: Pod "downwardapi-volume-528e78e2-e7a6-4557-8165-1e501459d4c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006420911s
STEP: Saw pod success 05/06/23 07:07:16.982
May  6 07:07:16.982: INFO: Pod "downwardapi-volume-528e78e2-e7a6-4557-8165-1e501459d4c0" satisfied condition "Succeeded or Failed"
May  6 07:07:16.984: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-528e78e2-e7a6-4557-8165-1e501459d4c0 container client-container: <nil>
STEP: delete the pod 05/06/23 07:07:16.99
May  6 07:07:17.002: INFO: Waiting for pod downwardapi-volume-528e78e2-e7a6-4557-8165-1e501459d4c0 to disappear
May  6 07:07:17.004: INFO: Pod downwardapi-volume-528e78e2-e7a6-4557-8165-1e501459d4c0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  6 07:07:17.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8147" for this suite. 05/06/23 07:07:17.008
------------------------------
â€¢ [SLOW TEST] [5.065 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:07:11.948
    May  6 07:07:11.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:07:11.949
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:07:12.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:07:12.966
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 05/06/23 07:07:12.968
    May  6 07:07:12.975: INFO: Waiting up to 5m0s for pod "downwardapi-volume-528e78e2-e7a6-4557-8165-1e501459d4c0" in namespace "projected-8147" to be "Succeeded or Failed"
    May  6 07:07:12.978: INFO: Pod "downwardapi-volume-528e78e2-e7a6-4557-8165-1e501459d4c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.296935ms
    May  6 07:07:14.981: INFO: Pod "downwardapi-volume-528e78e2-e7a6-4557-8165-1e501459d4c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005492227s
    May  6 07:07:16.982: INFO: Pod "downwardapi-volume-528e78e2-e7a6-4557-8165-1e501459d4c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006420911s
    STEP: Saw pod success 05/06/23 07:07:16.982
    May  6 07:07:16.982: INFO: Pod "downwardapi-volume-528e78e2-e7a6-4557-8165-1e501459d4c0" satisfied condition "Succeeded or Failed"
    May  6 07:07:16.984: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-528e78e2-e7a6-4557-8165-1e501459d4c0 container client-container: <nil>
    STEP: delete the pod 05/06/23 07:07:16.99
    May  6 07:07:17.002: INFO: Waiting for pod downwardapi-volume-528e78e2-e7a6-4557-8165-1e501459d4c0 to disappear
    May  6 07:07:17.004: INFO: Pod downwardapi-volume-528e78e2-e7a6-4557-8165-1e501459d4c0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  6 07:07:17.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8147" for this suite. 05/06/23 07:07:17.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:07:17.015
May  6 07:07:17.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename webhook 05/06/23 07:07:17.015
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:07:18.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:07:18.033
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/06/23 07:07:20.053
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:07:20.173
STEP: Deploying the webhook pod 05/06/23 07:07:20.178
STEP: Wait for the deployment to be ready 05/06/23 07:07:20.189
May  6 07:07:20.195: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/06/23 07:07:22.203
STEP: Verifying the service has paired with the endpoint 05/06/23 07:07:22.214
May  6 07:07:23.214: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
May  6 07:07:23.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Registering the custom resource webhook via the AdmissionRegistration API 05/06/23 07:07:23.726
STEP: Creating a custom resource that should be denied by the webhook 05/06/23 07:07:24.745
STEP: Creating a custom resource whose deletion would be denied by the webhook 05/06/23 07:07:27.785
STEP: Updating the custom resource with disallowed data should be denied 05/06/23 07:07:27.791
STEP: Deleting the custom resource should be denied 05/06/23 07:07:27.798
STEP: Remove the offending key and value from the custom resource data 05/06/23 07:07:27.802
STEP: Deleting the updated custom resource should be successful 05/06/23 07:07:27.81
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:07:28.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7862" for this suite. 05/06/23 07:07:28.377
STEP: Destroying namespace "webhook-7862-markers" for this suite. 05/06/23 07:07:28.385
------------------------------
â€¢ [SLOW TEST] [11.377 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:07:17.015
    May  6 07:07:17.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename webhook 05/06/23 07:07:17.015
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:07:18.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:07:18.033
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/06/23 07:07:20.053
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:07:20.173
    STEP: Deploying the webhook pod 05/06/23 07:07:20.178
    STEP: Wait for the deployment to be ready 05/06/23 07:07:20.189
    May  6 07:07:20.195: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/06/23 07:07:22.203
    STEP: Verifying the service has paired with the endpoint 05/06/23 07:07:22.214
    May  6 07:07:23.214: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    May  6 07:07:23.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 05/06/23 07:07:23.726
    STEP: Creating a custom resource that should be denied by the webhook 05/06/23 07:07:24.745
    STEP: Creating a custom resource whose deletion would be denied by the webhook 05/06/23 07:07:27.785
    STEP: Updating the custom resource with disallowed data should be denied 05/06/23 07:07:27.791
    STEP: Deleting the custom resource should be denied 05/06/23 07:07:27.798
    STEP: Remove the offending key and value from the custom resource data 05/06/23 07:07:27.802
    STEP: Deleting the updated custom resource should be successful 05/06/23 07:07:27.81
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:07:28.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7862" for this suite. 05/06/23 07:07:28.377
    STEP: Destroying namespace "webhook-7862-markers" for this suite. 05/06/23 07:07:28.385
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:07:28.392
May  6 07:07:28.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename namespaces 05/06/23 07:07:28.393
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:07:29.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:07:29.411
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 05/06/23 07:07:29.413
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:07:30.428
STEP: Creating a service in the namespace 05/06/23 07:07:30.43
STEP: Deleting the namespace 05/06/23 07:07:30.448
STEP: Waiting for the namespace to be removed. 05/06/23 07:07:30.454
STEP: Recreating the namespace 05/06/23 07:07:36.457
STEP: Verifying there is no service in the namespace 05/06/23 07:07:37.473
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 07:07:37.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5413" for this suite. 05/06/23 07:07:37.479
STEP: Destroying namespace "nsdeletetest-6950" for this suite. 05/06/23 07:07:37.488
May  6 07:07:37.489: INFO: Namespace nsdeletetest-6950 was already deleted
STEP: Destroying namespace "nsdeletetest-1015" for this suite. 05/06/23 07:07:37.489
------------------------------
â€¢ [SLOW TEST] [9.102 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:07:28.392
    May  6 07:07:28.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename namespaces 05/06/23 07:07:28.393
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:07:29.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:07:29.411
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 05/06/23 07:07:29.413
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:07:30.428
    STEP: Creating a service in the namespace 05/06/23 07:07:30.43
    STEP: Deleting the namespace 05/06/23 07:07:30.448
    STEP: Waiting for the namespace to be removed. 05/06/23 07:07:30.454
    STEP: Recreating the namespace 05/06/23 07:07:36.457
    STEP: Verifying there is no service in the namespace 05/06/23 07:07:37.473
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 07:07:37.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5413" for this suite. 05/06/23 07:07:37.479
    STEP: Destroying namespace "nsdeletetest-6950" for this suite. 05/06/23 07:07:37.488
    May  6 07:07:37.489: INFO: Namespace nsdeletetest-6950 was already deleted
    STEP: Destroying namespace "nsdeletetest-1015" for this suite. 05/06/23 07:07:37.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:07:37.494
May  6 07:07:37.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename replication-controller 05/06/23 07:07:37.495
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:07:38.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:07:38.521
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
May  6 07:07:38.523: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 05/06/23 07:07:39.534
STEP: Checking rc "condition-test" has the desired failure condition set 05/06/23 07:07:39.54
STEP: Scaling down rc "condition-test" to satisfy pod quota 05/06/23 07:07:40.545
May  6 07:07:40.562: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 05/06/23 07:07:40.562
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May  6 07:07:41.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7815" for this suite. 05/06/23 07:07:41.57
------------------------------
â€¢ [4.083 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:07:37.494
    May  6 07:07:37.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename replication-controller 05/06/23 07:07:37.495
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:07:38.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:07:38.521
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    May  6 07:07:38.523: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 05/06/23 07:07:39.534
    STEP: Checking rc "condition-test" has the desired failure condition set 05/06/23 07:07:39.54
    STEP: Scaling down rc "condition-test" to satisfy pod quota 05/06/23 07:07:40.545
    May  6 07:07:40.562: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 05/06/23 07:07:40.562
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May  6 07:07:41.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7815" for this suite. 05/06/23 07:07:41.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:07:41.578
May  6 07:07:41.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename endpointslice 05/06/23 07:07:41.579
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:07:42.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:07:42.595
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 05/06/23 07:07:47.673
STEP: referencing matching pods with named port 05/06/23 07:07:52.68
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 05/06/23 07:07:57.688
STEP: recreating EndpointSlices after they've been deleted 05/06/23 07:08:02.695
May  6 07:08:02.711: INFO: EndpointSlice for Service endpointslice-7428/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
May  6 07:08:12.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-7428" for this suite. 05/06/23 07:08:12.725
------------------------------
â€¢ [SLOW TEST] [31.152 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:07:41.578
    May  6 07:07:41.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename endpointslice 05/06/23 07:07:41.579
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:07:42.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:07:42.595
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 05/06/23 07:07:47.673
    STEP: referencing matching pods with named port 05/06/23 07:07:52.68
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 05/06/23 07:07:57.688
    STEP: recreating EndpointSlices after they've been deleted 05/06/23 07:08:02.695
    May  6 07:08:02.711: INFO: EndpointSlice for Service endpointslice-7428/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    May  6 07:08:12.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-7428" for this suite. 05/06/23 07:08:12.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:08:12.731
May  6 07:08:12.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:08:12.731
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:08:13.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:08:13.756
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-eacab21e-f9a8-4d5c-8e15-edd4fa4b29a4 05/06/23 07:08:13.758
STEP: Creating a pod to test consume secrets 05/06/23 07:08:13.762
May  6 07:08:13.768: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-66aab875-2e89-482d-8b1c-c864f73ff618" in namespace "projected-2487" to be "Succeeded or Failed"
May  6 07:08:13.770: INFO: Pod "pod-projected-secrets-66aab875-2e89-482d-8b1c-c864f73ff618": Phase="Pending", Reason="", readiness=false. Elapsed: 1.962349ms
May  6 07:08:15.774: INFO: Pod "pod-projected-secrets-66aab875-2e89-482d-8b1c-c864f73ff618": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005338053s
May  6 07:08:17.774: INFO: Pod "pod-projected-secrets-66aab875-2e89-482d-8b1c-c864f73ff618": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005608786s
STEP: Saw pod success 05/06/23 07:08:17.774
May  6 07:08:17.774: INFO: Pod "pod-projected-secrets-66aab875-2e89-482d-8b1c-c864f73ff618" satisfied condition "Succeeded or Failed"
May  6 07:08:17.777: INFO: Trying to get logs from node cncf-2 pod pod-projected-secrets-66aab875-2e89-482d-8b1c-c864f73ff618 container secret-volume-test: <nil>
STEP: delete the pod 05/06/23 07:08:17.787
May  6 07:08:17.798: INFO: Waiting for pod pod-projected-secrets-66aab875-2e89-482d-8b1c-c864f73ff618 to disappear
May  6 07:08:17.800: INFO: Pod pod-projected-secrets-66aab875-2e89-482d-8b1c-c864f73ff618 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May  6 07:08:17.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2487" for this suite. 05/06/23 07:08:17.804
------------------------------
â€¢ [SLOW TEST] [5.079 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:08:12.731
    May  6 07:08:12.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:08:12.731
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:08:13.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:08:13.756
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-eacab21e-f9a8-4d5c-8e15-edd4fa4b29a4 05/06/23 07:08:13.758
    STEP: Creating a pod to test consume secrets 05/06/23 07:08:13.762
    May  6 07:08:13.768: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-66aab875-2e89-482d-8b1c-c864f73ff618" in namespace "projected-2487" to be "Succeeded or Failed"
    May  6 07:08:13.770: INFO: Pod "pod-projected-secrets-66aab875-2e89-482d-8b1c-c864f73ff618": Phase="Pending", Reason="", readiness=false. Elapsed: 1.962349ms
    May  6 07:08:15.774: INFO: Pod "pod-projected-secrets-66aab875-2e89-482d-8b1c-c864f73ff618": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005338053s
    May  6 07:08:17.774: INFO: Pod "pod-projected-secrets-66aab875-2e89-482d-8b1c-c864f73ff618": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005608786s
    STEP: Saw pod success 05/06/23 07:08:17.774
    May  6 07:08:17.774: INFO: Pod "pod-projected-secrets-66aab875-2e89-482d-8b1c-c864f73ff618" satisfied condition "Succeeded or Failed"
    May  6 07:08:17.777: INFO: Trying to get logs from node cncf-2 pod pod-projected-secrets-66aab875-2e89-482d-8b1c-c864f73ff618 container secret-volume-test: <nil>
    STEP: delete the pod 05/06/23 07:08:17.787
    May  6 07:08:17.798: INFO: Waiting for pod pod-projected-secrets-66aab875-2e89-482d-8b1c-c864f73ff618 to disappear
    May  6 07:08:17.800: INFO: Pod pod-projected-secrets-66aab875-2e89-482d-8b1c-c864f73ff618 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May  6 07:08:17.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2487" for this suite. 05/06/23 07:08:17.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:08:17.812
May  6 07:08:17.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename custom-resource-definition 05/06/23 07:08:17.813
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:08:18.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:08:18.828
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
May  6 07:08:18.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:08:21.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-4626" for this suite. 05/06/23 07:08:21.938
------------------------------
â€¢ [4.132 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:08:17.812
    May  6 07:08:17.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename custom-resource-definition 05/06/23 07:08:17.813
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:08:18.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:08:18.828
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    May  6 07:08:18.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:08:21.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-4626" for this suite. 05/06/23 07:08:21.938
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:08:21.944
May  6 07:08:21.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename job 05/06/23 07:08:21.945
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:08:22.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:08:22.962
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 05/06/23 07:08:22.964
STEP: Ensure pods equal to parallelism count is attached to the job 05/06/23 07:08:22.974
STEP: patching /status 05/06/23 07:08:24.977
STEP: updating /status 05/06/23 07:08:24.985
STEP: get /status 05/06/23 07:08:25.01
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May  6 07:08:25.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2925" for this suite. 05/06/23 07:08:25.015
------------------------------
â€¢ [3.078 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:08:21.944
    May  6 07:08:21.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename job 05/06/23 07:08:21.945
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:08:22.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:08:22.962
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 05/06/23 07:08:22.964
    STEP: Ensure pods equal to parallelism count is attached to the job 05/06/23 07:08:22.974
    STEP: patching /status 05/06/23 07:08:24.977
    STEP: updating /status 05/06/23 07:08:24.985
    STEP: get /status 05/06/23 07:08:25.01
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May  6 07:08:25.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2925" for this suite. 05/06/23 07:08:25.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:08:25.023
May  6 07:08:25.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename var-expansion 05/06/23 07:08:25.023
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:08:26.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:08:26.041
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 05/06/23 07:08:26.043
May  6 07:08:26.051: INFO: Waiting up to 2m0s for pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e" in namespace "var-expansion-5963" to be "running"
May  6 07:08:26.053: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.257379ms
May  6 07:08:28.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006165896s
May  6 07:08:30.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006395482s
May  6 07:08:32.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005350966s
May  6 07:08:34.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006335268s
May  6 07:08:36.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005635388s
May  6 07:08:38.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005433646s
May  6 07:08:40.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005803287s
May  6 07:08:42.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006301675s
May  6 07:08:44.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005732005s
May  6 07:08:46.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005638538s
May  6 07:08:48.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005343411s
May  6 07:08:50.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004960907s
May  6 07:08:52.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 26.005589526s
May  6 07:08:54.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006676174s
May  6 07:08:56.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005406232s
May  6 07:08:58.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005806888s
May  6 07:09:00.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 34.005097894s
May  6 07:09:02.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 36.007108229s
May  6 07:09:04.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00538038s
May  6 07:09:06.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007222826s
May  6 07:09:08.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006988024s
May  6 07:09:10.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006270755s
May  6 07:09:12.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 46.005099815s
May  6 07:09:14.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 48.005271758s
May  6 07:09:16.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 50.006461524s
May  6 07:09:18.059: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 52.007689383s
May  6 07:09:20.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005714086s
May  6 07:09:22.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006435052s
May  6 07:09:24.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 58.005596586s
May  6 07:09:26.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.006630748s
May  6 07:09:28.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005205991s
May  6 07:09:30.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.005132591s
May  6 07:09:32.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.005498144s
May  6 07:09:34.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006323522s
May  6 07:09:36.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006114172s
May  6 07:09:38.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007237594s
May  6 07:09:40.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.006122876s
May  6 07:09:42.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007294544s
May  6 07:09:44.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.006247764s
May  6 07:09:46.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005035722s
May  6 07:09:48.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006808743s
May  6 07:09:50.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006101912s
May  6 07:09:52.067: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.015773794s
May  6 07:09:54.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.005580877s
May  6 07:09:56.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.005857896s
May  6 07:09:58.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.005563949s
May  6 07:10:00.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.00595726s
May  6 07:10:02.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005450751s
May  6 07:10:04.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.0060903s
May  6 07:10:06.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.006041733s
May  6 07:10:08.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005941466s
May  6 07:10:10.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006088009s
May  6 07:10:12.059: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.007583644s
May  6 07:10:14.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.006813805s
May  6 07:10:16.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006593903s
May  6 07:10:18.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006814016s
May  6 07:10:20.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.005881249s
May  6 07:10:22.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006684952s
May  6 07:10:24.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006638589s
May  6 07:10:26.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006873266s
May  6 07:10:26.061: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009681603s
STEP: updating the pod 05/06/23 07:10:26.061
May  6 07:10:26.573: INFO: Successfully updated pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e"
STEP: waiting for pod running 05/06/23 07:10:26.573
May  6 07:10:26.573: INFO: Waiting up to 2m0s for pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e" in namespace "var-expansion-5963" to be "running"
May  6 07:10:26.576: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.560947ms
May  6 07:10:28.582: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008233053s
May  6 07:10:28.582: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e" satisfied condition "running"
STEP: deleting the pod gracefully 05/06/23 07:10:28.582
May  6 07:10:28.582: INFO: Deleting pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e" in namespace "var-expansion-5963"
May  6 07:10:28.590: INFO: Wait up to 5m0s for pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May  6 07:11:00.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5963" for this suite. 05/06/23 07:11:00.6
------------------------------
â€¢ [SLOW TEST] [155.583 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:08:25.023
    May  6 07:08:25.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename var-expansion 05/06/23 07:08:25.023
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:08:26.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:08:26.041
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 05/06/23 07:08:26.043
    May  6 07:08:26.051: INFO: Waiting up to 2m0s for pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e" in namespace "var-expansion-5963" to be "running"
    May  6 07:08:26.053: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.257379ms
    May  6 07:08:28.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006165896s
    May  6 07:08:30.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006395482s
    May  6 07:08:32.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005350966s
    May  6 07:08:34.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006335268s
    May  6 07:08:36.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005635388s
    May  6 07:08:38.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005433646s
    May  6 07:08:40.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005803287s
    May  6 07:08:42.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006301675s
    May  6 07:08:44.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005732005s
    May  6 07:08:46.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005638538s
    May  6 07:08:48.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005343411s
    May  6 07:08:50.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004960907s
    May  6 07:08:52.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 26.005589526s
    May  6 07:08:54.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006676174s
    May  6 07:08:56.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005406232s
    May  6 07:08:58.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005806888s
    May  6 07:09:00.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 34.005097894s
    May  6 07:09:02.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 36.007108229s
    May  6 07:09:04.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00538038s
    May  6 07:09:06.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007222826s
    May  6 07:09:08.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006988024s
    May  6 07:09:10.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006270755s
    May  6 07:09:12.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 46.005099815s
    May  6 07:09:14.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 48.005271758s
    May  6 07:09:16.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 50.006461524s
    May  6 07:09:18.059: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 52.007689383s
    May  6 07:09:20.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005714086s
    May  6 07:09:22.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006435052s
    May  6 07:09:24.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 58.005596586s
    May  6 07:09:26.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.006630748s
    May  6 07:09:28.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005205991s
    May  6 07:09:30.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.005132591s
    May  6 07:09:32.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.005498144s
    May  6 07:09:34.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006323522s
    May  6 07:09:36.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006114172s
    May  6 07:09:38.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007237594s
    May  6 07:09:40.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.006122876s
    May  6 07:09:42.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007294544s
    May  6 07:09:44.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.006247764s
    May  6 07:09:46.056: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005035722s
    May  6 07:09:48.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006808743s
    May  6 07:09:50.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006101912s
    May  6 07:09:52.067: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.015773794s
    May  6 07:09:54.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.005580877s
    May  6 07:09:56.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.005857896s
    May  6 07:09:58.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.005563949s
    May  6 07:10:00.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.00595726s
    May  6 07:10:02.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005450751s
    May  6 07:10:04.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.0060903s
    May  6 07:10:06.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.006041733s
    May  6 07:10:08.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005941466s
    May  6 07:10:10.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006088009s
    May  6 07:10:12.059: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.007583644s
    May  6 07:10:14.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.006813805s
    May  6 07:10:16.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006593903s
    May  6 07:10:18.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006814016s
    May  6 07:10:20.057: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.005881249s
    May  6 07:10:22.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006684952s
    May  6 07:10:24.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006638589s
    May  6 07:10:26.058: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006873266s
    May  6 07:10:26.061: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009681603s
    STEP: updating the pod 05/06/23 07:10:26.061
    May  6 07:10:26.573: INFO: Successfully updated pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e"
    STEP: waiting for pod running 05/06/23 07:10:26.573
    May  6 07:10:26.573: INFO: Waiting up to 2m0s for pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e" in namespace "var-expansion-5963" to be "running"
    May  6 07:10:26.576: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.560947ms
    May  6 07:10:28.582: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008233053s
    May  6 07:10:28.582: INFO: Pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e" satisfied condition "running"
    STEP: deleting the pod gracefully 05/06/23 07:10:28.582
    May  6 07:10:28.582: INFO: Deleting pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e" in namespace "var-expansion-5963"
    May  6 07:10:28.590: INFO: Wait up to 5m0s for pod "var-expansion-780ee35d-5e6b-4ecb-9224-5293c1da220e" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May  6 07:11:00.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5963" for this suite. 05/06/23 07:11:00.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:11:00.607
May  6 07:11:00.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename var-expansion 05/06/23 07:11:00.607
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:11:01.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:11:01.629
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
May  6 07:11:01.638: INFO: Waiting up to 2m0s for pod "var-expansion-ed68a357-a45e-462a-812a-05889c673561" in namespace "var-expansion-3977" to be "container 0 failed with reason CreateContainerConfigError"
May  6 07:11:01.641: INFO: Pod "var-expansion-ed68a357-a45e-462a-812a-05889c673561": Phase="Pending", Reason="", readiness=false. Elapsed: 2.747551ms
May  6 07:11:03.644: INFO: Pod "var-expansion-ed68a357-a45e-462a-812a-05889c673561": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005535306s
May  6 07:11:03.644: INFO: Pod "var-expansion-ed68a357-a45e-462a-812a-05889c673561" satisfied condition "container 0 failed with reason CreateContainerConfigError"
May  6 07:11:03.644: INFO: Deleting pod "var-expansion-ed68a357-a45e-462a-812a-05889c673561" in namespace "var-expansion-3977"
May  6 07:11:03.651: INFO: Wait up to 5m0s for pod "var-expansion-ed68a357-a45e-462a-812a-05889c673561" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May  6 07:11:05.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3977" for this suite. 05/06/23 07:11:05.662
------------------------------
â€¢ [SLOW TEST] [5.062 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:11:00.607
    May  6 07:11:00.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename var-expansion 05/06/23 07:11:00.607
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:11:01.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:11:01.629
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    May  6 07:11:01.638: INFO: Waiting up to 2m0s for pod "var-expansion-ed68a357-a45e-462a-812a-05889c673561" in namespace "var-expansion-3977" to be "container 0 failed with reason CreateContainerConfigError"
    May  6 07:11:01.641: INFO: Pod "var-expansion-ed68a357-a45e-462a-812a-05889c673561": Phase="Pending", Reason="", readiness=false. Elapsed: 2.747551ms
    May  6 07:11:03.644: INFO: Pod "var-expansion-ed68a357-a45e-462a-812a-05889c673561": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005535306s
    May  6 07:11:03.644: INFO: Pod "var-expansion-ed68a357-a45e-462a-812a-05889c673561" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    May  6 07:11:03.644: INFO: Deleting pod "var-expansion-ed68a357-a45e-462a-812a-05889c673561" in namespace "var-expansion-3977"
    May  6 07:11:03.651: INFO: Wait up to 5m0s for pod "var-expansion-ed68a357-a45e-462a-812a-05889c673561" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May  6 07:11:05.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3977" for this suite. 05/06/23 07:11:05.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:11:05.67
May  6 07:11:05.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename container-runtime 05/06/23 07:11:05.67
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:11:06.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:11:06.688
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 05/06/23 07:11:06.69
STEP: wait for the container to reach Succeeded 05/06/23 07:11:06.698
STEP: get the container status 05/06/23 07:11:10.717
STEP: the container should be terminated 05/06/23 07:11:10.719
STEP: the termination message should be set 05/06/23 07:11:10.719
May  6 07:11:10.719: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 05/06/23 07:11:10.719
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May  6 07:11:10.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6161" for this suite. 05/06/23 07:11:10.738
------------------------------
â€¢ [SLOW TEST] [5.074 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:11:05.67
    May  6 07:11:05.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename container-runtime 05/06/23 07:11:05.67
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:11:06.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:11:06.688
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 05/06/23 07:11:06.69
    STEP: wait for the container to reach Succeeded 05/06/23 07:11:06.698
    STEP: get the container status 05/06/23 07:11:10.717
    STEP: the container should be terminated 05/06/23 07:11:10.719
    STEP: the termination message should be set 05/06/23 07:11:10.719
    May  6 07:11:10.719: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 05/06/23 07:11:10.719
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May  6 07:11:10.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6161" for this suite. 05/06/23 07:11:10.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:11:10.744
May  6 07:11:10.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:11:10.745
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:11:11.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:11:11.763
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 05/06/23 07:11:11.765
May  6 07:11:11.772: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c67bd3c-7889-402c-80a5-23123a452fd7" in namespace "projected-812" to be "Succeeded or Failed"
May  6 07:11:11.774: INFO: Pod "downwardapi-volume-3c67bd3c-7889-402c-80a5-23123a452fd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.496073ms
May  6 07:11:13.777: INFO: Pod "downwardapi-volume-3c67bd3c-7889-402c-80a5-23123a452fd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005398489s
May  6 07:11:15.778: INFO: Pod "downwardapi-volume-3c67bd3c-7889-402c-80a5-23123a452fd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006401335s
STEP: Saw pod success 05/06/23 07:11:15.778
May  6 07:11:15.778: INFO: Pod "downwardapi-volume-3c67bd3c-7889-402c-80a5-23123a452fd7" satisfied condition "Succeeded or Failed"
May  6 07:11:15.781: INFO: Trying to get logs from node cncf-3 pod downwardapi-volume-3c67bd3c-7889-402c-80a5-23123a452fd7 container client-container: <nil>
STEP: delete the pod 05/06/23 07:11:15.791
May  6 07:11:15.811: INFO: Waiting for pod downwardapi-volume-3c67bd3c-7889-402c-80a5-23123a452fd7 to disappear
May  6 07:11:15.814: INFO: Pod downwardapi-volume-3c67bd3c-7889-402c-80a5-23123a452fd7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  6 07:11:15.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-812" for this suite. 05/06/23 07:11:15.818
------------------------------
â€¢ [SLOW TEST] [5.080 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:11:10.744
    May  6 07:11:10.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:11:10.745
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:11:11.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:11:11.763
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 05/06/23 07:11:11.765
    May  6 07:11:11.772: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c67bd3c-7889-402c-80a5-23123a452fd7" in namespace "projected-812" to be "Succeeded or Failed"
    May  6 07:11:11.774: INFO: Pod "downwardapi-volume-3c67bd3c-7889-402c-80a5-23123a452fd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.496073ms
    May  6 07:11:13.777: INFO: Pod "downwardapi-volume-3c67bd3c-7889-402c-80a5-23123a452fd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005398489s
    May  6 07:11:15.778: INFO: Pod "downwardapi-volume-3c67bd3c-7889-402c-80a5-23123a452fd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006401335s
    STEP: Saw pod success 05/06/23 07:11:15.778
    May  6 07:11:15.778: INFO: Pod "downwardapi-volume-3c67bd3c-7889-402c-80a5-23123a452fd7" satisfied condition "Succeeded or Failed"
    May  6 07:11:15.781: INFO: Trying to get logs from node cncf-3 pod downwardapi-volume-3c67bd3c-7889-402c-80a5-23123a452fd7 container client-container: <nil>
    STEP: delete the pod 05/06/23 07:11:15.791
    May  6 07:11:15.811: INFO: Waiting for pod downwardapi-volume-3c67bd3c-7889-402c-80a5-23123a452fd7 to disappear
    May  6 07:11:15.814: INFO: Pod downwardapi-volume-3c67bd3c-7889-402c-80a5-23123a452fd7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  6 07:11:15.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-812" for this suite. 05/06/23 07:11:15.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:11:15.824
May  6 07:11:15.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubelet-test 05/06/23 07:11:15.825
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:11:16.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:11:16.842
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
May  6 07:11:16.850: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsba971aff-a365-4108-a4f9-33a89fb1d70b" in namespace "kubelet-test-5253" to be "running and ready"
May  6 07:11:16.853: INFO: Pod "busybox-readonly-fsba971aff-a365-4108-a4f9-33a89fb1d70b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.417624ms
May  6 07:11:16.853: INFO: The phase of Pod busybox-readonly-fsba971aff-a365-4108-a4f9-33a89fb1d70b is Pending, waiting for it to be Running (with Ready = true)
May  6 07:11:18.856: INFO: Pod "busybox-readonly-fsba971aff-a365-4108-a4f9-33a89fb1d70b": Phase="Running", Reason="", readiness=true. Elapsed: 2.005461512s
May  6 07:11:18.856: INFO: The phase of Pod busybox-readonly-fsba971aff-a365-4108-a4f9-33a89fb1d70b is Running (Ready = true)
May  6 07:11:18.856: INFO: Pod "busybox-readonly-fsba971aff-a365-4108-a4f9-33a89fb1d70b" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May  6 07:11:18.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5253" for this suite. 05/06/23 07:11:18.875
------------------------------
â€¢ [3.057 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:11:15.824
    May  6 07:11:15.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubelet-test 05/06/23 07:11:15.825
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:11:16.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:11:16.842
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    May  6 07:11:16.850: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsba971aff-a365-4108-a4f9-33a89fb1d70b" in namespace "kubelet-test-5253" to be "running and ready"
    May  6 07:11:16.853: INFO: Pod "busybox-readonly-fsba971aff-a365-4108-a4f9-33a89fb1d70b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.417624ms
    May  6 07:11:16.853: INFO: The phase of Pod busybox-readonly-fsba971aff-a365-4108-a4f9-33a89fb1d70b is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:11:18.856: INFO: Pod "busybox-readonly-fsba971aff-a365-4108-a4f9-33a89fb1d70b": Phase="Running", Reason="", readiness=true. Elapsed: 2.005461512s
    May  6 07:11:18.856: INFO: The phase of Pod busybox-readonly-fsba971aff-a365-4108-a4f9-33a89fb1d70b is Running (Ready = true)
    May  6 07:11:18.856: INFO: Pod "busybox-readonly-fsba971aff-a365-4108-a4f9-33a89fb1d70b" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May  6 07:11:18.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5253" for this suite. 05/06/23 07:11:18.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:11:18.882
May  6 07:11:18.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename replicaset 05/06/23 07:11:18.882
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:11:19.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:11:19.9
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 05/06/23 07:11:19.904
STEP: Verify that the required pods have come up. 05/06/23 07:11:19.909
May  6 07:11:19.911: INFO: Pod name sample-pod: Found 0 pods out of 1
May  6 07:11:24.915: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/06/23 07:11:24.915
STEP: Getting /status 05/06/23 07:11:24.915
May  6 07:11:24.918: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 05/06/23 07:11:24.918
May  6 07:11:24.927: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 05/06/23 07:11:24.927
May  6 07:11:24.928: INFO: Observed &ReplicaSet event: ADDED
May  6 07:11:24.928: INFO: Observed &ReplicaSet event: MODIFIED
May  6 07:11:24.928: INFO: Observed &ReplicaSet event: MODIFIED
May  6 07:11:24.928: INFO: Observed &ReplicaSet event: MODIFIED
May  6 07:11:24.928: INFO: Found replicaset test-rs in namespace replicaset-8158 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  6 07:11:24.928: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 05/06/23 07:11:24.928
May  6 07:11:24.929: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May  6 07:11:24.935: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 05/06/23 07:11:24.935
May  6 07:11:24.936: INFO: Observed &ReplicaSet event: ADDED
May  6 07:11:24.936: INFO: Observed &ReplicaSet event: MODIFIED
May  6 07:11:24.936: INFO: Observed &ReplicaSet event: MODIFIED
May  6 07:11:24.936: INFO: Observed &ReplicaSet event: MODIFIED
May  6 07:11:24.936: INFO: Observed replicaset test-rs in namespace replicaset-8158 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  6 07:11:24.937: INFO: Observed &ReplicaSet event: MODIFIED
May  6 07:11:24.937: INFO: Found replicaset test-rs in namespace replicaset-8158 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
May  6 07:11:24.937: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May  6 07:11:24.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8158" for this suite. 05/06/23 07:11:24.94
------------------------------
â€¢ [SLOW TEST] [6.064 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:11:18.882
    May  6 07:11:18.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename replicaset 05/06/23 07:11:18.882
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:11:19.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:11:19.9
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 05/06/23 07:11:19.904
    STEP: Verify that the required pods have come up. 05/06/23 07:11:19.909
    May  6 07:11:19.911: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  6 07:11:24.915: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/06/23 07:11:24.915
    STEP: Getting /status 05/06/23 07:11:24.915
    May  6 07:11:24.918: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 05/06/23 07:11:24.918
    May  6 07:11:24.927: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 05/06/23 07:11:24.927
    May  6 07:11:24.928: INFO: Observed &ReplicaSet event: ADDED
    May  6 07:11:24.928: INFO: Observed &ReplicaSet event: MODIFIED
    May  6 07:11:24.928: INFO: Observed &ReplicaSet event: MODIFIED
    May  6 07:11:24.928: INFO: Observed &ReplicaSet event: MODIFIED
    May  6 07:11:24.928: INFO: Found replicaset test-rs in namespace replicaset-8158 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  6 07:11:24.928: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 05/06/23 07:11:24.928
    May  6 07:11:24.929: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May  6 07:11:24.935: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 05/06/23 07:11:24.935
    May  6 07:11:24.936: INFO: Observed &ReplicaSet event: ADDED
    May  6 07:11:24.936: INFO: Observed &ReplicaSet event: MODIFIED
    May  6 07:11:24.936: INFO: Observed &ReplicaSet event: MODIFIED
    May  6 07:11:24.936: INFO: Observed &ReplicaSet event: MODIFIED
    May  6 07:11:24.936: INFO: Observed replicaset test-rs in namespace replicaset-8158 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  6 07:11:24.937: INFO: Observed &ReplicaSet event: MODIFIED
    May  6 07:11:24.937: INFO: Found replicaset test-rs in namespace replicaset-8158 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    May  6 07:11:24.937: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May  6 07:11:24.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8158" for this suite. 05/06/23 07:11:24.94
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:11:24.946
May  6 07:11:24.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename pods 05/06/23 07:11:24.946
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:11:25.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:11:25.964
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
May  6 07:11:25.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: creating the pod 05/06/23 07:11:25.967
STEP: submitting the pod to kubernetes 05/06/23 07:11:25.967
May  6 07:11:25.975: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-7b587266-77a0-42d2-a7dc-505453cb4022" in namespace "pods-5729" to be "running and ready"
May  6 07:11:25.977: INFO: Pod "pod-exec-websocket-7b587266-77a0-42d2-a7dc-505453cb4022": Phase="Pending", Reason="", readiness=false. Elapsed: 2.077908ms
May  6 07:11:25.977: INFO: The phase of Pod pod-exec-websocket-7b587266-77a0-42d2-a7dc-505453cb4022 is Pending, waiting for it to be Running (with Ready = true)
May  6 07:11:27.980: INFO: Pod "pod-exec-websocket-7b587266-77a0-42d2-a7dc-505453cb4022": Phase="Running", Reason="", readiness=true. Elapsed: 2.005468275s
May  6 07:11:27.980: INFO: The phase of Pod pod-exec-websocket-7b587266-77a0-42d2-a7dc-505453cb4022 is Running (Ready = true)
May  6 07:11:27.980: INFO: Pod "pod-exec-websocket-7b587266-77a0-42d2-a7dc-505453cb4022" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  6 07:11:28.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5729" for this suite. 05/06/23 07:11:28.05
------------------------------
â€¢ [3.111 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:11:24.946
    May  6 07:11:24.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename pods 05/06/23 07:11:24.946
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:11:25.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:11:25.964
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    May  6 07:11:25.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: creating the pod 05/06/23 07:11:25.967
    STEP: submitting the pod to kubernetes 05/06/23 07:11:25.967
    May  6 07:11:25.975: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-7b587266-77a0-42d2-a7dc-505453cb4022" in namespace "pods-5729" to be "running and ready"
    May  6 07:11:25.977: INFO: Pod "pod-exec-websocket-7b587266-77a0-42d2-a7dc-505453cb4022": Phase="Pending", Reason="", readiness=false. Elapsed: 2.077908ms
    May  6 07:11:25.977: INFO: The phase of Pod pod-exec-websocket-7b587266-77a0-42d2-a7dc-505453cb4022 is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:11:27.980: INFO: Pod "pod-exec-websocket-7b587266-77a0-42d2-a7dc-505453cb4022": Phase="Running", Reason="", readiness=true. Elapsed: 2.005468275s
    May  6 07:11:27.980: INFO: The phase of Pod pod-exec-websocket-7b587266-77a0-42d2-a7dc-505453cb4022 is Running (Ready = true)
    May  6 07:11:27.980: INFO: Pod "pod-exec-websocket-7b587266-77a0-42d2-a7dc-505453cb4022" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  6 07:11:28.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5729" for this suite. 05/06/23 07:11:28.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:11:28.057
May  6 07:11:28.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename configmap 05/06/23 07:11:28.058
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:11:29.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:11:29.078
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-5c12a59a-b2c9-4177-911f-75ead66590cd 05/06/23 07:11:29.08
STEP: Creating a pod to test consume configMaps 05/06/23 07:11:29.084
May  6 07:11:29.092: INFO: Waiting up to 5m0s for pod "pod-configmaps-30916a27-fd2d-4697-bf71-b40b5cd22c97" in namespace "configmap-2469" to be "Succeeded or Failed"
May  6 07:11:29.094: INFO: Pod "pod-configmaps-30916a27-fd2d-4697-bf71-b40b5cd22c97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.201254ms
May  6 07:11:31.097: INFO: Pod "pod-configmaps-30916a27-fd2d-4697-bf71-b40b5cd22c97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005315764s
May  6 07:11:33.098: INFO: Pod "pod-configmaps-30916a27-fd2d-4697-bf71-b40b5cd22c97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005860924s
STEP: Saw pod success 05/06/23 07:11:33.098
May  6 07:11:33.098: INFO: Pod "pod-configmaps-30916a27-fd2d-4697-bf71-b40b5cd22c97" satisfied condition "Succeeded or Failed"
May  6 07:11:33.101: INFO: Trying to get logs from node cncf-0 pod pod-configmaps-30916a27-fd2d-4697-bf71-b40b5cd22c97 container agnhost-container: <nil>
STEP: delete the pod 05/06/23 07:11:33.106
May  6 07:11:33.120: INFO: Waiting for pod pod-configmaps-30916a27-fd2d-4697-bf71-b40b5cd22c97 to disappear
May  6 07:11:33.122: INFO: Pod pod-configmaps-30916a27-fd2d-4697-bf71-b40b5cd22c97 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  6 07:11:33.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2469" for this suite. 05/06/23 07:11:33.125
------------------------------
â€¢ [SLOW TEST] [5.074 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:11:28.057
    May  6 07:11:28.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename configmap 05/06/23 07:11:28.058
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:11:29.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:11:29.078
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-5c12a59a-b2c9-4177-911f-75ead66590cd 05/06/23 07:11:29.08
    STEP: Creating a pod to test consume configMaps 05/06/23 07:11:29.084
    May  6 07:11:29.092: INFO: Waiting up to 5m0s for pod "pod-configmaps-30916a27-fd2d-4697-bf71-b40b5cd22c97" in namespace "configmap-2469" to be "Succeeded or Failed"
    May  6 07:11:29.094: INFO: Pod "pod-configmaps-30916a27-fd2d-4697-bf71-b40b5cd22c97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.201254ms
    May  6 07:11:31.097: INFO: Pod "pod-configmaps-30916a27-fd2d-4697-bf71-b40b5cd22c97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005315764s
    May  6 07:11:33.098: INFO: Pod "pod-configmaps-30916a27-fd2d-4697-bf71-b40b5cd22c97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005860924s
    STEP: Saw pod success 05/06/23 07:11:33.098
    May  6 07:11:33.098: INFO: Pod "pod-configmaps-30916a27-fd2d-4697-bf71-b40b5cd22c97" satisfied condition "Succeeded or Failed"
    May  6 07:11:33.101: INFO: Trying to get logs from node cncf-0 pod pod-configmaps-30916a27-fd2d-4697-bf71-b40b5cd22c97 container agnhost-container: <nil>
    STEP: delete the pod 05/06/23 07:11:33.106
    May  6 07:11:33.120: INFO: Waiting for pod pod-configmaps-30916a27-fd2d-4697-bf71-b40b5cd22c97 to disappear
    May  6 07:11:33.122: INFO: Pod pod-configmaps-30916a27-fd2d-4697-bf71-b40b5cd22c97 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  6 07:11:33.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2469" for this suite. 05/06/23 07:11:33.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:11:33.132
May  6 07:11:33.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename emptydir 05/06/23 07:11:33.133
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:11:34.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:11:34.152
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 05/06/23 07:11:34.154
May  6 07:11:34.161: INFO: Waiting up to 5m0s for pod "pod-41d7c278-57bd-4d72-b74c-c5a821d0d570" in namespace "emptydir-9597" to be "Succeeded or Failed"
May  6 07:11:34.165: INFO: Pod "pod-41d7c278-57bd-4d72-b74c-c5a821d0d570": Phase="Pending", Reason="", readiness=false. Elapsed: 4.194933ms
May  6 07:11:36.168: INFO: Pod "pod-41d7c278-57bd-4d72-b74c-c5a821d0d570": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006862075s
May  6 07:11:38.168: INFO: Pod "pod-41d7c278-57bd-4d72-b74c-c5a821d0d570": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007186946s
STEP: Saw pod success 05/06/23 07:11:38.168
May  6 07:11:38.168: INFO: Pod "pod-41d7c278-57bd-4d72-b74c-c5a821d0d570" satisfied condition "Succeeded or Failed"
May  6 07:11:38.171: INFO: Trying to get logs from node cncf-2 pod pod-41d7c278-57bd-4d72-b74c-c5a821d0d570 container test-container: <nil>
STEP: delete the pod 05/06/23 07:11:38.179
May  6 07:11:38.189: INFO: Waiting for pod pod-41d7c278-57bd-4d72-b74c-c5a821d0d570 to disappear
May  6 07:11:38.192: INFO: Pod pod-41d7c278-57bd-4d72-b74c-c5a821d0d570 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  6 07:11:38.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9597" for this suite. 05/06/23 07:11:38.195
------------------------------
â€¢ [SLOW TEST] [5.068 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:11:33.132
    May  6 07:11:33.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename emptydir 05/06/23 07:11:33.133
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:11:34.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:11:34.152
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 05/06/23 07:11:34.154
    May  6 07:11:34.161: INFO: Waiting up to 5m0s for pod "pod-41d7c278-57bd-4d72-b74c-c5a821d0d570" in namespace "emptydir-9597" to be "Succeeded or Failed"
    May  6 07:11:34.165: INFO: Pod "pod-41d7c278-57bd-4d72-b74c-c5a821d0d570": Phase="Pending", Reason="", readiness=false. Elapsed: 4.194933ms
    May  6 07:11:36.168: INFO: Pod "pod-41d7c278-57bd-4d72-b74c-c5a821d0d570": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006862075s
    May  6 07:11:38.168: INFO: Pod "pod-41d7c278-57bd-4d72-b74c-c5a821d0d570": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007186946s
    STEP: Saw pod success 05/06/23 07:11:38.168
    May  6 07:11:38.168: INFO: Pod "pod-41d7c278-57bd-4d72-b74c-c5a821d0d570" satisfied condition "Succeeded or Failed"
    May  6 07:11:38.171: INFO: Trying to get logs from node cncf-2 pod pod-41d7c278-57bd-4d72-b74c-c5a821d0d570 container test-container: <nil>
    STEP: delete the pod 05/06/23 07:11:38.179
    May  6 07:11:38.189: INFO: Waiting for pod pod-41d7c278-57bd-4d72-b74c-c5a821d0d570 to disappear
    May  6 07:11:38.192: INFO: Pod pod-41d7c278-57bd-4d72-b74c-c5a821d0d570 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  6 07:11:38.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9597" for this suite. 05/06/23 07:11:38.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:11:38.201
May  6 07:11:38.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename dns 05/06/23 07:11:38.202
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:11:39.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:11:39.221
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 05/06/23 07:11:39.223
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8117.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8117.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8117.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8117.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8117.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8117.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8117.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8117.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8117.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 127.231.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.231.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.231.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.231.127_tcp@PTR;sleep 1; done
 05/06/23 07:11:39.242
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8117.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8117.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8117.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8117.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8117.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8117.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8117.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8117.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8117.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8117.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 127.231.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.231.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.231.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.231.127_tcp@PTR;sleep 1; done
 05/06/23 07:11:39.242
STEP: creating a pod to probe DNS 05/06/23 07:11:39.242
STEP: submitting the pod to kubernetes 05/06/23 07:11:39.242
May  6 07:11:39.255: INFO: Waiting up to 15m0s for pod "dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1" in namespace "dns-8117" to be "running"
May  6 07:11:39.258: INFO: Pod "dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.81523ms
May  6 07:11:41.263: INFO: Pod "dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007703936s
May  6 07:11:43.262: INFO: Pod "dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006709288s
May  6 07:11:45.261: INFO: Pod "dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006317369s
May  6 07:11:47.262: INFO: Pod "dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1": Phase="Running", Reason="", readiness=true. Elapsed: 8.007466921s
May  6 07:11:47.262: INFO: Pod "dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1" satisfied condition "running"
STEP: retrieving the pod 05/06/23 07:11:47.262
STEP: looking for the results for each expected name from probers 05/06/23 07:11:47.265
May  6 07:11:47.269: INFO: Unable to read wheezy_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:47.272: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:47.275: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:47.278: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:47.297: INFO: Unable to read jessie_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:47.300: INFO: Unable to read jessie_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:47.302: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:47.305: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:47.319: INFO: Lookups using dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1 failed for: [wheezy_udp@dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_udp@dns-test-service.dns-8117.svc.cluster.local jessie_tcp@dns-test-service.dns-8117.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local]

May  6 07:11:52.324: INFO: Unable to read wheezy_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:52.327: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:52.330: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:52.334: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:52.349: INFO: Unable to read jessie_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:52.352: INFO: Unable to read jessie_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:52.354: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:52.358: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:52.370: INFO: Lookups using dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1 failed for: [wheezy_udp@dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_udp@dns-test-service.dns-8117.svc.cluster.local jessie_tcp@dns-test-service.dns-8117.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local]

May  6 07:11:57.324: INFO: Unable to read wheezy_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:57.327: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:57.331: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:57.333: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:57.348: INFO: Unable to read jessie_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:57.351: INFO: Unable to read jessie_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:57.354: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:57.356: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:11:57.368: INFO: Lookups using dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1 failed for: [wheezy_udp@dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_udp@dns-test-service.dns-8117.svc.cluster.local jessie_tcp@dns-test-service.dns-8117.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local]

May  6 07:12:02.326: INFO: Unable to read wheezy_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:12:02.329: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:12:02.332: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:12:02.336: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:12:02.351: INFO: Unable to read jessie_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:12:02.354: INFO: Unable to read jessie_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:12:02.357: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:12:02.359: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:12:02.370: INFO: Lookups using dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1 failed for: [wheezy_udp@dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_udp@dns-test-service.dns-8117.svc.cluster.local jessie_tcp@dns-test-service.dns-8117.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local]

May  6 07:12:07.325: INFO: Unable to read wheezy_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:12:07.329: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:12:07.332: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:12:07.335: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:12:07.349: INFO: Unable to read jessie_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:12:07.352: INFO: Unable to read jessie_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:12:07.355: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:12:07.358: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
May  6 07:12:07.370: INFO: Lookups using dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1 failed for: [wheezy_udp@dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_udp@dns-test-service.dns-8117.svc.cluster.local jessie_tcp@dns-test-service.dns-8117.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local]

May  6 07:12:12.367: INFO: DNS probes using dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1 succeeded

STEP: deleting the pod 05/06/23 07:12:12.367
STEP: deleting the test service 05/06/23 07:12:12.386
STEP: deleting the test headless service 05/06/23 07:12:12.44
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May  6 07:12:12.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8117" for this suite. 05/06/23 07:12:12.479
------------------------------
â€¢ [SLOW TEST] [34.310 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:11:38.201
    May  6 07:11:38.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename dns 05/06/23 07:11:38.202
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:11:39.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:11:39.221
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 05/06/23 07:11:39.223
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8117.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8117.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8117.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8117.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8117.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8117.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8117.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8117.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8117.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 127.231.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.231.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.231.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.231.127_tcp@PTR;sleep 1; done
     05/06/23 07:11:39.242
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8117.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8117.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8117.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8117.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8117.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8117.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8117.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8117.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8117.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8117.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 127.231.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.231.127_udp@PTR;check="$$(dig +tcp +noall +answer +search 127.231.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.231.127_tcp@PTR;sleep 1; done
     05/06/23 07:11:39.242
    STEP: creating a pod to probe DNS 05/06/23 07:11:39.242
    STEP: submitting the pod to kubernetes 05/06/23 07:11:39.242
    May  6 07:11:39.255: INFO: Waiting up to 15m0s for pod "dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1" in namespace "dns-8117" to be "running"
    May  6 07:11:39.258: INFO: Pod "dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.81523ms
    May  6 07:11:41.263: INFO: Pod "dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007703936s
    May  6 07:11:43.262: INFO: Pod "dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006709288s
    May  6 07:11:45.261: INFO: Pod "dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006317369s
    May  6 07:11:47.262: INFO: Pod "dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1": Phase="Running", Reason="", readiness=true. Elapsed: 8.007466921s
    May  6 07:11:47.262: INFO: Pod "dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1" satisfied condition "running"
    STEP: retrieving the pod 05/06/23 07:11:47.262
    STEP: looking for the results for each expected name from probers 05/06/23 07:11:47.265
    May  6 07:11:47.269: INFO: Unable to read wheezy_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:47.272: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:47.275: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:47.278: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:47.297: INFO: Unable to read jessie_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:47.300: INFO: Unable to read jessie_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:47.302: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:47.305: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:47.319: INFO: Lookups using dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1 failed for: [wheezy_udp@dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_udp@dns-test-service.dns-8117.svc.cluster.local jessie_tcp@dns-test-service.dns-8117.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local]

    May  6 07:11:52.324: INFO: Unable to read wheezy_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:52.327: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:52.330: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:52.334: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:52.349: INFO: Unable to read jessie_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:52.352: INFO: Unable to read jessie_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:52.354: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:52.358: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:52.370: INFO: Lookups using dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1 failed for: [wheezy_udp@dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_udp@dns-test-service.dns-8117.svc.cluster.local jessie_tcp@dns-test-service.dns-8117.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local]

    May  6 07:11:57.324: INFO: Unable to read wheezy_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:57.327: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:57.331: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:57.333: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:57.348: INFO: Unable to read jessie_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:57.351: INFO: Unable to read jessie_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:57.354: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:57.356: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:11:57.368: INFO: Lookups using dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1 failed for: [wheezy_udp@dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_udp@dns-test-service.dns-8117.svc.cluster.local jessie_tcp@dns-test-service.dns-8117.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local]

    May  6 07:12:02.326: INFO: Unable to read wheezy_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:12:02.329: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:12:02.332: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:12:02.336: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:12:02.351: INFO: Unable to read jessie_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:12:02.354: INFO: Unable to read jessie_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:12:02.357: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:12:02.359: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:12:02.370: INFO: Lookups using dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1 failed for: [wheezy_udp@dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_udp@dns-test-service.dns-8117.svc.cluster.local jessie_tcp@dns-test-service.dns-8117.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local]

    May  6 07:12:07.325: INFO: Unable to read wheezy_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:12:07.329: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:12:07.332: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:12:07.335: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:12:07.349: INFO: Unable to read jessie_udp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:12:07.352: INFO: Unable to read jessie_tcp@dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:12:07.355: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:12:07.358: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local from pod dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1: the server could not find the requested resource (get pods dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1)
    May  6 07:12:07.370: INFO: Lookups using dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1 failed for: [wheezy_udp@dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@dns-test-service.dns-8117.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_udp@dns-test-service.dns-8117.svc.cluster.local jessie_tcp@dns-test-service.dns-8117.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8117.svc.cluster.local]

    May  6 07:12:12.367: INFO: DNS probes using dns-8117/dns-test-8e8311fb-296c-4244-9a24-e84a3bd13bc1 succeeded

    STEP: deleting the pod 05/06/23 07:12:12.367
    STEP: deleting the test service 05/06/23 07:12:12.386
    STEP: deleting the test headless service 05/06/23 07:12:12.44
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May  6 07:12:12.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8117" for this suite. 05/06/23 07:12:12.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:12:12.512
May  6 07:12:12.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename emptydir-wrapper 05/06/23 07:12:12.512
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:12:13.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:12:13.529
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
May  6 07:12:13.547: INFO: Waiting up to 5m0s for pod "pod-secrets-7d3a9344-aa81-431b-ad11-476ab351ff8d" in namespace "emptydir-wrapper-8587" to be "running and ready"
May  6 07:12:13.549: INFO: Pod "pod-secrets-7d3a9344-aa81-431b-ad11-476ab351ff8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.226822ms
May  6 07:12:13.549: INFO: The phase of Pod pod-secrets-7d3a9344-aa81-431b-ad11-476ab351ff8d is Pending, waiting for it to be Running (with Ready = true)
May  6 07:12:15.552: INFO: Pod "pod-secrets-7d3a9344-aa81-431b-ad11-476ab351ff8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005209194s
May  6 07:12:15.552: INFO: The phase of Pod pod-secrets-7d3a9344-aa81-431b-ad11-476ab351ff8d is Running (Ready = true)
May  6 07:12:15.552: INFO: Pod "pod-secrets-7d3a9344-aa81-431b-ad11-476ab351ff8d" satisfied condition "running and ready"
STEP: Cleaning up the secret 05/06/23 07:12:15.555
STEP: Cleaning up the configmap 05/06/23 07:12:15.56
STEP: Cleaning up the pod 05/06/23 07:12:15.565
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
May  6 07:12:15.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-8587" for this suite. 05/06/23 07:12:15.581
------------------------------
â€¢ [3.076 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:12:12.512
    May  6 07:12:12.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename emptydir-wrapper 05/06/23 07:12:12.512
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:12:13.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:12:13.529
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    May  6 07:12:13.547: INFO: Waiting up to 5m0s for pod "pod-secrets-7d3a9344-aa81-431b-ad11-476ab351ff8d" in namespace "emptydir-wrapper-8587" to be "running and ready"
    May  6 07:12:13.549: INFO: Pod "pod-secrets-7d3a9344-aa81-431b-ad11-476ab351ff8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.226822ms
    May  6 07:12:13.549: INFO: The phase of Pod pod-secrets-7d3a9344-aa81-431b-ad11-476ab351ff8d is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:12:15.552: INFO: Pod "pod-secrets-7d3a9344-aa81-431b-ad11-476ab351ff8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005209194s
    May  6 07:12:15.552: INFO: The phase of Pod pod-secrets-7d3a9344-aa81-431b-ad11-476ab351ff8d is Running (Ready = true)
    May  6 07:12:15.552: INFO: Pod "pod-secrets-7d3a9344-aa81-431b-ad11-476ab351ff8d" satisfied condition "running and ready"
    STEP: Cleaning up the secret 05/06/23 07:12:15.555
    STEP: Cleaning up the configmap 05/06/23 07:12:15.56
    STEP: Cleaning up the pod 05/06/23 07:12:15.565
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    May  6 07:12:15.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-8587" for this suite. 05/06/23 07:12:15.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:12:15.588
May  6 07:12:15.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename crd-publish-openapi 05/06/23 07:12:15.589
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:12:16.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:12:16.607
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 05/06/23 07:12:16.609
May  6 07:12:16.610: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:12:18.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:12:25.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2015" for this suite. 05/06/23 07:12:25.783
------------------------------
â€¢ [SLOW TEST] [10.201 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:12:15.588
    May  6 07:12:15.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename crd-publish-openapi 05/06/23 07:12:15.589
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:12:16.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:12:16.607
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 05/06/23 07:12:16.609
    May  6 07:12:16.610: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:12:18.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:12:25.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2015" for this suite. 05/06/23 07:12:25.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:12:25.791
May  6 07:12:25.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename configmap 05/06/23 07:12:25.792
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:12:26.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:12:26.809
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-8660a1be-a375-4e10-84d9-3b1e6138cf86 05/06/23 07:12:26.811
STEP: Creating a pod to test consume configMaps 05/06/23 07:12:26.815
May  6 07:12:26.823: INFO: Waiting up to 5m0s for pod "pod-configmaps-336846d3-0171-4a5b-add3-2d9cf6c8631d" in namespace "configmap-2642" to be "Succeeded or Failed"
May  6 07:12:26.826: INFO: Pod "pod-configmaps-336846d3-0171-4a5b-add3-2d9cf6c8631d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054424ms
May  6 07:12:28.829: INFO: Pod "pod-configmaps-336846d3-0171-4a5b-add3-2d9cf6c8631d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005382495s
May  6 07:12:30.830: INFO: Pod "pod-configmaps-336846d3-0171-4a5b-add3-2d9cf6c8631d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005998581s
STEP: Saw pod success 05/06/23 07:12:30.83
May  6 07:12:30.830: INFO: Pod "pod-configmaps-336846d3-0171-4a5b-add3-2d9cf6c8631d" satisfied condition "Succeeded or Failed"
May  6 07:12:30.833: INFO: Trying to get logs from node cncf-0 pod pod-configmaps-336846d3-0171-4a5b-add3-2d9cf6c8631d container agnhost-container: <nil>
STEP: delete the pod 05/06/23 07:12:30.844
May  6 07:12:30.856: INFO: Waiting for pod pod-configmaps-336846d3-0171-4a5b-add3-2d9cf6c8631d to disappear
May  6 07:12:30.858: INFO: Pod pod-configmaps-336846d3-0171-4a5b-add3-2d9cf6c8631d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  6 07:12:30.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2642" for this suite. 05/06/23 07:12:30.861
------------------------------
â€¢ [SLOW TEST] [5.078 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:12:25.791
    May  6 07:12:25.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename configmap 05/06/23 07:12:25.792
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:12:26.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:12:26.809
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-8660a1be-a375-4e10-84d9-3b1e6138cf86 05/06/23 07:12:26.811
    STEP: Creating a pod to test consume configMaps 05/06/23 07:12:26.815
    May  6 07:12:26.823: INFO: Waiting up to 5m0s for pod "pod-configmaps-336846d3-0171-4a5b-add3-2d9cf6c8631d" in namespace "configmap-2642" to be "Succeeded or Failed"
    May  6 07:12:26.826: INFO: Pod "pod-configmaps-336846d3-0171-4a5b-add3-2d9cf6c8631d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054424ms
    May  6 07:12:28.829: INFO: Pod "pod-configmaps-336846d3-0171-4a5b-add3-2d9cf6c8631d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005382495s
    May  6 07:12:30.830: INFO: Pod "pod-configmaps-336846d3-0171-4a5b-add3-2d9cf6c8631d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005998581s
    STEP: Saw pod success 05/06/23 07:12:30.83
    May  6 07:12:30.830: INFO: Pod "pod-configmaps-336846d3-0171-4a5b-add3-2d9cf6c8631d" satisfied condition "Succeeded or Failed"
    May  6 07:12:30.833: INFO: Trying to get logs from node cncf-0 pod pod-configmaps-336846d3-0171-4a5b-add3-2d9cf6c8631d container agnhost-container: <nil>
    STEP: delete the pod 05/06/23 07:12:30.844
    May  6 07:12:30.856: INFO: Waiting for pod pod-configmaps-336846d3-0171-4a5b-add3-2d9cf6c8631d to disappear
    May  6 07:12:30.858: INFO: Pod pod-configmaps-336846d3-0171-4a5b-add3-2d9cf6c8631d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  6 07:12:30.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2642" for this suite. 05/06/23 07:12:30.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:12:30.87
May  6 07:12:30.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename services 05/06/23 07:12:30.87
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:12:31.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:12:31.887
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-8150 05/06/23 07:12:31.889
STEP: creating service affinity-clusterip in namespace services-8150 05/06/23 07:12:31.889
STEP: creating replication controller affinity-clusterip in namespace services-8150 05/06/23 07:12:31.902
I0506 07:12:31.915493      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-8150, replica count: 3
I0506 07:12:34.967562      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 07:12:34.972: INFO: Creating new exec pod
May  6 07:12:34.977: INFO: Waiting up to 5m0s for pod "execpod-affinityhxfn5" in namespace "services-8150" to be "running"
May  6 07:12:34.979: INFO: Pod "execpod-affinityhxfn5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.359454ms
May  6 07:12:36.983: INFO: Pod "execpod-affinityhxfn5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006150954s
May  6 07:12:36.983: INFO: Pod "execpod-affinityhxfn5" satisfied condition "running"
May  6 07:12:37.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-8150 exec execpod-affinityhxfn5 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
May  6 07:12:38.104: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May  6 07:12:38.104: INFO: stdout: ""
May  6 07:12:38.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-8150 exec execpod-affinityhxfn5 -- /bin/sh -x -c nc -v -z -w 2 10.111.205.5 80'
May  6 07:12:38.217: INFO: stderr: "+ nc -v -z -w 2 10.111.205.5 80\nConnection to 10.111.205.5 80 port [tcp/http] succeeded!\n"
May  6 07:12:38.217: INFO: stdout: ""
May  6 07:12:38.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-8150 exec execpod-affinityhxfn5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.111.205.5:80/ ; done'
May  6 07:12:38.408: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n"
May  6 07:12:38.409: INFO: stdout: "\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg"
May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
May  6 07:12:38.409: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-8150, will wait for the garbage collector to delete the pods 05/06/23 07:12:38.42
May  6 07:12:38.481: INFO: Deleting ReplicationController affinity-clusterip took: 7.484715ms
May  6 07:12:38.582: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.74166ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  6 07:12:40.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8150" for this suite. 05/06/23 07:12:40.805
------------------------------
â€¢ [SLOW TEST] [9.941 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:12:30.87
    May  6 07:12:30.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename services 05/06/23 07:12:30.87
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:12:31.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:12:31.887
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-8150 05/06/23 07:12:31.889
    STEP: creating service affinity-clusterip in namespace services-8150 05/06/23 07:12:31.889
    STEP: creating replication controller affinity-clusterip in namespace services-8150 05/06/23 07:12:31.902
    I0506 07:12:31.915493      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-8150, replica count: 3
    I0506 07:12:34.967562      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  6 07:12:34.972: INFO: Creating new exec pod
    May  6 07:12:34.977: INFO: Waiting up to 5m0s for pod "execpod-affinityhxfn5" in namespace "services-8150" to be "running"
    May  6 07:12:34.979: INFO: Pod "execpod-affinityhxfn5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.359454ms
    May  6 07:12:36.983: INFO: Pod "execpod-affinityhxfn5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006150954s
    May  6 07:12:36.983: INFO: Pod "execpod-affinityhxfn5" satisfied condition "running"
    May  6 07:12:37.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-8150 exec execpod-affinityhxfn5 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    May  6 07:12:38.104: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    May  6 07:12:38.104: INFO: stdout: ""
    May  6 07:12:38.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-8150 exec execpod-affinityhxfn5 -- /bin/sh -x -c nc -v -z -w 2 10.111.205.5 80'
    May  6 07:12:38.217: INFO: stderr: "+ nc -v -z -w 2 10.111.205.5 80\nConnection to 10.111.205.5 80 port [tcp/http] succeeded!\n"
    May  6 07:12:38.217: INFO: stdout: ""
    May  6 07:12:38.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-8150 exec execpod-affinityhxfn5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.111.205.5:80/ ; done'
    May  6 07:12:38.408: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.205.5:80/\n"
    May  6 07:12:38.409: INFO: stdout: "\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg\naffinity-clusterip-sb2mg"
    May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
    May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
    May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
    May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
    May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
    May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
    May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
    May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
    May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
    May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
    May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
    May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
    May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
    May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
    May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
    May  6 07:12:38.409: INFO: Received response from host: affinity-clusterip-sb2mg
    May  6 07:12:38.409: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-8150, will wait for the garbage collector to delete the pods 05/06/23 07:12:38.42
    May  6 07:12:38.481: INFO: Deleting ReplicationController affinity-clusterip took: 7.484715ms
    May  6 07:12:38.582: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.74166ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  6 07:12:40.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8150" for this suite. 05/06/23 07:12:40.805
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:12:40.812
May  6 07:12:40.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename svcaccounts 05/06/23 07:12:40.813
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:12:41.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:12:41.828
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 05/06/23 07:12:41.83
STEP: watching for the ServiceAccount to be added 05/06/23 07:12:41.836
STEP: patching the ServiceAccount 05/06/23 07:12:41.837
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 05/06/23 07:12:41.842
STEP: deleting the ServiceAccount 05/06/23 07:12:41.845
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May  6 07:12:41.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2463" for this suite. 05/06/23 07:12:41.859
------------------------------
â€¢ [1.052 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:12:40.812
    May  6 07:12:40.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename svcaccounts 05/06/23 07:12:40.813
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:12:41.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:12:41.828
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 05/06/23 07:12:41.83
    STEP: watching for the ServiceAccount to be added 05/06/23 07:12:41.836
    STEP: patching the ServiceAccount 05/06/23 07:12:41.837
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 05/06/23 07:12:41.842
    STEP: deleting the ServiceAccount 05/06/23 07:12:41.845
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May  6 07:12:41.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2463" for this suite. 05/06/23 07:12:41.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:12:41.865
May  6 07:12:41.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename container-probe 05/06/23 07:12:41.866
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:12:42.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:12:42.88
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-58483d0f-7171-42c9-8519-b7b9cf2dc6d0 in namespace container-probe-8141 05/06/23 07:12:42.882
May  6 07:12:42.891: INFO: Waiting up to 5m0s for pod "busybox-58483d0f-7171-42c9-8519-b7b9cf2dc6d0" in namespace "container-probe-8141" to be "not pending"
May  6 07:12:42.893: INFO: Pod "busybox-58483d0f-7171-42c9-8519-b7b9cf2dc6d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019007ms
May  6 07:12:44.896: INFO: Pod "busybox-58483d0f-7171-42c9-8519-b7b9cf2dc6d0": Phase="Running", Reason="", readiness=true. Elapsed: 2.004443129s
May  6 07:12:44.896: INFO: Pod "busybox-58483d0f-7171-42c9-8519-b7b9cf2dc6d0" satisfied condition "not pending"
May  6 07:12:44.896: INFO: Started pod busybox-58483d0f-7171-42c9-8519-b7b9cf2dc6d0 in namespace container-probe-8141
STEP: checking the pod's current state and verifying that restartCount is present 05/06/23 07:12:44.896
May  6 07:12:44.898: INFO: Initial restart count of pod busybox-58483d0f-7171-42c9-8519-b7b9cf2dc6d0 is 0
May  6 07:13:34.983: INFO: Restart count of pod container-probe-8141/busybox-58483d0f-7171-42c9-8519-b7b9cf2dc6d0 is now 1 (50.084901723s elapsed)
STEP: deleting the pod 05/06/23 07:13:34.983
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May  6 07:13:35.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8141" for this suite. 05/06/23 07:13:35.008
------------------------------
â€¢ [SLOW TEST] [53.150 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:12:41.865
    May  6 07:12:41.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename container-probe 05/06/23 07:12:41.866
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:12:42.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:12:42.88
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-58483d0f-7171-42c9-8519-b7b9cf2dc6d0 in namespace container-probe-8141 05/06/23 07:12:42.882
    May  6 07:12:42.891: INFO: Waiting up to 5m0s for pod "busybox-58483d0f-7171-42c9-8519-b7b9cf2dc6d0" in namespace "container-probe-8141" to be "not pending"
    May  6 07:12:42.893: INFO: Pod "busybox-58483d0f-7171-42c9-8519-b7b9cf2dc6d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019007ms
    May  6 07:12:44.896: INFO: Pod "busybox-58483d0f-7171-42c9-8519-b7b9cf2dc6d0": Phase="Running", Reason="", readiness=true. Elapsed: 2.004443129s
    May  6 07:12:44.896: INFO: Pod "busybox-58483d0f-7171-42c9-8519-b7b9cf2dc6d0" satisfied condition "not pending"
    May  6 07:12:44.896: INFO: Started pod busybox-58483d0f-7171-42c9-8519-b7b9cf2dc6d0 in namespace container-probe-8141
    STEP: checking the pod's current state and verifying that restartCount is present 05/06/23 07:12:44.896
    May  6 07:12:44.898: INFO: Initial restart count of pod busybox-58483d0f-7171-42c9-8519-b7b9cf2dc6d0 is 0
    May  6 07:13:34.983: INFO: Restart count of pod container-probe-8141/busybox-58483d0f-7171-42c9-8519-b7b9cf2dc6d0 is now 1 (50.084901723s elapsed)
    STEP: deleting the pod 05/06/23 07:13:34.983
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May  6 07:13:35.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8141" for this suite. 05/06/23 07:13:35.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:13:35.016
May  6 07:13:35.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename emptydir 05/06/23 07:13:35.017
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:13:36.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:13:36.031
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 05/06/23 07:13:36.033
May  6 07:13:36.042: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-39416ae4-bdac-4e9e-b322-288b4fd9db25" in namespace "emptydir-5486" to be "running"
May  6 07:13:36.044: INFO: Pod "pod-sharedvolume-39416ae4-bdac-4e9e-b322-288b4fd9db25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.162499ms
May  6 07:13:38.048: INFO: Pod "pod-sharedvolume-39416ae4-bdac-4e9e-b322-288b4fd9db25": Phase="Running", Reason="", readiness=false. Elapsed: 2.00556195s
May  6 07:13:38.048: INFO: Pod "pod-sharedvolume-39416ae4-bdac-4e9e-b322-288b4fd9db25" satisfied condition "running"
STEP: Reading file content from the nginx-container 05/06/23 07:13:38.048
May  6 07:13:38.048: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5486 PodName:pod-sharedvolume-39416ae4-bdac-4e9e-b322-288b4fd9db25 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 07:13:38.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:13:38.048: INFO: ExecWithOptions: Clientset creation
May  6 07:13:38.048: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-5486/pods/pod-sharedvolume-39416ae4-bdac-4e9e-b322-288b4fd9db25/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
May  6 07:13:38.096: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  6 07:13:38.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5486" for this suite. 05/06/23 07:13:38.099
------------------------------
â€¢ [3.088 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:13:35.016
    May  6 07:13:35.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename emptydir 05/06/23 07:13:35.017
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:13:36.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:13:36.031
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 05/06/23 07:13:36.033
    May  6 07:13:36.042: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-39416ae4-bdac-4e9e-b322-288b4fd9db25" in namespace "emptydir-5486" to be "running"
    May  6 07:13:36.044: INFO: Pod "pod-sharedvolume-39416ae4-bdac-4e9e-b322-288b4fd9db25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.162499ms
    May  6 07:13:38.048: INFO: Pod "pod-sharedvolume-39416ae4-bdac-4e9e-b322-288b4fd9db25": Phase="Running", Reason="", readiness=false. Elapsed: 2.00556195s
    May  6 07:13:38.048: INFO: Pod "pod-sharedvolume-39416ae4-bdac-4e9e-b322-288b4fd9db25" satisfied condition "running"
    STEP: Reading file content from the nginx-container 05/06/23 07:13:38.048
    May  6 07:13:38.048: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5486 PodName:pod-sharedvolume-39416ae4-bdac-4e9e-b322-288b4fd9db25 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 07:13:38.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:13:38.048: INFO: ExecWithOptions: Clientset creation
    May  6 07:13:38.048: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-5486/pods/pod-sharedvolume-39416ae4-bdac-4e9e-b322-288b4fd9db25/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    May  6 07:13:38.096: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  6 07:13:38.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5486" for this suite. 05/06/23 07:13:38.099
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:13:38.105
May  6 07:13:38.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename resourcequota 05/06/23 07:13:38.105
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:13:39.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:13:39.122
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 05/06/23 07:13:39.123
STEP: Counting existing ResourceQuota 05/06/23 07:13:44.127
STEP: Creating a ResourceQuota 05/06/23 07:13:49.13
STEP: Ensuring resource quota status is calculated 05/06/23 07:13:49.137
STEP: Creating a Secret 05/06/23 07:13:51.14
STEP: Ensuring resource quota status captures secret creation 05/06/23 07:13:51.15
STEP: Deleting a secret 05/06/23 07:13:53.155
STEP: Ensuring resource quota status released usage 05/06/23 07:13:53.16
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  6 07:13:55.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9554" for this suite. 05/06/23 07:13:55.167
------------------------------
â€¢ [SLOW TEST] [17.068 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:13:38.105
    May  6 07:13:38.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename resourcequota 05/06/23 07:13:38.105
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:13:39.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:13:39.122
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 05/06/23 07:13:39.123
    STEP: Counting existing ResourceQuota 05/06/23 07:13:44.127
    STEP: Creating a ResourceQuota 05/06/23 07:13:49.13
    STEP: Ensuring resource quota status is calculated 05/06/23 07:13:49.137
    STEP: Creating a Secret 05/06/23 07:13:51.14
    STEP: Ensuring resource quota status captures secret creation 05/06/23 07:13:51.15
    STEP: Deleting a secret 05/06/23 07:13:53.155
    STEP: Ensuring resource quota status released usage 05/06/23 07:13:53.16
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  6 07:13:55.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9554" for this suite. 05/06/23 07:13:55.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:13:55.173
May  6 07:13:55.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename certificates 05/06/23 07:13:55.174
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:13:56.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:13:56.191
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 05/06/23 07:13:56.511
STEP: getting /apis/certificates.k8s.io 05/06/23 07:13:56.512
STEP: getting /apis/certificates.k8s.io/v1 05/06/23 07:13:56.513
STEP: creating 05/06/23 07:13:56.514
STEP: getting 05/06/23 07:13:56.534
STEP: listing 05/06/23 07:13:56.536
STEP: watching 05/06/23 07:13:56.539
May  6 07:13:56.539: INFO: starting watch
STEP: patching 05/06/23 07:13:56.539
STEP: updating 05/06/23 07:13:56.543
May  6 07:13:56.549: INFO: waiting for watch events with expected annotations
May  6 07:13:56.549: INFO: saw patched and updated annotations
STEP: getting /approval 05/06/23 07:13:56.549
STEP: patching /approval 05/06/23 07:13:56.551
STEP: updating /approval 05/06/23 07:13:56.556
STEP: getting /status 05/06/23 07:13:56.562
STEP: patching /status 05/06/23 07:13:56.564
STEP: updating /status 05/06/23 07:13:56.57
STEP: deleting 05/06/23 07:13:56.577
STEP: deleting a collection 05/06/23 07:13:56.586
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:13:56.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-301" for this suite. 05/06/23 07:13:56.602
------------------------------
â€¢ [1.433 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:13:55.173
    May  6 07:13:55.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename certificates 05/06/23 07:13:55.174
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:13:56.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:13:56.191
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 05/06/23 07:13:56.511
    STEP: getting /apis/certificates.k8s.io 05/06/23 07:13:56.512
    STEP: getting /apis/certificates.k8s.io/v1 05/06/23 07:13:56.513
    STEP: creating 05/06/23 07:13:56.514
    STEP: getting 05/06/23 07:13:56.534
    STEP: listing 05/06/23 07:13:56.536
    STEP: watching 05/06/23 07:13:56.539
    May  6 07:13:56.539: INFO: starting watch
    STEP: patching 05/06/23 07:13:56.539
    STEP: updating 05/06/23 07:13:56.543
    May  6 07:13:56.549: INFO: waiting for watch events with expected annotations
    May  6 07:13:56.549: INFO: saw patched and updated annotations
    STEP: getting /approval 05/06/23 07:13:56.549
    STEP: patching /approval 05/06/23 07:13:56.551
    STEP: updating /approval 05/06/23 07:13:56.556
    STEP: getting /status 05/06/23 07:13:56.562
    STEP: patching /status 05/06/23 07:13:56.564
    STEP: updating /status 05/06/23 07:13:56.57
    STEP: deleting 05/06/23 07:13:56.577
    STEP: deleting a collection 05/06/23 07:13:56.586
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:13:56.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-301" for this suite. 05/06/23 07:13:56.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:13:56.607
May  6 07:13:56.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename crd-publish-openapi 05/06/23 07:13:56.608
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:13:57.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:13:57.622
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
May  6 07:13:57.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 05/06/23 07:13:59.558
May  6 07:13:59.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 --namespace=crd-publish-openapi-8075 create -f -'
May  6 07:14:00.142: INFO: stderr: ""
May  6 07:14:00.142: INFO: stdout: "e2e-test-crd-publish-openapi-6129-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May  6 07:14:00.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 --namespace=crd-publish-openapi-8075 delete e2e-test-crd-publish-openapi-6129-crds test-foo'
May  6 07:14:00.198: INFO: stderr: ""
May  6 07:14:00.198: INFO: stdout: "e2e-test-crd-publish-openapi-6129-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May  6 07:14:00.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 --namespace=crd-publish-openapi-8075 apply -f -'
May  6 07:14:00.686: INFO: stderr: ""
May  6 07:14:00.686: INFO: stdout: "e2e-test-crd-publish-openapi-6129-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May  6 07:14:00.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 --namespace=crd-publish-openapi-8075 delete e2e-test-crd-publish-openapi-6129-crds test-foo'
May  6 07:14:00.741: INFO: stderr: ""
May  6 07:14:00.741: INFO: stdout: "e2e-test-crd-publish-openapi-6129-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 05/06/23 07:14:00.741
May  6 07:14:00.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 --namespace=crd-publish-openapi-8075 create -f -'
May  6 07:14:01.154: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 05/06/23 07:14:01.154
May  6 07:14:01.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 --namespace=crd-publish-openapi-8075 create -f -'
May  6 07:14:01.299: INFO: rc: 1
May  6 07:14:01.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 --namespace=crd-publish-openapi-8075 apply -f -'
May  6 07:14:01.448: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 05/06/23 07:14:01.448
May  6 07:14:01.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 --namespace=crd-publish-openapi-8075 create -f -'
May  6 07:14:01.588: INFO: rc: 1
May  6 07:14:01.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 --namespace=crd-publish-openapi-8075 apply -f -'
May  6 07:14:01.746: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 05/06/23 07:14:01.746
May  6 07:14:01.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 explain e2e-test-crd-publish-openapi-6129-crds'
May  6 07:14:01.885: INFO: stderr: ""
May  6 07:14:01.885: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6129-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 05/06/23 07:14:01.885
May  6 07:14:01.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 explain e2e-test-crd-publish-openapi-6129-crds.metadata'
May  6 07:14:02.028: INFO: stderr: ""
May  6 07:14:02.028: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6129-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May  6 07:14:02.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 explain e2e-test-crd-publish-openapi-6129-crds.spec'
May  6 07:14:02.163: INFO: stderr: ""
May  6 07:14:02.163: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6129-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May  6 07:14:02.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 explain e2e-test-crd-publish-openapi-6129-crds.spec.bars'
May  6 07:14:02.306: INFO: stderr: ""
May  6 07:14:02.306: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6129-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 05/06/23 07:14:02.306
May  6 07:14:02.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 explain e2e-test-crd-publish-openapi-6129-crds.spec.bars2'
May  6 07:14:02.445: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:14:04.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8075" for this suite. 05/06/23 07:14:04.799
------------------------------
â€¢ [SLOW TEST] [8.200 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:13:56.607
    May  6 07:13:56.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename crd-publish-openapi 05/06/23 07:13:56.608
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:13:57.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:13:57.622
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    May  6 07:13:57.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 05/06/23 07:13:59.558
    May  6 07:13:59.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 --namespace=crd-publish-openapi-8075 create -f -'
    May  6 07:14:00.142: INFO: stderr: ""
    May  6 07:14:00.142: INFO: stdout: "e2e-test-crd-publish-openapi-6129-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    May  6 07:14:00.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 --namespace=crd-publish-openapi-8075 delete e2e-test-crd-publish-openapi-6129-crds test-foo'
    May  6 07:14:00.198: INFO: stderr: ""
    May  6 07:14:00.198: INFO: stdout: "e2e-test-crd-publish-openapi-6129-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    May  6 07:14:00.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 --namespace=crd-publish-openapi-8075 apply -f -'
    May  6 07:14:00.686: INFO: stderr: ""
    May  6 07:14:00.686: INFO: stdout: "e2e-test-crd-publish-openapi-6129-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    May  6 07:14:00.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 --namespace=crd-publish-openapi-8075 delete e2e-test-crd-publish-openapi-6129-crds test-foo'
    May  6 07:14:00.741: INFO: stderr: ""
    May  6 07:14:00.741: INFO: stdout: "e2e-test-crd-publish-openapi-6129-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 05/06/23 07:14:00.741
    May  6 07:14:00.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 --namespace=crd-publish-openapi-8075 create -f -'
    May  6 07:14:01.154: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 05/06/23 07:14:01.154
    May  6 07:14:01.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 --namespace=crd-publish-openapi-8075 create -f -'
    May  6 07:14:01.299: INFO: rc: 1
    May  6 07:14:01.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 --namespace=crd-publish-openapi-8075 apply -f -'
    May  6 07:14:01.448: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 05/06/23 07:14:01.448
    May  6 07:14:01.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 --namespace=crd-publish-openapi-8075 create -f -'
    May  6 07:14:01.588: INFO: rc: 1
    May  6 07:14:01.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 --namespace=crd-publish-openapi-8075 apply -f -'
    May  6 07:14:01.746: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 05/06/23 07:14:01.746
    May  6 07:14:01.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 explain e2e-test-crd-publish-openapi-6129-crds'
    May  6 07:14:01.885: INFO: stderr: ""
    May  6 07:14:01.885: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6129-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 05/06/23 07:14:01.885
    May  6 07:14:01.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 explain e2e-test-crd-publish-openapi-6129-crds.metadata'
    May  6 07:14:02.028: INFO: stderr: ""
    May  6 07:14:02.028: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6129-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    May  6 07:14:02.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 explain e2e-test-crd-publish-openapi-6129-crds.spec'
    May  6 07:14:02.163: INFO: stderr: ""
    May  6 07:14:02.163: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6129-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    May  6 07:14:02.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 explain e2e-test-crd-publish-openapi-6129-crds.spec.bars'
    May  6 07:14:02.306: INFO: stderr: ""
    May  6 07:14:02.306: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6129-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 05/06/23 07:14:02.306
    May  6 07:14:02.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-8075 explain e2e-test-crd-publish-openapi-6129-crds.spec.bars2'
    May  6 07:14:02.445: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:14:04.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8075" for this suite. 05/06/23 07:14:04.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:14:04.807
May  6 07:14:04.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename container-probe 05/06/23 07:14:04.808
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:14:05.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:14:05.827
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-4e73fc3a-231b-42cb-a8ca-bd5966fd5ce6 in namespace container-probe-3129 05/06/23 07:14:05.829
May  6 07:14:05.839: INFO: Waiting up to 5m0s for pod "liveness-4e73fc3a-231b-42cb-a8ca-bd5966fd5ce6" in namespace "container-probe-3129" to be "not pending"
May  6 07:14:05.841: INFO: Pod "liveness-4e73fc3a-231b-42cb-a8ca-bd5966fd5ce6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076887ms
May  6 07:14:07.844: INFO: Pod "liveness-4e73fc3a-231b-42cb-a8ca-bd5966fd5ce6": Phase="Running", Reason="", readiness=true. Elapsed: 2.005247136s
May  6 07:14:07.844: INFO: Pod "liveness-4e73fc3a-231b-42cb-a8ca-bd5966fd5ce6" satisfied condition "not pending"
May  6 07:14:07.844: INFO: Started pod liveness-4e73fc3a-231b-42cb-a8ca-bd5966fd5ce6 in namespace container-probe-3129
STEP: checking the pod's current state and verifying that restartCount is present 05/06/23 07:14:07.844
May  6 07:14:07.846: INFO: Initial restart count of pod liveness-4e73fc3a-231b-42cb-a8ca-bd5966fd5ce6 is 0
STEP: deleting the pod 05/06/23 07:18:08.27
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May  6 07:18:08.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3129" for this suite. 05/06/23 07:18:08.288
------------------------------
â€¢ [SLOW TEST] [243.486 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:14:04.807
    May  6 07:14:04.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename container-probe 05/06/23 07:14:04.808
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:14:05.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:14:05.827
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-4e73fc3a-231b-42cb-a8ca-bd5966fd5ce6 in namespace container-probe-3129 05/06/23 07:14:05.829
    May  6 07:14:05.839: INFO: Waiting up to 5m0s for pod "liveness-4e73fc3a-231b-42cb-a8ca-bd5966fd5ce6" in namespace "container-probe-3129" to be "not pending"
    May  6 07:14:05.841: INFO: Pod "liveness-4e73fc3a-231b-42cb-a8ca-bd5966fd5ce6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076887ms
    May  6 07:14:07.844: INFO: Pod "liveness-4e73fc3a-231b-42cb-a8ca-bd5966fd5ce6": Phase="Running", Reason="", readiness=true. Elapsed: 2.005247136s
    May  6 07:14:07.844: INFO: Pod "liveness-4e73fc3a-231b-42cb-a8ca-bd5966fd5ce6" satisfied condition "not pending"
    May  6 07:14:07.844: INFO: Started pod liveness-4e73fc3a-231b-42cb-a8ca-bd5966fd5ce6 in namespace container-probe-3129
    STEP: checking the pod's current state and verifying that restartCount is present 05/06/23 07:14:07.844
    May  6 07:14:07.846: INFO: Initial restart count of pod liveness-4e73fc3a-231b-42cb-a8ca-bd5966fd5ce6 is 0
    STEP: deleting the pod 05/06/23 07:18:08.27
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May  6 07:18:08.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3129" for this suite. 05/06/23 07:18:08.288
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:18:08.293
May  6 07:18:08.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename emptydir 05/06/23 07:18:08.294
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:09.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:09.32
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 05/06/23 07:18:09.321
May  6 07:18:09.337: INFO: Waiting up to 5m0s for pod "pod-eb98fdf7-7d3a-4c66-8d13-da135a7b9bfa" in namespace "emptydir-6980" to be "Succeeded or Failed"
May  6 07:18:09.339: INFO: Pod "pod-eb98fdf7-7d3a-4c66-8d13-da135a7b9bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030299ms
May  6 07:18:11.343: INFO: Pod "pod-eb98fdf7-7d3a-4c66-8d13-da135a7b9bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00576895s
May  6 07:18:13.344: INFO: Pod "pod-eb98fdf7-7d3a-4c66-8d13-da135a7b9bfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006586508s
STEP: Saw pod success 05/06/23 07:18:13.344
May  6 07:18:13.344: INFO: Pod "pod-eb98fdf7-7d3a-4c66-8d13-da135a7b9bfa" satisfied condition "Succeeded or Failed"
May  6 07:18:13.346: INFO: Trying to get logs from node cncf-0 pod pod-eb98fdf7-7d3a-4c66-8d13-da135a7b9bfa container test-container: <nil>
STEP: delete the pod 05/06/23 07:18:13.358
May  6 07:18:13.371: INFO: Waiting for pod pod-eb98fdf7-7d3a-4c66-8d13-da135a7b9bfa to disappear
May  6 07:18:13.375: INFO: Pod pod-eb98fdf7-7d3a-4c66-8d13-da135a7b9bfa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  6 07:18:13.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6980" for this suite. 05/06/23 07:18:13.378
------------------------------
â€¢ [SLOW TEST] [5.092 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:18:08.293
    May  6 07:18:08.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename emptydir 05/06/23 07:18:08.294
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:09.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:09.32
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 05/06/23 07:18:09.321
    May  6 07:18:09.337: INFO: Waiting up to 5m0s for pod "pod-eb98fdf7-7d3a-4c66-8d13-da135a7b9bfa" in namespace "emptydir-6980" to be "Succeeded or Failed"
    May  6 07:18:09.339: INFO: Pod "pod-eb98fdf7-7d3a-4c66-8d13-da135a7b9bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030299ms
    May  6 07:18:11.343: INFO: Pod "pod-eb98fdf7-7d3a-4c66-8d13-da135a7b9bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00576895s
    May  6 07:18:13.344: INFO: Pod "pod-eb98fdf7-7d3a-4c66-8d13-da135a7b9bfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006586508s
    STEP: Saw pod success 05/06/23 07:18:13.344
    May  6 07:18:13.344: INFO: Pod "pod-eb98fdf7-7d3a-4c66-8d13-da135a7b9bfa" satisfied condition "Succeeded or Failed"
    May  6 07:18:13.346: INFO: Trying to get logs from node cncf-0 pod pod-eb98fdf7-7d3a-4c66-8d13-da135a7b9bfa container test-container: <nil>
    STEP: delete the pod 05/06/23 07:18:13.358
    May  6 07:18:13.371: INFO: Waiting for pod pod-eb98fdf7-7d3a-4c66-8d13-da135a7b9bfa to disappear
    May  6 07:18:13.375: INFO: Pod pod-eb98fdf7-7d3a-4c66-8d13-da135a7b9bfa no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  6 07:18:13.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6980" for this suite. 05/06/23 07:18:13.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:18:13.387
May  6 07:18:13.387: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename runtimeclass 05/06/23 07:18:13.387
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:14.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:14.406
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
May  6 07:18:14.430: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8771 to be scheduled
May  6 07:18:14.433: INFO: 1 pods are not scheduled: [runtimeclass-8771/test-runtimeclass-runtimeclass-8771-preconfigured-handler-mfml2(ff59491d-b8e2-42b1-ae01-c410f7173ca8)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May  6 07:18:16.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8771" for this suite. 05/06/23 07:18:16.446
------------------------------
â€¢ [3.065 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:18:13.387
    May  6 07:18:13.387: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename runtimeclass 05/06/23 07:18:13.387
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:14.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:14.406
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    May  6 07:18:14.430: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8771 to be scheduled
    May  6 07:18:14.433: INFO: 1 pods are not scheduled: [runtimeclass-8771/test-runtimeclass-runtimeclass-8771-preconfigured-handler-mfml2(ff59491d-b8e2-42b1-ae01-c410f7173ca8)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May  6 07:18:16.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8771" for this suite. 05/06/23 07:18:16.446
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:18:16.455
May  6 07:18:16.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename webhook 05/06/23 07:18:16.456
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:17.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:17.473
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/06/23 07:18:19.487
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:18:19.758
STEP: Deploying the webhook pod 05/06/23 07:18:19.768
STEP: Wait for the deployment to be ready 05/06/23 07:18:19.777
May  6 07:18:19.781: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/06/23 07:18:21.79
STEP: Verifying the service has paired with the endpoint 05/06/23 07:18:21.802
May  6 07:18:22.802: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 05/06/23 07:18:22.806
STEP: Creating a custom resource definition that should be denied by the webhook 05/06/23 07:18:22.824
May  6 07:18:22.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:18:22.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9302" for this suite. 05/06/23 07:18:22.883
STEP: Destroying namespace "webhook-9302-markers" for this suite. 05/06/23 07:18:22.891
------------------------------
â€¢ [SLOW TEST] [6.443 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:18:16.455
    May  6 07:18:16.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename webhook 05/06/23 07:18:16.456
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:17.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:17.473
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/06/23 07:18:19.487
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:18:19.758
    STEP: Deploying the webhook pod 05/06/23 07:18:19.768
    STEP: Wait for the deployment to be ready 05/06/23 07:18:19.777
    May  6 07:18:19.781: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/06/23 07:18:21.79
    STEP: Verifying the service has paired with the endpoint 05/06/23 07:18:21.802
    May  6 07:18:22.802: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 05/06/23 07:18:22.806
    STEP: Creating a custom resource definition that should be denied by the webhook 05/06/23 07:18:22.824
    May  6 07:18:22.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:18:22.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9302" for this suite. 05/06/23 07:18:22.883
    STEP: Destroying namespace "webhook-9302-markers" for this suite. 05/06/23 07:18:22.891
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:18:22.899
May  6 07:18:22.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename disruption 05/06/23 07:18:22.9
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:23.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:23.919
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 05/06/23 07:18:23.926
STEP: Waiting for all pods to be running 05/06/23 07:18:25.962
May  6 07:18:25.964: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May  6 07:18:27.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8809" for this suite. 05/06/23 07:18:27.979
------------------------------
â€¢ [SLOW TEST] [5.086 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:18:22.899
    May  6 07:18:22.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename disruption 05/06/23 07:18:22.9
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:23.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:23.919
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 05/06/23 07:18:23.926
    STEP: Waiting for all pods to be running 05/06/23 07:18:25.962
    May  6 07:18:25.964: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May  6 07:18:27.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8809" for this suite. 05/06/23 07:18:27.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:18:27.987
May  6 07:18:27.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename pods 05/06/23 07:18:27.987
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:29.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:29.005
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 05/06/23 07:18:29.007
May  6 07:18:29.015: INFO: Waiting up to 5m0s for pod "pod-kw9r9" in namespace "pods-2235" to be "running"
May  6 07:18:29.017: INFO: Pod "pod-kw9r9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.907554ms
May  6 07:18:31.021: INFO: Pod "pod-kw9r9": Phase="Running", Reason="", readiness=true. Elapsed: 2.005622106s
May  6 07:18:31.021: INFO: Pod "pod-kw9r9" satisfied condition "running"
STEP: patching /status 05/06/23 07:18:31.021
May  6 07:18:31.030: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  6 07:18:31.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2235" for this suite. 05/06/23 07:18:31.032
------------------------------
â€¢ [3.051 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:18:27.987
    May  6 07:18:27.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename pods 05/06/23 07:18:27.987
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:29.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:29.005
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 05/06/23 07:18:29.007
    May  6 07:18:29.015: INFO: Waiting up to 5m0s for pod "pod-kw9r9" in namespace "pods-2235" to be "running"
    May  6 07:18:29.017: INFO: Pod "pod-kw9r9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.907554ms
    May  6 07:18:31.021: INFO: Pod "pod-kw9r9": Phase="Running", Reason="", readiness=true. Elapsed: 2.005622106s
    May  6 07:18:31.021: INFO: Pod "pod-kw9r9" satisfied condition "running"
    STEP: patching /status 05/06/23 07:18:31.021
    May  6 07:18:31.030: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  6 07:18:31.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2235" for this suite. 05/06/23 07:18:31.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:18:31.039
May  6 07:18:31.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubectl 05/06/23 07:18:31.039
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:32.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:32.056
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 05/06/23 07:18:32.058
May  6 07:18:32.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-4571 create -f -'
May  6 07:18:32.609: INFO: stderr: ""
May  6 07:18:32.609: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/06/23 07:18:32.609
May  6 07:18:33.613: INFO: Selector matched 1 pods for map[app:agnhost]
May  6 07:18:33.613: INFO: Found 1 / 1
May  6 07:18:33.613: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 05/06/23 07:18:33.613
May  6 07:18:33.617: INFO: Selector matched 1 pods for map[app:agnhost]
May  6 07:18:33.617: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  6 07:18:33.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-4571 patch pod agnhost-primary-w6q97 -p {"metadata":{"annotations":{"x":"y"}}}'
May  6 07:18:33.676: INFO: stderr: ""
May  6 07:18:33.676: INFO: stdout: "pod/agnhost-primary-w6q97 patched\n"
STEP: checking annotations 05/06/23 07:18:33.676
May  6 07:18:33.679: INFO: Selector matched 1 pods for map[app:agnhost]
May  6 07:18:33.679: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  6 07:18:33.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4571" for this suite. 05/06/23 07:18:33.682
------------------------------
â€¢ [2.648 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:18:31.039
    May  6 07:18:31.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubectl 05/06/23 07:18:31.039
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:32.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:32.056
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 05/06/23 07:18:32.058
    May  6 07:18:32.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-4571 create -f -'
    May  6 07:18:32.609: INFO: stderr: ""
    May  6 07:18:32.609: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/06/23 07:18:32.609
    May  6 07:18:33.613: INFO: Selector matched 1 pods for map[app:agnhost]
    May  6 07:18:33.613: INFO: Found 1 / 1
    May  6 07:18:33.613: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 05/06/23 07:18:33.613
    May  6 07:18:33.617: INFO: Selector matched 1 pods for map[app:agnhost]
    May  6 07:18:33.617: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May  6 07:18:33.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-4571 patch pod agnhost-primary-w6q97 -p {"metadata":{"annotations":{"x":"y"}}}'
    May  6 07:18:33.676: INFO: stderr: ""
    May  6 07:18:33.676: INFO: stdout: "pod/agnhost-primary-w6q97 patched\n"
    STEP: checking annotations 05/06/23 07:18:33.676
    May  6 07:18:33.679: INFO: Selector matched 1 pods for map[app:agnhost]
    May  6 07:18:33.679: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  6 07:18:33.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4571" for this suite. 05/06/23 07:18:33.682
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:18:33.688
May  6 07:18:33.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:18:33.689
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:34.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:34.705
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 05/06/23 07:18:34.707
May  6 07:18:34.716: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f97c36b5-d52b-43e8-95f8-f4bba166861f" in namespace "projected-9186" to be "Succeeded or Failed"
May  6 07:18:34.719: INFO: Pod "downwardapi-volume-f97c36b5-d52b-43e8-95f8-f4bba166861f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.294661ms
May  6 07:18:36.723: INFO: Pod "downwardapi-volume-f97c36b5-d52b-43e8-95f8-f4bba166861f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006372483s
May  6 07:18:38.724: INFO: Pod "downwardapi-volume-f97c36b5-d52b-43e8-95f8-f4bba166861f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008181288s
STEP: Saw pod success 05/06/23 07:18:38.724
May  6 07:18:38.725: INFO: Pod "downwardapi-volume-f97c36b5-d52b-43e8-95f8-f4bba166861f" satisfied condition "Succeeded or Failed"
May  6 07:18:38.728: INFO: Trying to get logs from node cncf-3 pod downwardapi-volume-f97c36b5-d52b-43e8-95f8-f4bba166861f container client-container: <nil>
STEP: delete the pod 05/06/23 07:18:38.738
May  6 07:18:38.750: INFO: Waiting for pod downwardapi-volume-f97c36b5-d52b-43e8-95f8-f4bba166861f to disappear
May  6 07:18:38.752: INFO: Pod downwardapi-volume-f97c36b5-d52b-43e8-95f8-f4bba166861f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  6 07:18:38.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9186" for this suite. 05/06/23 07:18:38.754
------------------------------
â€¢ [SLOW TEST] [5.073 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:18:33.688
    May  6 07:18:33.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:18:33.689
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:34.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:34.705
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 05/06/23 07:18:34.707
    May  6 07:18:34.716: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f97c36b5-d52b-43e8-95f8-f4bba166861f" in namespace "projected-9186" to be "Succeeded or Failed"
    May  6 07:18:34.719: INFO: Pod "downwardapi-volume-f97c36b5-d52b-43e8-95f8-f4bba166861f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.294661ms
    May  6 07:18:36.723: INFO: Pod "downwardapi-volume-f97c36b5-d52b-43e8-95f8-f4bba166861f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006372483s
    May  6 07:18:38.724: INFO: Pod "downwardapi-volume-f97c36b5-d52b-43e8-95f8-f4bba166861f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008181288s
    STEP: Saw pod success 05/06/23 07:18:38.724
    May  6 07:18:38.725: INFO: Pod "downwardapi-volume-f97c36b5-d52b-43e8-95f8-f4bba166861f" satisfied condition "Succeeded or Failed"
    May  6 07:18:38.728: INFO: Trying to get logs from node cncf-3 pod downwardapi-volume-f97c36b5-d52b-43e8-95f8-f4bba166861f container client-container: <nil>
    STEP: delete the pod 05/06/23 07:18:38.738
    May  6 07:18:38.750: INFO: Waiting for pod downwardapi-volume-f97c36b5-d52b-43e8-95f8-f4bba166861f to disappear
    May  6 07:18:38.752: INFO: Pod downwardapi-volume-f97c36b5-d52b-43e8-95f8-f4bba166861f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  6 07:18:38.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9186" for this suite. 05/06/23 07:18:38.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:18:38.761
May  6 07:18:38.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename downward-api 05/06/23 07:18:38.762
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:39.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:39.789
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 05/06/23 07:18:39.791
May  6 07:18:39.799: INFO: Waiting up to 5m0s for pod "labelsupdatea2b4c263-f937-4627-ad32-4247b27237fe" in namespace "downward-api-1174" to be "running and ready"
May  6 07:18:39.801: INFO: Pod "labelsupdatea2b4c263-f937-4627-ad32-4247b27237fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079221ms
May  6 07:18:39.802: INFO: The phase of Pod labelsupdatea2b4c263-f937-4627-ad32-4247b27237fe is Pending, waiting for it to be Running (with Ready = true)
May  6 07:18:41.805: INFO: Pod "labelsupdatea2b4c263-f937-4627-ad32-4247b27237fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.0051105s
May  6 07:18:41.805: INFO: The phase of Pod labelsupdatea2b4c263-f937-4627-ad32-4247b27237fe is Running (Ready = true)
May  6 07:18:41.805: INFO: Pod "labelsupdatea2b4c263-f937-4627-ad32-4247b27237fe" satisfied condition "running and ready"
May  6 07:18:42.323: INFO: Successfully updated pod "labelsupdatea2b4c263-f937-4627-ad32-4247b27237fe"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  6 07:18:46.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1174" for this suite. 05/06/23 07:18:46.343
------------------------------
â€¢ [SLOW TEST] [7.589 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:18:38.761
    May  6 07:18:38.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename downward-api 05/06/23 07:18:38.762
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:39.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:39.789
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 05/06/23 07:18:39.791
    May  6 07:18:39.799: INFO: Waiting up to 5m0s for pod "labelsupdatea2b4c263-f937-4627-ad32-4247b27237fe" in namespace "downward-api-1174" to be "running and ready"
    May  6 07:18:39.801: INFO: Pod "labelsupdatea2b4c263-f937-4627-ad32-4247b27237fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079221ms
    May  6 07:18:39.802: INFO: The phase of Pod labelsupdatea2b4c263-f937-4627-ad32-4247b27237fe is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:18:41.805: INFO: Pod "labelsupdatea2b4c263-f937-4627-ad32-4247b27237fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.0051105s
    May  6 07:18:41.805: INFO: The phase of Pod labelsupdatea2b4c263-f937-4627-ad32-4247b27237fe is Running (Ready = true)
    May  6 07:18:41.805: INFO: Pod "labelsupdatea2b4c263-f937-4627-ad32-4247b27237fe" satisfied condition "running and ready"
    May  6 07:18:42.323: INFO: Successfully updated pod "labelsupdatea2b4c263-f937-4627-ad32-4247b27237fe"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  6 07:18:46.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1174" for this suite. 05/06/23 07:18:46.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:18:46.352
May  6 07:18:46.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:18:46.353
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:47.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:47.37
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-cd1489ff-ef30-4a04-af5d-a8a95b3d14f9 05/06/23 07:18:47.372
STEP: Creating secret with name secret-projected-all-test-volume-d3e542aa-78bc-4414-b1b5-9daf2c33e5a1 05/06/23 07:18:47.376
STEP: Creating a pod to test Check all projections for projected volume plugin 05/06/23 07:18:47.379
May  6 07:18:47.386: INFO: Waiting up to 5m0s for pod "projected-volume-ccae81c2-fa1b-46ef-989d-8dca48f283a6" in namespace "projected-1738" to be "Succeeded or Failed"
May  6 07:18:47.388: INFO: Pod "projected-volume-ccae81c2-fa1b-46ef-989d-8dca48f283a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.584933ms
May  6 07:18:49.392: INFO: Pod "projected-volume-ccae81c2-fa1b-46ef-989d-8dca48f283a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00592964s
May  6 07:18:51.392: INFO: Pod "projected-volume-ccae81c2-fa1b-46ef-989d-8dca48f283a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00620242s
STEP: Saw pod success 05/06/23 07:18:51.392
May  6 07:18:51.392: INFO: Pod "projected-volume-ccae81c2-fa1b-46ef-989d-8dca48f283a6" satisfied condition "Succeeded or Failed"
May  6 07:18:51.395: INFO: Trying to get logs from node cncf-2 pod projected-volume-ccae81c2-fa1b-46ef-989d-8dca48f283a6 container projected-all-volume-test: <nil>
STEP: delete the pod 05/06/23 07:18:51.407
May  6 07:18:51.421: INFO: Waiting for pod projected-volume-ccae81c2-fa1b-46ef-989d-8dca48f283a6 to disappear
May  6 07:18:51.423: INFO: Pod projected-volume-ccae81c2-fa1b-46ef-989d-8dca48f283a6 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
May  6 07:18:51.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1738" for this suite. 05/06/23 07:18:51.426
------------------------------
â€¢ [SLOW TEST] [5.079 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:18:46.352
    May  6 07:18:46.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:18:46.353
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:47.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:47.37
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-cd1489ff-ef30-4a04-af5d-a8a95b3d14f9 05/06/23 07:18:47.372
    STEP: Creating secret with name secret-projected-all-test-volume-d3e542aa-78bc-4414-b1b5-9daf2c33e5a1 05/06/23 07:18:47.376
    STEP: Creating a pod to test Check all projections for projected volume plugin 05/06/23 07:18:47.379
    May  6 07:18:47.386: INFO: Waiting up to 5m0s for pod "projected-volume-ccae81c2-fa1b-46ef-989d-8dca48f283a6" in namespace "projected-1738" to be "Succeeded or Failed"
    May  6 07:18:47.388: INFO: Pod "projected-volume-ccae81c2-fa1b-46ef-989d-8dca48f283a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.584933ms
    May  6 07:18:49.392: INFO: Pod "projected-volume-ccae81c2-fa1b-46ef-989d-8dca48f283a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00592964s
    May  6 07:18:51.392: INFO: Pod "projected-volume-ccae81c2-fa1b-46ef-989d-8dca48f283a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00620242s
    STEP: Saw pod success 05/06/23 07:18:51.392
    May  6 07:18:51.392: INFO: Pod "projected-volume-ccae81c2-fa1b-46ef-989d-8dca48f283a6" satisfied condition "Succeeded or Failed"
    May  6 07:18:51.395: INFO: Trying to get logs from node cncf-2 pod projected-volume-ccae81c2-fa1b-46ef-989d-8dca48f283a6 container projected-all-volume-test: <nil>
    STEP: delete the pod 05/06/23 07:18:51.407
    May  6 07:18:51.421: INFO: Waiting for pod projected-volume-ccae81c2-fa1b-46ef-989d-8dca48f283a6 to disappear
    May  6 07:18:51.423: INFO: Pod projected-volume-ccae81c2-fa1b-46ef-989d-8dca48f283a6 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    May  6 07:18:51.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1738" for this suite. 05/06/23 07:18:51.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:18:51.433
May  6 07:18:51.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename downward-api 05/06/23 07:18:51.433
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:52.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:52.451
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 05/06/23 07:18:52.453
May  6 07:18:52.459: INFO: Waiting up to 5m0s for pod "downward-api-65b53ef6-79ca-4adf-a8fd-6b722fa5169e" in namespace "downward-api-2696" to be "Succeeded or Failed"
May  6 07:18:52.462: INFO: Pod "downward-api-65b53ef6-79ca-4adf-a8fd-6b722fa5169e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.632854ms
May  6 07:18:54.466: INFO: Pod "downward-api-65b53ef6-79ca-4adf-a8fd-6b722fa5169e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006407068s
May  6 07:18:56.465: INFO: Pod "downward-api-65b53ef6-79ca-4adf-a8fd-6b722fa5169e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005884059s
STEP: Saw pod success 05/06/23 07:18:56.465
May  6 07:18:56.465: INFO: Pod "downward-api-65b53ef6-79ca-4adf-a8fd-6b722fa5169e" satisfied condition "Succeeded or Failed"
May  6 07:18:56.468: INFO: Trying to get logs from node cncf-3 pod downward-api-65b53ef6-79ca-4adf-a8fd-6b722fa5169e container dapi-container: <nil>
STEP: delete the pod 05/06/23 07:18:56.473
May  6 07:18:56.492: INFO: Waiting for pod downward-api-65b53ef6-79ca-4adf-a8fd-6b722fa5169e to disappear
May  6 07:18:56.496: INFO: Pod downward-api-65b53ef6-79ca-4adf-a8fd-6b722fa5169e no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May  6 07:18:56.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2696" for this suite. 05/06/23 07:18:56.499
------------------------------
â€¢ [SLOW TEST] [5.072 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:18:51.433
    May  6 07:18:51.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename downward-api 05/06/23 07:18:51.433
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:52.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:52.451
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 05/06/23 07:18:52.453
    May  6 07:18:52.459: INFO: Waiting up to 5m0s for pod "downward-api-65b53ef6-79ca-4adf-a8fd-6b722fa5169e" in namespace "downward-api-2696" to be "Succeeded or Failed"
    May  6 07:18:52.462: INFO: Pod "downward-api-65b53ef6-79ca-4adf-a8fd-6b722fa5169e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.632854ms
    May  6 07:18:54.466: INFO: Pod "downward-api-65b53ef6-79ca-4adf-a8fd-6b722fa5169e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006407068s
    May  6 07:18:56.465: INFO: Pod "downward-api-65b53ef6-79ca-4adf-a8fd-6b722fa5169e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005884059s
    STEP: Saw pod success 05/06/23 07:18:56.465
    May  6 07:18:56.465: INFO: Pod "downward-api-65b53ef6-79ca-4adf-a8fd-6b722fa5169e" satisfied condition "Succeeded or Failed"
    May  6 07:18:56.468: INFO: Trying to get logs from node cncf-3 pod downward-api-65b53ef6-79ca-4adf-a8fd-6b722fa5169e container dapi-container: <nil>
    STEP: delete the pod 05/06/23 07:18:56.473
    May  6 07:18:56.492: INFO: Waiting for pod downward-api-65b53ef6-79ca-4adf-a8fd-6b722fa5169e to disappear
    May  6 07:18:56.496: INFO: Pod downward-api-65b53ef6-79ca-4adf-a8fd-6b722fa5169e no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May  6 07:18:56.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2696" for this suite. 05/06/23 07:18:56.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:18:56.507
May  6 07:18:56.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:18:56.508
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:57.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:57.525
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-e1351dcb-f45c-4452-88b7-d9ea580d0113 05/06/23 07:18:57.527
STEP: Creating a pod to test consume secrets 05/06/23 07:18:57.531
May  6 07:18:57.539: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5ca7e6aa-5695-409e-810f-c7226318ad68" in namespace "projected-435" to be "Succeeded or Failed"
May  6 07:18:57.543: INFO: Pod "pod-projected-secrets-5ca7e6aa-5695-409e-810f-c7226318ad68": Phase="Pending", Reason="", readiness=false. Elapsed: 3.671337ms
May  6 07:18:59.546: INFO: Pod "pod-projected-secrets-5ca7e6aa-5695-409e-810f-c7226318ad68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007558428s
May  6 07:19:01.546: INFO: Pod "pod-projected-secrets-5ca7e6aa-5695-409e-810f-c7226318ad68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007047809s
STEP: Saw pod success 05/06/23 07:19:01.546
May  6 07:19:01.546: INFO: Pod "pod-projected-secrets-5ca7e6aa-5695-409e-810f-c7226318ad68" satisfied condition "Succeeded or Failed"
May  6 07:19:01.549: INFO: Trying to get logs from node cncf-0 pod pod-projected-secrets-5ca7e6aa-5695-409e-810f-c7226318ad68 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/06/23 07:19:01.554
May  6 07:19:01.565: INFO: Waiting for pod pod-projected-secrets-5ca7e6aa-5695-409e-810f-c7226318ad68 to disappear
May  6 07:19:01.567: INFO: Pod pod-projected-secrets-5ca7e6aa-5695-409e-810f-c7226318ad68 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May  6 07:19:01.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-435" for this suite. 05/06/23 07:19:01.57
------------------------------
â€¢ [SLOW TEST] [5.068 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:18:56.507
    May  6 07:18:56.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:18:56.508
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:18:57.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:18:57.525
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-e1351dcb-f45c-4452-88b7-d9ea580d0113 05/06/23 07:18:57.527
    STEP: Creating a pod to test consume secrets 05/06/23 07:18:57.531
    May  6 07:18:57.539: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5ca7e6aa-5695-409e-810f-c7226318ad68" in namespace "projected-435" to be "Succeeded or Failed"
    May  6 07:18:57.543: INFO: Pod "pod-projected-secrets-5ca7e6aa-5695-409e-810f-c7226318ad68": Phase="Pending", Reason="", readiness=false. Elapsed: 3.671337ms
    May  6 07:18:59.546: INFO: Pod "pod-projected-secrets-5ca7e6aa-5695-409e-810f-c7226318ad68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007558428s
    May  6 07:19:01.546: INFO: Pod "pod-projected-secrets-5ca7e6aa-5695-409e-810f-c7226318ad68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007047809s
    STEP: Saw pod success 05/06/23 07:19:01.546
    May  6 07:19:01.546: INFO: Pod "pod-projected-secrets-5ca7e6aa-5695-409e-810f-c7226318ad68" satisfied condition "Succeeded or Failed"
    May  6 07:19:01.549: INFO: Trying to get logs from node cncf-0 pod pod-projected-secrets-5ca7e6aa-5695-409e-810f-c7226318ad68 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/06/23 07:19:01.554
    May  6 07:19:01.565: INFO: Waiting for pod pod-projected-secrets-5ca7e6aa-5695-409e-810f-c7226318ad68 to disappear
    May  6 07:19:01.567: INFO: Pod pod-projected-secrets-5ca7e6aa-5695-409e-810f-c7226318ad68 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May  6 07:19:01.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-435" for this suite. 05/06/23 07:19:01.57
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:19:01.576
May  6 07:19:01.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename sched-pred 05/06/23 07:19:01.576
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:19:02.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:19:02.594
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
May  6 07:19:02.596: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  6 07:19:02.602: INFO: Waiting for terminating namespaces to be deleted...
May  6 07:19:02.604: INFO: 
Logging pods the apiserver thinks is on node cncf-0 before test
May  6 07:19:02.610: INFO: calico-node-vvl9m from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.610: INFO: 	Container calico-node ready: true, restart count 0
May  6 07:19:02.610: INFO: kube-apiserver-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.610: INFO: 	Container kube-apiserver ready: true, restart count 1
May  6 07:19:02.610: INFO: kube-controller-manager-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.610: INFO: 	Container kube-controller-manager ready: true, restart count 0
May  6 07:19:02.610: INFO: kube-proxy-cq72q from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.610: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 07:19:02.610: INFO: kube-scheduler-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.610: INFO: 	Container kube-scheduler ready: true, restart count 1
May  6 07:19:02.610: INFO: kubelet-rubber-stamp-c6b74568-5mtx7 from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.610: INFO: 	Container kubelet-rubber-stamp ready: true, restart count 1
May  6 07:19:02.610: INFO: tigera-operator-585fc94df6-xnxld from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.610: INFO: 	Container tigera-operator ready: true, restart count 0
May  6 07:19:02.610: INFO: externalsvc-hjgnp from services-2585 started at 2023-05-06 06:58:05 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.610: INFO: 	Container externalsvc ready: true, restart count 0
May  6 07:19:02.610: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-np4v2 from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 07:19:02.610: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 07:19:02.610: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 07:19:02.610: INFO: 
Logging pods the apiserver thinks is on node cncf-1 before test
May  6 07:19:02.615: INFO: calico-node-kz2jp from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.615: INFO: 	Container calico-node ready: true, restart count 0
May  6 07:19:02.615: INFO: calico-typha-7d85b4d7df-79mt7 from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.615: INFO: 	Container calico-typha ready: true, restart count 0
May  6 07:19:02.615: INFO: dns-test-6ffeff72-54ba-4fa1-9bbb-359bba68b89e from dns-3718 started at 2023-05-06 06:44:04 +0000 UTC (3 container statuses recorded)
May  6 07:19:02.615: INFO: 	Container jessie-querier ready: true, restart count 3
May  6 07:19:02.615: INFO: 	Container querier ready: true, restart count 2
May  6 07:19:02.615: INFO: 	Container webserver ready: true, restart count 0
May  6 07:19:02.615: INFO: coredns-5b98b988fd-8gbpp from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.615: INFO: 	Container coredns ready: true, restart count 0
May  6 07:19:02.615: INFO: kube-apiserver-cncf-1 from kube-system started at 2023-05-05 20:24:38 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.615: INFO: 	Container kube-apiserver ready: true, restart count 1
May  6 07:19:02.615: INFO: kube-controller-manager-cncf-1 from kube-system started at 2023-05-05 20:24:14 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.615: INFO: 	Container kube-controller-manager ready: true, restart count 0
May  6 07:19:02.615: INFO: kube-proxy-4wtf5 from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.615: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 07:19:02.615: INFO: kube-scheduler-cncf-1 from kube-system started at 2023-05-05 20:23:17 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.615: INFO: 	Container kube-scheduler ready: true, restart count 1
May  6 07:19:02.615: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-5lxlb from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 07:19:02.615: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 07:19:02.615: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 07:19:02.615: INFO: 
Logging pods the apiserver thinks is on node cncf-2 before test
May  6 07:19:02.621: INFO: calico-kube-controllers-574d8db6c-nnb5w from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.621: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  6 07:19:02.621: INFO: calico-node-q42q6 from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.621: INFO: 	Container calico-node ready: true, restart count 0
May  6 07:19:02.621: INFO: calico-typha-7d85b4d7df-6s76x from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.621: INFO: 	Container calico-typha ready: true, restart count 0
May  6 07:19:02.621: INFO: coredns-5b98b988fd-7r5vm from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.621: INFO: 	Container coredns ready: true, restart count 0
May  6 07:19:02.621: INFO: kube-apiserver-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.621: INFO: 	Container kube-apiserver ready: true, restart count 1
May  6 07:19:02.621: INFO: kube-controller-manager-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.621: INFO: 	Container kube-controller-manager ready: true, restart count 0
May  6 07:19:02.621: INFO: kube-proxy-jl76p from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.621: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 07:19:02.621: INFO: kube-scheduler-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.621: INFO: 	Container kube-scheduler ready: true, restart count 1
May  6 07:19:02.621: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-mjqth from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 07:19:02.621: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 07:19:02.621: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 07:19:02.621: INFO: 
Logging pods the apiserver thinks is on node cncf-3 before test
May  6 07:19:02.625: INFO: calico-node-sv4fh from calico-system started at 2023-05-05 20:28:36 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.625: INFO: 	Container calico-node ready: true, restart count 0
May  6 07:19:02.625: INFO: kube-proxy-n8v5r from kube-system started at 2023-05-05 20:28:36 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.625: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 07:19:02.625: INFO: controller-manager-v3-6b6b89c4f6-skqkm from rafay-system started at 2023-05-05 20:32:05 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.625: INFO: 	Container manager ready: true, restart count 0
May  6 07:19:02.625: INFO: edge-client-59448698df-gxb8q from rafay-system started at 2023-05-05 20:31:45 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.625: INFO: 	Container edge-client ready: true, restart count 0
May  6 07:19:02.625: INFO: rafay-connector-v3-79f986b9c6-rxjdn from rafay-system started at 2023-05-05 20:32:04 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.625: INFO: 	Container connector ready: true, restart count 0
May  6 07:19:02.625: INFO: v2-relay-agent-6b76bc6c6f-m6hgg from rafay-system started at 2023-05-05 20:32:06 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.625: INFO: 	Container relay-agent ready: true, restart count 0
May  6 07:19:02.625: INFO: externalsvc-45jwj from services-2585 started at 2023-05-06 06:58:05 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.625: INFO: 	Container externalsvc ready: true, restart count 0
May  6 07:19:02.625: INFO: sonobuoy from sonobuoy started at 2023-05-06 06:59:12 +0000 UTC (1 container statuses recorded)
May  6 07:19:02.625: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  6 07:19:02.625: INFO: sonobuoy-e2e-job-47910262329e4558 from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 07:19:02.625: INFO: 	Container e2e ready: true, restart count 0
May  6 07:19:02.625: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 07:19:02.625: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-ts2xw from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 07:19:02.625: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 07:19:02.625: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/06/23 07:19:02.626
May  6 07:19:02.632: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2396" to be "running"
May  6 07:19:02.634: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.849795ms
May  6 07:19:04.637: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.00516942s
May  6 07:19:04.637: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/06/23 07:19:04.639
STEP: Trying to apply a random label on the found node. 05/06/23 07:19:04.653
STEP: verifying the node has the label kubernetes.io/e2e-b5d031f6-117a-4b5c-8df6-f20ff3c29831 95 05/06/23 07:19:04.661
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 05/06/23 07:19:04.664
May  6 07:19:04.670: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2396" to be "not pending"
May  6 07:19:04.673: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.517675ms
May  6 07:19:06.676: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005203185s
May  6 07:19:06.676: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.0.134 on the node which pod4 resides and expect not scheduled 05/06/23 07:19:06.676
May  6 07:19:06.682: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2396" to be "not pending"
May  6 07:19:06.684: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.849626ms
May  6 07:19:08.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004453909s
May  6 07:19:10.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005105063s
May  6 07:19:12.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004847027s
May  6 07:19:14.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005455529s
May  6 07:19:16.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005644433s
May  6 07:19:18.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006734629s
May  6 07:19:20.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004727023s
May  6 07:19:22.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006145577s
May  6 07:19:24.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005286661s
May  6 07:19:26.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.006108703s
May  6 07:19:28.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005129994s
May  6 07:19:30.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.005778282s
May  6 07:19:32.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.006742293s
May  6 07:19:34.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006450398s
May  6 07:19:36.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006386144s
May  6 07:19:38.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00560596s
May  6 07:19:40.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006268139s
May  6 07:19:42.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.004814755s
May  6 07:19:44.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.005297422s
May  6 07:19:46.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.006424632s
May  6 07:19:48.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.005983243s
May  6 07:19:50.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006147967s
May  6 07:19:52.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.005264361s
May  6 07:19:54.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.00579877s
May  6 07:19:56.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005486768s
May  6 07:19:58.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005961461s
May  6 07:20:00.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.004620186s
May  6 07:20:02.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006119597s
May  6 07:20:04.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.004846759s
May  6 07:20:06.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005264336s
May  6 07:20:08.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005558316s
May  6 07:20:10.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.005313518s
May  6 07:20:12.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006243288s
May  6 07:20:14.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005085381s
May  6 07:20:16.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006911806s
May  6 07:20:18.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.006677955s
May  6 07:20:20.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.006366644s
May  6 07:20:22.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.005602478s
May  6 07:20:24.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.00597899s
May  6 07:20:26.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005609479s
May  6 07:20:28.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006722491s
May  6 07:20:30.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005697712s
May  6 07:20:32.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005870333s
May  6 07:20:34.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.005632145s
May  6 07:20:36.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.00717769s
May  6 07:20:38.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006182575s
May  6 07:20:40.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005317729s
May  6 07:20:42.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005313229s
May  6 07:20:44.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00600852s
May  6 07:20:46.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005540271s
May  6 07:20:48.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.006835533s
May  6 07:20:50.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.005602029s
May  6 07:20:52.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.006124606s
May  6 07:20:54.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.004973554s
May  6 07:20:56.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.005346038s
May  6 07:20:58.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004710337s
May  6 07:21:00.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006268074s
May  6 07:21:02.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006638758s
May  6 07:21:04.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006086497s
May  6 07:21:06.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005724868s
May  6 07:21:08.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.006795723s
May  6 07:21:10.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.004617018s
May  6 07:21:12.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.006585071s
May  6 07:21:14.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.005437412s
May  6 07:21:16.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.006394954s
May  6 07:21:18.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.005892552s
May  6 07:21:20.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.004992704s
May  6 07:21:22.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.004772069s
May  6 07:21:24.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.005204043s
May  6 07:21:26.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.005020035s
May  6 07:21:28.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.005404859s
May  6 07:21:30.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.004813538s
May  6 07:21:32.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.007107963s
May  6 07:21:34.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.005096307s
May  6 07:21:36.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.006569563s
May  6 07:21:38.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.005240114s
May  6 07:21:40.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.004837795s
May  6 07:21:42.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.005934463s
May  6 07:21:44.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.004784704s
May  6 07:21:46.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.007490121s
May  6 07:21:48.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.005874309s
May  6 07:21:50.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.006445031s
May  6 07:21:52.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.005609633s
May  6 07:21:54.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005842782s
May  6 07:21:56.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.006339796s
May  6 07:21:58.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.006254158s
May  6 07:22:00.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.00517771s
May  6 07:22:02.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.006276176s
May  6 07:22:04.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.005062617s
May  6 07:22:06.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.006413842s
May  6 07:22:08.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.005606754s
May  6 07:22:10.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.006027412s
May  6 07:22:12.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.00586303s
May  6 07:22:14.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.004949041s
May  6 07:22:16.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.005664388s
May  6 07:22:18.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.006899947s
May  6 07:22:20.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.005355448s
May  6 07:22:22.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.006999975s
May  6 07:22:24.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.004801407s
May  6 07:22:26.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.007096702s
May  6 07:22:28.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.004586388s
May  6 07:22:30.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.005933958s
May  6 07:22:32.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.006965369s
May  6 07:22:34.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.005405925s
May  6 07:22:36.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.00457084s
May  6 07:22:38.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.005562264s
May  6 07:22:40.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.004844193s
May  6 07:22:42.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.006264006s
May  6 07:22:44.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.006994133s
May  6 07:22:46.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.007280752s
May  6 07:22:48.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.006083441s
May  6 07:22:50.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.006544852s
May  6 07:22:52.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.005795665s
May  6 07:22:54.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.005641158s
May  6 07:22:56.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.006415445s
May  6 07:22:58.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.00528797s
May  6 07:23:00.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.005562658s
May  6 07:23:02.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.006826195s
May  6 07:23:04.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.005655007s
May  6 07:23:06.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.00685632s
May  6 07:23:08.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.005570523s
May  6 07:23:10.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.005862957s
May  6 07:23:12.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.005538177s
May  6 07:23:14.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.006221305s
May  6 07:23:16.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.004542161s
May  6 07:23:18.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.004868928s
May  6 07:23:20.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.005224906s
May  6 07:23:22.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.006498636s
May  6 07:23:24.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.005341945s
May  6 07:23:26.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.005269887s
May  6 07:23:28.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.005158435s
May  6 07:23:30.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.005463385s
May  6 07:23:32.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.005810887s
May  6 07:23:34.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.005476518s
May  6 07:23:36.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.005888181s
May  6 07:23:38.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.006391073s
May  6 07:23:40.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.005371343s
May  6 07:23:42.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.004792695s
May  6 07:23:44.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.00509055s
May  6 07:23:46.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.005478396s
May  6 07:23:48.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.005583454s
May  6 07:23:50.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.006992049s
May  6 07:23:52.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.006189845s
May  6 07:23:54.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.004969939s
May  6 07:23:56.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.006191594s
May  6 07:23:58.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.004357951s
May  6 07:24:00.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.005975395s
May  6 07:24:02.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.006120293s
May  6 07:24:04.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.005453859s
May  6 07:24:06.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.00517569s
May  6 07:24:06.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.00763805s
STEP: removing the label kubernetes.io/e2e-b5d031f6-117a-4b5c-8df6-f20ff3c29831 off the node cncf-0 05/06/23 07:24:06.69
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b5d031f6-117a-4b5c-8df6-f20ff3c29831 05/06/23 07:24:06.702
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 07:24:06.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2396" for this suite. 05/06/23 07:24:06.708
------------------------------
â€¢ [SLOW TEST] [305.139 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:19:01.576
    May  6 07:19:01.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename sched-pred 05/06/23 07:19:01.576
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:19:02.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:19:02.594
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    May  6 07:19:02.596: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  6 07:19:02.602: INFO: Waiting for terminating namespaces to be deleted...
    May  6 07:19:02.604: INFO: 
    Logging pods the apiserver thinks is on node cncf-0 before test
    May  6 07:19:02.610: INFO: calico-node-vvl9m from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.610: INFO: 	Container calico-node ready: true, restart count 0
    May  6 07:19:02.610: INFO: kube-apiserver-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.610: INFO: 	Container kube-apiserver ready: true, restart count 1
    May  6 07:19:02.610: INFO: kube-controller-manager-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.610: INFO: 	Container kube-controller-manager ready: true, restart count 0
    May  6 07:19:02.610: INFO: kube-proxy-cq72q from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.610: INFO: 	Container kube-proxy ready: true, restart count 0
    May  6 07:19:02.610: INFO: kube-scheduler-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.610: INFO: 	Container kube-scheduler ready: true, restart count 1
    May  6 07:19:02.610: INFO: kubelet-rubber-stamp-c6b74568-5mtx7 from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.610: INFO: 	Container kubelet-rubber-stamp ready: true, restart count 1
    May  6 07:19:02.610: INFO: tigera-operator-585fc94df6-xnxld from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.610: INFO: 	Container tigera-operator ready: true, restart count 0
    May  6 07:19:02.610: INFO: externalsvc-hjgnp from services-2585 started at 2023-05-06 06:58:05 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.610: INFO: 	Container externalsvc ready: true, restart count 0
    May  6 07:19:02.610: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-np4v2 from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 07:19:02.610: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 07:19:02.610: INFO: 	Container systemd-logs ready: true, restart count 0
    May  6 07:19:02.610: INFO: 
    Logging pods the apiserver thinks is on node cncf-1 before test
    May  6 07:19:02.615: INFO: calico-node-kz2jp from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.615: INFO: 	Container calico-node ready: true, restart count 0
    May  6 07:19:02.615: INFO: calico-typha-7d85b4d7df-79mt7 from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.615: INFO: 	Container calico-typha ready: true, restart count 0
    May  6 07:19:02.615: INFO: dns-test-6ffeff72-54ba-4fa1-9bbb-359bba68b89e from dns-3718 started at 2023-05-06 06:44:04 +0000 UTC (3 container statuses recorded)
    May  6 07:19:02.615: INFO: 	Container jessie-querier ready: true, restart count 3
    May  6 07:19:02.615: INFO: 	Container querier ready: true, restart count 2
    May  6 07:19:02.615: INFO: 	Container webserver ready: true, restart count 0
    May  6 07:19:02.615: INFO: coredns-5b98b988fd-8gbpp from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.615: INFO: 	Container coredns ready: true, restart count 0
    May  6 07:19:02.615: INFO: kube-apiserver-cncf-1 from kube-system started at 2023-05-05 20:24:38 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.615: INFO: 	Container kube-apiserver ready: true, restart count 1
    May  6 07:19:02.615: INFO: kube-controller-manager-cncf-1 from kube-system started at 2023-05-05 20:24:14 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.615: INFO: 	Container kube-controller-manager ready: true, restart count 0
    May  6 07:19:02.615: INFO: kube-proxy-4wtf5 from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.615: INFO: 	Container kube-proxy ready: true, restart count 0
    May  6 07:19:02.615: INFO: kube-scheduler-cncf-1 from kube-system started at 2023-05-05 20:23:17 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.615: INFO: 	Container kube-scheduler ready: true, restart count 1
    May  6 07:19:02.615: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-5lxlb from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 07:19:02.615: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 07:19:02.615: INFO: 	Container systemd-logs ready: true, restart count 0
    May  6 07:19:02.615: INFO: 
    Logging pods the apiserver thinks is on node cncf-2 before test
    May  6 07:19:02.621: INFO: calico-kube-controllers-574d8db6c-nnb5w from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.621: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  6 07:19:02.621: INFO: calico-node-q42q6 from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.621: INFO: 	Container calico-node ready: true, restart count 0
    May  6 07:19:02.621: INFO: calico-typha-7d85b4d7df-6s76x from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.621: INFO: 	Container calico-typha ready: true, restart count 0
    May  6 07:19:02.621: INFO: coredns-5b98b988fd-7r5vm from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.621: INFO: 	Container coredns ready: true, restart count 0
    May  6 07:19:02.621: INFO: kube-apiserver-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.621: INFO: 	Container kube-apiserver ready: true, restart count 1
    May  6 07:19:02.621: INFO: kube-controller-manager-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.621: INFO: 	Container kube-controller-manager ready: true, restart count 0
    May  6 07:19:02.621: INFO: kube-proxy-jl76p from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.621: INFO: 	Container kube-proxy ready: true, restart count 0
    May  6 07:19:02.621: INFO: kube-scheduler-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.621: INFO: 	Container kube-scheduler ready: true, restart count 1
    May  6 07:19:02.621: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-mjqth from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 07:19:02.621: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 07:19:02.621: INFO: 	Container systemd-logs ready: true, restart count 0
    May  6 07:19:02.621: INFO: 
    Logging pods the apiserver thinks is on node cncf-3 before test
    May  6 07:19:02.625: INFO: calico-node-sv4fh from calico-system started at 2023-05-05 20:28:36 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.625: INFO: 	Container calico-node ready: true, restart count 0
    May  6 07:19:02.625: INFO: kube-proxy-n8v5r from kube-system started at 2023-05-05 20:28:36 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.625: INFO: 	Container kube-proxy ready: true, restart count 0
    May  6 07:19:02.625: INFO: controller-manager-v3-6b6b89c4f6-skqkm from rafay-system started at 2023-05-05 20:32:05 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.625: INFO: 	Container manager ready: true, restart count 0
    May  6 07:19:02.625: INFO: edge-client-59448698df-gxb8q from rafay-system started at 2023-05-05 20:31:45 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.625: INFO: 	Container edge-client ready: true, restart count 0
    May  6 07:19:02.625: INFO: rafay-connector-v3-79f986b9c6-rxjdn from rafay-system started at 2023-05-05 20:32:04 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.625: INFO: 	Container connector ready: true, restart count 0
    May  6 07:19:02.625: INFO: v2-relay-agent-6b76bc6c6f-m6hgg from rafay-system started at 2023-05-05 20:32:06 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.625: INFO: 	Container relay-agent ready: true, restart count 0
    May  6 07:19:02.625: INFO: externalsvc-45jwj from services-2585 started at 2023-05-06 06:58:05 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.625: INFO: 	Container externalsvc ready: true, restart count 0
    May  6 07:19:02.625: INFO: sonobuoy from sonobuoy started at 2023-05-06 06:59:12 +0000 UTC (1 container statuses recorded)
    May  6 07:19:02.625: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  6 07:19:02.625: INFO: sonobuoy-e2e-job-47910262329e4558 from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 07:19:02.625: INFO: 	Container e2e ready: true, restart count 0
    May  6 07:19:02.625: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 07:19:02.625: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-ts2xw from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 07:19:02.625: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 07:19:02.625: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/06/23 07:19:02.626
    May  6 07:19:02.632: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2396" to be "running"
    May  6 07:19:02.634: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.849795ms
    May  6 07:19:04.637: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.00516942s
    May  6 07:19:04.637: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/06/23 07:19:04.639
    STEP: Trying to apply a random label on the found node. 05/06/23 07:19:04.653
    STEP: verifying the node has the label kubernetes.io/e2e-b5d031f6-117a-4b5c-8df6-f20ff3c29831 95 05/06/23 07:19:04.661
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 05/06/23 07:19:04.664
    May  6 07:19:04.670: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2396" to be "not pending"
    May  6 07:19:04.673: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.517675ms
    May  6 07:19:06.676: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005203185s
    May  6 07:19:06.676: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.0.134 on the node which pod4 resides and expect not scheduled 05/06/23 07:19:06.676
    May  6 07:19:06.682: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2396" to be "not pending"
    May  6 07:19:06.684: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.849626ms
    May  6 07:19:08.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004453909s
    May  6 07:19:10.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005105063s
    May  6 07:19:12.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004847027s
    May  6 07:19:14.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005455529s
    May  6 07:19:16.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005644433s
    May  6 07:19:18.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006734629s
    May  6 07:19:20.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004727023s
    May  6 07:19:22.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006145577s
    May  6 07:19:24.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005286661s
    May  6 07:19:26.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.006108703s
    May  6 07:19:28.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005129994s
    May  6 07:19:30.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.005778282s
    May  6 07:19:32.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.006742293s
    May  6 07:19:34.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006450398s
    May  6 07:19:36.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006386144s
    May  6 07:19:38.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00560596s
    May  6 07:19:40.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006268139s
    May  6 07:19:42.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.004814755s
    May  6 07:19:44.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.005297422s
    May  6 07:19:46.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.006424632s
    May  6 07:19:48.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.005983243s
    May  6 07:19:50.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006147967s
    May  6 07:19:52.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.005264361s
    May  6 07:19:54.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.00579877s
    May  6 07:19:56.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005486768s
    May  6 07:19:58.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005961461s
    May  6 07:20:00.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.004620186s
    May  6 07:20:02.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006119597s
    May  6 07:20:04.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.004846759s
    May  6 07:20:06.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005264336s
    May  6 07:20:08.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005558316s
    May  6 07:20:10.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.005313518s
    May  6 07:20:12.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006243288s
    May  6 07:20:14.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005085381s
    May  6 07:20:16.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006911806s
    May  6 07:20:18.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.006677955s
    May  6 07:20:20.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.006366644s
    May  6 07:20:22.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.005602478s
    May  6 07:20:24.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.00597899s
    May  6 07:20:26.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005609479s
    May  6 07:20:28.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006722491s
    May  6 07:20:30.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005697712s
    May  6 07:20:32.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005870333s
    May  6 07:20:34.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.005632145s
    May  6 07:20:36.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.00717769s
    May  6 07:20:38.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006182575s
    May  6 07:20:40.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005317729s
    May  6 07:20:42.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005313229s
    May  6 07:20:44.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00600852s
    May  6 07:20:46.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005540271s
    May  6 07:20:48.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.006835533s
    May  6 07:20:50.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.005602029s
    May  6 07:20:52.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.006124606s
    May  6 07:20:54.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.004973554s
    May  6 07:20:56.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.005346038s
    May  6 07:20:58.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004710337s
    May  6 07:21:00.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006268074s
    May  6 07:21:02.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006638758s
    May  6 07:21:04.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006086497s
    May  6 07:21:06.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005724868s
    May  6 07:21:08.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.006795723s
    May  6 07:21:10.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.004617018s
    May  6 07:21:12.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.006585071s
    May  6 07:21:14.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.005437412s
    May  6 07:21:16.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.006394954s
    May  6 07:21:18.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.005892552s
    May  6 07:21:20.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.004992704s
    May  6 07:21:22.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.004772069s
    May  6 07:21:24.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.005204043s
    May  6 07:21:26.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.005020035s
    May  6 07:21:28.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.005404859s
    May  6 07:21:30.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.004813538s
    May  6 07:21:32.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.007107963s
    May  6 07:21:34.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.005096307s
    May  6 07:21:36.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.006569563s
    May  6 07:21:38.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.005240114s
    May  6 07:21:40.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.004837795s
    May  6 07:21:42.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.005934463s
    May  6 07:21:44.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.004784704s
    May  6 07:21:46.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.007490121s
    May  6 07:21:48.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.005874309s
    May  6 07:21:50.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.006445031s
    May  6 07:21:52.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.005609633s
    May  6 07:21:54.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005842782s
    May  6 07:21:56.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.006339796s
    May  6 07:21:58.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.006254158s
    May  6 07:22:00.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.00517771s
    May  6 07:22:02.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.006276176s
    May  6 07:22:04.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.005062617s
    May  6 07:22:06.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.006413842s
    May  6 07:22:08.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.005606754s
    May  6 07:22:10.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.006027412s
    May  6 07:22:12.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.00586303s
    May  6 07:22:14.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.004949041s
    May  6 07:22:16.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.005664388s
    May  6 07:22:18.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.006899947s
    May  6 07:22:20.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.005355448s
    May  6 07:22:22.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.006999975s
    May  6 07:22:24.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.004801407s
    May  6 07:22:26.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.007096702s
    May  6 07:22:28.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.004586388s
    May  6 07:22:30.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.005933958s
    May  6 07:22:32.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.006965369s
    May  6 07:22:34.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.005405925s
    May  6 07:22:36.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.00457084s
    May  6 07:22:38.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.005562264s
    May  6 07:22:40.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.004844193s
    May  6 07:22:42.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.006264006s
    May  6 07:22:44.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.006994133s
    May  6 07:22:46.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.007280752s
    May  6 07:22:48.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.006083441s
    May  6 07:22:50.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.006544852s
    May  6 07:22:52.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.005795665s
    May  6 07:22:54.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.005641158s
    May  6 07:22:56.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.006415445s
    May  6 07:22:58.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.00528797s
    May  6 07:23:00.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.005562658s
    May  6 07:23:02.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.006826195s
    May  6 07:23:04.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.005655007s
    May  6 07:23:06.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.00685632s
    May  6 07:23:08.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.005570523s
    May  6 07:23:10.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.005862957s
    May  6 07:23:12.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.005538177s
    May  6 07:23:14.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.006221305s
    May  6 07:23:16.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.004542161s
    May  6 07:23:18.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.004868928s
    May  6 07:23:20.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.005224906s
    May  6 07:23:22.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.006498636s
    May  6 07:23:24.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.005341945s
    May  6 07:23:26.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.005269887s
    May  6 07:23:28.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.005158435s
    May  6 07:23:30.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.005463385s
    May  6 07:23:32.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.005810887s
    May  6 07:23:34.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.005476518s
    May  6 07:23:36.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.005888181s
    May  6 07:23:38.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.006391073s
    May  6 07:23:40.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.005371343s
    May  6 07:23:42.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.004792695s
    May  6 07:23:44.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.00509055s
    May  6 07:23:46.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.005478396s
    May  6 07:23:48.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.005583454s
    May  6 07:23:50.689: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.006992049s
    May  6 07:23:52.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.006189845s
    May  6 07:23:54.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.004969939s
    May  6 07:23:56.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.006191594s
    May  6 07:23:58.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.004357951s
    May  6 07:24:00.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.005975395s
    May  6 07:24:02.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.006120293s
    May  6 07:24:04.688: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.005453859s
    May  6 07:24:06.687: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.00517569s
    May  6 07:24:06.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.00763805s
    STEP: removing the label kubernetes.io/e2e-b5d031f6-117a-4b5c-8df6-f20ff3c29831 off the node cncf-0 05/06/23 07:24:06.69
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-b5d031f6-117a-4b5c-8df6-f20ff3c29831 05/06/23 07:24:06.702
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 07:24:06.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2396" for this suite. 05/06/23 07:24:06.708
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:24:06.716
May  6 07:24:06.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename container-probe 05/06/23 07:24:06.716
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:24:07.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:24:07.734
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511 in namespace container-probe-6670 05/06/23 07:24:07.736
May  6 07:24:07.742: INFO: Waiting up to 5m0s for pod "liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511" in namespace "container-probe-6670" to be "not pending"
May  6 07:24:07.745: INFO: Pod "liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511": Phase="Pending", Reason="", readiness=false. Elapsed: 3.284122ms
May  6 07:24:09.749: INFO: Pod "liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511": Phase="Running", Reason="", readiness=true. Elapsed: 2.007149497s
May  6 07:24:09.749: INFO: Pod "liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511" satisfied condition "not pending"
May  6 07:24:09.749: INFO: Started pod liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511 in namespace container-probe-6670
STEP: checking the pod's current state and verifying that restartCount is present 05/06/23 07:24:09.749
May  6 07:24:09.751: INFO: Initial restart count of pod liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511 is 0
May  6 07:24:29.787: INFO: Restart count of pod container-probe-6670/liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511 is now 1 (20.03566617s elapsed)
May  6 07:24:49.824: INFO: Restart count of pod container-probe-6670/liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511 is now 2 (40.072449565s elapsed)
May  6 07:25:09.867: INFO: Restart count of pod container-probe-6670/liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511 is now 3 (1m0.11531469s elapsed)
May  6 07:25:29.901: INFO: Restart count of pod container-probe-6670/liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511 is now 4 (1m20.149623727s elapsed)
May  6 07:26:42.027: INFO: Restart count of pod container-probe-6670/liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511 is now 5 (2m32.276189945s elapsed)
STEP: deleting the pod 05/06/23 07:26:42.028
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May  6 07:26:42.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6670" for this suite. 05/06/23 07:26:42.043
------------------------------
â€¢ [SLOW TEST] [155.337 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:24:06.716
    May  6 07:24:06.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename container-probe 05/06/23 07:24:06.716
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:24:07.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:24:07.734
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511 in namespace container-probe-6670 05/06/23 07:24:07.736
    May  6 07:24:07.742: INFO: Waiting up to 5m0s for pod "liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511" in namespace "container-probe-6670" to be "not pending"
    May  6 07:24:07.745: INFO: Pod "liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511": Phase="Pending", Reason="", readiness=false. Elapsed: 3.284122ms
    May  6 07:24:09.749: INFO: Pod "liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511": Phase="Running", Reason="", readiness=true. Elapsed: 2.007149497s
    May  6 07:24:09.749: INFO: Pod "liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511" satisfied condition "not pending"
    May  6 07:24:09.749: INFO: Started pod liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511 in namespace container-probe-6670
    STEP: checking the pod's current state and verifying that restartCount is present 05/06/23 07:24:09.749
    May  6 07:24:09.751: INFO: Initial restart count of pod liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511 is 0
    May  6 07:24:29.787: INFO: Restart count of pod container-probe-6670/liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511 is now 1 (20.03566617s elapsed)
    May  6 07:24:49.824: INFO: Restart count of pod container-probe-6670/liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511 is now 2 (40.072449565s elapsed)
    May  6 07:25:09.867: INFO: Restart count of pod container-probe-6670/liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511 is now 3 (1m0.11531469s elapsed)
    May  6 07:25:29.901: INFO: Restart count of pod container-probe-6670/liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511 is now 4 (1m20.149623727s elapsed)
    May  6 07:26:42.027: INFO: Restart count of pod container-probe-6670/liveness-61fe7636-fc3d-415b-9f5d-6b2a1e52f511 is now 5 (2m32.276189945s elapsed)
    STEP: deleting the pod 05/06/23 07:26:42.028
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May  6 07:26:42.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6670" for this suite. 05/06/23 07:26:42.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:26:42.054
May  6 07:26:42.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename watch 05/06/23 07:26:42.055
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:26:43.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:26:43.078
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 05/06/23 07:26:43.08
STEP: modifying the configmap once 05/06/23 07:26:43.085
STEP: modifying the configmap a second time 05/06/23 07:26:43.093
STEP: deleting the configmap 05/06/23 07:26:43.1
STEP: creating a watch on configmaps from the resource version returned by the first update 05/06/23 07:26:43.105
STEP: Expecting to observe notifications for all changes to the configmap after the first update 05/06/23 07:26:43.106
May  6 07:26:43.106: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8416  76db2745-d242-4c10-9475-49dc359012ce 150460 0 2023-05-06 07:26:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-06 07:26:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  6 07:26:43.107: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8416  76db2745-d242-4c10-9475-49dc359012ce 150461 0 2023-05-06 07:26:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-06 07:26:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May  6 07:26:43.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8416" for this suite. 05/06/23 07:26:43.11
------------------------------
â€¢ [1.061 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:26:42.054
    May  6 07:26:42.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename watch 05/06/23 07:26:42.055
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:26:43.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:26:43.078
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 05/06/23 07:26:43.08
    STEP: modifying the configmap once 05/06/23 07:26:43.085
    STEP: modifying the configmap a second time 05/06/23 07:26:43.093
    STEP: deleting the configmap 05/06/23 07:26:43.1
    STEP: creating a watch on configmaps from the resource version returned by the first update 05/06/23 07:26:43.105
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 05/06/23 07:26:43.106
    May  6 07:26:43.106: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8416  76db2745-d242-4c10-9475-49dc359012ce 150460 0 2023-05-06 07:26:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-06 07:26:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  6 07:26:43.107: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8416  76db2745-d242-4c10-9475-49dc359012ce 150461 0 2023-05-06 07:26:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-06 07:26:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May  6 07:26:43.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8416" for this suite. 05/06/23 07:26:43.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:26:43.115
May  6 07:26:43.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename services 05/06/23 07:26:43.116
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:26:44.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:26:44.132
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 05/06/23 07:26:44.134
May  6 07:26:44.134: INFO: Creating e2e-svc-a-v9bsj
May  6 07:26:44.146: INFO: Creating e2e-svc-b-fc76g
May  6 07:26:44.173: INFO: Creating e2e-svc-c-glqhf
STEP: deleting service collection 05/06/23 07:26:44.189
May  6 07:26:44.225: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  6 07:26:44.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3328" for this suite. 05/06/23 07:26:44.227
------------------------------
â€¢ [1.117 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:26:43.115
    May  6 07:26:43.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename services 05/06/23 07:26:43.116
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:26:44.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:26:44.132
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 05/06/23 07:26:44.134
    May  6 07:26:44.134: INFO: Creating e2e-svc-a-v9bsj
    May  6 07:26:44.146: INFO: Creating e2e-svc-b-fc76g
    May  6 07:26:44.173: INFO: Creating e2e-svc-c-glqhf
    STEP: deleting service collection 05/06/23 07:26:44.189
    May  6 07:26:44.225: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  6 07:26:44.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3328" for this suite. 05/06/23 07:26:44.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:26:44.233
May  6 07:26:44.233: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename gc 05/06/23 07:26:44.234
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:26:45.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:26:45.252
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 05/06/23 07:26:45.257
STEP: delete the rc 05/06/23 07:26:50.274
STEP: wait for the rc to be deleted 05/06/23 07:26:50.3
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 05/06/23 07:26:55.303
STEP: Gathering metrics 05/06/23 07:27:25.313
May  6 07:27:25.374: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-2" in namespace "kube-system" to be "running and ready"
May  6 07:27:25.377: INFO: Pod "kube-controller-manager-cncf-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.963522ms
May  6 07:27:25.377: INFO: The phase of Pod kube-controller-manager-cncf-2 is Running (Ready = true)
May  6 07:27:25.377: INFO: Pod "kube-controller-manager-cncf-2" satisfied condition "running and ready"
May  6 07:27:25.422: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May  6 07:27:25.422: INFO: Deleting pod "simpletest.rc-26xrd" in namespace "gc-9555"
May  6 07:27:25.437: INFO: Deleting pod "simpletest.rc-2sg7w" in namespace "gc-9555"
May  6 07:27:25.445: INFO: Deleting pod "simpletest.rc-45m6s" in namespace "gc-9555"
May  6 07:27:25.460: INFO: Deleting pod "simpletest.rc-4g2jn" in namespace "gc-9555"
May  6 07:27:25.471: INFO: Deleting pod "simpletest.rc-4mfws" in namespace "gc-9555"
May  6 07:27:25.488: INFO: Deleting pod "simpletest.rc-4rn24" in namespace "gc-9555"
May  6 07:27:25.503: INFO: Deleting pod "simpletest.rc-4zprq" in namespace "gc-9555"
May  6 07:27:25.522: INFO: Deleting pod "simpletest.rc-5hrqm" in namespace "gc-9555"
May  6 07:27:25.534: INFO: Deleting pod "simpletest.rc-5qkvf" in namespace "gc-9555"
May  6 07:27:25.551: INFO: Deleting pod "simpletest.rc-6fc9z" in namespace "gc-9555"
May  6 07:27:25.586: INFO: Deleting pod "simpletest.rc-6kwqj" in namespace "gc-9555"
May  6 07:27:25.603: INFO: Deleting pod "simpletest.rc-6lkzl" in namespace "gc-9555"
May  6 07:27:25.641: INFO: Deleting pod "simpletest.rc-6pjx9" in namespace "gc-9555"
May  6 07:27:25.658: INFO: Deleting pod "simpletest.rc-6zl6p" in namespace "gc-9555"
May  6 07:27:25.679: INFO: Deleting pod "simpletest.rc-72vsh" in namespace "gc-9555"
May  6 07:27:25.728: INFO: Deleting pod "simpletest.rc-74tww" in namespace "gc-9555"
May  6 07:27:25.760: INFO: Deleting pod "simpletest.rc-75mxq" in namespace "gc-9555"
May  6 07:27:25.802: INFO: Deleting pod "simpletest.rc-7jtqb" in namespace "gc-9555"
May  6 07:27:25.820: INFO: Deleting pod "simpletest.rc-86zdl" in namespace "gc-9555"
May  6 07:27:25.841: INFO: Deleting pod "simpletest.rc-8v965" in namespace "gc-9555"
May  6 07:27:25.867: INFO: Deleting pod "simpletest.rc-8zjnj" in namespace "gc-9555"
May  6 07:27:25.886: INFO: Deleting pod "simpletest.rc-9bbcs" in namespace "gc-9555"
May  6 07:27:25.924: INFO: Deleting pod "simpletest.rc-9bm8q" in namespace "gc-9555"
May  6 07:27:25.958: INFO: Deleting pod "simpletest.rc-9czsk" in namespace "gc-9555"
May  6 07:27:25.984: INFO: Deleting pod "simpletest.rc-9h5vv" in namespace "gc-9555"
May  6 07:27:26.019: INFO: Deleting pod "simpletest.rc-9pc5x" in namespace "gc-9555"
May  6 07:27:26.036: INFO: Deleting pod "simpletest.rc-9vlnr" in namespace "gc-9555"
May  6 07:27:26.066: INFO: Deleting pod "simpletest.rc-9xvmn" in namespace "gc-9555"
May  6 07:27:26.089: INFO: Deleting pod "simpletest.rc-bc529" in namespace "gc-9555"
May  6 07:27:26.146: INFO: Deleting pod "simpletest.rc-cg9w4" in namespace "gc-9555"
May  6 07:27:26.181: INFO: Deleting pod "simpletest.rc-ctdn6" in namespace "gc-9555"
May  6 07:27:26.243: INFO: Deleting pod "simpletest.rc-d2lnd" in namespace "gc-9555"
May  6 07:27:26.270: INFO: Deleting pod "simpletest.rc-d4854" in namespace "gc-9555"
May  6 07:27:26.315: INFO: Deleting pod "simpletest.rc-d4f26" in namespace "gc-9555"
May  6 07:27:26.356: INFO: Deleting pod "simpletest.rc-d8g9c" in namespace "gc-9555"
May  6 07:27:26.401: INFO: Deleting pod "simpletest.rc-dc7d6" in namespace "gc-9555"
May  6 07:27:26.463: INFO: Deleting pod "simpletest.rc-dfgzj" in namespace "gc-9555"
May  6 07:27:26.500: INFO: Deleting pod "simpletest.rc-dqlsg" in namespace "gc-9555"
May  6 07:27:26.529: INFO: Deleting pod "simpletest.rc-dx9v6" in namespace "gc-9555"
May  6 07:27:26.587: INFO: Deleting pod "simpletest.rc-f28w8" in namespace "gc-9555"
May  6 07:27:26.635: INFO: Deleting pod "simpletest.rc-f47hb" in namespace "gc-9555"
May  6 07:27:26.667: INFO: Deleting pod "simpletest.rc-fk62h" in namespace "gc-9555"
May  6 07:27:26.701: INFO: Deleting pod "simpletest.rc-ft5r7" in namespace "gc-9555"
May  6 07:27:26.724: INFO: Deleting pod "simpletest.rc-g4scm" in namespace "gc-9555"
May  6 07:27:26.769: INFO: Deleting pod "simpletest.rc-gcdz2" in namespace "gc-9555"
May  6 07:27:26.796: INFO: Deleting pod "simpletest.rc-gj6bz" in namespace "gc-9555"
May  6 07:27:26.826: INFO: Deleting pod "simpletest.rc-gj96j" in namespace "gc-9555"
May  6 07:27:26.916: INFO: Deleting pod "simpletest.rc-gpz4l" in namespace "gc-9555"
May  6 07:27:26.996: INFO: Deleting pod "simpletest.rc-gr8vk" in namespace "gc-9555"
May  6 07:27:27.047: INFO: Deleting pod "simpletest.rc-h48hj" in namespace "gc-9555"
May  6 07:27:27.097: INFO: Deleting pod "simpletest.rc-h66n8" in namespace "gc-9555"
May  6 07:27:27.142: INFO: Deleting pod "simpletest.rc-hh4tv" in namespace "gc-9555"
May  6 07:27:27.169: INFO: Deleting pod "simpletest.rc-hl5dt" in namespace "gc-9555"
May  6 07:27:27.217: INFO: Deleting pod "simpletest.rc-htrs6" in namespace "gc-9555"
May  6 07:27:27.265: INFO: Deleting pod "simpletest.rc-hwvbf" in namespace "gc-9555"
May  6 07:27:27.318: INFO: Deleting pod "simpletest.rc-jhzf2" in namespace "gc-9555"
May  6 07:27:27.352: INFO: Deleting pod "simpletest.rc-klx4p" in namespace "gc-9555"
May  6 07:27:27.393: INFO: Deleting pod "simpletest.rc-l4hsn" in namespace "gc-9555"
May  6 07:27:27.431: INFO: Deleting pod "simpletest.rc-lshmw" in namespace "gc-9555"
May  6 07:27:27.462: INFO: Deleting pod "simpletest.rc-m64c5" in namespace "gc-9555"
May  6 07:27:27.491: INFO: Deleting pod "simpletest.rc-mdrzq" in namespace "gc-9555"
May  6 07:27:27.524: INFO: Deleting pod "simpletest.rc-mg575" in namespace "gc-9555"
May  6 07:27:27.563: INFO: Deleting pod "simpletest.rc-nf758" in namespace "gc-9555"
May  6 07:27:27.614: INFO: Deleting pod "simpletest.rc-nfk2z" in namespace "gc-9555"
May  6 07:27:27.654: INFO: Deleting pod "simpletest.rc-nz6rz" in namespace "gc-9555"
May  6 07:27:27.684: INFO: Deleting pod "simpletest.rc-p4wvx" in namespace "gc-9555"
May  6 07:27:27.711: INFO: Deleting pod "simpletest.rc-p6fw7" in namespace "gc-9555"
May  6 07:27:27.743: INFO: Deleting pod "simpletest.rc-p6zmk" in namespace "gc-9555"
May  6 07:27:27.773: INFO: Deleting pod "simpletest.rc-pqdlq" in namespace "gc-9555"
May  6 07:27:27.810: INFO: Deleting pod "simpletest.rc-prh66" in namespace "gc-9555"
May  6 07:27:27.850: INFO: Deleting pod "simpletest.rc-ptflk" in namespace "gc-9555"
May  6 07:27:27.878: INFO: Deleting pod "simpletest.rc-pzpxk" in namespace "gc-9555"
May  6 07:27:27.903: INFO: Deleting pod "simpletest.rc-qb75f" in namespace "gc-9555"
May  6 07:27:27.934: INFO: Deleting pod "simpletest.rc-qc4c7" in namespace "gc-9555"
May  6 07:27:27.967: INFO: Deleting pod "simpletest.rc-r7vlb" in namespace "gc-9555"
May  6 07:27:28.001: INFO: Deleting pod "simpletest.rc-rf5wn" in namespace "gc-9555"
May  6 07:27:28.025: INFO: Deleting pod "simpletest.rc-s2bxw" in namespace "gc-9555"
May  6 07:27:28.053: INFO: Deleting pod "simpletest.rc-s6mqx" in namespace "gc-9555"
May  6 07:27:28.117: INFO: Deleting pod "simpletest.rc-sfwvg" in namespace "gc-9555"
May  6 07:27:28.156: INFO: Deleting pod "simpletest.rc-sr49w" in namespace "gc-9555"
May  6 07:27:28.217: INFO: Deleting pod "simpletest.rc-svsbt" in namespace "gc-9555"
May  6 07:27:28.241: INFO: Deleting pod "simpletest.rc-t5mr2" in namespace "gc-9555"
May  6 07:27:28.320: INFO: Deleting pod "simpletest.rc-tbvgs" in namespace "gc-9555"
May  6 07:27:28.369: INFO: Deleting pod "simpletest.rc-tkqb9" in namespace "gc-9555"
May  6 07:27:28.403: INFO: Deleting pod "simpletest.rc-ts8xd" in namespace "gc-9555"
May  6 07:27:28.426: INFO: Deleting pod "simpletest.rc-v4hgb" in namespace "gc-9555"
May  6 07:27:28.466: INFO: Deleting pod "simpletest.rc-v9nvq" in namespace "gc-9555"
May  6 07:27:28.521: INFO: Deleting pod "simpletest.rc-vm87t" in namespace "gc-9555"
May  6 07:27:28.558: INFO: Deleting pod "simpletest.rc-w8g5f" in namespace "gc-9555"
May  6 07:27:28.586: INFO: Deleting pod "simpletest.rc-wb7wv" in namespace "gc-9555"
May  6 07:27:28.621: INFO: Deleting pod "simpletest.rc-wbgg7" in namespace "gc-9555"
May  6 07:27:28.651: INFO: Deleting pod "simpletest.rc-wm8gz" in namespace "gc-9555"
May  6 07:27:28.680: INFO: Deleting pod "simpletest.rc-wmjpw" in namespace "gc-9555"
May  6 07:27:28.700: INFO: Deleting pod "simpletest.rc-wmk2f" in namespace "gc-9555"
May  6 07:27:28.742: INFO: Deleting pod "simpletest.rc-wsw9p" in namespace "gc-9555"
May  6 07:27:28.785: INFO: Deleting pod "simpletest.rc-x442c" in namespace "gc-9555"
May  6 07:27:28.829: INFO: Deleting pod "simpletest.rc-xmxlb" in namespace "gc-9555"
May  6 07:27:28.872: INFO: Deleting pod "simpletest.rc-z68x9" in namespace "gc-9555"
May  6 07:27:28.919: INFO: Deleting pod "simpletest.rc-zk7gz" in namespace "gc-9555"
May  6 07:27:28.951: INFO: Deleting pod "simpletest.rc-zxf4s" in namespace "gc-9555"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May  6 07:27:28.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9555" for this suite. 05/06/23 07:27:29
------------------------------
â€¢ [SLOW TEST] [44.787 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:26:44.233
    May  6 07:26:44.233: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename gc 05/06/23 07:26:44.234
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:26:45.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:26:45.252
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 05/06/23 07:26:45.257
    STEP: delete the rc 05/06/23 07:26:50.274
    STEP: wait for the rc to be deleted 05/06/23 07:26:50.3
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 05/06/23 07:26:55.303
    STEP: Gathering metrics 05/06/23 07:27:25.313
    May  6 07:27:25.374: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-2" in namespace "kube-system" to be "running and ready"
    May  6 07:27:25.377: INFO: Pod "kube-controller-manager-cncf-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.963522ms
    May  6 07:27:25.377: INFO: The phase of Pod kube-controller-manager-cncf-2 is Running (Ready = true)
    May  6 07:27:25.377: INFO: Pod "kube-controller-manager-cncf-2" satisfied condition "running and ready"
    May  6 07:27:25.422: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    May  6 07:27:25.422: INFO: Deleting pod "simpletest.rc-26xrd" in namespace "gc-9555"
    May  6 07:27:25.437: INFO: Deleting pod "simpletest.rc-2sg7w" in namespace "gc-9555"
    May  6 07:27:25.445: INFO: Deleting pod "simpletest.rc-45m6s" in namespace "gc-9555"
    May  6 07:27:25.460: INFO: Deleting pod "simpletest.rc-4g2jn" in namespace "gc-9555"
    May  6 07:27:25.471: INFO: Deleting pod "simpletest.rc-4mfws" in namespace "gc-9555"
    May  6 07:27:25.488: INFO: Deleting pod "simpletest.rc-4rn24" in namespace "gc-9555"
    May  6 07:27:25.503: INFO: Deleting pod "simpletest.rc-4zprq" in namespace "gc-9555"
    May  6 07:27:25.522: INFO: Deleting pod "simpletest.rc-5hrqm" in namespace "gc-9555"
    May  6 07:27:25.534: INFO: Deleting pod "simpletest.rc-5qkvf" in namespace "gc-9555"
    May  6 07:27:25.551: INFO: Deleting pod "simpletest.rc-6fc9z" in namespace "gc-9555"
    May  6 07:27:25.586: INFO: Deleting pod "simpletest.rc-6kwqj" in namespace "gc-9555"
    May  6 07:27:25.603: INFO: Deleting pod "simpletest.rc-6lkzl" in namespace "gc-9555"
    May  6 07:27:25.641: INFO: Deleting pod "simpletest.rc-6pjx9" in namespace "gc-9555"
    May  6 07:27:25.658: INFO: Deleting pod "simpletest.rc-6zl6p" in namespace "gc-9555"
    May  6 07:27:25.679: INFO: Deleting pod "simpletest.rc-72vsh" in namespace "gc-9555"
    May  6 07:27:25.728: INFO: Deleting pod "simpletest.rc-74tww" in namespace "gc-9555"
    May  6 07:27:25.760: INFO: Deleting pod "simpletest.rc-75mxq" in namespace "gc-9555"
    May  6 07:27:25.802: INFO: Deleting pod "simpletest.rc-7jtqb" in namespace "gc-9555"
    May  6 07:27:25.820: INFO: Deleting pod "simpletest.rc-86zdl" in namespace "gc-9555"
    May  6 07:27:25.841: INFO: Deleting pod "simpletest.rc-8v965" in namespace "gc-9555"
    May  6 07:27:25.867: INFO: Deleting pod "simpletest.rc-8zjnj" in namespace "gc-9555"
    May  6 07:27:25.886: INFO: Deleting pod "simpletest.rc-9bbcs" in namespace "gc-9555"
    May  6 07:27:25.924: INFO: Deleting pod "simpletest.rc-9bm8q" in namespace "gc-9555"
    May  6 07:27:25.958: INFO: Deleting pod "simpletest.rc-9czsk" in namespace "gc-9555"
    May  6 07:27:25.984: INFO: Deleting pod "simpletest.rc-9h5vv" in namespace "gc-9555"
    May  6 07:27:26.019: INFO: Deleting pod "simpletest.rc-9pc5x" in namespace "gc-9555"
    May  6 07:27:26.036: INFO: Deleting pod "simpletest.rc-9vlnr" in namespace "gc-9555"
    May  6 07:27:26.066: INFO: Deleting pod "simpletest.rc-9xvmn" in namespace "gc-9555"
    May  6 07:27:26.089: INFO: Deleting pod "simpletest.rc-bc529" in namespace "gc-9555"
    May  6 07:27:26.146: INFO: Deleting pod "simpletest.rc-cg9w4" in namespace "gc-9555"
    May  6 07:27:26.181: INFO: Deleting pod "simpletest.rc-ctdn6" in namespace "gc-9555"
    May  6 07:27:26.243: INFO: Deleting pod "simpletest.rc-d2lnd" in namespace "gc-9555"
    May  6 07:27:26.270: INFO: Deleting pod "simpletest.rc-d4854" in namespace "gc-9555"
    May  6 07:27:26.315: INFO: Deleting pod "simpletest.rc-d4f26" in namespace "gc-9555"
    May  6 07:27:26.356: INFO: Deleting pod "simpletest.rc-d8g9c" in namespace "gc-9555"
    May  6 07:27:26.401: INFO: Deleting pod "simpletest.rc-dc7d6" in namespace "gc-9555"
    May  6 07:27:26.463: INFO: Deleting pod "simpletest.rc-dfgzj" in namespace "gc-9555"
    May  6 07:27:26.500: INFO: Deleting pod "simpletest.rc-dqlsg" in namespace "gc-9555"
    May  6 07:27:26.529: INFO: Deleting pod "simpletest.rc-dx9v6" in namespace "gc-9555"
    May  6 07:27:26.587: INFO: Deleting pod "simpletest.rc-f28w8" in namespace "gc-9555"
    May  6 07:27:26.635: INFO: Deleting pod "simpletest.rc-f47hb" in namespace "gc-9555"
    May  6 07:27:26.667: INFO: Deleting pod "simpletest.rc-fk62h" in namespace "gc-9555"
    May  6 07:27:26.701: INFO: Deleting pod "simpletest.rc-ft5r7" in namespace "gc-9555"
    May  6 07:27:26.724: INFO: Deleting pod "simpletest.rc-g4scm" in namespace "gc-9555"
    May  6 07:27:26.769: INFO: Deleting pod "simpletest.rc-gcdz2" in namespace "gc-9555"
    May  6 07:27:26.796: INFO: Deleting pod "simpletest.rc-gj6bz" in namespace "gc-9555"
    May  6 07:27:26.826: INFO: Deleting pod "simpletest.rc-gj96j" in namespace "gc-9555"
    May  6 07:27:26.916: INFO: Deleting pod "simpletest.rc-gpz4l" in namespace "gc-9555"
    May  6 07:27:26.996: INFO: Deleting pod "simpletest.rc-gr8vk" in namespace "gc-9555"
    May  6 07:27:27.047: INFO: Deleting pod "simpletest.rc-h48hj" in namespace "gc-9555"
    May  6 07:27:27.097: INFO: Deleting pod "simpletest.rc-h66n8" in namespace "gc-9555"
    May  6 07:27:27.142: INFO: Deleting pod "simpletest.rc-hh4tv" in namespace "gc-9555"
    May  6 07:27:27.169: INFO: Deleting pod "simpletest.rc-hl5dt" in namespace "gc-9555"
    May  6 07:27:27.217: INFO: Deleting pod "simpletest.rc-htrs6" in namespace "gc-9555"
    May  6 07:27:27.265: INFO: Deleting pod "simpletest.rc-hwvbf" in namespace "gc-9555"
    May  6 07:27:27.318: INFO: Deleting pod "simpletest.rc-jhzf2" in namespace "gc-9555"
    May  6 07:27:27.352: INFO: Deleting pod "simpletest.rc-klx4p" in namespace "gc-9555"
    May  6 07:27:27.393: INFO: Deleting pod "simpletest.rc-l4hsn" in namespace "gc-9555"
    May  6 07:27:27.431: INFO: Deleting pod "simpletest.rc-lshmw" in namespace "gc-9555"
    May  6 07:27:27.462: INFO: Deleting pod "simpletest.rc-m64c5" in namespace "gc-9555"
    May  6 07:27:27.491: INFO: Deleting pod "simpletest.rc-mdrzq" in namespace "gc-9555"
    May  6 07:27:27.524: INFO: Deleting pod "simpletest.rc-mg575" in namespace "gc-9555"
    May  6 07:27:27.563: INFO: Deleting pod "simpletest.rc-nf758" in namespace "gc-9555"
    May  6 07:27:27.614: INFO: Deleting pod "simpletest.rc-nfk2z" in namespace "gc-9555"
    May  6 07:27:27.654: INFO: Deleting pod "simpletest.rc-nz6rz" in namespace "gc-9555"
    May  6 07:27:27.684: INFO: Deleting pod "simpletest.rc-p4wvx" in namespace "gc-9555"
    May  6 07:27:27.711: INFO: Deleting pod "simpletest.rc-p6fw7" in namespace "gc-9555"
    May  6 07:27:27.743: INFO: Deleting pod "simpletest.rc-p6zmk" in namespace "gc-9555"
    May  6 07:27:27.773: INFO: Deleting pod "simpletest.rc-pqdlq" in namespace "gc-9555"
    May  6 07:27:27.810: INFO: Deleting pod "simpletest.rc-prh66" in namespace "gc-9555"
    May  6 07:27:27.850: INFO: Deleting pod "simpletest.rc-ptflk" in namespace "gc-9555"
    May  6 07:27:27.878: INFO: Deleting pod "simpletest.rc-pzpxk" in namespace "gc-9555"
    May  6 07:27:27.903: INFO: Deleting pod "simpletest.rc-qb75f" in namespace "gc-9555"
    May  6 07:27:27.934: INFO: Deleting pod "simpletest.rc-qc4c7" in namespace "gc-9555"
    May  6 07:27:27.967: INFO: Deleting pod "simpletest.rc-r7vlb" in namespace "gc-9555"
    May  6 07:27:28.001: INFO: Deleting pod "simpletest.rc-rf5wn" in namespace "gc-9555"
    May  6 07:27:28.025: INFO: Deleting pod "simpletest.rc-s2bxw" in namespace "gc-9555"
    May  6 07:27:28.053: INFO: Deleting pod "simpletest.rc-s6mqx" in namespace "gc-9555"
    May  6 07:27:28.117: INFO: Deleting pod "simpletest.rc-sfwvg" in namespace "gc-9555"
    May  6 07:27:28.156: INFO: Deleting pod "simpletest.rc-sr49w" in namespace "gc-9555"
    May  6 07:27:28.217: INFO: Deleting pod "simpletest.rc-svsbt" in namespace "gc-9555"
    May  6 07:27:28.241: INFO: Deleting pod "simpletest.rc-t5mr2" in namespace "gc-9555"
    May  6 07:27:28.320: INFO: Deleting pod "simpletest.rc-tbvgs" in namespace "gc-9555"
    May  6 07:27:28.369: INFO: Deleting pod "simpletest.rc-tkqb9" in namespace "gc-9555"
    May  6 07:27:28.403: INFO: Deleting pod "simpletest.rc-ts8xd" in namespace "gc-9555"
    May  6 07:27:28.426: INFO: Deleting pod "simpletest.rc-v4hgb" in namespace "gc-9555"
    May  6 07:27:28.466: INFO: Deleting pod "simpletest.rc-v9nvq" in namespace "gc-9555"
    May  6 07:27:28.521: INFO: Deleting pod "simpletest.rc-vm87t" in namespace "gc-9555"
    May  6 07:27:28.558: INFO: Deleting pod "simpletest.rc-w8g5f" in namespace "gc-9555"
    May  6 07:27:28.586: INFO: Deleting pod "simpletest.rc-wb7wv" in namespace "gc-9555"
    May  6 07:27:28.621: INFO: Deleting pod "simpletest.rc-wbgg7" in namespace "gc-9555"
    May  6 07:27:28.651: INFO: Deleting pod "simpletest.rc-wm8gz" in namespace "gc-9555"
    May  6 07:27:28.680: INFO: Deleting pod "simpletest.rc-wmjpw" in namespace "gc-9555"
    May  6 07:27:28.700: INFO: Deleting pod "simpletest.rc-wmk2f" in namespace "gc-9555"
    May  6 07:27:28.742: INFO: Deleting pod "simpletest.rc-wsw9p" in namespace "gc-9555"
    May  6 07:27:28.785: INFO: Deleting pod "simpletest.rc-x442c" in namespace "gc-9555"
    May  6 07:27:28.829: INFO: Deleting pod "simpletest.rc-xmxlb" in namespace "gc-9555"
    May  6 07:27:28.872: INFO: Deleting pod "simpletest.rc-z68x9" in namespace "gc-9555"
    May  6 07:27:28.919: INFO: Deleting pod "simpletest.rc-zk7gz" in namespace "gc-9555"
    May  6 07:27:28.951: INFO: Deleting pod "simpletest.rc-zxf4s" in namespace "gc-9555"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May  6 07:27:28.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9555" for this suite. 05/06/23 07:27:29
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:27:29.022
May  6 07:27:29.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename secrets 05/06/23 07:27:29.023
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:27:30.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:27:30.042
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-96e2a56f-6fc0-4633-8da9-3ca585f65b67 05/06/23 07:27:30.045
STEP: Creating a pod to test consume secrets 05/06/23 07:27:30.05
May  6 07:27:30.069: INFO: Waiting up to 5m0s for pod "pod-secrets-1cd11b0f-5072-4570-8227-de24e768eef2" in namespace "secrets-4445" to be "Succeeded or Failed"
May  6 07:27:30.075: INFO: Pod "pod-secrets-1cd11b0f-5072-4570-8227-de24e768eef2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.84062ms
May  6 07:27:32.078: INFO: Pod "pod-secrets-1cd11b0f-5072-4570-8227-de24e768eef2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008353399s
May  6 07:27:34.079: INFO: Pod "pod-secrets-1cd11b0f-5072-4570-8227-de24e768eef2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009316361s
May  6 07:27:36.083: INFO: Pod "pod-secrets-1cd11b0f-5072-4570-8227-de24e768eef2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013288866s
STEP: Saw pod success 05/06/23 07:27:36.083
May  6 07:27:36.083: INFO: Pod "pod-secrets-1cd11b0f-5072-4570-8227-de24e768eef2" satisfied condition "Succeeded or Failed"
May  6 07:27:36.087: INFO: Trying to get logs from node cncf-0 pod pod-secrets-1cd11b0f-5072-4570-8227-de24e768eef2 container secret-volume-test: <nil>
STEP: delete the pod 05/06/23 07:27:36.099
May  6 07:27:36.111: INFO: Waiting for pod pod-secrets-1cd11b0f-5072-4570-8227-de24e768eef2 to disappear
May  6 07:27:36.113: INFO: Pod pod-secrets-1cd11b0f-5072-4570-8227-de24e768eef2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May  6 07:27:36.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4445" for this suite. 05/06/23 07:27:36.117
------------------------------
â€¢ [SLOW TEST] [7.103 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:27:29.022
    May  6 07:27:29.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename secrets 05/06/23 07:27:29.023
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:27:30.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:27:30.042
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-96e2a56f-6fc0-4633-8da9-3ca585f65b67 05/06/23 07:27:30.045
    STEP: Creating a pod to test consume secrets 05/06/23 07:27:30.05
    May  6 07:27:30.069: INFO: Waiting up to 5m0s for pod "pod-secrets-1cd11b0f-5072-4570-8227-de24e768eef2" in namespace "secrets-4445" to be "Succeeded or Failed"
    May  6 07:27:30.075: INFO: Pod "pod-secrets-1cd11b0f-5072-4570-8227-de24e768eef2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.84062ms
    May  6 07:27:32.078: INFO: Pod "pod-secrets-1cd11b0f-5072-4570-8227-de24e768eef2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008353399s
    May  6 07:27:34.079: INFO: Pod "pod-secrets-1cd11b0f-5072-4570-8227-de24e768eef2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009316361s
    May  6 07:27:36.083: INFO: Pod "pod-secrets-1cd11b0f-5072-4570-8227-de24e768eef2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013288866s
    STEP: Saw pod success 05/06/23 07:27:36.083
    May  6 07:27:36.083: INFO: Pod "pod-secrets-1cd11b0f-5072-4570-8227-de24e768eef2" satisfied condition "Succeeded or Failed"
    May  6 07:27:36.087: INFO: Trying to get logs from node cncf-0 pod pod-secrets-1cd11b0f-5072-4570-8227-de24e768eef2 container secret-volume-test: <nil>
    STEP: delete the pod 05/06/23 07:27:36.099
    May  6 07:27:36.111: INFO: Waiting for pod pod-secrets-1cd11b0f-5072-4570-8227-de24e768eef2 to disappear
    May  6 07:27:36.113: INFO: Pod pod-secrets-1cd11b0f-5072-4570-8227-de24e768eef2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May  6 07:27:36.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4445" for this suite. 05/06/23 07:27:36.117
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:27:36.125
May  6 07:27:36.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename podtemplate 05/06/23 07:27:36.126
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:27:37.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:27:37.144
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 05/06/23 07:27:37.146
May  6 07:27:37.152: INFO: created test-podtemplate-1
May  6 07:27:37.158: INFO: created test-podtemplate-2
May  6 07:27:37.163: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 05/06/23 07:27:37.163
STEP: delete collection of pod templates 05/06/23 07:27:37.166
May  6 07:27:37.166: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 05/06/23 07:27:37.183
May  6 07:27:37.183: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
May  6 07:27:37.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4499" for this suite. 05/06/23 07:27:37.192
------------------------------
â€¢ [1.075 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:27:36.125
    May  6 07:27:36.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename podtemplate 05/06/23 07:27:36.126
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:27:37.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:27:37.144
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 05/06/23 07:27:37.146
    May  6 07:27:37.152: INFO: created test-podtemplate-1
    May  6 07:27:37.158: INFO: created test-podtemplate-2
    May  6 07:27:37.163: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 05/06/23 07:27:37.163
    STEP: delete collection of pod templates 05/06/23 07:27:37.166
    May  6 07:27:37.166: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 05/06/23 07:27:37.183
    May  6 07:27:37.183: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    May  6 07:27:37.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4499" for this suite. 05/06/23 07:27:37.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:27:37.202
May  6 07:27:37.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename deployment 05/06/23 07:27:37.202
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:27:38.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:27:38.221
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
May  6 07:27:38.222: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May  6 07:27:38.229: INFO: Pod name sample-pod: Found 0 pods out of 1
May  6 07:27:43.234: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/06/23 07:27:43.234
May  6 07:27:43.234: INFO: Creating deployment "test-rolling-update-deployment"
May  6 07:27:43.241: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May  6 07:27:43.246: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May  6 07:27:45.253: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May  6 07:27:45.256: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  6 07:27:45.264: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-252  40580677-81a9-4824-95c7-dadec84f4a33 153160 1 2023-05-06 07:27:43 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-06 07:27:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:27:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c6d788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-06 07:27:43 +0000 UTC,LastTransitionTime:2023-05-06 07:27:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-05-06 07:27:44 +0000 UTC,LastTransitionTime:2023-05-06 07:27:43 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  6 07:27:45.267: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-252  d6fd9b69-23d3-4c74-a349-964c17c7673a 153150 1 2023-05-06 07:27:43 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 40580677-81a9-4824-95c7-dadec84f4a33 0xc002eda827 0xc002eda828}] [] [{kube-controller-manager Update apps/v1 2023-05-06 07:27:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40580677-81a9-4824-95c7-dadec84f4a33\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:27:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002eda8e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  6 07:27:45.267: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May  6 07:27:45.267: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-252  925608d2-44dd-4885-9a4e-1f4bcf05784a 153159 2 2023-05-06 07:27:38 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 40580677-81a9-4824-95c7-dadec84f4a33 0xc002eda6e7 0xc002eda6e8}] [] [{e2e.test Update apps/v1 2023-05-06 07:27:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:27:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40580677-81a9-4824-95c7-dadec84f4a33\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:27:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002eda7b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 07:27:45.269: INFO: Pod "test-rolling-update-deployment-7549d9f46d-82hc8" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-82hc8 test-rolling-update-deployment-7549d9f46d- deployment-252  1fe404d9-6298-4a81-8975-dcfe9a4eb5df 153149 0 2023-05-06 07:27:43 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:3d48d911609e02772bfd72a00bfe39109eb95d82dc7b8751bfdc6c3131429294 cni.projectcalico.org/podIP:10.244.20.147/32 cni.projectcalico.org/podIPs:10.244.20.147/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d d6fd9b69-23d3-4c74-a349-964c17c7673a 0xc002edad97 0xc002edad98}] [] [{calico Update v1 2023-05-06 07:27:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-06 07:27:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6fd9b69-23d3-4c74-a349-964c17c7673a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:27:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.20.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q4ln2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q4ln2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:27:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:27:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:27:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:27:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:10.244.20.147,StartTime:2023-05-06 07:27:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:27:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://59c1761187458fc96893713236b14910b40887c401bc35918f0f1fecd81db874,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.20.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May  6 07:27:45.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-252" for this suite. 05/06/23 07:27:45.273
------------------------------
â€¢ [SLOW TEST] [8.078 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:27:37.202
    May  6 07:27:37.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename deployment 05/06/23 07:27:37.202
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:27:38.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:27:38.221
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    May  6 07:27:38.222: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    May  6 07:27:38.229: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  6 07:27:43.234: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/06/23 07:27:43.234
    May  6 07:27:43.234: INFO: Creating deployment "test-rolling-update-deployment"
    May  6 07:27:43.241: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    May  6 07:27:43.246: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    May  6 07:27:45.253: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    May  6 07:27:45.256: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  6 07:27:45.264: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-252  40580677-81a9-4824-95c7-dadec84f4a33 153160 1 2023-05-06 07:27:43 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-06 07:27:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:27:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c6d788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-06 07:27:43 +0000 UTC,LastTransitionTime:2023-05-06 07:27:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-05-06 07:27:44 +0000 UTC,LastTransitionTime:2023-05-06 07:27:43 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  6 07:27:45.267: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-252  d6fd9b69-23d3-4c74-a349-964c17c7673a 153150 1 2023-05-06 07:27:43 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 40580677-81a9-4824-95c7-dadec84f4a33 0xc002eda827 0xc002eda828}] [] [{kube-controller-manager Update apps/v1 2023-05-06 07:27:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40580677-81a9-4824-95c7-dadec84f4a33\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:27:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002eda8e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  6 07:27:45.267: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    May  6 07:27:45.267: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-252  925608d2-44dd-4885-9a4e-1f4bcf05784a 153159 2 2023-05-06 07:27:38 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 40580677-81a9-4824-95c7-dadec84f4a33 0xc002eda6e7 0xc002eda6e8}] [] [{e2e.test Update apps/v1 2023-05-06 07:27:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:27:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40580677-81a9-4824-95c7-dadec84f4a33\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:27:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002eda7b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  6 07:27:45.269: INFO: Pod "test-rolling-update-deployment-7549d9f46d-82hc8" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-82hc8 test-rolling-update-deployment-7549d9f46d- deployment-252  1fe404d9-6298-4a81-8975-dcfe9a4eb5df 153149 0 2023-05-06 07:27:43 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:3d48d911609e02772bfd72a00bfe39109eb95d82dc7b8751bfdc6c3131429294 cni.projectcalico.org/podIP:10.244.20.147/32 cni.projectcalico.org/podIPs:10.244.20.147/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d d6fd9b69-23d3-4c74-a349-964c17c7673a 0xc002edad97 0xc002edad98}] [] [{calico Update v1 2023-05-06 07:27:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-06 07:27:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6fd9b69-23d3-4c74-a349-964c17c7673a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:27:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.20.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q4ln2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q4ln2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:27:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:27:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:27:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:27:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:10.244.20.147,StartTime:2023-05-06 07:27:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:27:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://59c1761187458fc96893713236b14910b40887c401bc35918f0f1fecd81db874,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.20.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May  6 07:27:45.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-252" for this suite. 05/06/23 07:27:45.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:27:45.281
May  6 07:27:45.281: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename container-probe 05/06/23 07:27:45.282
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:27:46.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:27:46.299
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-df0497e2-4523-4b7b-9616-60a6e2c39b10 in namespace container-probe-1413 05/06/23 07:27:46.301
May  6 07:27:46.308: INFO: Waiting up to 5m0s for pod "test-webserver-df0497e2-4523-4b7b-9616-60a6e2c39b10" in namespace "container-probe-1413" to be "not pending"
May  6 07:27:46.311: INFO: Pod "test-webserver-df0497e2-4523-4b7b-9616-60a6e2c39b10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.864184ms
May  6 07:27:48.316: INFO: Pod "test-webserver-df0497e2-4523-4b7b-9616-60a6e2c39b10": Phase="Running", Reason="", readiness=true. Elapsed: 2.007152704s
May  6 07:27:48.316: INFO: Pod "test-webserver-df0497e2-4523-4b7b-9616-60a6e2c39b10" satisfied condition "not pending"
May  6 07:27:48.316: INFO: Started pod test-webserver-df0497e2-4523-4b7b-9616-60a6e2c39b10 in namespace container-probe-1413
STEP: checking the pod's current state and verifying that restartCount is present 05/06/23 07:27:48.316
May  6 07:27:48.318: INFO: Initial restart count of pod test-webserver-df0497e2-4523-4b7b-9616-60a6e2c39b10 is 0
STEP: deleting the pod 05/06/23 07:31:48.752
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May  6 07:31:48.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1413" for this suite. 05/06/23 07:31:48.771
------------------------------
â€¢ [SLOW TEST] [243.496 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:27:45.281
    May  6 07:27:45.281: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename container-probe 05/06/23 07:27:45.282
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:27:46.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:27:46.299
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-df0497e2-4523-4b7b-9616-60a6e2c39b10 in namespace container-probe-1413 05/06/23 07:27:46.301
    May  6 07:27:46.308: INFO: Waiting up to 5m0s for pod "test-webserver-df0497e2-4523-4b7b-9616-60a6e2c39b10" in namespace "container-probe-1413" to be "not pending"
    May  6 07:27:46.311: INFO: Pod "test-webserver-df0497e2-4523-4b7b-9616-60a6e2c39b10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.864184ms
    May  6 07:27:48.316: INFO: Pod "test-webserver-df0497e2-4523-4b7b-9616-60a6e2c39b10": Phase="Running", Reason="", readiness=true. Elapsed: 2.007152704s
    May  6 07:27:48.316: INFO: Pod "test-webserver-df0497e2-4523-4b7b-9616-60a6e2c39b10" satisfied condition "not pending"
    May  6 07:27:48.316: INFO: Started pod test-webserver-df0497e2-4523-4b7b-9616-60a6e2c39b10 in namespace container-probe-1413
    STEP: checking the pod's current state and verifying that restartCount is present 05/06/23 07:27:48.316
    May  6 07:27:48.318: INFO: Initial restart count of pod test-webserver-df0497e2-4523-4b7b-9616-60a6e2c39b10 is 0
    STEP: deleting the pod 05/06/23 07:31:48.752
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May  6 07:31:48.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1413" for this suite. 05/06/23 07:31:48.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:31:48.778
May  6 07:31:48.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename webhook 05/06/23 07:31:48.778
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:31:49.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:31:49.804
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/06/23 07:31:51.821
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:31:52.001
STEP: Deploying the webhook pod 05/06/23 07:31:52.007
STEP: Wait for the deployment to be ready 05/06/23 07:31:52.02
May  6 07:31:52.025: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/06/23 07:31:54.033
STEP: Verifying the service has paired with the endpoint 05/06/23 07:31:54.048
May  6 07:31:55.048: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
May  6 07:31:55.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6201-crds.webhook.example.com via the AdmissionRegistration API 05/06/23 07:31:55.559
STEP: Creating a custom resource that should be mutated by the webhook 05/06/23 07:31:56.595
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:32:00.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2009" for this suite. 05/06/23 07:32:00.204
STEP: Destroying namespace "webhook-2009-markers" for this suite. 05/06/23 07:32:00.209
------------------------------
â€¢ [SLOW TEST] [11.441 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:31:48.778
    May  6 07:31:48.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename webhook 05/06/23 07:31:48.778
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:31:49.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:31:49.804
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/06/23 07:31:51.821
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:31:52.001
    STEP: Deploying the webhook pod 05/06/23 07:31:52.007
    STEP: Wait for the deployment to be ready 05/06/23 07:31:52.02
    May  6 07:31:52.025: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/06/23 07:31:54.033
    STEP: Verifying the service has paired with the endpoint 05/06/23 07:31:54.048
    May  6 07:31:55.048: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    May  6 07:31:55.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6201-crds.webhook.example.com via the AdmissionRegistration API 05/06/23 07:31:55.559
    STEP: Creating a custom resource that should be mutated by the webhook 05/06/23 07:31:56.595
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:32:00.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2009" for this suite. 05/06/23 07:32:00.204
    STEP: Destroying namespace "webhook-2009-markers" for this suite. 05/06/23 07:32:00.209
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:32:00.219
May  6 07:32:00.219: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename sched-pred 05/06/23 07:32:00.22
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:32:01.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:32:01.24
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
May  6 07:32:01.243: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  6 07:32:01.248: INFO: Waiting for terminating namespaces to be deleted...
May  6 07:32:01.250: INFO: 
Logging pods the apiserver thinks is on node cncf-0 before test
May  6 07:32:01.255: INFO: calico-node-vvl9m from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.255: INFO: 	Container calico-node ready: true, restart count 0
May  6 07:32:01.255: INFO: kube-apiserver-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.255: INFO: 	Container kube-apiserver ready: true, restart count 1
May  6 07:32:01.255: INFO: kube-controller-manager-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.255: INFO: 	Container kube-controller-manager ready: true, restart count 0
May  6 07:32:01.255: INFO: kube-proxy-cq72q from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.255: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 07:32:01.255: INFO: kube-scheduler-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.255: INFO: 	Container kube-scheduler ready: true, restart count 1
May  6 07:32:01.255: INFO: kubelet-rubber-stamp-c6b74568-5mtx7 from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.255: INFO: 	Container kubelet-rubber-stamp ready: true, restart count 1
May  6 07:32:01.255: INFO: tigera-operator-585fc94df6-xnxld from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.255: INFO: 	Container tigera-operator ready: true, restart count 0
May  6 07:32:01.255: INFO: externalsvc-hjgnp from services-2585 started at 2023-05-06 06:58:05 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.255: INFO: 	Container externalsvc ready: true, restart count 0
May  6 07:32:01.255: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-np4v2 from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 07:32:01.255: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 07:32:01.255: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 07:32:01.255: INFO: 
Logging pods the apiserver thinks is on node cncf-1 before test
May  6 07:32:01.261: INFO: calico-node-kz2jp from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.261: INFO: 	Container calico-node ready: true, restart count 0
May  6 07:32:01.261: INFO: calico-typha-7d85b4d7df-79mt7 from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.261: INFO: 	Container calico-typha ready: true, restart count 0
May  6 07:32:01.261: INFO: dns-test-6ffeff72-54ba-4fa1-9bbb-359bba68b89e from dns-3718 started at 2023-05-06 06:44:04 +0000 UTC (3 container statuses recorded)
May  6 07:32:01.261: INFO: 	Container jessie-querier ready: true, restart count 4
May  6 07:32:01.261: INFO: 	Container querier ready: true, restart count 4
May  6 07:32:01.261: INFO: 	Container webserver ready: true, restart count 0
May  6 07:32:01.261: INFO: coredns-5b98b988fd-8gbpp from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.261: INFO: 	Container coredns ready: true, restart count 0
May  6 07:32:01.261: INFO: kube-apiserver-cncf-1 from kube-system started at 2023-05-05 20:24:38 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.261: INFO: 	Container kube-apiserver ready: true, restart count 1
May  6 07:32:01.261: INFO: kube-controller-manager-cncf-1 from kube-system started at 2023-05-05 20:24:14 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.261: INFO: 	Container kube-controller-manager ready: true, restart count 0
May  6 07:32:01.261: INFO: kube-proxy-4wtf5 from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.261: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 07:32:01.261: INFO: kube-scheduler-cncf-1 from kube-system started at 2023-05-05 20:23:17 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.261: INFO: 	Container kube-scheduler ready: true, restart count 1
May  6 07:32:01.261: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-5lxlb from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 07:32:01.261: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 07:32:01.261: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 07:32:01.261: INFO: 
Logging pods the apiserver thinks is on node cncf-2 before test
May  6 07:32:01.267: INFO: calico-kube-controllers-574d8db6c-nnb5w from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.267: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  6 07:32:01.267: INFO: calico-node-q42q6 from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.267: INFO: 	Container calico-node ready: true, restart count 0
May  6 07:32:01.267: INFO: calico-typha-7d85b4d7df-6s76x from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.267: INFO: 	Container calico-typha ready: true, restart count 0
May  6 07:32:01.267: INFO: coredns-5b98b988fd-7r5vm from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.267: INFO: 	Container coredns ready: true, restart count 0
May  6 07:32:01.267: INFO: kube-apiserver-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.267: INFO: 	Container kube-apiserver ready: true, restart count 1
May  6 07:32:01.267: INFO: kube-controller-manager-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.267: INFO: 	Container kube-controller-manager ready: true, restart count 0
May  6 07:32:01.267: INFO: kube-proxy-jl76p from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.267: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 07:32:01.267: INFO: kube-scheduler-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.267: INFO: 	Container kube-scheduler ready: true, restart count 1
May  6 07:32:01.267: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-mjqth from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 07:32:01.267: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 07:32:01.267: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 07:32:01.267: INFO: 
Logging pods the apiserver thinks is on node cncf-3 before test
May  6 07:32:01.274: INFO: calico-node-sv4fh from calico-system started at 2023-05-05 20:28:36 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.274: INFO: 	Container calico-node ready: true, restart count 0
May  6 07:32:01.274: INFO: kube-proxy-n8v5r from kube-system started at 2023-05-05 20:28:36 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.274: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 07:32:01.274: INFO: controller-manager-v3-6b6b89c4f6-skqkm from rafay-system started at 2023-05-05 20:32:05 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.274: INFO: 	Container manager ready: true, restart count 0
May  6 07:32:01.274: INFO: edge-client-59448698df-gxb8q from rafay-system started at 2023-05-05 20:31:45 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.274: INFO: 	Container edge-client ready: true, restart count 0
May  6 07:32:01.274: INFO: rafay-connector-v3-79f986b9c6-rxjdn from rafay-system started at 2023-05-05 20:32:04 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.274: INFO: 	Container connector ready: true, restart count 0
May  6 07:32:01.274: INFO: v2-relay-agent-6b76bc6c6f-m6hgg from rafay-system started at 2023-05-05 20:32:06 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.274: INFO: 	Container relay-agent ready: true, restart count 0
May  6 07:32:01.274: INFO: externalsvc-45jwj from services-2585 started at 2023-05-06 06:58:05 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.274: INFO: 	Container externalsvc ready: true, restart count 0
May  6 07:32:01.274: INFO: sonobuoy from sonobuoy started at 2023-05-06 06:59:12 +0000 UTC (1 container statuses recorded)
May  6 07:32:01.274: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  6 07:32:01.274: INFO: sonobuoy-e2e-job-47910262329e4558 from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 07:32:01.274: INFO: 	Container e2e ready: true, restart count 0
May  6 07:32:01.274: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 07:32:01.274: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-ts2xw from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 07:32:01.274: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 07:32:01.274: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 05/06/23 07:32:01.274
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.175c7d79724da8a8], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match Pod's node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling..] 05/06/23 07:32:01.297
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 07:32:02.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5030" for this suite. 05/06/23 07:32:02.299
------------------------------
â€¢ [2.085 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:32:00.219
    May  6 07:32:00.219: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename sched-pred 05/06/23 07:32:00.22
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:32:01.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:32:01.24
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    May  6 07:32:01.243: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  6 07:32:01.248: INFO: Waiting for terminating namespaces to be deleted...
    May  6 07:32:01.250: INFO: 
    Logging pods the apiserver thinks is on node cncf-0 before test
    May  6 07:32:01.255: INFO: calico-node-vvl9m from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.255: INFO: 	Container calico-node ready: true, restart count 0
    May  6 07:32:01.255: INFO: kube-apiserver-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.255: INFO: 	Container kube-apiserver ready: true, restart count 1
    May  6 07:32:01.255: INFO: kube-controller-manager-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.255: INFO: 	Container kube-controller-manager ready: true, restart count 0
    May  6 07:32:01.255: INFO: kube-proxy-cq72q from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.255: INFO: 	Container kube-proxy ready: true, restart count 0
    May  6 07:32:01.255: INFO: kube-scheduler-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.255: INFO: 	Container kube-scheduler ready: true, restart count 1
    May  6 07:32:01.255: INFO: kubelet-rubber-stamp-c6b74568-5mtx7 from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.255: INFO: 	Container kubelet-rubber-stamp ready: true, restart count 1
    May  6 07:32:01.255: INFO: tigera-operator-585fc94df6-xnxld from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.255: INFO: 	Container tigera-operator ready: true, restart count 0
    May  6 07:32:01.255: INFO: externalsvc-hjgnp from services-2585 started at 2023-05-06 06:58:05 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.255: INFO: 	Container externalsvc ready: true, restart count 0
    May  6 07:32:01.255: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-np4v2 from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 07:32:01.255: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 07:32:01.255: INFO: 	Container systemd-logs ready: true, restart count 0
    May  6 07:32:01.255: INFO: 
    Logging pods the apiserver thinks is on node cncf-1 before test
    May  6 07:32:01.261: INFO: calico-node-kz2jp from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.261: INFO: 	Container calico-node ready: true, restart count 0
    May  6 07:32:01.261: INFO: calico-typha-7d85b4d7df-79mt7 from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.261: INFO: 	Container calico-typha ready: true, restart count 0
    May  6 07:32:01.261: INFO: dns-test-6ffeff72-54ba-4fa1-9bbb-359bba68b89e from dns-3718 started at 2023-05-06 06:44:04 +0000 UTC (3 container statuses recorded)
    May  6 07:32:01.261: INFO: 	Container jessie-querier ready: true, restart count 4
    May  6 07:32:01.261: INFO: 	Container querier ready: true, restart count 4
    May  6 07:32:01.261: INFO: 	Container webserver ready: true, restart count 0
    May  6 07:32:01.261: INFO: coredns-5b98b988fd-8gbpp from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.261: INFO: 	Container coredns ready: true, restart count 0
    May  6 07:32:01.261: INFO: kube-apiserver-cncf-1 from kube-system started at 2023-05-05 20:24:38 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.261: INFO: 	Container kube-apiserver ready: true, restart count 1
    May  6 07:32:01.261: INFO: kube-controller-manager-cncf-1 from kube-system started at 2023-05-05 20:24:14 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.261: INFO: 	Container kube-controller-manager ready: true, restart count 0
    May  6 07:32:01.261: INFO: kube-proxy-4wtf5 from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.261: INFO: 	Container kube-proxy ready: true, restart count 0
    May  6 07:32:01.261: INFO: kube-scheduler-cncf-1 from kube-system started at 2023-05-05 20:23:17 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.261: INFO: 	Container kube-scheduler ready: true, restart count 1
    May  6 07:32:01.261: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-5lxlb from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 07:32:01.261: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 07:32:01.261: INFO: 	Container systemd-logs ready: true, restart count 0
    May  6 07:32:01.261: INFO: 
    Logging pods the apiserver thinks is on node cncf-2 before test
    May  6 07:32:01.267: INFO: calico-kube-controllers-574d8db6c-nnb5w from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.267: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  6 07:32:01.267: INFO: calico-node-q42q6 from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.267: INFO: 	Container calico-node ready: true, restart count 0
    May  6 07:32:01.267: INFO: calico-typha-7d85b4d7df-6s76x from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.267: INFO: 	Container calico-typha ready: true, restart count 0
    May  6 07:32:01.267: INFO: coredns-5b98b988fd-7r5vm from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.267: INFO: 	Container coredns ready: true, restart count 0
    May  6 07:32:01.267: INFO: kube-apiserver-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.267: INFO: 	Container kube-apiserver ready: true, restart count 1
    May  6 07:32:01.267: INFO: kube-controller-manager-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.267: INFO: 	Container kube-controller-manager ready: true, restart count 0
    May  6 07:32:01.267: INFO: kube-proxy-jl76p from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.267: INFO: 	Container kube-proxy ready: true, restart count 0
    May  6 07:32:01.267: INFO: kube-scheduler-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.267: INFO: 	Container kube-scheduler ready: true, restart count 1
    May  6 07:32:01.267: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-mjqth from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 07:32:01.267: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 07:32:01.267: INFO: 	Container systemd-logs ready: true, restart count 0
    May  6 07:32:01.267: INFO: 
    Logging pods the apiserver thinks is on node cncf-3 before test
    May  6 07:32:01.274: INFO: calico-node-sv4fh from calico-system started at 2023-05-05 20:28:36 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.274: INFO: 	Container calico-node ready: true, restart count 0
    May  6 07:32:01.274: INFO: kube-proxy-n8v5r from kube-system started at 2023-05-05 20:28:36 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.274: INFO: 	Container kube-proxy ready: true, restart count 0
    May  6 07:32:01.274: INFO: controller-manager-v3-6b6b89c4f6-skqkm from rafay-system started at 2023-05-05 20:32:05 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.274: INFO: 	Container manager ready: true, restart count 0
    May  6 07:32:01.274: INFO: edge-client-59448698df-gxb8q from rafay-system started at 2023-05-05 20:31:45 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.274: INFO: 	Container edge-client ready: true, restart count 0
    May  6 07:32:01.274: INFO: rafay-connector-v3-79f986b9c6-rxjdn from rafay-system started at 2023-05-05 20:32:04 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.274: INFO: 	Container connector ready: true, restart count 0
    May  6 07:32:01.274: INFO: v2-relay-agent-6b76bc6c6f-m6hgg from rafay-system started at 2023-05-05 20:32:06 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.274: INFO: 	Container relay-agent ready: true, restart count 0
    May  6 07:32:01.274: INFO: externalsvc-45jwj from services-2585 started at 2023-05-06 06:58:05 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.274: INFO: 	Container externalsvc ready: true, restart count 0
    May  6 07:32:01.274: INFO: sonobuoy from sonobuoy started at 2023-05-06 06:59:12 +0000 UTC (1 container statuses recorded)
    May  6 07:32:01.274: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  6 07:32:01.274: INFO: sonobuoy-e2e-job-47910262329e4558 from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 07:32:01.274: INFO: 	Container e2e ready: true, restart count 0
    May  6 07:32:01.274: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 07:32:01.274: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-ts2xw from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 07:32:01.274: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 07:32:01.274: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 05/06/23 07:32:01.274
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.175c7d79724da8a8], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match Pod's node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling..] 05/06/23 07:32:01.297
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 07:32:02.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5030" for this suite. 05/06/23 07:32:02.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:32:02.305
May  6 07:32:02.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename resourcequota 05/06/23 07:32:02.306
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:32:03.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:32:03.323
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-6rbqd" 05/06/23 07:32:03.327
May  6 07:32:03.333: INFO: Resource quota "e2e-rq-status-6rbqd" reports spec: hard cpu limit of 500m
May  6 07:32:03.333: INFO: Resource quota "e2e-rq-status-6rbqd" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-6rbqd" /status 05/06/23 07:32:03.333
STEP: Confirm /status for "e2e-rq-status-6rbqd" resourceQuota via watch 05/06/23 07:32:03.355
May  6 07:32:03.356: INFO: observed resourceQuota "e2e-rq-status-6rbqd" in namespace "resourcequota-2846" with hard status: v1.ResourceList(nil)
May  6 07:32:03.356: INFO: Found resourceQuota "e2e-rq-status-6rbqd" in namespace "resourcequota-2846" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
May  6 07:32:03.356: INFO: ResourceQuota "e2e-rq-status-6rbqd" /status was updated
STEP: Patching hard spec values for cpu & memory 05/06/23 07:32:03.358
May  6 07:32:03.365: INFO: Resource quota "e2e-rq-status-6rbqd" reports spec: hard cpu limit of 1
May  6 07:32:03.365: INFO: Resource quota "e2e-rq-status-6rbqd" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-6rbqd" /status 05/06/23 07:32:03.365
STEP: Confirm /status for "e2e-rq-status-6rbqd" resourceQuota via watch 05/06/23 07:32:03.372
May  6 07:32:03.373: INFO: observed resourceQuota "e2e-rq-status-6rbqd" in namespace "resourcequota-2846" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
May  6 07:32:03.373: INFO: Found resourceQuota "e2e-rq-status-6rbqd" in namespace "resourcequota-2846" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
May  6 07:32:03.373: INFO: ResourceQuota "e2e-rq-status-6rbqd" /status was patched
STEP: Get "e2e-rq-status-6rbqd" /status 05/06/23 07:32:03.373
May  6 07:32:03.375: INFO: Resourcequota "e2e-rq-status-6rbqd" reports status: hard cpu of 1
May  6 07:32:03.375: INFO: Resourcequota "e2e-rq-status-6rbqd" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-6rbqd" /status before checking Spec is unchanged 05/06/23 07:32:03.377
May  6 07:32:03.382: INFO: Resourcequota "e2e-rq-status-6rbqd" reports status: hard cpu of 2
May  6 07:32:03.382: INFO: Resourcequota "e2e-rq-status-6rbqd" reports status: hard memory of 2Gi
May  6 07:32:03.382: INFO: Found resourceQuota "e2e-rq-status-6rbqd" in namespace "resourcequota-2846" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
May  6 07:32:08.387: INFO: ResourceQuota "e2e-rq-status-6rbqd" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  6 07:32:08.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2846" for this suite. 05/06/23 07:32:08.391
------------------------------
â€¢ [SLOW TEST] [6.093 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:32:02.305
    May  6 07:32:02.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename resourcequota 05/06/23 07:32:02.306
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:32:03.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:32:03.323
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-6rbqd" 05/06/23 07:32:03.327
    May  6 07:32:03.333: INFO: Resource quota "e2e-rq-status-6rbqd" reports spec: hard cpu limit of 500m
    May  6 07:32:03.333: INFO: Resource quota "e2e-rq-status-6rbqd" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-6rbqd" /status 05/06/23 07:32:03.333
    STEP: Confirm /status for "e2e-rq-status-6rbqd" resourceQuota via watch 05/06/23 07:32:03.355
    May  6 07:32:03.356: INFO: observed resourceQuota "e2e-rq-status-6rbqd" in namespace "resourcequota-2846" with hard status: v1.ResourceList(nil)
    May  6 07:32:03.356: INFO: Found resourceQuota "e2e-rq-status-6rbqd" in namespace "resourcequota-2846" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    May  6 07:32:03.356: INFO: ResourceQuota "e2e-rq-status-6rbqd" /status was updated
    STEP: Patching hard spec values for cpu & memory 05/06/23 07:32:03.358
    May  6 07:32:03.365: INFO: Resource quota "e2e-rq-status-6rbqd" reports spec: hard cpu limit of 1
    May  6 07:32:03.365: INFO: Resource quota "e2e-rq-status-6rbqd" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-6rbqd" /status 05/06/23 07:32:03.365
    STEP: Confirm /status for "e2e-rq-status-6rbqd" resourceQuota via watch 05/06/23 07:32:03.372
    May  6 07:32:03.373: INFO: observed resourceQuota "e2e-rq-status-6rbqd" in namespace "resourcequota-2846" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    May  6 07:32:03.373: INFO: Found resourceQuota "e2e-rq-status-6rbqd" in namespace "resourcequota-2846" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    May  6 07:32:03.373: INFO: ResourceQuota "e2e-rq-status-6rbqd" /status was patched
    STEP: Get "e2e-rq-status-6rbqd" /status 05/06/23 07:32:03.373
    May  6 07:32:03.375: INFO: Resourcequota "e2e-rq-status-6rbqd" reports status: hard cpu of 1
    May  6 07:32:03.375: INFO: Resourcequota "e2e-rq-status-6rbqd" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-6rbqd" /status before checking Spec is unchanged 05/06/23 07:32:03.377
    May  6 07:32:03.382: INFO: Resourcequota "e2e-rq-status-6rbqd" reports status: hard cpu of 2
    May  6 07:32:03.382: INFO: Resourcequota "e2e-rq-status-6rbqd" reports status: hard memory of 2Gi
    May  6 07:32:03.382: INFO: Found resourceQuota "e2e-rq-status-6rbqd" in namespace "resourcequota-2846" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    May  6 07:32:08.387: INFO: ResourceQuota "e2e-rq-status-6rbqd" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  6 07:32:08.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2846" for this suite. 05/06/23 07:32:08.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:32:08.399
May  6 07:32:08.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename replication-controller 05/06/23 07:32:08.4
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:32:09.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:32:09.42
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-hm4gq" 05/06/23 07:32:09.422
May  6 07:32:09.427: INFO: Get Replication Controller "e2e-rc-hm4gq" to confirm replicas
May  6 07:32:10.429: INFO: Get Replication Controller "e2e-rc-hm4gq" to confirm replicas
May  6 07:32:10.432: INFO: Found 1 replicas for "e2e-rc-hm4gq" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-hm4gq" 05/06/23 07:32:10.432
STEP: Updating a scale subresource 05/06/23 07:32:10.438
STEP: Verifying replicas where modified for replication controller "e2e-rc-hm4gq" 05/06/23 07:32:10.443
May  6 07:32:10.443: INFO: Get Replication Controller "e2e-rc-hm4gq" to confirm replicas
May  6 07:32:11.445: INFO: Get Replication Controller "e2e-rc-hm4gq" to confirm replicas
May  6 07:32:11.448: INFO: Found 2 replicas for "e2e-rc-hm4gq" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May  6 07:32:11.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1345" for this suite. 05/06/23 07:32:11.454
------------------------------
â€¢ [3.067 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:32:08.399
    May  6 07:32:08.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename replication-controller 05/06/23 07:32:08.4
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:32:09.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:32:09.42
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-hm4gq" 05/06/23 07:32:09.422
    May  6 07:32:09.427: INFO: Get Replication Controller "e2e-rc-hm4gq" to confirm replicas
    May  6 07:32:10.429: INFO: Get Replication Controller "e2e-rc-hm4gq" to confirm replicas
    May  6 07:32:10.432: INFO: Found 1 replicas for "e2e-rc-hm4gq" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-hm4gq" 05/06/23 07:32:10.432
    STEP: Updating a scale subresource 05/06/23 07:32:10.438
    STEP: Verifying replicas where modified for replication controller "e2e-rc-hm4gq" 05/06/23 07:32:10.443
    May  6 07:32:10.443: INFO: Get Replication Controller "e2e-rc-hm4gq" to confirm replicas
    May  6 07:32:11.445: INFO: Get Replication Controller "e2e-rc-hm4gq" to confirm replicas
    May  6 07:32:11.448: INFO: Found 2 replicas for "e2e-rc-hm4gq" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May  6 07:32:11.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1345" for this suite. 05/06/23 07:32:11.454
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:32:11.468
May  6 07:32:11.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename services 05/06/23 07:32:11.469
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:32:12.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:32:12.485
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3410 05/06/23 07:32:12.487
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/06/23 07:32:12.497
STEP: creating service externalsvc in namespace services-3410 05/06/23 07:32:12.497
STEP: creating replication controller externalsvc in namespace services-3410 05/06/23 07:32:12.512
I0506 07:32:12.518814      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3410, replica count: 2
I0506 07:32:15.569887      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 05/06/23 07:32:15.572
May  6 07:32:15.586: INFO: Creating new exec pod
May  6 07:32:15.594: INFO: Waiting up to 5m0s for pod "execpod9tgnz" in namespace "services-3410" to be "running"
May  6 07:32:15.597: INFO: Pod "execpod9tgnz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.291775ms
May  6 07:32:17.600: INFO: Pod "execpod9tgnz": Phase="Running", Reason="", readiness=true. Elapsed: 2.005646296s
May  6 07:32:17.600: INFO: Pod "execpod9tgnz" satisfied condition "running"
May  6 07:32:17.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-3410 exec execpod9tgnz -- /bin/sh -x -c nslookup clusterip-service.services-3410.svc.cluster.local'
May  6 07:32:17.749: INFO: stderr: "+ nslookup clusterip-service.services-3410.svc.cluster.local\n"
May  6 07:32:17.749: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-3410.svc.cluster.local\tcanonical name = externalsvc.services-3410.svc.cluster.local.\nName:\texternalsvc.services-3410.svc.cluster.local\nAddress: 10.97.210.244\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3410, will wait for the garbage collector to delete the pods 05/06/23 07:32:17.749
May  6 07:32:17.809: INFO: Deleting ReplicationController externalsvc took: 5.831283ms
May  6 07:32:17.910: INFO: Terminating ReplicationController externalsvc pods took: 100.653162ms
May  6 07:32:19.928: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  6 07:32:19.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3410" for this suite. 05/06/23 07:32:19.94
------------------------------
â€¢ [SLOW TEST] [8.478 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:32:11.468
    May  6 07:32:11.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename services 05/06/23 07:32:11.469
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:32:12.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:32:12.485
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3410 05/06/23 07:32:12.487
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/06/23 07:32:12.497
    STEP: creating service externalsvc in namespace services-3410 05/06/23 07:32:12.497
    STEP: creating replication controller externalsvc in namespace services-3410 05/06/23 07:32:12.512
    I0506 07:32:12.518814      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3410, replica count: 2
    I0506 07:32:15.569887      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 05/06/23 07:32:15.572
    May  6 07:32:15.586: INFO: Creating new exec pod
    May  6 07:32:15.594: INFO: Waiting up to 5m0s for pod "execpod9tgnz" in namespace "services-3410" to be "running"
    May  6 07:32:15.597: INFO: Pod "execpod9tgnz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.291775ms
    May  6 07:32:17.600: INFO: Pod "execpod9tgnz": Phase="Running", Reason="", readiness=true. Elapsed: 2.005646296s
    May  6 07:32:17.600: INFO: Pod "execpod9tgnz" satisfied condition "running"
    May  6 07:32:17.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-3410 exec execpod9tgnz -- /bin/sh -x -c nslookup clusterip-service.services-3410.svc.cluster.local'
    May  6 07:32:17.749: INFO: stderr: "+ nslookup clusterip-service.services-3410.svc.cluster.local\n"
    May  6 07:32:17.749: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-3410.svc.cluster.local\tcanonical name = externalsvc.services-3410.svc.cluster.local.\nName:\texternalsvc.services-3410.svc.cluster.local\nAddress: 10.97.210.244\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3410, will wait for the garbage collector to delete the pods 05/06/23 07:32:17.749
    May  6 07:32:17.809: INFO: Deleting ReplicationController externalsvc took: 5.831283ms
    May  6 07:32:17.910: INFO: Terminating ReplicationController externalsvc pods took: 100.653162ms
    May  6 07:32:19.928: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  6 07:32:19.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3410" for this suite. 05/06/23 07:32:19.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:32:19.946
May  6 07:32:19.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename secrets 05/06/23 07:32:19.947
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:32:20.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:32:20.966
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-29530110-6c22-4fa9-befe-519abec3e5ff 05/06/23 07:32:20.968
STEP: Creating a pod to test consume secrets 05/06/23 07:32:20.973
May  6 07:32:20.981: INFO: Waiting up to 5m0s for pod "pod-secrets-6974d29a-b041-4023-9bfa-dc713f583ab0" in namespace "secrets-2713" to be "Succeeded or Failed"
May  6 07:32:20.983: INFO: Pod "pod-secrets-6974d29a-b041-4023-9bfa-dc713f583ab0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.569843ms
May  6 07:32:22.986: INFO: Pod "pod-secrets-6974d29a-b041-4023-9bfa-dc713f583ab0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005440117s
May  6 07:32:24.987: INFO: Pod "pod-secrets-6974d29a-b041-4023-9bfa-dc713f583ab0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005841518s
STEP: Saw pod success 05/06/23 07:32:24.987
May  6 07:32:24.987: INFO: Pod "pod-secrets-6974d29a-b041-4023-9bfa-dc713f583ab0" satisfied condition "Succeeded or Failed"
May  6 07:32:24.989: INFO: Trying to get logs from node cncf-0 pod pod-secrets-6974d29a-b041-4023-9bfa-dc713f583ab0 container secret-env-test: <nil>
STEP: delete the pod 05/06/23 07:32:24.999
May  6 07:32:25.010: INFO: Waiting for pod pod-secrets-6974d29a-b041-4023-9bfa-dc713f583ab0 to disappear
May  6 07:32:25.012: INFO: Pod pod-secrets-6974d29a-b041-4023-9bfa-dc713f583ab0 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
May  6 07:32:25.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2713" for this suite. 05/06/23 07:32:25.015
------------------------------
â€¢ [SLOW TEST] [5.075 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:32:19.946
    May  6 07:32:19.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename secrets 05/06/23 07:32:19.947
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:32:20.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:32:20.966
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-29530110-6c22-4fa9-befe-519abec3e5ff 05/06/23 07:32:20.968
    STEP: Creating a pod to test consume secrets 05/06/23 07:32:20.973
    May  6 07:32:20.981: INFO: Waiting up to 5m0s for pod "pod-secrets-6974d29a-b041-4023-9bfa-dc713f583ab0" in namespace "secrets-2713" to be "Succeeded or Failed"
    May  6 07:32:20.983: INFO: Pod "pod-secrets-6974d29a-b041-4023-9bfa-dc713f583ab0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.569843ms
    May  6 07:32:22.986: INFO: Pod "pod-secrets-6974d29a-b041-4023-9bfa-dc713f583ab0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005440117s
    May  6 07:32:24.987: INFO: Pod "pod-secrets-6974d29a-b041-4023-9bfa-dc713f583ab0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005841518s
    STEP: Saw pod success 05/06/23 07:32:24.987
    May  6 07:32:24.987: INFO: Pod "pod-secrets-6974d29a-b041-4023-9bfa-dc713f583ab0" satisfied condition "Succeeded or Failed"
    May  6 07:32:24.989: INFO: Trying to get logs from node cncf-0 pod pod-secrets-6974d29a-b041-4023-9bfa-dc713f583ab0 container secret-env-test: <nil>
    STEP: delete the pod 05/06/23 07:32:24.999
    May  6 07:32:25.010: INFO: Waiting for pod pod-secrets-6974d29a-b041-4023-9bfa-dc713f583ab0 to disappear
    May  6 07:32:25.012: INFO: Pod pod-secrets-6974d29a-b041-4023-9bfa-dc713f583ab0 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    May  6 07:32:25.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2713" for this suite. 05/06/23 07:32:25.015
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:32:25.022
May  6 07:32:25.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename dns 05/06/23 07:32:25.022
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:32:26.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:32:26.04
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 05/06/23 07:32:26.042
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 05/06/23 07:32:26.042
STEP: creating a pod to probe DNS 05/06/23 07:32:26.042
STEP: submitting the pod to kubernetes 05/06/23 07:32:26.042
May  6 07:32:26.050: INFO: Waiting up to 15m0s for pod "dns-test-b70a3cba-d0f3-442f-8abc-1cfdf9ff9e1f" in namespace "dns-1300" to be "running"
May  6 07:32:26.052: INFO: Pod "dns-test-b70a3cba-d0f3-442f-8abc-1cfdf9ff9e1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.097767ms
May  6 07:32:28.057: INFO: Pod "dns-test-b70a3cba-d0f3-442f-8abc-1cfdf9ff9e1f": Phase="Running", Reason="", readiness=true. Elapsed: 2.006412957s
May  6 07:32:28.057: INFO: Pod "dns-test-b70a3cba-d0f3-442f-8abc-1cfdf9ff9e1f" satisfied condition "running"
STEP: retrieving the pod 05/06/23 07:32:28.057
STEP: looking for the results for each expected name from probers 05/06/23 07:32:28.059
May  6 07:32:28.073: INFO: DNS probes using dns-1300/dns-test-b70a3cba-d0f3-442f-8abc-1cfdf9ff9e1f succeeded

STEP: deleting the pod 05/06/23 07:32:28.073
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May  6 07:32:28.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1300" for this suite. 05/06/23 07:32:28.091
------------------------------
â€¢ [3.074 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:32:25.022
    May  6 07:32:25.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename dns 05/06/23 07:32:25.022
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:32:26.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:32:26.04
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     05/06/23 07:32:26.042
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     05/06/23 07:32:26.042
    STEP: creating a pod to probe DNS 05/06/23 07:32:26.042
    STEP: submitting the pod to kubernetes 05/06/23 07:32:26.042
    May  6 07:32:26.050: INFO: Waiting up to 15m0s for pod "dns-test-b70a3cba-d0f3-442f-8abc-1cfdf9ff9e1f" in namespace "dns-1300" to be "running"
    May  6 07:32:26.052: INFO: Pod "dns-test-b70a3cba-d0f3-442f-8abc-1cfdf9ff9e1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.097767ms
    May  6 07:32:28.057: INFO: Pod "dns-test-b70a3cba-d0f3-442f-8abc-1cfdf9ff9e1f": Phase="Running", Reason="", readiness=true. Elapsed: 2.006412957s
    May  6 07:32:28.057: INFO: Pod "dns-test-b70a3cba-d0f3-442f-8abc-1cfdf9ff9e1f" satisfied condition "running"
    STEP: retrieving the pod 05/06/23 07:32:28.057
    STEP: looking for the results for each expected name from probers 05/06/23 07:32:28.059
    May  6 07:32:28.073: INFO: DNS probes using dns-1300/dns-test-b70a3cba-d0f3-442f-8abc-1cfdf9ff9e1f succeeded

    STEP: deleting the pod 05/06/23 07:32:28.073
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May  6 07:32:28.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1300" for this suite. 05/06/23 07:32:28.091
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:32:28.096
May  6 07:32:28.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename sched-preemption 05/06/23 07:32:28.097
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:32:29.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:32:29.117
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
May  6 07:32:29.133: INFO: Waiting up to 1m0s for all nodes to be ready
May  6 07:33:29.155: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 05/06/23 07:33:29.158
May  6 07:33:29.179: INFO: Created pod: pod0-0-sched-preemption-low-priority
May  6 07:33:29.185: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May  6 07:33:29.203: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May  6 07:33:29.209: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May  6 07:33:29.238: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May  6 07:33:29.248: INFO: Created pod: pod2-1-sched-preemption-medium-priority
May  6 07:33:29.268: INFO: Created pod: pod3-0-sched-preemption-medium-priority
May  6 07:33:29.274: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 05/06/23 07:33:29.274
May  6 07:33:29.274: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1669" to be "running"
May  6 07:33:29.281: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.397819ms
May  6 07:33:31.284: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.010717069s
May  6 07:33:31.284: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
May  6 07:33:31.284: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1669" to be "running"
May  6 07:33:31.286: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.904489ms
May  6 07:33:31.286: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
May  6 07:33:31.286: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1669" to be "running"
May  6 07:33:31.289: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.255155ms
May  6 07:33:31.289: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
May  6 07:33:31.289: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1669" to be "running"
May  6 07:33:31.291: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.811452ms
May  6 07:33:31.291: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
May  6 07:33:31.291: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1669" to be "running"
May  6 07:33:31.293: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.13659ms
May  6 07:33:31.293: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
May  6 07:33:31.293: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1669" to be "running"
May  6 07:33:31.295: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.880644ms
May  6 07:33:31.295: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
May  6 07:33:31.295: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-1669" to be "running"
May  6 07:33:31.297: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.107025ms
May  6 07:33:31.297: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
May  6 07:33:31.297: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-1669" to be "running"
May  6 07:33:31.298: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.662601ms
May  6 07:33:31.298: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 05/06/23 07:33:31.298
May  6 07:33:31.306: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
May  6 07:33:31.308: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.943984ms
May  6 07:33:33.312: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005757335s
May  6 07:33:35.311: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004855886s
May  6 07:33:37.312: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006008161s
May  6 07:33:37.312: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 07:33:37.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1669" for this suite. 05/06/23 07:33:37.389
------------------------------
â€¢ [SLOW TEST] [69.300 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:32:28.096
    May  6 07:32:28.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename sched-preemption 05/06/23 07:32:28.097
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:32:29.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:32:29.117
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    May  6 07:32:29.133: INFO: Waiting up to 1m0s for all nodes to be ready
    May  6 07:33:29.155: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 05/06/23 07:33:29.158
    May  6 07:33:29.179: INFO: Created pod: pod0-0-sched-preemption-low-priority
    May  6 07:33:29.185: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    May  6 07:33:29.203: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    May  6 07:33:29.209: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    May  6 07:33:29.238: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    May  6 07:33:29.248: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    May  6 07:33:29.268: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    May  6 07:33:29.274: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 05/06/23 07:33:29.274
    May  6 07:33:29.274: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1669" to be "running"
    May  6 07:33:29.281: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.397819ms
    May  6 07:33:31.284: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.010717069s
    May  6 07:33:31.284: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    May  6 07:33:31.284: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1669" to be "running"
    May  6 07:33:31.286: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.904489ms
    May  6 07:33:31.286: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    May  6 07:33:31.286: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1669" to be "running"
    May  6 07:33:31.289: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.255155ms
    May  6 07:33:31.289: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    May  6 07:33:31.289: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1669" to be "running"
    May  6 07:33:31.291: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.811452ms
    May  6 07:33:31.291: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    May  6 07:33:31.291: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1669" to be "running"
    May  6 07:33:31.293: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.13659ms
    May  6 07:33:31.293: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    May  6 07:33:31.293: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1669" to be "running"
    May  6 07:33:31.295: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.880644ms
    May  6 07:33:31.295: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    May  6 07:33:31.295: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-1669" to be "running"
    May  6 07:33:31.297: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.107025ms
    May  6 07:33:31.297: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    May  6 07:33:31.297: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-1669" to be "running"
    May  6 07:33:31.298: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.662601ms
    May  6 07:33:31.298: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 05/06/23 07:33:31.298
    May  6 07:33:31.306: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    May  6 07:33:31.308: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.943984ms
    May  6 07:33:33.312: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005757335s
    May  6 07:33:35.311: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004855886s
    May  6 07:33:37.312: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006008161s
    May  6 07:33:37.312: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 07:33:37.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1669" for this suite. 05/06/23 07:33:37.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:33:37.396
May  6 07:33:37.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename webhook 05/06/23 07:33:37.397
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:33:38.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:33:38.415
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/06/23 07:33:38.43
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:33:38.652
STEP: Deploying the webhook pod 05/06/23 07:33:38.658
STEP: Wait for the deployment to be ready 05/06/23 07:33:38.667
May  6 07:33:38.674: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/06/23 07:33:40.682
STEP: Verifying the service has paired with the endpoint 05/06/23 07:33:40.695
May  6 07:33:41.695: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 05/06/23 07:33:41.698
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 05/06/23 07:33:41.7
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 05/06/23 07:33:41.7
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 05/06/23 07:33:41.7
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 05/06/23 07:33:41.701
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 05/06/23 07:33:41.701
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 05/06/23 07:33:41.702
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:33:41.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-739" for this suite. 05/06/23 07:33:41.743
STEP: Destroying namespace "webhook-739-markers" for this suite. 05/06/23 07:33:41.752
------------------------------
â€¢ [4.363 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:33:37.396
    May  6 07:33:37.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename webhook 05/06/23 07:33:37.397
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:33:38.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:33:38.415
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/06/23 07:33:38.43
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:33:38.652
    STEP: Deploying the webhook pod 05/06/23 07:33:38.658
    STEP: Wait for the deployment to be ready 05/06/23 07:33:38.667
    May  6 07:33:38.674: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/06/23 07:33:40.682
    STEP: Verifying the service has paired with the endpoint 05/06/23 07:33:40.695
    May  6 07:33:41.695: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 05/06/23 07:33:41.698
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 05/06/23 07:33:41.7
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 05/06/23 07:33:41.7
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 05/06/23 07:33:41.7
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 05/06/23 07:33:41.701
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 05/06/23 07:33:41.701
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 05/06/23 07:33:41.702
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:33:41.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-739" for this suite. 05/06/23 07:33:41.743
    STEP: Destroying namespace "webhook-739-markers" for this suite. 05/06/23 07:33:41.752
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:33:41.76
May  6 07:33:41.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename namespaces 05/06/23 07:33:41.76
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:33:41.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:33:41.777
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 05/06/23 07:33:41.779
STEP: patching the Namespace 05/06/23 07:33:41.79
STEP: get the Namespace and ensuring it has the label 05/06/23 07:33:41.796
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 07:33:41.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1059" for this suite. 05/06/23 07:33:41.801
STEP: Destroying namespace "nspatchtest-ef0868f9-ecbc-4f4e-942d-5b76524de237-901" for this suite. 05/06/23 07:33:41.805
------------------------------
â€¢ [0.051 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:33:41.76
    May  6 07:33:41.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename namespaces 05/06/23 07:33:41.76
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:33:41.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:33:41.777
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 05/06/23 07:33:41.779
    STEP: patching the Namespace 05/06/23 07:33:41.79
    STEP: get the Namespace and ensuring it has the label 05/06/23 07:33:41.796
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 07:33:41.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1059" for this suite. 05/06/23 07:33:41.801
    STEP: Destroying namespace "nspatchtest-ef0868f9-ecbc-4f4e-942d-5b76524de237-901" for this suite. 05/06/23 07:33:41.805
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:33:41.811
May  6 07:33:41.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename custom-resource-definition 05/06/23 07:33:41.812
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:33:41.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:33:41.827
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
May  6 07:33:41.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:33:42.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9036" for this suite. 05/06/23 07:33:42.374
------------------------------
â€¢ [0.574 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:33:41.811
    May  6 07:33:41.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename custom-resource-definition 05/06/23 07:33:41.812
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:33:41.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:33:41.827
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    May  6 07:33:41.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:33:42.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9036" for this suite. 05/06/23 07:33:42.374
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:33:42.385
May  6 07:33:42.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:33:42.386
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:33:42.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:33:42.404
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 05/06/23 07:33:42.406
May  6 07:33:42.413: INFO: Waiting up to 5m0s for pod "annotationupdate2b0ef02b-fe25-4cb9-afec-7210a677e1df" in namespace "projected-8491" to be "running and ready"
May  6 07:33:42.415: INFO: Pod "annotationupdate2b0ef02b-fe25-4cb9-afec-7210a677e1df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.113616ms
May  6 07:33:42.415: INFO: The phase of Pod annotationupdate2b0ef02b-fe25-4cb9-afec-7210a677e1df is Pending, waiting for it to be Running (with Ready = true)
May  6 07:33:44.420: INFO: Pod "annotationupdate2b0ef02b-fe25-4cb9-afec-7210a677e1df": Phase="Running", Reason="", readiness=true. Elapsed: 2.006366207s
May  6 07:33:44.420: INFO: The phase of Pod annotationupdate2b0ef02b-fe25-4cb9-afec-7210a677e1df is Running (Ready = true)
May  6 07:33:44.420: INFO: Pod "annotationupdate2b0ef02b-fe25-4cb9-afec-7210a677e1df" satisfied condition "running and ready"
May  6 07:33:44.942: INFO: Successfully updated pod "annotationupdate2b0ef02b-fe25-4cb9-afec-7210a677e1df"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  6 07:33:46.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8491" for this suite. 05/06/23 07:33:46.958
------------------------------
â€¢ [4.580 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:33:42.385
    May  6 07:33:42.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:33:42.386
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:33:42.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:33:42.404
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 05/06/23 07:33:42.406
    May  6 07:33:42.413: INFO: Waiting up to 5m0s for pod "annotationupdate2b0ef02b-fe25-4cb9-afec-7210a677e1df" in namespace "projected-8491" to be "running and ready"
    May  6 07:33:42.415: INFO: Pod "annotationupdate2b0ef02b-fe25-4cb9-afec-7210a677e1df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.113616ms
    May  6 07:33:42.415: INFO: The phase of Pod annotationupdate2b0ef02b-fe25-4cb9-afec-7210a677e1df is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:33:44.420: INFO: Pod "annotationupdate2b0ef02b-fe25-4cb9-afec-7210a677e1df": Phase="Running", Reason="", readiness=true. Elapsed: 2.006366207s
    May  6 07:33:44.420: INFO: The phase of Pod annotationupdate2b0ef02b-fe25-4cb9-afec-7210a677e1df is Running (Ready = true)
    May  6 07:33:44.420: INFO: Pod "annotationupdate2b0ef02b-fe25-4cb9-afec-7210a677e1df" satisfied condition "running and ready"
    May  6 07:33:44.942: INFO: Successfully updated pod "annotationupdate2b0ef02b-fe25-4cb9-afec-7210a677e1df"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  6 07:33:46.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8491" for this suite. 05/06/23 07:33:46.958
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:33:46.965
May  6 07:33:46.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename ingressclass 05/06/23 07:33:46.966
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:33:46.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:33:46.988
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 05/06/23 07:33:46.991
STEP: getting /apis/networking.k8s.io 05/06/23 07:33:46.993
STEP: getting /apis/networking.k8s.iov1 05/06/23 07:33:46.994
STEP: creating 05/06/23 07:33:46.995
STEP: getting 05/06/23 07:33:47.017
STEP: listing 05/06/23 07:33:47.022
STEP: watching 05/06/23 07:33:47.024
May  6 07:33:47.024: INFO: starting watch
STEP: patching 05/06/23 07:33:47.025
STEP: updating 05/06/23 07:33:47.03
May  6 07:33:47.036: INFO: waiting for watch events with expected annotations
May  6 07:33:47.036: INFO: saw patched and updated annotations
STEP: deleting 05/06/23 07:33:47.036
STEP: deleting a collection 05/06/23 07:33:47.052
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
May  6 07:33:47.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-9695" for this suite. 05/06/23 07:33:47.069
------------------------------
â€¢ [0.109 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:33:46.965
    May  6 07:33:46.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename ingressclass 05/06/23 07:33:46.966
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:33:46.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:33:46.988
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 05/06/23 07:33:46.991
    STEP: getting /apis/networking.k8s.io 05/06/23 07:33:46.993
    STEP: getting /apis/networking.k8s.iov1 05/06/23 07:33:46.994
    STEP: creating 05/06/23 07:33:46.995
    STEP: getting 05/06/23 07:33:47.017
    STEP: listing 05/06/23 07:33:47.022
    STEP: watching 05/06/23 07:33:47.024
    May  6 07:33:47.024: INFO: starting watch
    STEP: patching 05/06/23 07:33:47.025
    STEP: updating 05/06/23 07:33:47.03
    May  6 07:33:47.036: INFO: waiting for watch events with expected annotations
    May  6 07:33:47.036: INFO: saw patched and updated annotations
    STEP: deleting 05/06/23 07:33:47.036
    STEP: deleting a collection 05/06/23 07:33:47.052
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    May  6 07:33:47.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-9695" for this suite. 05/06/23 07:33:47.069
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:33:47.075
May  6 07:33:47.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename init-container 05/06/23 07:33:47.076
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:33:47.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:33:47.092
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 05/06/23 07:33:47.094
May  6 07:33:47.094: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May  6 07:33:52.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-6273" for this suite. 05/06/23 07:33:52.465
------------------------------
â€¢ [SLOW TEST] [5.396 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:33:47.075
    May  6 07:33:47.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename init-container 05/06/23 07:33:47.076
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:33:47.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:33:47.092
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 05/06/23 07:33:47.094
    May  6 07:33:47.094: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May  6 07:33:52.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-6273" for this suite. 05/06/23 07:33:52.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:33:52.471
May  6 07:33:52.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename dns 05/06/23 07:33:52.472
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:33:52.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:33:52.489
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 05/06/23 07:33:52.491
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7638.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7638.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 05/06/23 07:33:52.495
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7638.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7638.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 05/06/23 07:33:52.495
STEP: creating a pod to probe DNS 05/06/23 07:33:52.495
STEP: submitting the pod to kubernetes 05/06/23 07:33:52.495
May  6 07:33:52.503: INFO: Waiting up to 15m0s for pod "dns-test-26c6acc1-f3d3-4844-b82c-eabb63678287" in namespace "dns-7638" to be "running"
May  6 07:33:52.507: INFO: Pod "dns-test-26c6acc1-f3d3-4844-b82c-eabb63678287": Phase="Pending", Reason="", readiness=false. Elapsed: 3.02538ms
May  6 07:33:54.510: INFO: Pod "dns-test-26c6acc1-f3d3-4844-b82c-eabb63678287": Phase="Running", Reason="", readiness=true. Elapsed: 2.006849642s
May  6 07:33:54.510: INFO: Pod "dns-test-26c6acc1-f3d3-4844-b82c-eabb63678287" satisfied condition "running"
STEP: retrieving the pod 05/06/23 07:33:54.51
STEP: looking for the results for each expected name from probers 05/06/23 07:33:54.513
May  6 07:33:54.525: INFO: DNS probes using dns-7638/dns-test-26c6acc1-f3d3-4844-b82c-eabb63678287 succeeded

STEP: deleting the pod 05/06/23 07:33:54.525
STEP: deleting the test headless service 05/06/23 07:33:54.539
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May  6 07:33:54.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7638" for this suite. 05/06/23 07:33:54.556
------------------------------
â€¢ [2.091 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:33:52.471
    May  6 07:33:52.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename dns 05/06/23 07:33:52.472
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:33:52.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:33:52.489
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 05/06/23 07:33:52.491
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7638.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7638.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     05/06/23 07:33:52.495
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7638.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7638.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     05/06/23 07:33:52.495
    STEP: creating a pod to probe DNS 05/06/23 07:33:52.495
    STEP: submitting the pod to kubernetes 05/06/23 07:33:52.495
    May  6 07:33:52.503: INFO: Waiting up to 15m0s for pod "dns-test-26c6acc1-f3d3-4844-b82c-eabb63678287" in namespace "dns-7638" to be "running"
    May  6 07:33:52.507: INFO: Pod "dns-test-26c6acc1-f3d3-4844-b82c-eabb63678287": Phase="Pending", Reason="", readiness=false. Elapsed: 3.02538ms
    May  6 07:33:54.510: INFO: Pod "dns-test-26c6acc1-f3d3-4844-b82c-eabb63678287": Phase="Running", Reason="", readiness=true. Elapsed: 2.006849642s
    May  6 07:33:54.510: INFO: Pod "dns-test-26c6acc1-f3d3-4844-b82c-eabb63678287" satisfied condition "running"
    STEP: retrieving the pod 05/06/23 07:33:54.51
    STEP: looking for the results for each expected name from probers 05/06/23 07:33:54.513
    May  6 07:33:54.525: INFO: DNS probes using dns-7638/dns-test-26c6acc1-f3d3-4844-b82c-eabb63678287 succeeded

    STEP: deleting the pod 05/06/23 07:33:54.525
    STEP: deleting the test headless service 05/06/23 07:33:54.539
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May  6 07:33:54.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7638" for this suite. 05/06/23 07:33:54.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:33:54.563
May  6 07:33:54.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubelet-test 05/06/23 07:33:54.564
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:33:54.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:33:54.579
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May  6 07:33:58.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1067" for this suite. 05/06/23 07:33:58.595
------------------------------
â€¢ [4.038 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:33:54.563
    May  6 07:33:54.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubelet-test 05/06/23 07:33:54.564
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:33:54.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:33:54.579
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May  6 07:33:58.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1067" for this suite. 05/06/23 07:33:58.595
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:33:58.602
May  6 07:33:58.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename daemonsets 05/06/23 07:33:58.603
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:33:58.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:33:58.619
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 05/06/23 07:33:58.639
STEP: Check that daemon pods launch on every node of the cluster. 05/06/23 07:33:58.643
May  6 07:33:58.650: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 07:33:58.650: INFO: Node cncf-0 is running 0 daemon pod, expected 1
May  6 07:33:59.660: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  6 07:33:59.660: INFO: Node cncf-0 is running 0 daemon pod, expected 1
May  6 07:34:00.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
May  6 07:34:00.657: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 05/06/23 07:34:00.659
May  6 07:34:00.674: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  6 07:34:00.674: INFO: Node cncf-2 is running 0 daemon pod, expected 1
May  6 07:34:01.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  6 07:34:01.681: INFO: Node cncf-2 is running 0 daemon pod, expected 1
May  6 07:34:02.682: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  6 07:34:02.682: INFO: Node cncf-2 is running 0 daemon pod, expected 1
May  6 07:34:03.680: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
May  6 07:34:03.680: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/06/23 07:34:03.682
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4329, will wait for the garbage collector to delete the pods 05/06/23 07:34:03.682
May  6 07:34:03.741: INFO: Deleting DaemonSet.extensions daemon-set took: 5.871861ms
May  6 07:34:03.842: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.513731ms
May  6 07:34:06.745: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 07:34:06.745: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  6 07:34:06.747: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"155551"},"items":null}

May  6 07:34:06.750: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"155551"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 07:34:06.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4329" for this suite. 05/06/23 07:34:06.765
------------------------------
â€¢ [SLOW TEST] [8.204 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:33:58.602
    May  6 07:33:58.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename daemonsets 05/06/23 07:33:58.603
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:33:58.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:33:58.619
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 05/06/23 07:33:58.639
    STEP: Check that daemon pods launch on every node of the cluster. 05/06/23 07:33:58.643
    May  6 07:33:58.650: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 07:33:58.650: INFO: Node cncf-0 is running 0 daemon pod, expected 1
    May  6 07:33:59.660: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  6 07:33:59.660: INFO: Node cncf-0 is running 0 daemon pod, expected 1
    May  6 07:34:00.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    May  6 07:34:00.657: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 05/06/23 07:34:00.659
    May  6 07:34:00.674: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  6 07:34:00.674: INFO: Node cncf-2 is running 0 daemon pod, expected 1
    May  6 07:34:01.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  6 07:34:01.681: INFO: Node cncf-2 is running 0 daemon pod, expected 1
    May  6 07:34:02.682: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  6 07:34:02.682: INFO: Node cncf-2 is running 0 daemon pod, expected 1
    May  6 07:34:03.680: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    May  6 07:34:03.680: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/06/23 07:34:03.682
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4329, will wait for the garbage collector to delete the pods 05/06/23 07:34:03.682
    May  6 07:34:03.741: INFO: Deleting DaemonSet.extensions daemon-set took: 5.871861ms
    May  6 07:34:03.842: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.513731ms
    May  6 07:34:06.745: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 07:34:06.745: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  6 07:34:06.747: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"155551"},"items":null}

    May  6 07:34:06.750: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"155551"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 07:34:06.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4329" for this suite. 05/06/23 07:34:06.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:34:06.806
May  6 07:34:06.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename secrets 05/06/23 07:34:06.807
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:34:06.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:34:06.863
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-780686c4-922e-47c1-b337-34a611b64495 05/06/23 07:34:06.949
STEP: Creating secret with name s-test-opt-upd-27cebf07-9eae-4a45-8279-c8f9e644210c 05/06/23 07:34:07.029
STEP: Creating the pod 05/06/23 07:34:07.078
May  6 07:34:07.109: INFO: Waiting up to 5m0s for pod "pod-secrets-b4b56a38-395f-4bde-8c5d-eb74e7f1c64a" in namespace "secrets-9121" to be "running and ready"
May  6 07:34:07.119: INFO: Pod "pod-secrets-b4b56a38-395f-4bde-8c5d-eb74e7f1c64a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.57093ms
May  6 07:34:07.119: INFO: The phase of Pod pod-secrets-b4b56a38-395f-4bde-8c5d-eb74e7f1c64a is Pending, waiting for it to be Running (with Ready = true)
May  6 07:34:09.476: INFO: Pod "pod-secrets-b4b56a38-395f-4bde-8c5d-eb74e7f1c64a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.36642798s
May  6 07:34:09.476: INFO: The phase of Pod pod-secrets-b4b56a38-395f-4bde-8c5d-eb74e7f1c64a is Pending, waiting for it to be Running (with Ready = true)
May  6 07:34:11.122: INFO: Pod "pod-secrets-b4b56a38-395f-4bde-8c5d-eb74e7f1c64a": Phase="Running", Reason="", readiness=true. Elapsed: 4.012562591s
May  6 07:34:11.122: INFO: The phase of Pod pod-secrets-b4b56a38-395f-4bde-8c5d-eb74e7f1c64a is Running (Ready = true)
May  6 07:34:11.122: INFO: Pod "pod-secrets-b4b56a38-395f-4bde-8c5d-eb74e7f1c64a" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-780686c4-922e-47c1-b337-34a611b64495 05/06/23 07:34:11.137
STEP: Updating secret s-test-opt-upd-27cebf07-9eae-4a45-8279-c8f9e644210c 05/06/23 07:34:11.142
STEP: Creating secret with name s-test-opt-create-fa8dfbcf-0586-47d8-b4cf-3ff145c8470e 05/06/23 07:34:11.148
STEP: waiting to observe update in volume 05/06/23 07:34:11.151
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May  6 07:35:39.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9121" for this suite. 05/06/23 07:35:39.499
------------------------------
â€¢ [SLOW TEST] [92.698 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:34:06.806
    May  6 07:34:06.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename secrets 05/06/23 07:34:06.807
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:34:06.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:34:06.863
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-780686c4-922e-47c1-b337-34a611b64495 05/06/23 07:34:06.949
    STEP: Creating secret with name s-test-opt-upd-27cebf07-9eae-4a45-8279-c8f9e644210c 05/06/23 07:34:07.029
    STEP: Creating the pod 05/06/23 07:34:07.078
    May  6 07:34:07.109: INFO: Waiting up to 5m0s for pod "pod-secrets-b4b56a38-395f-4bde-8c5d-eb74e7f1c64a" in namespace "secrets-9121" to be "running and ready"
    May  6 07:34:07.119: INFO: Pod "pod-secrets-b4b56a38-395f-4bde-8c5d-eb74e7f1c64a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.57093ms
    May  6 07:34:07.119: INFO: The phase of Pod pod-secrets-b4b56a38-395f-4bde-8c5d-eb74e7f1c64a is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:34:09.476: INFO: Pod "pod-secrets-b4b56a38-395f-4bde-8c5d-eb74e7f1c64a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.36642798s
    May  6 07:34:09.476: INFO: The phase of Pod pod-secrets-b4b56a38-395f-4bde-8c5d-eb74e7f1c64a is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:34:11.122: INFO: Pod "pod-secrets-b4b56a38-395f-4bde-8c5d-eb74e7f1c64a": Phase="Running", Reason="", readiness=true. Elapsed: 4.012562591s
    May  6 07:34:11.122: INFO: The phase of Pod pod-secrets-b4b56a38-395f-4bde-8c5d-eb74e7f1c64a is Running (Ready = true)
    May  6 07:34:11.122: INFO: Pod "pod-secrets-b4b56a38-395f-4bde-8c5d-eb74e7f1c64a" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-780686c4-922e-47c1-b337-34a611b64495 05/06/23 07:34:11.137
    STEP: Updating secret s-test-opt-upd-27cebf07-9eae-4a45-8279-c8f9e644210c 05/06/23 07:34:11.142
    STEP: Creating secret with name s-test-opt-create-fa8dfbcf-0586-47d8-b4cf-3ff145c8470e 05/06/23 07:34:11.148
    STEP: waiting to observe update in volume 05/06/23 07:34:11.151
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May  6 07:35:39.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9121" for this suite. 05/06/23 07:35:39.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:35:39.504
May  6 07:35:39.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename var-expansion 05/06/23 07:35:39.505
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:35:40.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:35:40.523
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 05/06/23 07:35:40.524
May  6 07:35:40.532: INFO: Waiting up to 5m0s for pod "var-expansion-95795ed2-11cc-43c4-b84e-dec9f8671926" in namespace "var-expansion-9998" to be "Succeeded or Failed"
May  6 07:35:40.535: INFO: Pod "var-expansion-95795ed2-11cc-43c4-b84e-dec9f8671926": Phase="Pending", Reason="", readiness=false. Elapsed: 2.971778ms
May  6 07:35:42.538: INFO: Pod "var-expansion-95795ed2-11cc-43c4-b84e-dec9f8671926": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005891604s
May  6 07:35:44.539: INFO: Pod "var-expansion-95795ed2-11cc-43c4-b84e-dec9f8671926": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006629342s
STEP: Saw pod success 05/06/23 07:35:44.539
May  6 07:35:44.539: INFO: Pod "var-expansion-95795ed2-11cc-43c4-b84e-dec9f8671926" satisfied condition "Succeeded or Failed"
May  6 07:35:44.541: INFO: Trying to get logs from node cncf-2 pod var-expansion-95795ed2-11cc-43c4-b84e-dec9f8671926 container dapi-container: <nil>
STEP: delete the pod 05/06/23 07:35:44.552
May  6 07:35:44.563: INFO: Waiting for pod var-expansion-95795ed2-11cc-43c4-b84e-dec9f8671926 to disappear
May  6 07:35:44.565: INFO: Pod var-expansion-95795ed2-11cc-43c4-b84e-dec9f8671926 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May  6 07:35:44.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9998" for this suite. 05/06/23 07:35:44.567
------------------------------
â€¢ [SLOW TEST] [5.070 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:35:39.504
    May  6 07:35:39.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename var-expansion 05/06/23 07:35:39.505
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:35:40.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:35:40.523
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 05/06/23 07:35:40.524
    May  6 07:35:40.532: INFO: Waiting up to 5m0s for pod "var-expansion-95795ed2-11cc-43c4-b84e-dec9f8671926" in namespace "var-expansion-9998" to be "Succeeded or Failed"
    May  6 07:35:40.535: INFO: Pod "var-expansion-95795ed2-11cc-43c4-b84e-dec9f8671926": Phase="Pending", Reason="", readiness=false. Elapsed: 2.971778ms
    May  6 07:35:42.538: INFO: Pod "var-expansion-95795ed2-11cc-43c4-b84e-dec9f8671926": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005891604s
    May  6 07:35:44.539: INFO: Pod "var-expansion-95795ed2-11cc-43c4-b84e-dec9f8671926": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006629342s
    STEP: Saw pod success 05/06/23 07:35:44.539
    May  6 07:35:44.539: INFO: Pod "var-expansion-95795ed2-11cc-43c4-b84e-dec9f8671926" satisfied condition "Succeeded or Failed"
    May  6 07:35:44.541: INFO: Trying to get logs from node cncf-2 pod var-expansion-95795ed2-11cc-43c4-b84e-dec9f8671926 container dapi-container: <nil>
    STEP: delete the pod 05/06/23 07:35:44.552
    May  6 07:35:44.563: INFO: Waiting for pod var-expansion-95795ed2-11cc-43c4-b84e-dec9f8671926 to disappear
    May  6 07:35:44.565: INFO: Pod var-expansion-95795ed2-11cc-43c4-b84e-dec9f8671926 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May  6 07:35:44.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9998" for this suite. 05/06/23 07:35:44.567
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:35:44.575
May  6 07:35:44.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename custom-resource-definition 05/06/23 07:35:44.576
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:35:45.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:35:45.596
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
May  6 07:35:45.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:35:46.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3385" for this suite. 05/06/23 07:35:46.621
------------------------------
â€¢ [2.053 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:35:44.575
    May  6 07:35:44.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename custom-resource-definition 05/06/23 07:35:44.576
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:35:45.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:35:45.596
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    May  6 07:35:45.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:35:46.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3385" for this suite. 05/06/23 07:35:46.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:35:46.629
May  6 07:35:46.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename resourcequota 05/06/23 07:35:46.63
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:35:47.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:35:47.65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 05/06/23 07:35:47.652
STEP: Creating a ResourceQuota 05/06/23 07:35:52.655
STEP: Ensuring resource quota status is calculated 05/06/23 07:35:52.661
STEP: Creating a ReplicationController 05/06/23 07:35:54.664
STEP: Ensuring resource quota status captures replication controller creation 05/06/23 07:35:54.675
STEP: Deleting a ReplicationController 05/06/23 07:35:56.679
STEP: Ensuring resource quota status released usage 05/06/23 07:35:56.69
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  6 07:35:58.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-224" for this suite. 05/06/23 07:35:58.698
------------------------------
â€¢ [SLOW TEST] [12.075 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:35:46.629
    May  6 07:35:46.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename resourcequota 05/06/23 07:35:46.63
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:35:47.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:35:47.65
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 05/06/23 07:35:47.652
    STEP: Creating a ResourceQuota 05/06/23 07:35:52.655
    STEP: Ensuring resource quota status is calculated 05/06/23 07:35:52.661
    STEP: Creating a ReplicationController 05/06/23 07:35:54.664
    STEP: Ensuring resource quota status captures replication controller creation 05/06/23 07:35:54.675
    STEP: Deleting a ReplicationController 05/06/23 07:35:56.679
    STEP: Ensuring resource quota status released usage 05/06/23 07:35:56.69
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  6 07:35:58.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-224" for this suite. 05/06/23 07:35:58.698
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:35:58.705
May  6 07:35:58.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename downward-api 05/06/23 07:35:58.706
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:35:59.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:35:59.723
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 05/06/23 07:35:59.725
May  6 07:35:59.733: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fb12ca57-f329-4378-b345-b46077b214b9" in namespace "downward-api-9475" to be "Succeeded or Failed"
May  6 07:35:59.735: INFO: Pod "downwardapi-volume-fb12ca57-f329-4378-b345-b46077b214b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.356117ms
May  6 07:36:01.739: INFO: Pod "downwardapi-volume-fb12ca57-f329-4378-b345-b46077b214b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006102384s
May  6 07:36:03.738: INFO: Pod "downwardapi-volume-fb12ca57-f329-4378-b345-b46077b214b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00550121s
STEP: Saw pod success 05/06/23 07:36:03.738
May  6 07:36:03.738: INFO: Pod "downwardapi-volume-fb12ca57-f329-4378-b345-b46077b214b9" satisfied condition "Succeeded or Failed"
May  6 07:36:03.744: INFO: Trying to get logs from node cncf-3 pod downwardapi-volume-fb12ca57-f329-4378-b345-b46077b214b9 container client-container: <nil>
STEP: delete the pod 05/06/23 07:36:03.756
May  6 07:36:03.766: INFO: Waiting for pod downwardapi-volume-fb12ca57-f329-4378-b345-b46077b214b9 to disappear
May  6 07:36:03.769: INFO: Pod downwardapi-volume-fb12ca57-f329-4378-b345-b46077b214b9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  6 07:36:03.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9475" for this suite. 05/06/23 07:36:03.772
------------------------------
â€¢ [SLOW TEST] [5.072 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:35:58.705
    May  6 07:35:58.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename downward-api 05/06/23 07:35:58.706
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:35:59.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:35:59.723
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 05/06/23 07:35:59.725
    May  6 07:35:59.733: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fb12ca57-f329-4378-b345-b46077b214b9" in namespace "downward-api-9475" to be "Succeeded or Failed"
    May  6 07:35:59.735: INFO: Pod "downwardapi-volume-fb12ca57-f329-4378-b345-b46077b214b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.356117ms
    May  6 07:36:01.739: INFO: Pod "downwardapi-volume-fb12ca57-f329-4378-b345-b46077b214b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006102384s
    May  6 07:36:03.738: INFO: Pod "downwardapi-volume-fb12ca57-f329-4378-b345-b46077b214b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00550121s
    STEP: Saw pod success 05/06/23 07:36:03.738
    May  6 07:36:03.738: INFO: Pod "downwardapi-volume-fb12ca57-f329-4378-b345-b46077b214b9" satisfied condition "Succeeded or Failed"
    May  6 07:36:03.744: INFO: Trying to get logs from node cncf-3 pod downwardapi-volume-fb12ca57-f329-4378-b345-b46077b214b9 container client-container: <nil>
    STEP: delete the pod 05/06/23 07:36:03.756
    May  6 07:36:03.766: INFO: Waiting for pod downwardapi-volume-fb12ca57-f329-4378-b345-b46077b214b9 to disappear
    May  6 07:36:03.769: INFO: Pod downwardapi-volume-fb12ca57-f329-4378-b345-b46077b214b9 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  6 07:36:03.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9475" for this suite. 05/06/23 07:36:03.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:36:03.779
May  6 07:36:03.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename statefulset 05/06/23 07:36:03.78
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:36:04.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:36:04.802
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9399 05/06/23 07:36:04.804
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-9399 05/06/23 07:36:04.808
May  6 07:36:04.818: INFO: Found 0 stateful pods, waiting for 1
May  6 07:36:14.822: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 05/06/23 07:36:14.827
STEP: updating a scale subresource 05/06/23 07:36:14.829
STEP: verifying the statefulset Spec.Replicas was modified 05/06/23 07:36:14.836
STEP: Patch a scale subresource 05/06/23 07:36:14.837
STEP: verifying the statefulset Spec.Replicas was modified 05/06/23 07:36:14.846
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May  6 07:36:14.849: INFO: Deleting all statefulset in ns statefulset-9399
May  6 07:36:14.855: INFO: Scaling statefulset ss to 0
May  6 07:36:24.875: INFO: Waiting for statefulset status.replicas updated to 0
May  6 07:36:24.877: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May  6 07:36:24.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9399" for this suite. 05/06/23 07:36:24.89
------------------------------
â€¢ [SLOW TEST] [21.117 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:36:03.779
    May  6 07:36:03.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename statefulset 05/06/23 07:36:03.78
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:36:04.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:36:04.802
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9399 05/06/23 07:36:04.804
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-9399 05/06/23 07:36:04.808
    May  6 07:36:04.818: INFO: Found 0 stateful pods, waiting for 1
    May  6 07:36:14.822: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 05/06/23 07:36:14.827
    STEP: updating a scale subresource 05/06/23 07:36:14.829
    STEP: verifying the statefulset Spec.Replicas was modified 05/06/23 07:36:14.836
    STEP: Patch a scale subresource 05/06/23 07:36:14.837
    STEP: verifying the statefulset Spec.Replicas was modified 05/06/23 07:36:14.846
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May  6 07:36:14.849: INFO: Deleting all statefulset in ns statefulset-9399
    May  6 07:36:14.855: INFO: Scaling statefulset ss to 0
    May  6 07:36:24.875: INFO: Waiting for statefulset status.replicas updated to 0
    May  6 07:36:24.877: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May  6 07:36:24.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9399" for this suite. 05/06/23 07:36:24.89
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:36:24.897
May  6 07:36:24.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubectl 05/06/23 07:36:24.897
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:36:25.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:36:25.914
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/06/23 07:36:25.916
May  6 07:36:25.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-9171 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May  6 07:36:25.971: INFO: stderr: ""
May  6 07:36:25.972: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 05/06/23 07:36:25.972
May  6 07:36:25.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-9171 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
May  6 07:36:26.743: INFO: stderr: ""
May  6 07:36:26.743: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/06/23 07:36:26.743
May  6 07:36:26.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-9171 delete pods e2e-test-httpd-pod'
May  6 07:36:29.364: INFO: stderr: ""
May  6 07:36:29.364: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  6 07:36:29.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9171" for this suite. 05/06/23 07:36:29.367
------------------------------
â€¢ [4.477 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:36:24.897
    May  6 07:36:24.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubectl 05/06/23 07:36:24.897
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:36:25.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:36:25.914
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/06/23 07:36:25.916
    May  6 07:36:25.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-9171 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    May  6 07:36:25.971: INFO: stderr: ""
    May  6 07:36:25.972: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 05/06/23 07:36:25.972
    May  6 07:36:25.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-9171 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    May  6 07:36:26.743: INFO: stderr: ""
    May  6 07:36:26.743: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/06/23 07:36:26.743
    May  6 07:36:26.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-9171 delete pods e2e-test-httpd-pod'
    May  6 07:36:29.364: INFO: stderr: ""
    May  6 07:36:29.364: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  6 07:36:29.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9171" for this suite. 05/06/23 07:36:29.367
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:36:29.374
May  6 07:36:29.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename sched-preemption 05/06/23 07:36:29.374
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:36:30.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:36:30.391
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
May  6 07:36:30.405: INFO: Waiting up to 1m0s for all nodes to be ready
May  6 07:37:30.426: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:37:30.429
May  6 07:37:30.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename sched-preemption-path 05/06/23 07:37:30.43
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:37:31.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:37:31.448
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 05/06/23 07:37:31.45
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/06/23 07:37:31.45
May  6 07:37:31.457: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-5791" to be "running"
May  6 07:37:31.465: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.604532ms
May  6 07:37:33.468: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011019455s
May  6 07:37:33.468: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/06/23 07:37:33.47
May  6 07:37:33.482: INFO: found a healthy node: cncf-0
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
May  6 07:37:39.543: INFO: pods created so far: [1 1 1]
May  6 07:37:39.543: INFO: length of pods created so far: 3
May  6 07:37:41.553: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
May  6 07:37:48.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 07:37:48.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-5791" for this suite. 05/06/23 07:37:48.651
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1978" for this suite. 05/06/23 07:37:48.657
------------------------------
â€¢ [SLOW TEST] [79.288 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:36:29.374
    May  6 07:36:29.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename sched-preemption 05/06/23 07:36:29.374
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:36:30.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:36:30.391
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    May  6 07:36:30.405: INFO: Waiting up to 1m0s for all nodes to be ready
    May  6 07:37:30.426: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:37:30.429
    May  6 07:37:30.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename sched-preemption-path 05/06/23 07:37:30.43
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:37:31.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:37:31.448
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 05/06/23 07:37:31.45
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/06/23 07:37:31.45
    May  6 07:37:31.457: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-5791" to be "running"
    May  6 07:37:31.465: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.604532ms
    May  6 07:37:33.468: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011019455s
    May  6 07:37:33.468: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/06/23 07:37:33.47
    May  6 07:37:33.482: INFO: found a healthy node: cncf-0
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    May  6 07:37:39.543: INFO: pods created so far: [1 1 1]
    May  6 07:37:39.543: INFO: length of pods created so far: 3
    May  6 07:37:41.553: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    May  6 07:37:48.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 07:37:48.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-5791" for this suite. 05/06/23 07:37:48.651
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1978" for this suite. 05/06/23 07:37:48.657
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:37:48.662
May  6 07:37:48.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubectl 05/06/23 07:37:48.663
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:37:49.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:37:49.681
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 05/06/23 07:37:49.682
May  6 07:37:49.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-9031 api-versions'
May  6 07:37:49.743: INFO: stderr: ""
May  6 07:37:49.743: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncluster.rafay.dev/v2\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noperator.tigera.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nsystem.k8smgmt.io/v3\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  6 07:37:49.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9031" for this suite. 05/06/23 07:37:49.745
------------------------------
â€¢ [1.090 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:37:48.662
    May  6 07:37:48.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubectl 05/06/23 07:37:48.663
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:37:49.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:37:49.681
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 05/06/23 07:37:49.682
    May  6 07:37:49.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-9031 api-versions'
    May  6 07:37:49.743: INFO: stderr: ""
    May  6 07:37:49.743: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncluster.rafay.dev/v2\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noperator.tigera.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nsystem.k8smgmt.io/v3\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  6 07:37:49.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9031" for this suite. 05/06/23 07:37:49.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:37:49.754
May  6 07:37:49.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename configmap 05/06/23 07:37:49.754
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:37:50.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:37:50.773
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-4357811a-268c-4fbf-af35-fe9efb136f4a 05/06/23 07:37:50.775
STEP: Creating a pod to test consume configMaps 05/06/23 07:37:50.78
May  6 07:37:50.786: INFO: Waiting up to 5m0s for pod "pod-configmaps-c2afba56-ec06-4fbc-8b95-8f3a40732cf0" in namespace "configmap-2966" to be "Succeeded or Failed"
May  6 07:37:50.788: INFO: Pod "pod-configmaps-c2afba56-ec06-4fbc-8b95-8f3a40732cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.899439ms
May  6 07:37:52.791: INFO: Pod "pod-configmaps-c2afba56-ec06-4fbc-8b95-8f3a40732cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005456471s
May  6 07:37:54.791: INFO: Pod "pod-configmaps-c2afba56-ec06-4fbc-8b95-8f3a40732cf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005435345s
STEP: Saw pod success 05/06/23 07:37:54.791
May  6 07:37:54.792: INFO: Pod "pod-configmaps-c2afba56-ec06-4fbc-8b95-8f3a40732cf0" satisfied condition "Succeeded or Failed"
May  6 07:37:54.794: INFO: Trying to get logs from node cncf-2 pod pod-configmaps-c2afba56-ec06-4fbc-8b95-8f3a40732cf0 container agnhost-container: <nil>
STEP: delete the pod 05/06/23 07:37:54.801
May  6 07:37:54.812: INFO: Waiting for pod pod-configmaps-c2afba56-ec06-4fbc-8b95-8f3a40732cf0 to disappear
May  6 07:37:54.814: INFO: Pod pod-configmaps-c2afba56-ec06-4fbc-8b95-8f3a40732cf0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  6 07:37:54.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2966" for this suite. 05/06/23 07:37:54.82
------------------------------
â€¢ [SLOW TEST] [5.071 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:37:49.754
    May  6 07:37:49.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename configmap 05/06/23 07:37:49.754
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:37:50.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:37:50.773
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-4357811a-268c-4fbf-af35-fe9efb136f4a 05/06/23 07:37:50.775
    STEP: Creating a pod to test consume configMaps 05/06/23 07:37:50.78
    May  6 07:37:50.786: INFO: Waiting up to 5m0s for pod "pod-configmaps-c2afba56-ec06-4fbc-8b95-8f3a40732cf0" in namespace "configmap-2966" to be "Succeeded or Failed"
    May  6 07:37:50.788: INFO: Pod "pod-configmaps-c2afba56-ec06-4fbc-8b95-8f3a40732cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.899439ms
    May  6 07:37:52.791: INFO: Pod "pod-configmaps-c2afba56-ec06-4fbc-8b95-8f3a40732cf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005456471s
    May  6 07:37:54.791: INFO: Pod "pod-configmaps-c2afba56-ec06-4fbc-8b95-8f3a40732cf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005435345s
    STEP: Saw pod success 05/06/23 07:37:54.791
    May  6 07:37:54.792: INFO: Pod "pod-configmaps-c2afba56-ec06-4fbc-8b95-8f3a40732cf0" satisfied condition "Succeeded or Failed"
    May  6 07:37:54.794: INFO: Trying to get logs from node cncf-2 pod pod-configmaps-c2afba56-ec06-4fbc-8b95-8f3a40732cf0 container agnhost-container: <nil>
    STEP: delete the pod 05/06/23 07:37:54.801
    May  6 07:37:54.812: INFO: Waiting for pod pod-configmaps-c2afba56-ec06-4fbc-8b95-8f3a40732cf0 to disappear
    May  6 07:37:54.814: INFO: Pod pod-configmaps-c2afba56-ec06-4fbc-8b95-8f3a40732cf0 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  6 07:37:54.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2966" for this suite. 05/06/23 07:37:54.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:37:54.827
May  6 07:37:54.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename disruption 05/06/23 07:37:54.827
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:37:55.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:37:55.847
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 05/06/23 07:37:55.854
STEP: Updating PodDisruptionBudget status 05/06/23 07:37:57.858
STEP: Waiting for all pods to be running 05/06/23 07:37:57.865
May  6 07:37:57.868: INFO: running pods: 0 < 1
STEP: locating a running pod 05/06/23 07:37:59.872
STEP: Waiting for the pdb to be processed 05/06/23 07:37:59.881
STEP: Patching PodDisruptionBudget status 05/06/23 07:37:59.888
STEP: Waiting for the pdb to be processed 05/06/23 07:37:59.895
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May  6 07:37:59.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8116" for this suite. 05/06/23 07:37:59.901
------------------------------
â€¢ [SLOW TEST] [5.079 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:37:54.827
    May  6 07:37:54.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename disruption 05/06/23 07:37:54.827
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:37:55.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:37:55.847
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 05/06/23 07:37:55.854
    STEP: Updating PodDisruptionBudget status 05/06/23 07:37:57.858
    STEP: Waiting for all pods to be running 05/06/23 07:37:57.865
    May  6 07:37:57.868: INFO: running pods: 0 < 1
    STEP: locating a running pod 05/06/23 07:37:59.872
    STEP: Waiting for the pdb to be processed 05/06/23 07:37:59.881
    STEP: Patching PodDisruptionBudget status 05/06/23 07:37:59.888
    STEP: Waiting for the pdb to be processed 05/06/23 07:37:59.895
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May  6 07:37:59.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8116" for this suite. 05/06/23 07:37:59.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:37:59.907
May  6 07:37:59.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename crd-webhook 05/06/23 07:37:59.907
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:38:00.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:38:00.925
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 05/06/23 07:38:00.927
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/06/23 07:38:01.21
STEP: Deploying the custom resource conversion webhook pod 05/06/23 07:38:01.216
STEP: Wait for the deployment to be ready 05/06/23 07:38:01.227
May  6 07:38:01.232: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/06/23 07:38:03.24
STEP: Verifying the service has paired with the endpoint 05/06/23 07:38:03.253
May  6 07:38:04.253: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
May  6 07:38:04.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Creating a v1 custom resource 05/06/23 07:38:06.823
STEP: v2 custom resource should be converted 05/06/23 07:38:06.828
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:38:07.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-9456" for this suite. 05/06/23 07:38:07.388
------------------------------
â€¢ [SLOW TEST] [7.490 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:37:59.907
    May  6 07:37:59.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename crd-webhook 05/06/23 07:37:59.907
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:38:00.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:38:00.925
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 05/06/23 07:38:00.927
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/06/23 07:38:01.21
    STEP: Deploying the custom resource conversion webhook pod 05/06/23 07:38:01.216
    STEP: Wait for the deployment to be ready 05/06/23 07:38:01.227
    May  6 07:38:01.232: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/06/23 07:38:03.24
    STEP: Verifying the service has paired with the endpoint 05/06/23 07:38:03.253
    May  6 07:38:04.253: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    May  6 07:38:04.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Creating a v1 custom resource 05/06/23 07:38:06.823
    STEP: v2 custom resource should be converted 05/06/23 07:38:06.828
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:38:07.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-9456" for this suite. 05/06/23 07:38:07.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:38:07.399
May  6 07:38:07.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename init-container 05/06/23 07:38:07.4
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:38:08.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:38:08.42
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 05/06/23 07:38:08.422
May  6 07:38:08.422: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May  6 07:38:11.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-767" for this suite. 05/06/23 07:38:11.567
------------------------------
â€¢ [4.173 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:38:07.399
    May  6 07:38:07.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename init-container 05/06/23 07:38:07.4
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:38:08.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:38:08.42
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 05/06/23 07:38:08.422
    May  6 07:38:08.422: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May  6 07:38:11.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-767" for this suite. 05/06/23 07:38:11.567
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:38:11.573
May  6 07:38:11.573: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename statefulset 05/06/23 07:38:11.574
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:38:12.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:38:12.593
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8871 05/06/23 07:38:12.595
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
May  6 07:38:12.613: INFO: Found 0 stateful pods, waiting for 1
May  6 07:38:22.616: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 05/06/23 07:38:22.621
W0506 07:38:22.632148      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May  6 07:38:22.638: INFO: Found 1 stateful pods, waiting for 2
May  6 07:38:32.642: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  6 07:38:32.642: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 05/06/23 07:38:32.647
STEP: Delete all of the StatefulSets 05/06/23 07:38:32.649
STEP: Verify that StatefulSets have been deleted 05/06/23 07:38:32.655
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May  6 07:38:32.658: INFO: Deleting all statefulset in ns statefulset-8871
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May  6 07:38:32.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8871" for this suite. 05/06/23 07:38:32.666
------------------------------
â€¢ [SLOW TEST] [21.102 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:38:11.573
    May  6 07:38:11.573: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename statefulset 05/06/23 07:38:11.574
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:38:12.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:38:12.593
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8871 05/06/23 07:38:12.595
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    May  6 07:38:12.613: INFO: Found 0 stateful pods, waiting for 1
    May  6 07:38:22.616: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 05/06/23 07:38:22.621
    W0506 07:38:22.632148      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May  6 07:38:22.638: INFO: Found 1 stateful pods, waiting for 2
    May  6 07:38:32.642: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May  6 07:38:32.642: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 05/06/23 07:38:32.647
    STEP: Delete all of the StatefulSets 05/06/23 07:38:32.649
    STEP: Verify that StatefulSets have been deleted 05/06/23 07:38:32.655
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May  6 07:38:32.658: INFO: Deleting all statefulset in ns statefulset-8871
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May  6 07:38:32.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8871" for this suite. 05/06/23 07:38:32.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:38:32.676
May  6 07:38:32.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename ingress 05/06/23 07:38:32.676
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:38:33.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:38:33.694
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 05/06/23 07:38:33.696
STEP: getting /apis/networking.k8s.io 05/06/23 07:38:33.698
STEP: getting /apis/networking.k8s.iov1 05/06/23 07:38:33.699
STEP: creating 05/06/23 07:38:33.7
STEP: getting 05/06/23 07:38:33.718
STEP: listing 05/06/23 07:38:33.72
STEP: watching 05/06/23 07:38:33.722
May  6 07:38:33.722: INFO: starting watch
STEP: cluster-wide listing 05/06/23 07:38:33.723
STEP: cluster-wide watching 05/06/23 07:38:33.725
May  6 07:38:33.725: INFO: starting watch
STEP: patching 05/06/23 07:38:33.726
STEP: updating 05/06/23 07:38:33.735
May  6 07:38:33.742: INFO: waiting for watch events with expected annotations
May  6 07:38:33.742: INFO: saw patched and updated annotations
STEP: patching /status 05/06/23 07:38:33.743
STEP: updating /status 05/06/23 07:38:33.749
STEP: get /status 05/06/23 07:38:33.756
STEP: deleting 05/06/23 07:38:33.758
STEP: deleting a collection 05/06/23 07:38:33.767
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
May  6 07:38:33.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-5606" for this suite. 05/06/23 07:38:33.781
------------------------------
â€¢ [1.111 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:38:32.676
    May  6 07:38:32.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename ingress 05/06/23 07:38:32.676
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:38:33.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:38:33.694
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 05/06/23 07:38:33.696
    STEP: getting /apis/networking.k8s.io 05/06/23 07:38:33.698
    STEP: getting /apis/networking.k8s.iov1 05/06/23 07:38:33.699
    STEP: creating 05/06/23 07:38:33.7
    STEP: getting 05/06/23 07:38:33.718
    STEP: listing 05/06/23 07:38:33.72
    STEP: watching 05/06/23 07:38:33.722
    May  6 07:38:33.722: INFO: starting watch
    STEP: cluster-wide listing 05/06/23 07:38:33.723
    STEP: cluster-wide watching 05/06/23 07:38:33.725
    May  6 07:38:33.725: INFO: starting watch
    STEP: patching 05/06/23 07:38:33.726
    STEP: updating 05/06/23 07:38:33.735
    May  6 07:38:33.742: INFO: waiting for watch events with expected annotations
    May  6 07:38:33.742: INFO: saw patched and updated annotations
    STEP: patching /status 05/06/23 07:38:33.743
    STEP: updating /status 05/06/23 07:38:33.749
    STEP: get /status 05/06/23 07:38:33.756
    STEP: deleting 05/06/23 07:38:33.758
    STEP: deleting a collection 05/06/23 07:38:33.767
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    May  6 07:38:33.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-5606" for this suite. 05/06/23 07:38:33.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:38:33.788
May  6 07:38:33.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename pod-network-test 05/06/23 07:38:33.788
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:38:34.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:38:34.805
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-2264 05/06/23 07:38:34.807
STEP: creating a selector 05/06/23 07:38:34.807
STEP: Creating the service pods in kubernetes 05/06/23 07:38:34.807
May  6 07:38:34.807: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  6 07:38:34.844: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2264" to be "running and ready"
May  6 07:38:34.853: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.095286ms
May  6 07:38:34.853: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  6 07:38:36.856: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011321251s
May  6 07:38:36.856: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 07:38:38.858: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013538589s
May  6 07:38:38.858: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 07:38:40.857: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01209534s
May  6 07:38:40.857: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 07:38:42.856: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.0119459s
May  6 07:38:42.856: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 07:38:44.856: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011850303s
May  6 07:38:44.856: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 07:38:46.856: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011353691s
May  6 07:38:46.856: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  6 07:38:46.856: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  6 07:38:46.858: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2264" to be "running and ready"
May  6 07:38:46.861: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.97804ms
May  6 07:38:46.861: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  6 07:38:46.861: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  6 07:38:46.863: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2264" to be "running and ready"
May  6 07:38:46.865: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.427604ms
May  6 07:38:46.865: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  6 07:38:46.865: INFO: Pod "netserver-2" satisfied condition "running and ready"
May  6 07:38:46.867: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-2264" to be "running and ready"
May  6 07:38:46.869: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.251128ms
May  6 07:38:46.869: INFO: The phase of Pod netserver-3 is Running (Ready = true)
May  6 07:38:46.869: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 05/06/23 07:38:46.871
May  6 07:38:46.883: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2264" to be "running"
May  6 07:38:46.886: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.821111ms
May  6 07:38:48.890: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006998252s
May  6 07:38:48.890: INFO: Pod "test-container-pod" satisfied condition "running"
May  6 07:38:48.892: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2264" to be "running"
May  6 07:38:48.895: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.340647ms
May  6 07:38:48.895: INFO: Pod "host-test-container-pod" satisfied condition "running"
May  6 07:38:48.897: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
May  6 07:38:48.897: INFO: Going to poll 10.244.174.181 on port 8083 at least 0 times, with a maximum of 46 tries before failing
May  6 07:38:48.899: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.174.181:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 07:38:48.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:38:48.900: INFO: ExecWithOptions: Clientset creation
May  6 07:38:48.900: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.174.181%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  6 07:38:48.967: INFO: Found all 1 expected endpoints: [netserver-0]
May  6 07:38:48.967: INFO: Going to poll 10.244.21.75 on port 8083 at least 0 times, with a maximum of 46 tries before failing
May  6 07:38:48.970: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.21.75:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 07:38:48.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:38:48.970: INFO: ExecWithOptions: Clientset creation
May  6 07:38:48.970: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.21.75%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  6 07:38:49.019: INFO: Found all 1 expected endpoints: [netserver-1]
May  6 07:38:49.019: INFO: Going to poll 10.244.20.167 on port 8083 at least 0 times, with a maximum of 46 tries before failing
May  6 07:38:49.021: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.20.167:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 07:38:49.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:38:49.021: INFO: ExecWithOptions: Clientset creation
May  6 07:38:49.021: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.20.167%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  6 07:38:49.055: INFO: Found all 1 expected endpoints: [netserver-2]
May  6 07:38:49.055: INFO: Going to poll 10.244.245.112 on port 8083 at least 0 times, with a maximum of 46 tries before failing
May  6 07:38:49.057: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.245.112:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 07:38:49.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:38:49.058: INFO: ExecWithOptions: Clientset creation
May  6 07:38:49.058: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.245.112%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  6 07:38:49.110: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
May  6 07:38:49.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2264" for this suite. 05/06/23 07:38:49.115
------------------------------
â€¢ [SLOW TEST] [15.333 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:38:33.788
    May  6 07:38:33.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename pod-network-test 05/06/23 07:38:33.788
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:38:34.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:38:34.805
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-2264 05/06/23 07:38:34.807
    STEP: creating a selector 05/06/23 07:38:34.807
    STEP: Creating the service pods in kubernetes 05/06/23 07:38:34.807
    May  6 07:38:34.807: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  6 07:38:34.844: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2264" to be "running and ready"
    May  6 07:38:34.853: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.095286ms
    May  6 07:38:34.853: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:38:36.856: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011321251s
    May  6 07:38:36.856: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 07:38:38.858: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013538589s
    May  6 07:38:38.858: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 07:38:40.857: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01209534s
    May  6 07:38:40.857: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 07:38:42.856: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.0119459s
    May  6 07:38:42.856: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 07:38:44.856: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011850303s
    May  6 07:38:44.856: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 07:38:46.856: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011353691s
    May  6 07:38:46.856: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  6 07:38:46.856: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  6 07:38:46.858: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2264" to be "running and ready"
    May  6 07:38:46.861: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.97804ms
    May  6 07:38:46.861: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  6 07:38:46.861: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  6 07:38:46.863: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2264" to be "running and ready"
    May  6 07:38:46.865: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.427604ms
    May  6 07:38:46.865: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  6 07:38:46.865: INFO: Pod "netserver-2" satisfied condition "running and ready"
    May  6 07:38:46.867: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-2264" to be "running and ready"
    May  6 07:38:46.869: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.251128ms
    May  6 07:38:46.869: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    May  6 07:38:46.869: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 05/06/23 07:38:46.871
    May  6 07:38:46.883: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2264" to be "running"
    May  6 07:38:46.886: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.821111ms
    May  6 07:38:48.890: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006998252s
    May  6 07:38:48.890: INFO: Pod "test-container-pod" satisfied condition "running"
    May  6 07:38:48.892: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2264" to be "running"
    May  6 07:38:48.895: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.340647ms
    May  6 07:38:48.895: INFO: Pod "host-test-container-pod" satisfied condition "running"
    May  6 07:38:48.897: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    May  6 07:38:48.897: INFO: Going to poll 10.244.174.181 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    May  6 07:38:48.899: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.174.181:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 07:38:48.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:38:48.900: INFO: ExecWithOptions: Clientset creation
    May  6 07:38:48.900: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.174.181%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  6 07:38:48.967: INFO: Found all 1 expected endpoints: [netserver-0]
    May  6 07:38:48.967: INFO: Going to poll 10.244.21.75 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    May  6 07:38:48.970: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.21.75:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 07:38:48.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:38:48.970: INFO: ExecWithOptions: Clientset creation
    May  6 07:38:48.970: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.21.75%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  6 07:38:49.019: INFO: Found all 1 expected endpoints: [netserver-1]
    May  6 07:38:49.019: INFO: Going to poll 10.244.20.167 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    May  6 07:38:49.021: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.20.167:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 07:38:49.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:38:49.021: INFO: ExecWithOptions: Clientset creation
    May  6 07:38:49.021: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.20.167%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  6 07:38:49.055: INFO: Found all 1 expected endpoints: [netserver-2]
    May  6 07:38:49.055: INFO: Going to poll 10.244.245.112 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    May  6 07:38:49.057: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.245.112:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2264 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 07:38:49.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:38:49.058: INFO: ExecWithOptions: Clientset creation
    May  6 07:38:49.058: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2264/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.245.112%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  6 07:38:49.110: INFO: Found all 1 expected endpoints: [netserver-3]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    May  6 07:38:49.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2264" for this suite. 05/06/23 07:38:49.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:38:49.121
May  6 07:38:49.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename dns 05/06/23 07:38:49.121
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:38:50.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:38:50.14
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-401.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-401.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 05/06/23 07:38:50.142
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-401.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-401.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 05/06/23 07:38:50.142
STEP: creating a pod to probe /etc/hosts 05/06/23 07:38:50.142
STEP: submitting the pod to kubernetes 05/06/23 07:38:50.142
May  6 07:38:50.150: INFO: Waiting up to 15m0s for pod "dns-test-a15b06cd-2e26-4487-b721-b4d611945b8b" in namespace "dns-401" to be "running"
May  6 07:38:50.153: INFO: Pod "dns-test-a15b06cd-2e26-4487-b721-b4d611945b8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.662812ms
May  6 07:38:52.155: INFO: Pod "dns-test-a15b06cd-2e26-4487-b721-b4d611945b8b": Phase="Running", Reason="", readiness=true. Elapsed: 2.005460132s
May  6 07:38:52.156: INFO: Pod "dns-test-a15b06cd-2e26-4487-b721-b4d611945b8b" satisfied condition "running"
STEP: retrieving the pod 05/06/23 07:38:52.156
STEP: looking for the results for each expected name from probers 05/06/23 07:38:52.158
May  6 07:38:52.173: INFO: DNS probes using dns-401/dns-test-a15b06cd-2e26-4487-b721-b4d611945b8b succeeded

STEP: deleting the pod 05/06/23 07:38:52.173
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May  6 07:38:52.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-401" for this suite. 05/06/23 07:38:52.186
------------------------------
â€¢ [3.071 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:38:49.121
    May  6 07:38:49.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename dns 05/06/23 07:38:49.121
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:38:50.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:38:50.14
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-401.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-401.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     05/06/23 07:38:50.142
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-401.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-401.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     05/06/23 07:38:50.142
    STEP: creating a pod to probe /etc/hosts 05/06/23 07:38:50.142
    STEP: submitting the pod to kubernetes 05/06/23 07:38:50.142
    May  6 07:38:50.150: INFO: Waiting up to 15m0s for pod "dns-test-a15b06cd-2e26-4487-b721-b4d611945b8b" in namespace "dns-401" to be "running"
    May  6 07:38:50.153: INFO: Pod "dns-test-a15b06cd-2e26-4487-b721-b4d611945b8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.662812ms
    May  6 07:38:52.155: INFO: Pod "dns-test-a15b06cd-2e26-4487-b721-b4d611945b8b": Phase="Running", Reason="", readiness=true. Elapsed: 2.005460132s
    May  6 07:38:52.156: INFO: Pod "dns-test-a15b06cd-2e26-4487-b721-b4d611945b8b" satisfied condition "running"
    STEP: retrieving the pod 05/06/23 07:38:52.156
    STEP: looking for the results for each expected name from probers 05/06/23 07:38:52.158
    May  6 07:38:52.173: INFO: DNS probes using dns-401/dns-test-a15b06cd-2e26-4487-b721-b4d611945b8b succeeded

    STEP: deleting the pod 05/06/23 07:38:52.173
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May  6 07:38:52.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-401" for this suite. 05/06/23 07:38:52.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:38:52.192
May  6 07:38:52.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename conformance-tests 05/06/23 07:38:52.193
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:38:53.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:38:53.215
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 05/06/23 07:38:53.217
May  6 07:38:53.217: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
May  6 07:38:53.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-8011" for this suite. 05/06/23 07:38:53.224
------------------------------
â€¢ [1.039 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:38:52.192
    May  6 07:38:52.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename conformance-tests 05/06/23 07:38:52.193
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:38:53.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:38:53.215
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 05/06/23 07:38:53.217
    May  6 07:38:53.217: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    May  6 07:38:53.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-8011" for this suite. 05/06/23 07:38:53.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:38:53.232
May  6 07:38:53.232: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename containers 05/06/23 07:38:53.232
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:38:54.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:38:54.252
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 05/06/23 07:38:54.254
May  6 07:38:54.265: INFO: Waiting up to 5m0s for pod "client-containers-cbe9d089-89d3-4dfe-89fb-5bed62a75483" in namespace "containers-7811" to be "Succeeded or Failed"
May  6 07:38:54.270: INFO: Pod "client-containers-cbe9d089-89d3-4dfe-89fb-5bed62a75483": Phase="Pending", Reason="", readiness=false. Elapsed: 5.442242ms
May  6 07:38:56.274: INFO: Pod "client-containers-cbe9d089-89d3-4dfe-89fb-5bed62a75483": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008958318s
May  6 07:38:58.275: INFO: Pod "client-containers-cbe9d089-89d3-4dfe-89fb-5bed62a75483": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010552843s
STEP: Saw pod success 05/06/23 07:38:58.275
May  6 07:38:58.275: INFO: Pod "client-containers-cbe9d089-89d3-4dfe-89fb-5bed62a75483" satisfied condition "Succeeded or Failed"
May  6 07:38:58.277: INFO: Trying to get logs from node cncf-3 pod client-containers-cbe9d089-89d3-4dfe-89fb-5bed62a75483 container agnhost-container: <nil>
STEP: delete the pod 05/06/23 07:38:58.288
May  6 07:38:58.298: INFO: Waiting for pod client-containers-cbe9d089-89d3-4dfe-89fb-5bed62a75483 to disappear
May  6 07:38:58.301: INFO: Pod client-containers-cbe9d089-89d3-4dfe-89fb-5bed62a75483 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
May  6 07:38:58.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-7811" for this suite. 05/06/23 07:38:58.303
------------------------------
â€¢ [SLOW TEST] [5.079 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:38:53.232
    May  6 07:38:53.232: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename containers 05/06/23 07:38:53.232
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:38:54.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:38:54.252
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 05/06/23 07:38:54.254
    May  6 07:38:54.265: INFO: Waiting up to 5m0s for pod "client-containers-cbe9d089-89d3-4dfe-89fb-5bed62a75483" in namespace "containers-7811" to be "Succeeded or Failed"
    May  6 07:38:54.270: INFO: Pod "client-containers-cbe9d089-89d3-4dfe-89fb-5bed62a75483": Phase="Pending", Reason="", readiness=false. Elapsed: 5.442242ms
    May  6 07:38:56.274: INFO: Pod "client-containers-cbe9d089-89d3-4dfe-89fb-5bed62a75483": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008958318s
    May  6 07:38:58.275: INFO: Pod "client-containers-cbe9d089-89d3-4dfe-89fb-5bed62a75483": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010552843s
    STEP: Saw pod success 05/06/23 07:38:58.275
    May  6 07:38:58.275: INFO: Pod "client-containers-cbe9d089-89d3-4dfe-89fb-5bed62a75483" satisfied condition "Succeeded or Failed"
    May  6 07:38:58.277: INFO: Trying to get logs from node cncf-3 pod client-containers-cbe9d089-89d3-4dfe-89fb-5bed62a75483 container agnhost-container: <nil>
    STEP: delete the pod 05/06/23 07:38:58.288
    May  6 07:38:58.298: INFO: Waiting for pod client-containers-cbe9d089-89d3-4dfe-89fb-5bed62a75483 to disappear
    May  6 07:38:58.301: INFO: Pod client-containers-cbe9d089-89d3-4dfe-89fb-5bed62a75483 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    May  6 07:38:58.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-7811" for this suite. 05/06/23 07:38:58.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:38:58.311
May  6 07:38:58.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename disruption 05/06/23 07:38:58.312
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:38:59.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:38:59.328
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 05/06/23 07:38:59.33
STEP: Waiting for the pdb to be processed 05/06/23 07:38:59.334
STEP: First trying to evict a pod which shouldn't be evictable 05/06/23 07:39:01.345
STEP: Waiting for all pods to be running 05/06/23 07:39:01.345
May  6 07:39:01.347: INFO: pods: 0 < 3
STEP: locating a running pod 05/06/23 07:39:03.352
STEP: Updating the pdb to allow a pod to be evicted 05/06/23 07:39:03.364
STEP: Waiting for the pdb to be processed 05/06/23 07:39:03.371
STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/06/23 07:39:05.376
STEP: Waiting for all pods to be running 05/06/23 07:39:05.376
STEP: Waiting for the pdb to observed all healthy pods 05/06/23 07:39:05.379
STEP: Patching the pdb to disallow a pod to be evicted 05/06/23 07:39:05.403
STEP: Waiting for the pdb to be processed 05/06/23 07:39:05.419
STEP: Waiting for all pods to be running 05/06/23 07:39:07.426
STEP: locating a running pod 05/06/23 07:39:07.429
STEP: Deleting the pdb to allow a pod to be evicted 05/06/23 07:39:07.436
STEP: Waiting for the pdb to be deleted 05/06/23 07:39:07.443
STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/06/23 07:39:07.444
STEP: Waiting for all pods to be running 05/06/23 07:39:07.444
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May  6 07:39:07.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1428" for this suite. 05/06/23 07:39:07.463
------------------------------
â€¢ [SLOW TEST] [9.159 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:38:58.311
    May  6 07:38:58.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename disruption 05/06/23 07:38:58.312
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:38:59.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:38:59.328
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 05/06/23 07:38:59.33
    STEP: Waiting for the pdb to be processed 05/06/23 07:38:59.334
    STEP: First trying to evict a pod which shouldn't be evictable 05/06/23 07:39:01.345
    STEP: Waiting for all pods to be running 05/06/23 07:39:01.345
    May  6 07:39:01.347: INFO: pods: 0 < 3
    STEP: locating a running pod 05/06/23 07:39:03.352
    STEP: Updating the pdb to allow a pod to be evicted 05/06/23 07:39:03.364
    STEP: Waiting for the pdb to be processed 05/06/23 07:39:03.371
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/06/23 07:39:05.376
    STEP: Waiting for all pods to be running 05/06/23 07:39:05.376
    STEP: Waiting for the pdb to observed all healthy pods 05/06/23 07:39:05.379
    STEP: Patching the pdb to disallow a pod to be evicted 05/06/23 07:39:05.403
    STEP: Waiting for the pdb to be processed 05/06/23 07:39:05.419
    STEP: Waiting for all pods to be running 05/06/23 07:39:07.426
    STEP: locating a running pod 05/06/23 07:39:07.429
    STEP: Deleting the pdb to allow a pod to be evicted 05/06/23 07:39:07.436
    STEP: Waiting for the pdb to be deleted 05/06/23 07:39:07.443
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/06/23 07:39:07.444
    STEP: Waiting for all pods to be running 05/06/23 07:39:07.444
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May  6 07:39:07.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1428" for this suite. 05/06/23 07:39:07.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:39:07.471
May  6 07:39:07.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename namespaces 05/06/23 07:39:07.471
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:07.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:07.489
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 05/06/23 07:39:07.49
May  6 07:39:07.492: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 05/06/23 07:39:07.493
May  6 07:39:07.498: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 05/06/23 07:39:07.498
May  6 07:39:07.505: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 07:39:07.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6504" for this suite. 05/06/23 07:39:07.508
------------------------------
â€¢ [0.044 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:39:07.471
    May  6 07:39:07.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename namespaces 05/06/23 07:39:07.471
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:07.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:07.489
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 05/06/23 07:39:07.49
    May  6 07:39:07.492: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 05/06/23 07:39:07.493
    May  6 07:39:07.498: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 05/06/23 07:39:07.498
    May  6 07:39:07.505: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 07:39:07.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6504" for this suite. 05/06/23 07:39:07.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:39:07.516
May  6 07:39:07.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename emptydir 05/06/23 07:39:07.517
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:07.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:07.537
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 05/06/23 07:39:07.539
May  6 07:39:07.547: INFO: Waiting up to 5m0s for pod "pod-8abc9a52-3fca-4548-8b25-4dbf61bef499" in namespace "emptydir-8232" to be "Succeeded or Failed"
May  6 07:39:07.549: INFO: Pod "pod-8abc9a52-3fca-4548-8b25-4dbf61bef499": Phase="Pending", Reason="", readiness=false. Elapsed: 1.890463ms
May  6 07:39:09.552: INFO: Pod "pod-8abc9a52-3fca-4548-8b25-4dbf61bef499": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005081001s
May  6 07:39:11.553: INFO: Pod "pod-8abc9a52-3fca-4548-8b25-4dbf61bef499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006470188s
STEP: Saw pod success 05/06/23 07:39:11.553
May  6 07:39:11.553: INFO: Pod "pod-8abc9a52-3fca-4548-8b25-4dbf61bef499" satisfied condition "Succeeded or Failed"
May  6 07:39:11.556: INFO: Trying to get logs from node cncf-2 pod pod-8abc9a52-3fca-4548-8b25-4dbf61bef499 container test-container: <nil>
STEP: delete the pod 05/06/23 07:39:11.56
May  6 07:39:11.570: INFO: Waiting for pod pod-8abc9a52-3fca-4548-8b25-4dbf61bef499 to disappear
May  6 07:39:11.573: INFO: Pod pod-8abc9a52-3fca-4548-8b25-4dbf61bef499 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  6 07:39:11.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8232" for this suite. 05/06/23 07:39:11.576
------------------------------
â€¢ [4.065 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:39:07.516
    May  6 07:39:07.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename emptydir 05/06/23 07:39:07.517
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:07.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:07.537
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 05/06/23 07:39:07.539
    May  6 07:39:07.547: INFO: Waiting up to 5m0s for pod "pod-8abc9a52-3fca-4548-8b25-4dbf61bef499" in namespace "emptydir-8232" to be "Succeeded or Failed"
    May  6 07:39:07.549: INFO: Pod "pod-8abc9a52-3fca-4548-8b25-4dbf61bef499": Phase="Pending", Reason="", readiness=false. Elapsed: 1.890463ms
    May  6 07:39:09.552: INFO: Pod "pod-8abc9a52-3fca-4548-8b25-4dbf61bef499": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005081001s
    May  6 07:39:11.553: INFO: Pod "pod-8abc9a52-3fca-4548-8b25-4dbf61bef499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006470188s
    STEP: Saw pod success 05/06/23 07:39:11.553
    May  6 07:39:11.553: INFO: Pod "pod-8abc9a52-3fca-4548-8b25-4dbf61bef499" satisfied condition "Succeeded or Failed"
    May  6 07:39:11.556: INFO: Trying to get logs from node cncf-2 pod pod-8abc9a52-3fca-4548-8b25-4dbf61bef499 container test-container: <nil>
    STEP: delete the pod 05/06/23 07:39:11.56
    May  6 07:39:11.570: INFO: Waiting for pod pod-8abc9a52-3fca-4548-8b25-4dbf61bef499 to disappear
    May  6 07:39:11.573: INFO: Pod pod-8abc9a52-3fca-4548-8b25-4dbf61bef499 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  6 07:39:11.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8232" for this suite. 05/06/23 07:39:11.576
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:39:11.582
May  6 07:39:11.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubectl 05/06/23 07:39:11.582
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:11.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:11.598
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 05/06/23 07:39:11.6
May  6 07:39:11.600: INFO: namespace kubectl-7807
May  6 07:39:11.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7807 create -f -'
May  6 07:39:12.340: INFO: stderr: ""
May  6 07:39:12.340: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/06/23 07:39:12.34
May  6 07:39:13.344: INFO: Selector matched 1 pods for map[app:agnhost]
May  6 07:39:13.344: INFO: Found 0 / 1
May  6 07:39:14.343: INFO: Selector matched 1 pods for map[app:agnhost]
May  6 07:39:14.343: INFO: Found 1 / 1
May  6 07:39:14.344: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  6 07:39:14.345: INFO: Selector matched 1 pods for map[app:agnhost]
May  6 07:39:14.345: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  6 07:39:14.345: INFO: wait on agnhost-primary startup in kubectl-7807 
May  6 07:39:14.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7807 logs agnhost-primary-jd7xz agnhost-primary'
May  6 07:39:14.413: INFO: stderr: ""
May  6 07:39:14.413: INFO: stdout: "Paused\n"
STEP: exposing RC 05/06/23 07:39:14.413
May  6 07:39:14.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7807 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May  6 07:39:14.476: INFO: stderr: ""
May  6 07:39:14.476: INFO: stdout: "service/rm2 exposed\n"
May  6 07:39:14.480: INFO: Service rm2 in namespace kubectl-7807 found.
STEP: exposing service 05/06/23 07:39:16.485
May  6 07:39:16.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7807 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May  6 07:39:16.550: INFO: stderr: ""
May  6 07:39:16.550: INFO: stdout: "service/rm3 exposed\n"
May  6 07:39:16.555: INFO: Service rm3 in namespace kubectl-7807 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  6 07:39:18.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7807" for this suite. 05/06/23 07:39:18.563
------------------------------
â€¢ [SLOW TEST] [6.988 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:39:11.582
    May  6 07:39:11.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubectl 05/06/23 07:39:11.582
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:11.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:11.598
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 05/06/23 07:39:11.6
    May  6 07:39:11.600: INFO: namespace kubectl-7807
    May  6 07:39:11.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7807 create -f -'
    May  6 07:39:12.340: INFO: stderr: ""
    May  6 07:39:12.340: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/06/23 07:39:12.34
    May  6 07:39:13.344: INFO: Selector matched 1 pods for map[app:agnhost]
    May  6 07:39:13.344: INFO: Found 0 / 1
    May  6 07:39:14.343: INFO: Selector matched 1 pods for map[app:agnhost]
    May  6 07:39:14.343: INFO: Found 1 / 1
    May  6 07:39:14.344: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    May  6 07:39:14.345: INFO: Selector matched 1 pods for map[app:agnhost]
    May  6 07:39:14.345: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May  6 07:39:14.345: INFO: wait on agnhost-primary startup in kubectl-7807 
    May  6 07:39:14.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7807 logs agnhost-primary-jd7xz agnhost-primary'
    May  6 07:39:14.413: INFO: stderr: ""
    May  6 07:39:14.413: INFO: stdout: "Paused\n"
    STEP: exposing RC 05/06/23 07:39:14.413
    May  6 07:39:14.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7807 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    May  6 07:39:14.476: INFO: stderr: ""
    May  6 07:39:14.476: INFO: stdout: "service/rm2 exposed\n"
    May  6 07:39:14.480: INFO: Service rm2 in namespace kubectl-7807 found.
    STEP: exposing service 05/06/23 07:39:16.485
    May  6 07:39:16.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7807 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    May  6 07:39:16.550: INFO: stderr: ""
    May  6 07:39:16.550: INFO: stdout: "service/rm3 exposed\n"
    May  6 07:39:16.555: INFO: Service rm3 in namespace kubectl-7807 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  6 07:39:18.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7807" for this suite. 05/06/23 07:39:18.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:39:18.57
May  6 07:39:18.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename emptydir 05/06/23 07:39:18.571
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:18.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:18.588
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 05/06/23 07:39:18.59
May  6 07:39:18.598: INFO: Waiting up to 5m0s for pod "pod-b0b3976c-e9f4-4315-8967-e66b14c70691" in namespace "emptydir-7247" to be "Succeeded or Failed"
May  6 07:39:18.600: INFO: Pod "pod-b0b3976c-e9f4-4315-8967-e66b14c70691": Phase="Pending", Reason="", readiness=false. Elapsed: 2.17324ms
May  6 07:39:20.603: INFO: Pod "pod-b0b3976c-e9f4-4315-8967-e66b14c70691": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005309707s
May  6 07:39:22.603: INFO: Pod "pod-b0b3976c-e9f4-4315-8967-e66b14c70691": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005741293s
STEP: Saw pod success 05/06/23 07:39:22.604
May  6 07:39:22.604: INFO: Pod "pod-b0b3976c-e9f4-4315-8967-e66b14c70691" satisfied condition "Succeeded or Failed"
May  6 07:39:22.606: INFO: Trying to get logs from node cncf-2 pod pod-b0b3976c-e9f4-4315-8967-e66b14c70691 container test-container: <nil>
STEP: delete the pod 05/06/23 07:39:22.61
May  6 07:39:22.621: INFO: Waiting for pod pod-b0b3976c-e9f4-4315-8967-e66b14c70691 to disappear
May  6 07:39:22.624: INFO: Pod pod-b0b3976c-e9f4-4315-8967-e66b14c70691 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  6 07:39:22.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7247" for this suite. 05/06/23 07:39:22.627
------------------------------
â€¢ [4.063 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:39:18.57
    May  6 07:39:18.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename emptydir 05/06/23 07:39:18.571
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:18.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:18.588
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 05/06/23 07:39:18.59
    May  6 07:39:18.598: INFO: Waiting up to 5m0s for pod "pod-b0b3976c-e9f4-4315-8967-e66b14c70691" in namespace "emptydir-7247" to be "Succeeded or Failed"
    May  6 07:39:18.600: INFO: Pod "pod-b0b3976c-e9f4-4315-8967-e66b14c70691": Phase="Pending", Reason="", readiness=false. Elapsed: 2.17324ms
    May  6 07:39:20.603: INFO: Pod "pod-b0b3976c-e9f4-4315-8967-e66b14c70691": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005309707s
    May  6 07:39:22.603: INFO: Pod "pod-b0b3976c-e9f4-4315-8967-e66b14c70691": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005741293s
    STEP: Saw pod success 05/06/23 07:39:22.604
    May  6 07:39:22.604: INFO: Pod "pod-b0b3976c-e9f4-4315-8967-e66b14c70691" satisfied condition "Succeeded or Failed"
    May  6 07:39:22.606: INFO: Trying to get logs from node cncf-2 pod pod-b0b3976c-e9f4-4315-8967-e66b14c70691 container test-container: <nil>
    STEP: delete the pod 05/06/23 07:39:22.61
    May  6 07:39:22.621: INFO: Waiting for pod pod-b0b3976c-e9f4-4315-8967-e66b14c70691 to disappear
    May  6 07:39:22.624: INFO: Pod pod-b0b3976c-e9f4-4315-8967-e66b14c70691 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  6 07:39:22.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7247" for this suite. 05/06/23 07:39:22.627
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:39:22.634
May  6 07:39:22.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename replicaset 05/06/23 07:39:22.634
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:22.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:22.651
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 05/06/23 07:39:22.653
May  6 07:39:22.661: INFO: Pod name sample-pod: Found 0 pods out of 1
May  6 07:39:27.665: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/06/23 07:39:27.665
STEP: getting scale subresource 05/06/23 07:39:27.666
STEP: updating a scale subresource 05/06/23 07:39:27.668
STEP: verifying the replicaset Spec.Replicas was modified 05/06/23 07:39:27.673
STEP: Patch a scale subresource 05/06/23 07:39:27.675
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May  6 07:39:27.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4912" for this suite. 05/06/23 07:39:27.691
------------------------------
â€¢ [SLOW TEST] [5.068 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:39:22.634
    May  6 07:39:22.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename replicaset 05/06/23 07:39:22.634
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:22.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:22.651
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 05/06/23 07:39:22.653
    May  6 07:39:22.661: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  6 07:39:27.665: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/06/23 07:39:27.665
    STEP: getting scale subresource 05/06/23 07:39:27.666
    STEP: updating a scale subresource 05/06/23 07:39:27.668
    STEP: verifying the replicaset Spec.Replicas was modified 05/06/23 07:39:27.673
    STEP: Patch a scale subresource 05/06/23 07:39:27.675
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May  6 07:39:27.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4912" for this suite. 05/06/23 07:39:27.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:39:27.702
May  6 07:39:27.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename resourcequota 05/06/23 07:39:27.702
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:27.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:27.725
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 05/06/23 07:39:27.727
STEP: Creating a ResourceQuota 05/06/23 07:39:32.73
STEP: Ensuring resource quota status is calculated 05/06/23 07:39:32.736
STEP: Creating a Pod that fits quota 05/06/23 07:39:34.739
STEP: Ensuring ResourceQuota status captures the pod usage 05/06/23 07:39:34.753
STEP: Not allowing a pod to be created that exceeds remaining quota 05/06/23 07:39:36.756
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 05/06/23 07:39:36.759
STEP: Ensuring a pod cannot update its resource requirements 05/06/23 07:39:36.761
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 05/06/23 07:39:36.765
STEP: Deleting the pod 05/06/23 07:39:38.769
STEP: Ensuring resource quota status released the pod usage 05/06/23 07:39:38.78
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  6 07:39:40.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3861" for this suite. 05/06/23 07:39:40.787
------------------------------
â€¢ [SLOW TEST] [13.097 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:39:27.702
    May  6 07:39:27.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename resourcequota 05/06/23 07:39:27.702
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:27.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:27.725
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 05/06/23 07:39:27.727
    STEP: Creating a ResourceQuota 05/06/23 07:39:32.73
    STEP: Ensuring resource quota status is calculated 05/06/23 07:39:32.736
    STEP: Creating a Pod that fits quota 05/06/23 07:39:34.739
    STEP: Ensuring ResourceQuota status captures the pod usage 05/06/23 07:39:34.753
    STEP: Not allowing a pod to be created that exceeds remaining quota 05/06/23 07:39:36.756
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 05/06/23 07:39:36.759
    STEP: Ensuring a pod cannot update its resource requirements 05/06/23 07:39:36.761
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 05/06/23 07:39:36.765
    STEP: Deleting the pod 05/06/23 07:39:38.769
    STEP: Ensuring resource quota status released the pod usage 05/06/23 07:39:38.78
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  6 07:39:40.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3861" for this suite. 05/06/23 07:39:40.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:39:40.8
May  6 07:39:40.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:39:40.801
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:40.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:40.821
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-4a0bb2c2-7fcc-4207-b32b-a08cf26f097d 05/06/23 07:39:40.826
STEP: Creating the pod 05/06/23 07:39:40.831
May  6 07:39:40.840: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-af74c5df-efb9-44bb-b54f-c9c4f4dba120" in namespace "projected-670" to be "running and ready"
May  6 07:39:40.846: INFO: Pod "pod-projected-configmaps-af74c5df-efb9-44bb-b54f-c9c4f4dba120": Phase="Pending", Reason="", readiness=false. Elapsed: 5.757552ms
May  6 07:39:40.846: INFO: The phase of Pod pod-projected-configmaps-af74c5df-efb9-44bb-b54f-c9c4f4dba120 is Pending, waiting for it to be Running (with Ready = true)
May  6 07:39:42.849: INFO: Pod "pod-projected-configmaps-af74c5df-efb9-44bb-b54f-c9c4f4dba120": Phase="Running", Reason="", readiness=true. Elapsed: 2.008692035s
May  6 07:39:42.849: INFO: The phase of Pod pod-projected-configmaps-af74c5df-efb9-44bb-b54f-c9c4f4dba120 is Running (Ready = true)
May  6 07:39:42.849: INFO: Pod "pod-projected-configmaps-af74c5df-efb9-44bb-b54f-c9c4f4dba120" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-4a0bb2c2-7fcc-4207-b32b-a08cf26f097d 05/06/23 07:39:42.855
STEP: waiting to observe update in volume 05/06/23 07:39:42.859
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May  6 07:39:44.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-670" for this suite. 05/06/23 07:39:44.872
------------------------------
â€¢ [4.077 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:39:40.8
    May  6 07:39:40.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:39:40.801
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:40.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:40.821
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-4a0bb2c2-7fcc-4207-b32b-a08cf26f097d 05/06/23 07:39:40.826
    STEP: Creating the pod 05/06/23 07:39:40.831
    May  6 07:39:40.840: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-af74c5df-efb9-44bb-b54f-c9c4f4dba120" in namespace "projected-670" to be "running and ready"
    May  6 07:39:40.846: INFO: Pod "pod-projected-configmaps-af74c5df-efb9-44bb-b54f-c9c4f4dba120": Phase="Pending", Reason="", readiness=false. Elapsed: 5.757552ms
    May  6 07:39:40.846: INFO: The phase of Pod pod-projected-configmaps-af74c5df-efb9-44bb-b54f-c9c4f4dba120 is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:39:42.849: INFO: Pod "pod-projected-configmaps-af74c5df-efb9-44bb-b54f-c9c4f4dba120": Phase="Running", Reason="", readiness=true. Elapsed: 2.008692035s
    May  6 07:39:42.849: INFO: The phase of Pod pod-projected-configmaps-af74c5df-efb9-44bb-b54f-c9c4f4dba120 is Running (Ready = true)
    May  6 07:39:42.849: INFO: Pod "pod-projected-configmaps-af74c5df-efb9-44bb-b54f-c9c4f4dba120" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-4a0bb2c2-7fcc-4207-b32b-a08cf26f097d 05/06/23 07:39:42.855
    STEP: waiting to observe update in volume 05/06/23 07:39:42.859
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May  6 07:39:44.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-670" for this suite. 05/06/23 07:39:44.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:39:44.878
May  6 07:39:44.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubectl 05/06/23 07:39:44.879
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:44.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:44.894
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 05/06/23 07:39:44.896
May  6 07:39:44.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-2073 cluster-info'
May  6 07:39:44.943: INFO: stderr: ""
May  6 07:39:44.943: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  6 07:39:44.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2073" for this suite. 05/06/23 07:39:44.946
------------------------------
â€¢ [0.073 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:39:44.878
    May  6 07:39:44.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubectl 05/06/23 07:39:44.879
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:44.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:44.894
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 05/06/23 07:39:44.896
    May  6 07:39:44.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-2073 cluster-info'
    May  6 07:39:44.943: INFO: stderr: ""
    May  6 07:39:44.943: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  6 07:39:44.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2073" for this suite. 05/06/23 07:39:44.946
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:39:44.951
May  6 07:39:44.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubectl 05/06/23 07:39:44.952
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:44.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:44.968
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
May  6 07:39:44.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7258 version'
May  6 07:39:45.012: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
May  6 07:39:45.012: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:40:17Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:33:12Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  6 07:39:45.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7258" for this suite. 05/06/23 07:39:45.015
------------------------------
â€¢ [0.071 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:39:44.951
    May  6 07:39:44.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubectl 05/06/23 07:39:44.952
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:44.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:44.968
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    May  6 07:39:44.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7258 version'
    May  6 07:39:45.012: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    May  6 07:39:45.012: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:40:17Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:33:12Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  6 07:39:45.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7258" for this suite. 05/06/23 07:39:45.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:39:45.022
May  6 07:39:45.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename gc 05/06/23 07:39:45.023
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:45.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:45.038
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 05/06/23 07:39:45.04
STEP: Wait for the Deployment to create new ReplicaSet 05/06/23 07:39:45.045
STEP: delete the deployment 05/06/23 07:39:45.55
STEP: wait for all rs to be garbage collected 05/06/23 07:39:45.556
STEP: expected 0 rs, got 1 rs 05/06/23 07:39:45.56
STEP: expected 0 pods, got 2 pods 05/06/23 07:39:45.563
STEP: Gathering metrics 05/06/23 07:39:46.07
May  6 07:39:46.078: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-2" in namespace "kube-system" to be "running and ready"
May  6 07:39:46.080: INFO: Pod "kube-controller-manager-cncf-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.19342ms
May  6 07:39:46.080: INFO: The phase of Pod kube-controller-manager-cncf-2 is Running (Ready = true)
May  6 07:39:46.080: INFO: Pod "kube-controller-manager-cncf-2" satisfied condition "running and ready"
May  6 07:39:46.118: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May  6 07:39:46.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-240" for this suite. 05/06/23 07:39:46.122
------------------------------
â€¢ [1.107 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:39:45.022
    May  6 07:39:45.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename gc 05/06/23 07:39:45.023
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:45.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:45.038
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 05/06/23 07:39:45.04
    STEP: Wait for the Deployment to create new ReplicaSet 05/06/23 07:39:45.045
    STEP: delete the deployment 05/06/23 07:39:45.55
    STEP: wait for all rs to be garbage collected 05/06/23 07:39:45.556
    STEP: expected 0 rs, got 1 rs 05/06/23 07:39:45.56
    STEP: expected 0 pods, got 2 pods 05/06/23 07:39:45.563
    STEP: Gathering metrics 05/06/23 07:39:46.07
    May  6 07:39:46.078: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-2" in namespace "kube-system" to be "running and ready"
    May  6 07:39:46.080: INFO: Pod "kube-controller-manager-cncf-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.19342ms
    May  6 07:39:46.080: INFO: The phase of Pod kube-controller-manager-cncf-2 is Running (Ready = true)
    May  6 07:39:46.080: INFO: Pod "kube-controller-manager-cncf-2" satisfied condition "running and ready"
    May  6 07:39:46.118: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May  6 07:39:46.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-240" for this suite. 05/06/23 07:39:46.122
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:39:46.13
May  6 07:39:46.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename security-context 05/06/23 07:39:46.131
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:46.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:46.148
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/06/23 07:39:46.15
May  6 07:39:46.158: INFO: Waiting up to 5m0s for pod "security-context-117a5295-bc59-4aa8-b3ed-b86606c43ecf" in namespace "security-context-5211" to be "Succeeded or Failed"
May  6 07:39:46.163: INFO: Pod "security-context-117a5295-bc59-4aa8-b3ed-b86606c43ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.84125ms
May  6 07:39:48.167: INFO: Pod "security-context-117a5295-bc59-4aa8-b3ed-b86606c43ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008776997s
May  6 07:39:50.170: INFO: Pod "security-context-117a5295-bc59-4aa8-b3ed-b86606c43ecf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012603465s
STEP: Saw pod success 05/06/23 07:39:50.17
May  6 07:39:50.170: INFO: Pod "security-context-117a5295-bc59-4aa8-b3ed-b86606c43ecf" satisfied condition "Succeeded or Failed"
May  6 07:39:50.174: INFO: Trying to get logs from node cncf-3 pod security-context-117a5295-bc59-4aa8-b3ed-b86606c43ecf container test-container: <nil>
STEP: delete the pod 05/06/23 07:39:50.182
May  6 07:39:50.195: INFO: Waiting for pod security-context-117a5295-bc59-4aa8-b3ed-b86606c43ecf to disappear
May  6 07:39:50.198: INFO: Pod security-context-117a5295-bc59-4aa8-b3ed-b86606c43ecf no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May  6 07:39:50.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-5211" for this suite. 05/06/23 07:39:50.201
------------------------------
â€¢ [4.078 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:39:46.13
    May  6 07:39:46.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename security-context 05/06/23 07:39:46.131
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:46.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:46.148
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/06/23 07:39:46.15
    May  6 07:39:46.158: INFO: Waiting up to 5m0s for pod "security-context-117a5295-bc59-4aa8-b3ed-b86606c43ecf" in namespace "security-context-5211" to be "Succeeded or Failed"
    May  6 07:39:46.163: INFO: Pod "security-context-117a5295-bc59-4aa8-b3ed-b86606c43ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.84125ms
    May  6 07:39:48.167: INFO: Pod "security-context-117a5295-bc59-4aa8-b3ed-b86606c43ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008776997s
    May  6 07:39:50.170: INFO: Pod "security-context-117a5295-bc59-4aa8-b3ed-b86606c43ecf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012603465s
    STEP: Saw pod success 05/06/23 07:39:50.17
    May  6 07:39:50.170: INFO: Pod "security-context-117a5295-bc59-4aa8-b3ed-b86606c43ecf" satisfied condition "Succeeded or Failed"
    May  6 07:39:50.174: INFO: Trying to get logs from node cncf-3 pod security-context-117a5295-bc59-4aa8-b3ed-b86606c43ecf container test-container: <nil>
    STEP: delete the pod 05/06/23 07:39:50.182
    May  6 07:39:50.195: INFO: Waiting for pod security-context-117a5295-bc59-4aa8-b3ed-b86606c43ecf to disappear
    May  6 07:39:50.198: INFO: Pod security-context-117a5295-bc59-4aa8-b3ed-b86606c43ecf no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May  6 07:39:50.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-5211" for this suite. 05/06/23 07:39:50.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:39:50.21
May  6 07:39:50.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename downward-api 05/06/23 07:39:50.21
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:50.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:50.229
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 05/06/23 07:39:50.231
May  6 07:39:50.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75a34184-1aa1-477f-997e-ac94300d4368" in namespace "downward-api-8465" to be "Succeeded or Failed"
May  6 07:39:50.241: INFO: Pod "downwardapi-volume-75a34184-1aa1-477f-997e-ac94300d4368": Phase="Pending", Reason="", readiness=false. Elapsed: 3.071787ms
May  6 07:39:52.244: INFO: Pod "downwardapi-volume-75a34184-1aa1-477f-997e-ac94300d4368": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007010622s
May  6 07:39:54.244: INFO: Pod "downwardapi-volume-75a34184-1aa1-477f-997e-ac94300d4368": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006339324s
STEP: Saw pod success 05/06/23 07:39:54.244
May  6 07:39:54.244: INFO: Pod "downwardapi-volume-75a34184-1aa1-477f-997e-ac94300d4368" satisfied condition "Succeeded or Failed"
May  6 07:39:54.246: INFO: Trying to get logs from node cncf-3 pod downwardapi-volume-75a34184-1aa1-477f-997e-ac94300d4368 container client-container: <nil>
STEP: delete the pod 05/06/23 07:39:54.25
May  6 07:39:54.260: INFO: Waiting for pod downwardapi-volume-75a34184-1aa1-477f-997e-ac94300d4368 to disappear
May  6 07:39:54.264: INFO: Pod downwardapi-volume-75a34184-1aa1-477f-997e-ac94300d4368 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  6 07:39:54.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8465" for this suite. 05/06/23 07:39:54.267
------------------------------
â€¢ [4.063 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:39:50.21
    May  6 07:39:50.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename downward-api 05/06/23 07:39:50.21
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:50.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:50.229
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 05/06/23 07:39:50.231
    May  6 07:39:50.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75a34184-1aa1-477f-997e-ac94300d4368" in namespace "downward-api-8465" to be "Succeeded or Failed"
    May  6 07:39:50.241: INFO: Pod "downwardapi-volume-75a34184-1aa1-477f-997e-ac94300d4368": Phase="Pending", Reason="", readiness=false. Elapsed: 3.071787ms
    May  6 07:39:52.244: INFO: Pod "downwardapi-volume-75a34184-1aa1-477f-997e-ac94300d4368": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007010622s
    May  6 07:39:54.244: INFO: Pod "downwardapi-volume-75a34184-1aa1-477f-997e-ac94300d4368": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006339324s
    STEP: Saw pod success 05/06/23 07:39:54.244
    May  6 07:39:54.244: INFO: Pod "downwardapi-volume-75a34184-1aa1-477f-997e-ac94300d4368" satisfied condition "Succeeded or Failed"
    May  6 07:39:54.246: INFO: Trying to get logs from node cncf-3 pod downwardapi-volume-75a34184-1aa1-477f-997e-ac94300d4368 container client-container: <nil>
    STEP: delete the pod 05/06/23 07:39:54.25
    May  6 07:39:54.260: INFO: Waiting for pod downwardapi-volume-75a34184-1aa1-477f-997e-ac94300d4368 to disappear
    May  6 07:39:54.264: INFO: Pod downwardapi-volume-75a34184-1aa1-477f-997e-ac94300d4368 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  6 07:39:54.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8465" for this suite. 05/06/23 07:39:54.267
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:39:54.272
May  6 07:39:54.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename sysctl 05/06/23 07:39:54.273
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:54.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:54.289
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 05/06/23 07:39:54.292
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May  6 07:39:54.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-7757" for this suite. 05/06/23 07:39:54.298
------------------------------
â€¢ [0.031 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:39:54.272
    May  6 07:39:54.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename sysctl 05/06/23 07:39:54.273
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:54.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:54.289
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 05/06/23 07:39:54.292
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May  6 07:39:54.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-7757" for this suite. 05/06/23 07:39:54.298
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:39:54.303
May  6 07:39:54.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename dns 05/06/23 07:39:54.304
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:54.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:54.32
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 05/06/23 07:39:54.321
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5885 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5885;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5885 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5885;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5885.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5885.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5885.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5885.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5885.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5885.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5885.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5885.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5885.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5885.svc;check="$$(dig +notcp +noall +answer +search 39.150.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.150.39_udp@PTR;check="$$(dig +tcp +noall +answer +search 39.150.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.150.39_tcp@PTR;sleep 1; done
 05/06/23 07:39:54.338
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5885 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5885;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5885 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5885;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5885.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5885.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5885.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5885.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5885.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5885.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5885.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5885.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5885.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5885.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5885.svc;check="$$(dig +notcp +noall +answer +search 39.150.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.150.39_udp@PTR;check="$$(dig +tcp +noall +answer +search 39.150.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.150.39_tcp@PTR;sleep 1; done
 05/06/23 07:39:54.338
STEP: creating a pod to probe DNS 05/06/23 07:39:54.338
STEP: submitting the pod to kubernetes 05/06/23 07:39:54.338
May  6 07:39:54.350: INFO: Waiting up to 15m0s for pod "dns-test-46080421-9b0f-4183-80fd-6e2064c3678e" in namespace "dns-5885" to be "running"
May  6 07:39:54.353: INFO: Pod "dns-test-46080421-9b0f-4183-80fd-6e2064c3678e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.086696ms
May  6 07:39:56.356: INFO: Pod "dns-test-46080421-9b0f-4183-80fd-6e2064c3678e": Phase="Running", Reason="", readiness=true. Elapsed: 2.005842912s
May  6 07:39:56.356: INFO: Pod "dns-test-46080421-9b0f-4183-80fd-6e2064c3678e" satisfied condition "running"
STEP: retrieving the pod 05/06/23 07:39:56.356
STEP: looking for the results for each expected name from probers 05/06/23 07:39:56.359
May  6 07:39:56.362: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:39:56.366: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:39:56.371: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:39:56.374: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:39:56.376: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:39:56.380: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:39:56.383: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:39:56.385: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:39:56.401: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:39:56.404: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:39:56.406: INFO: Unable to read jessie_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:39:56.409: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:39:56.412: INFO: Unable to read jessie_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:39:56.415: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:39:56.418: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:39:56.421: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:39:56.432: INFO: Lookups using dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5885 wheezy_tcp@dns-test-service.dns-5885 wheezy_udp@dns-test-service.dns-5885.svc wheezy_tcp@dns-test-service.dns-5885.svc wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5885 jessie_tcp@dns-test-service.dns-5885 jessie_udp@dns-test-service.dns-5885.svc jessie_tcp@dns-test-service.dns-5885.svc jessie_udp@_http._tcp.dns-test-service.dns-5885.svc jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc]

May  6 07:40:01.436: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:01.440: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:01.442: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:01.445: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:01.448: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:01.451: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:01.456: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:01.460: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:01.475: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:01.478: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:01.481: INFO: Unable to read jessie_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:01.484: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:01.486: INFO: Unable to read jessie_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:01.490: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:01.493: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:01.496: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:01.508: INFO: Lookups using dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5885 wheezy_tcp@dns-test-service.dns-5885 wheezy_udp@dns-test-service.dns-5885.svc wheezy_tcp@dns-test-service.dns-5885.svc wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5885 jessie_tcp@dns-test-service.dns-5885 jessie_udp@dns-test-service.dns-5885.svc jessie_tcp@dns-test-service.dns-5885.svc jessie_udp@_http._tcp.dns-test-service.dns-5885.svc jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc]

May  6 07:40:06.436: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:06.439: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:06.441: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:06.445: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:06.447: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:06.450: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:06.453: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:06.455: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:06.469: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:06.472: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:06.474: INFO: Unable to read jessie_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:06.478: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:06.481: INFO: Unable to read jessie_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:06.484: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:06.486: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:06.489: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:06.500: INFO: Lookups using dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5885 wheezy_tcp@dns-test-service.dns-5885 wheezy_udp@dns-test-service.dns-5885.svc wheezy_tcp@dns-test-service.dns-5885.svc wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5885 jessie_tcp@dns-test-service.dns-5885 jessie_udp@dns-test-service.dns-5885.svc jessie_tcp@dns-test-service.dns-5885.svc jessie_udp@_http._tcp.dns-test-service.dns-5885.svc jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc]

May  6 07:40:11.437: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:11.441: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:11.443: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:11.446: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:11.449: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:11.453: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:11.456: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:11.459: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:11.474: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:11.478: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:11.481: INFO: Unable to read jessie_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:11.484: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:11.488: INFO: Unable to read jessie_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:11.490: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:11.493: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:11.497: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:11.510: INFO: Lookups using dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5885 wheezy_tcp@dns-test-service.dns-5885 wheezy_udp@dns-test-service.dns-5885.svc wheezy_tcp@dns-test-service.dns-5885.svc wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5885 jessie_tcp@dns-test-service.dns-5885 jessie_udp@dns-test-service.dns-5885.svc jessie_tcp@dns-test-service.dns-5885.svc jessie_udp@_http._tcp.dns-test-service.dns-5885.svc jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc]

May  6 07:40:16.436: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:16.439: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:16.442: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:16.445: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:16.447: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:16.451: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:16.453: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:16.456: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:16.470: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:16.473: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:16.476: INFO: Unable to read jessie_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:16.479: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:16.481: INFO: Unable to read jessie_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:16.484: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:16.487: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:16.490: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:16.501: INFO: Lookups using dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5885 wheezy_tcp@dns-test-service.dns-5885 wheezy_udp@dns-test-service.dns-5885.svc wheezy_tcp@dns-test-service.dns-5885.svc wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5885 jessie_tcp@dns-test-service.dns-5885 jessie_udp@dns-test-service.dns-5885.svc jessie_tcp@dns-test-service.dns-5885.svc jessie_udp@_http._tcp.dns-test-service.dns-5885.svc jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc]

May  6 07:40:21.436: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:21.440: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:21.442: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:21.445: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:21.448: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:21.451: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:21.453: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:21.457: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:21.471: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:21.474: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:21.477: INFO: Unable to read jessie_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:21.480: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:21.483: INFO: Unable to read jessie_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:21.486: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:21.489: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:21.492: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
May  6 07:40:21.507: INFO: Lookups using dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5885 wheezy_tcp@dns-test-service.dns-5885 wheezy_udp@dns-test-service.dns-5885.svc wheezy_tcp@dns-test-service.dns-5885.svc wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5885 jessie_tcp@dns-test-service.dns-5885 jessie_udp@dns-test-service.dns-5885.svc jessie_tcp@dns-test-service.dns-5885.svc jessie_udp@_http._tcp.dns-test-service.dns-5885.svc jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc]

May  6 07:40:26.503: INFO: DNS probes using dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e succeeded

STEP: deleting the pod 05/06/23 07:40:26.503
STEP: deleting the test service 05/06/23 07:40:26.52
STEP: deleting the test headless service 05/06/23 07:40:26.555
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May  6 07:40:26.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5885" for this suite. 05/06/23 07:40:26.575
------------------------------
â€¢ [SLOW TEST] [32.280 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:39:54.303
    May  6 07:39:54.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename dns 05/06/23 07:39:54.304
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:39:54.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:39:54.32
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 05/06/23 07:39:54.321
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5885 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5885;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5885 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5885;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5885.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5885.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5885.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5885.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5885.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5885.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5885.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5885.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5885.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5885.svc;check="$$(dig +notcp +noall +answer +search 39.150.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.150.39_udp@PTR;check="$$(dig +tcp +noall +answer +search 39.150.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.150.39_tcp@PTR;sleep 1; done
     05/06/23 07:39:54.338
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5885 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5885;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5885 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5885;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5885.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5885.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5885.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5885.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5885.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5885.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5885.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5885.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5885.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5885.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5885.svc;check="$$(dig +notcp +noall +answer +search 39.150.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.150.39_udp@PTR;check="$$(dig +tcp +noall +answer +search 39.150.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.150.39_tcp@PTR;sleep 1; done
     05/06/23 07:39:54.338
    STEP: creating a pod to probe DNS 05/06/23 07:39:54.338
    STEP: submitting the pod to kubernetes 05/06/23 07:39:54.338
    May  6 07:39:54.350: INFO: Waiting up to 15m0s for pod "dns-test-46080421-9b0f-4183-80fd-6e2064c3678e" in namespace "dns-5885" to be "running"
    May  6 07:39:54.353: INFO: Pod "dns-test-46080421-9b0f-4183-80fd-6e2064c3678e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.086696ms
    May  6 07:39:56.356: INFO: Pod "dns-test-46080421-9b0f-4183-80fd-6e2064c3678e": Phase="Running", Reason="", readiness=true. Elapsed: 2.005842912s
    May  6 07:39:56.356: INFO: Pod "dns-test-46080421-9b0f-4183-80fd-6e2064c3678e" satisfied condition "running"
    STEP: retrieving the pod 05/06/23 07:39:56.356
    STEP: looking for the results for each expected name from probers 05/06/23 07:39:56.359
    May  6 07:39:56.362: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:39:56.366: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:39:56.371: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:39:56.374: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:39:56.376: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:39:56.380: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:39:56.383: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:39:56.385: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:39:56.401: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:39:56.404: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:39:56.406: INFO: Unable to read jessie_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:39:56.409: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:39:56.412: INFO: Unable to read jessie_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:39:56.415: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:39:56.418: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:39:56.421: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:39:56.432: INFO: Lookups using dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5885 wheezy_tcp@dns-test-service.dns-5885 wheezy_udp@dns-test-service.dns-5885.svc wheezy_tcp@dns-test-service.dns-5885.svc wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5885 jessie_tcp@dns-test-service.dns-5885 jessie_udp@dns-test-service.dns-5885.svc jessie_tcp@dns-test-service.dns-5885.svc jessie_udp@_http._tcp.dns-test-service.dns-5885.svc jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc]

    May  6 07:40:01.436: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:01.440: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:01.442: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:01.445: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:01.448: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:01.451: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:01.456: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:01.460: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:01.475: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:01.478: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:01.481: INFO: Unable to read jessie_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:01.484: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:01.486: INFO: Unable to read jessie_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:01.490: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:01.493: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:01.496: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:01.508: INFO: Lookups using dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5885 wheezy_tcp@dns-test-service.dns-5885 wheezy_udp@dns-test-service.dns-5885.svc wheezy_tcp@dns-test-service.dns-5885.svc wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5885 jessie_tcp@dns-test-service.dns-5885 jessie_udp@dns-test-service.dns-5885.svc jessie_tcp@dns-test-service.dns-5885.svc jessie_udp@_http._tcp.dns-test-service.dns-5885.svc jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc]

    May  6 07:40:06.436: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:06.439: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:06.441: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:06.445: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:06.447: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:06.450: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:06.453: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:06.455: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:06.469: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:06.472: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:06.474: INFO: Unable to read jessie_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:06.478: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:06.481: INFO: Unable to read jessie_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:06.484: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:06.486: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:06.489: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:06.500: INFO: Lookups using dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5885 wheezy_tcp@dns-test-service.dns-5885 wheezy_udp@dns-test-service.dns-5885.svc wheezy_tcp@dns-test-service.dns-5885.svc wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5885 jessie_tcp@dns-test-service.dns-5885 jessie_udp@dns-test-service.dns-5885.svc jessie_tcp@dns-test-service.dns-5885.svc jessie_udp@_http._tcp.dns-test-service.dns-5885.svc jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc]

    May  6 07:40:11.437: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:11.441: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:11.443: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:11.446: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:11.449: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:11.453: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:11.456: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:11.459: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:11.474: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:11.478: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:11.481: INFO: Unable to read jessie_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:11.484: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:11.488: INFO: Unable to read jessie_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:11.490: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:11.493: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:11.497: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:11.510: INFO: Lookups using dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5885 wheezy_tcp@dns-test-service.dns-5885 wheezy_udp@dns-test-service.dns-5885.svc wheezy_tcp@dns-test-service.dns-5885.svc wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5885 jessie_tcp@dns-test-service.dns-5885 jessie_udp@dns-test-service.dns-5885.svc jessie_tcp@dns-test-service.dns-5885.svc jessie_udp@_http._tcp.dns-test-service.dns-5885.svc jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc]

    May  6 07:40:16.436: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:16.439: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:16.442: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:16.445: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:16.447: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:16.451: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:16.453: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:16.456: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:16.470: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:16.473: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:16.476: INFO: Unable to read jessie_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:16.479: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:16.481: INFO: Unable to read jessie_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:16.484: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:16.487: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:16.490: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:16.501: INFO: Lookups using dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5885 wheezy_tcp@dns-test-service.dns-5885 wheezy_udp@dns-test-service.dns-5885.svc wheezy_tcp@dns-test-service.dns-5885.svc wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5885 jessie_tcp@dns-test-service.dns-5885 jessie_udp@dns-test-service.dns-5885.svc jessie_tcp@dns-test-service.dns-5885.svc jessie_udp@_http._tcp.dns-test-service.dns-5885.svc jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc]

    May  6 07:40:21.436: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:21.440: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:21.442: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:21.445: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:21.448: INFO: Unable to read wheezy_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:21.451: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:21.453: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:21.457: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:21.471: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:21.474: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:21.477: INFO: Unable to read jessie_udp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:21.480: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885 from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:21.483: INFO: Unable to read jessie_udp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:21.486: INFO: Unable to read jessie_tcp@dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:21.489: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:21.492: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc from pod dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e: the server could not find the requested resource (get pods dns-test-46080421-9b0f-4183-80fd-6e2064c3678e)
    May  6 07:40:21.507: INFO: Lookups using dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5885 wheezy_tcp@dns-test-service.dns-5885 wheezy_udp@dns-test-service.dns-5885.svc wheezy_tcp@dns-test-service.dns-5885.svc wheezy_udp@_http._tcp.dns-test-service.dns-5885.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5885.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5885 jessie_tcp@dns-test-service.dns-5885 jessie_udp@dns-test-service.dns-5885.svc jessie_tcp@dns-test-service.dns-5885.svc jessie_udp@_http._tcp.dns-test-service.dns-5885.svc jessie_tcp@_http._tcp.dns-test-service.dns-5885.svc]

    May  6 07:40:26.503: INFO: DNS probes using dns-5885/dns-test-46080421-9b0f-4183-80fd-6e2064c3678e succeeded

    STEP: deleting the pod 05/06/23 07:40:26.503
    STEP: deleting the test service 05/06/23 07:40:26.52
    STEP: deleting the test headless service 05/06/23 07:40:26.555
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May  6 07:40:26.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5885" for this suite. 05/06/23 07:40:26.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:40:26.584
May  6 07:40:26.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename gc 05/06/23 07:40:26.585
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:40:26.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:40:26.603
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 05/06/23 07:40:26.605
STEP: delete the rc 05/06/23 07:40:31.614
STEP: wait for all pods to be garbage collected 05/06/23 07:40:31.619
STEP: Gathering metrics 05/06/23 07:40:36.624
May  6 07:40:36.633: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-2" in namespace "kube-system" to be "running and ready"
May  6 07:40:36.636: INFO: Pod "kube-controller-manager-cncf-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.135961ms
May  6 07:40:36.636: INFO: The phase of Pod kube-controller-manager-cncf-2 is Running (Ready = true)
May  6 07:40:36.636: INFO: Pod "kube-controller-manager-cncf-2" satisfied condition "running and ready"
May  6 07:40:36.667: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May  6 07:40:36.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1253" for this suite. 05/06/23 07:40:36.67
------------------------------
â€¢ [SLOW TEST] [10.093 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:40:26.584
    May  6 07:40:26.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename gc 05/06/23 07:40:26.585
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:40:26.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:40:26.603
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 05/06/23 07:40:26.605
    STEP: delete the rc 05/06/23 07:40:31.614
    STEP: wait for all pods to be garbage collected 05/06/23 07:40:31.619
    STEP: Gathering metrics 05/06/23 07:40:36.624
    May  6 07:40:36.633: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-2" in namespace "kube-system" to be "running and ready"
    May  6 07:40:36.636: INFO: Pod "kube-controller-manager-cncf-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.135961ms
    May  6 07:40:36.636: INFO: The phase of Pod kube-controller-manager-cncf-2 is Running (Ready = true)
    May  6 07:40:36.636: INFO: Pod "kube-controller-manager-cncf-2" satisfied condition "running and ready"
    May  6 07:40:36.667: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May  6 07:40:36.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1253" for this suite. 05/06/23 07:40:36.67
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:40:36.677
May  6 07:40:36.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename svcaccounts 05/06/23 07:40:36.678
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:40:36.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:40:36.694
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-w4crd"  05/06/23 07:40:36.696
May  6 07:40:36.700: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-w4crd"  05/06/23 07:40:36.7
May  6 07:40:36.706: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May  6 07:40:36.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9662" for this suite. 05/06/23 07:40:36.709
------------------------------
â€¢ [0.039 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:40:36.677
    May  6 07:40:36.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename svcaccounts 05/06/23 07:40:36.678
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:40:36.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:40:36.694
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-w4crd"  05/06/23 07:40:36.696
    May  6 07:40:36.700: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-w4crd"  05/06/23 07:40:36.7
    May  6 07:40:36.706: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May  6 07:40:36.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9662" for this suite. 05/06/23 07:40:36.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:40:36.718
May  6 07:40:36.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename dns 05/06/23 07:40:36.719
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:40:36.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:40:36.733
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 05/06/23 07:40:36.735
May  6 07:40:36.744: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3441  2f24a7ec-0335-4178-98a4-7924df6b8b8f 158677 0 2023-05-06 07:40:36 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-06 07:40:36 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5fvzv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5fvzv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:40:36.744: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3441" to be "running and ready"
May  6 07:40:36.746: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09454ms
May  6 07:40:36.746: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May  6 07:40:38.750: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.005853619s
May  6 07:40:38.750: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
May  6 07:40:38.750: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 05/06/23 07:40:38.75
May  6 07:40:38.750: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3441 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 07:40:38.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:40:38.751: INFO: ExecWithOptions: Clientset creation
May  6 07:40:38.751: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3441/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 05/06/23 07:40:38.811
May  6 07:40:38.811: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3441 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 07:40:38.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:40:38.812: INFO: ExecWithOptions: Clientset creation
May  6 07:40:38.812: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3441/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  6 07:40:38.855: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May  6 07:40:38.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3441" for this suite. 05/06/23 07:40:38.87
------------------------------
â€¢ [2.159 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:40:36.718
    May  6 07:40:36.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename dns 05/06/23 07:40:36.719
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:40:36.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:40:36.733
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 05/06/23 07:40:36.735
    May  6 07:40:36.744: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3441  2f24a7ec-0335-4178-98a4-7924df6b8b8f 158677 0 2023-05-06 07:40:36 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-06 07:40:36 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5fvzv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5fvzv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:40:36.744: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3441" to be "running and ready"
    May  6 07:40:36.746: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09454ms
    May  6 07:40:36.746: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:40:38.750: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.005853619s
    May  6 07:40:38.750: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    May  6 07:40:38.750: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 05/06/23 07:40:38.75
    May  6 07:40:38.750: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3441 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 07:40:38.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:40:38.751: INFO: ExecWithOptions: Clientset creation
    May  6 07:40:38.751: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3441/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 05/06/23 07:40:38.811
    May  6 07:40:38.811: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3441 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 07:40:38.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:40:38.812: INFO: ExecWithOptions: Clientset creation
    May  6 07:40:38.812: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3441/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  6 07:40:38.855: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May  6 07:40:38.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3441" for this suite. 05/06/23 07:40:38.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:40:38.877
May  6 07:40:38.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename crd-publish-openapi 05/06/23 07:40:38.878
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:40:38.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:40:38.894
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 05/06/23 07:40:38.896
May  6 07:40:38.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:40:41.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:40:48.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5812" for this suite. 05/06/23 07:40:48.496
------------------------------
â€¢ [SLOW TEST] [9.627 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:40:38.877
    May  6 07:40:38.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename crd-publish-openapi 05/06/23 07:40:38.878
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:40:38.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:40:38.894
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 05/06/23 07:40:38.896
    May  6 07:40:38.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:40:41.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:40:48.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5812" for this suite. 05/06/23 07:40:48.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:40:48.505
May  6 07:40:48.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename configmap 05/06/23 07:40:48.506
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:40:49.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:40:49.527
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-1747/configmap-test-519a0b6b-9870-438f-b206-686e877aeef9 05/06/23 07:40:49.529
STEP: Creating a pod to test consume configMaps 05/06/23 07:40:49.533
May  6 07:40:49.540: INFO: Waiting up to 5m0s for pod "pod-configmaps-e48746aa-194f-490e-9324-465fc1e8e537" in namespace "configmap-1747" to be "Succeeded or Failed"
May  6 07:40:49.543: INFO: Pod "pod-configmaps-e48746aa-194f-490e-9324-465fc1e8e537": Phase="Pending", Reason="", readiness=false. Elapsed: 2.611152ms
May  6 07:40:51.547: INFO: Pod "pod-configmaps-e48746aa-194f-490e-9324-465fc1e8e537": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006652629s
May  6 07:40:53.548: INFO: Pod "pod-configmaps-e48746aa-194f-490e-9324-465fc1e8e537": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007332189s
STEP: Saw pod success 05/06/23 07:40:53.548
May  6 07:40:53.548: INFO: Pod "pod-configmaps-e48746aa-194f-490e-9324-465fc1e8e537" satisfied condition "Succeeded or Failed"
May  6 07:40:53.551: INFO: Trying to get logs from node cncf-2 pod pod-configmaps-e48746aa-194f-490e-9324-465fc1e8e537 container env-test: <nil>
STEP: delete the pod 05/06/23 07:40:53.562
May  6 07:40:53.576: INFO: Waiting for pod pod-configmaps-e48746aa-194f-490e-9324-465fc1e8e537 to disappear
May  6 07:40:53.579: INFO: Pod pod-configmaps-e48746aa-194f-490e-9324-465fc1e8e537 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  6 07:40:53.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1747" for this suite. 05/06/23 07:40:53.582
------------------------------
â€¢ [SLOW TEST] [5.083 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:40:48.505
    May  6 07:40:48.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename configmap 05/06/23 07:40:48.506
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:40:49.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:40:49.527
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-1747/configmap-test-519a0b6b-9870-438f-b206-686e877aeef9 05/06/23 07:40:49.529
    STEP: Creating a pod to test consume configMaps 05/06/23 07:40:49.533
    May  6 07:40:49.540: INFO: Waiting up to 5m0s for pod "pod-configmaps-e48746aa-194f-490e-9324-465fc1e8e537" in namespace "configmap-1747" to be "Succeeded or Failed"
    May  6 07:40:49.543: INFO: Pod "pod-configmaps-e48746aa-194f-490e-9324-465fc1e8e537": Phase="Pending", Reason="", readiness=false. Elapsed: 2.611152ms
    May  6 07:40:51.547: INFO: Pod "pod-configmaps-e48746aa-194f-490e-9324-465fc1e8e537": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006652629s
    May  6 07:40:53.548: INFO: Pod "pod-configmaps-e48746aa-194f-490e-9324-465fc1e8e537": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007332189s
    STEP: Saw pod success 05/06/23 07:40:53.548
    May  6 07:40:53.548: INFO: Pod "pod-configmaps-e48746aa-194f-490e-9324-465fc1e8e537" satisfied condition "Succeeded or Failed"
    May  6 07:40:53.551: INFO: Trying to get logs from node cncf-2 pod pod-configmaps-e48746aa-194f-490e-9324-465fc1e8e537 container env-test: <nil>
    STEP: delete the pod 05/06/23 07:40:53.562
    May  6 07:40:53.576: INFO: Waiting for pod pod-configmaps-e48746aa-194f-490e-9324-465fc1e8e537 to disappear
    May  6 07:40:53.579: INFO: Pod pod-configmaps-e48746aa-194f-490e-9324-465fc1e8e537 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  6 07:40:53.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1747" for this suite. 05/06/23 07:40:53.582
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:40:53.588
May  6 07:40:53.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename configmap 05/06/23 07:40:53.589
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:40:54.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:40:54.608
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-f52df271-c4d4-49d8-a923-023b99bef6de 05/06/23 07:40:54.61
STEP: Creating a pod to test consume configMaps 05/06/23 07:40:54.614
May  6 07:40:54.622: INFO: Waiting up to 5m0s for pod "pod-configmaps-446287a1-038b-4154-81fb-4a20ad81de77" in namespace "configmap-9579" to be "Succeeded or Failed"
May  6 07:40:54.628: INFO: Pod "pod-configmaps-446287a1-038b-4154-81fb-4a20ad81de77": Phase="Pending", Reason="", readiness=false. Elapsed: 6.47774ms
May  6 07:40:56.632: INFO: Pod "pod-configmaps-446287a1-038b-4154-81fb-4a20ad81de77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009850517s
May  6 07:40:58.632: INFO: Pod "pod-configmaps-446287a1-038b-4154-81fb-4a20ad81de77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010386494s
STEP: Saw pod success 05/06/23 07:40:58.632
May  6 07:40:58.632: INFO: Pod "pod-configmaps-446287a1-038b-4154-81fb-4a20ad81de77" satisfied condition "Succeeded or Failed"
May  6 07:40:58.635: INFO: Trying to get logs from node cncf-2 pod pod-configmaps-446287a1-038b-4154-81fb-4a20ad81de77 container agnhost-container: <nil>
STEP: delete the pod 05/06/23 07:40:58.642
May  6 07:40:58.656: INFO: Waiting for pod pod-configmaps-446287a1-038b-4154-81fb-4a20ad81de77 to disappear
May  6 07:40:58.659: INFO: Pod pod-configmaps-446287a1-038b-4154-81fb-4a20ad81de77 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  6 07:40:58.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9579" for this suite. 05/06/23 07:40:58.662
------------------------------
â€¢ [SLOW TEST] [5.078 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:40:53.588
    May  6 07:40:53.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename configmap 05/06/23 07:40:53.589
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:40:54.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:40:54.608
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-f52df271-c4d4-49d8-a923-023b99bef6de 05/06/23 07:40:54.61
    STEP: Creating a pod to test consume configMaps 05/06/23 07:40:54.614
    May  6 07:40:54.622: INFO: Waiting up to 5m0s for pod "pod-configmaps-446287a1-038b-4154-81fb-4a20ad81de77" in namespace "configmap-9579" to be "Succeeded or Failed"
    May  6 07:40:54.628: INFO: Pod "pod-configmaps-446287a1-038b-4154-81fb-4a20ad81de77": Phase="Pending", Reason="", readiness=false. Elapsed: 6.47774ms
    May  6 07:40:56.632: INFO: Pod "pod-configmaps-446287a1-038b-4154-81fb-4a20ad81de77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009850517s
    May  6 07:40:58.632: INFO: Pod "pod-configmaps-446287a1-038b-4154-81fb-4a20ad81de77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010386494s
    STEP: Saw pod success 05/06/23 07:40:58.632
    May  6 07:40:58.632: INFO: Pod "pod-configmaps-446287a1-038b-4154-81fb-4a20ad81de77" satisfied condition "Succeeded or Failed"
    May  6 07:40:58.635: INFO: Trying to get logs from node cncf-2 pod pod-configmaps-446287a1-038b-4154-81fb-4a20ad81de77 container agnhost-container: <nil>
    STEP: delete the pod 05/06/23 07:40:58.642
    May  6 07:40:58.656: INFO: Waiting for pod pod-configmaps-446287a1-038b-4154-81fb-4a20ad81de77 to disappear
    May  6 07:40:58.659: INFO: Pod pod-configmaps-446287a1-038b-4154-81fb-4a20ad81de77 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  6 07:40:58.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9579" for this suite. 05/06/23 07:40:58.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:40:58.668
May  6 07:40:58.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename endpointslice 05/06/23 07:40:58.669
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:40:59.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:40:59.687
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
May  6 07:40:59.695: INFO: Endpoints addresses: [10.0.0.107 10.0.0.134 10.0.0.180] , ports: [6443]
May  6 07:40:59.695: INFO: EndpointSlices addresses: [10.0.0.107 10.0.0.134 10.0.0.180] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
May  6 07:40:59.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-8590" for this suite. 05/06/23 07:40:59.698
------------------------------
â€¢ [1.036 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:40:58.668
    May  6 07:40:58.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename endpointslice 05/06/23 07:40:58.669
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:40:59.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:40:59.687
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    May  6 07:40:59.695: INFO: Endpoints addresses: [10.0.0.107 10.0.0.134 10.0.0.180] , ports: [6443]
    May  6 07:40:59.695: INFO: EndpointSlices addresses: [10.0.0.107 10.0.0.134 10.0.0.180] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    May  6 07:40:59.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-8590" for this suite. 05/06/23 07:40:59.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:40:59.706
May  6 07:40:59.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename security-context 05/06/23 07:40:59.706
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:41:00.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:41:00.727
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/06/23 07:41:00.729
May  6 07:41:00.735: INFO: Waiting up to 5m0s for pod "security-context-4d4785ea-c143-43e0-8fe6-edd4f6b504b2" in namespace "security-context-7576" to be "Succeeded or Failed"
May  6 07:41:00.738: INFO: Pod "security-context-4d4785ea-c143-43e0-8fe6-edd4f6b504b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.652623ms
May  6 07:41:02.742: INFO: Pod "security-context-4d4785ea-c143-43e0-8fe6-edd4f6b504b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006674633s
May  6 07:41:04.741: INFO: Pod "security-context-4d4785ea-c143-43e0-8fe6-edd4f6b504b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005895609s
STEP: Saw pod success 05/06/23 07:41:04.741
May  6 07:41:04.741: INFO: Pod "security-context-4d4785ea-c143-43e0-8fe6-edd4f6b504b2" satisfied condition "Succeeded or Failed"
May  6 07:41:04.743: INFO: Trying to get logs from node cncf-2 pod security-context-4d4785ea-c143-43e0-8fe6-edd4f6b504b2 container test-container: <nil>
STEP: delete the pod 05/06/23 07:41:04.749
May  6 07:41:04.763: INFO: Waiting for pod security-context-4d4785ea-c143-43e0-8fe6-edd4f6b504b2 to disappear
May  6 07:41:04.766: INFO: Pod security-context-4d4785ea-c143-43e0-8fe6-edd4f6b504b2 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May  6 07:41:04.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-7576" for this suite. 05/06/23 07:41:04.768
------------------------------
â€¢ [SLOW TEST] [5.067 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:40:59.706
    May  6 07:40:59.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename security-context 05/06/23 07:40:59.706
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:41:00.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:41:00.727
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/06/23 07:41:00.729
    May  6 07:41:00.735: INFO: Waiting up to 5m0s for pod "security-context-4d4785ea-c143-43e0-8fe6-edd4f6b504b2" in namespace "security-context-7576" to be "Succeeded or Failed"
    May  6 07:41:00.738: INFO: Pod "security-context-4d4785ea-c143-43e0-8fe6-edd4f6b504b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.652623ms
    May  6 07:41:02.742: INFO: Pod "security-context-4d4785ea-c143-43e0-8fe6-edd4f6b504b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006674633s
    May  6 07:41:04.741: INFO: Pod "security-context-4d4785ea-c143-43e0-8fe6-edd4f6b504b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005895609s
    STEP: Saw pod success 05/06/23 07:41:04.741
    May  6 07:41:04.741: INFO: Pod "security-context-4d4785ea-c143-43e0-8fe6-edd4f6b504b2" satisfied condition "Succeeded or Failed"
    May  6 07:41:04.743: INFO: Trying to get logs from node cncf-2 pod security-context-4d4785ea-c143-43e0-8fe6-edd4f6b504b2 container test-container: <nil>
    STEP: delete the pod 05/06/23 07:41:04.749
    May  6 07:41:04.763: INFO: Waiting for pod security-context-4d4785ea-c143-43e0-8fe6-edd4f6b504b2 to disappear
    May  6 07:41:04.766: INFO: Pod security-context-4d4785ea-c143-43e0-8fe6-edd4f6b504b2 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May  6 07:41:04.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-7576" for this suite. 05/06/23 07:41:04.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:41:04.774
May  6 07:41:04.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename emptydir 05/06/23 07:41:04.775
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:41:05.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:41:05.794
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 05/06/23 07:41:05.796
May  6 07:41:05.804: INFO: Waiting up to 5m0s for pod "pod-1628aaac-f030-4baf-835a-bd3266482e1f" in namespace "emptydir-3778" to be "Succeeded or Failed"
May  6 07:41:05.807: INFO: Pod "pod-1628aaac-f030-4baf-835a-bd3266482e1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.251959ms
May  6 07:41:07.810: INFO: Pod "pod-1628aaac-f030-4baf-835a-bd3266482e1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006101162s
May  6 07:41:09.811: INFO: Pod "pod-1628aaac-f030-4baf-835a-bd3266482e1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006804636s
STEP: Saw pod success 05/06/23 07:41:09.811
May  6 07:41:09.811: INFO: Pod "pod-1628aaac-f030-4baf-835a-bd3266482e1f" satisfied condition "Succeeded or Failed"
May  6 07:41:09.814: INFO: Trying to get logs from node cncf-2 pod pod-1628aaac-f030-4baf-835a-bd3266482e1f container test-container: <nil>
STEP: delete the pod 05/06/23 07:41:09.82
May  6 07:41:09.830: INFO: Waiting for pod pod-1628aaac-f030-4baf-835a-bd3266482e1f to disappear
May  6 07:41:09.833: INFO: Pod pod-1628aaac-f030-4baf-835a-bd3266482e1f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  6 07:41:09.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3778" for this suite. 05/06/23 07:41:09.836
------------------------------
â€¢ [SLOW TEST] [5.069 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:41:04.774
    May  6 07:41:04.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename emptydir 05/06/23 07:41:04.775
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:41:05.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:41:05.794
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 05/06/23 07:41:05.796
    May  6 07:41:05.804: INFO: Waiting up to 5m0s for pod "pod-1628aaac-f030-4baf-835a-bd3266482e1f" in namespace "emptydir-3778" to be "Succeeded or Failed"
    May  6 07:41:05.807: INFO: Pod "pod-1628aaac-f030-4baf-835a-bd3266482e1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.251959ms
    May  6 07:41:07.810: INFO: Pod "pod-1628aaac-f030-4baf-835a-bd3266482e1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006101162s
    May  6 07:41:09.811: INFO: Pod "pod-1628aaac-f030-4baf-835a-bd3266482e1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006804636s
    STEP: Saw pod success 05/06/23 07:41:09.811
    May  6 07:41:09.811: INFO: Pod "pod-1628aaac-f030-4baf-835a-bd3266482e1f" satisfied condition "Succeeded or Failed"
    May  6 07:41:09.814: INFO: Trying to get logs from node cncf-2 pod pod-1628aaac-f030-4baf-835a-bd3266482e1f container test-container: <nil>
    STEP: delete the pod 05/06/23 07:41:09.82
    May  6 07:41:09.830: INFO: Waiting for pod pod-1628aaac-f030-4baf-835a-bd3266482e1f to disappear
    May  6 07:41:09.833: INFO: Pod pod-1628aaac-f030-4baf-835a-bd3266482e1f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  6 07:41:09.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3778" for this suite. 05/06/23 07:41:09.836
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:41:09.843
May  6 07:41:09.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename downward-api 05/06/23 07:41:09.844
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:41:10.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:41:10.863
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 05/06/23 07:41:10.866
May  6 07:41:10.874: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3d24300-85ff-4099-8b43-4b9edad8ab35" in namespace "downward-api-7278" to be "Succeeded or Failed"
May  6 07:41:10.877: INFO: Pod "downwardapi-volume-e3d24300-85ff-4099-8b43-4b9edad8ab35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.596134ms
May  6 07:41:12.880: INFO: Pod "downwardapi-volume-e3d24300-85ff-4099-8b43-4b9edad8ab35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006030758s
May  6 07:41:14.880: INFO: Pod "downwardapi-volume-e3d24300-85ff-4099-8b43-4b9edad8ab35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005887986s
STEP: Saw pod success 05/06/23 07:41:14.88
May  6 07:41:14.880: INFO: Pod "downwardapi-volume-e3d24300-85ff-4099-8b43-4b9edad8ab35" satisfied condition "Succeeded or Failed"
May  6 07:41:14.883: INFO: Trying to get logs from node cncf-2 pod downwardapi-volume-e3d24300-85ff-4099-8b43-4b9edad8ab35 container client-container: <nil>
STEP: delete the pod 05/06/23 07:41:14.888
May  6 07:41:14.903: INFO: Waiting for pod downwardapi-volume-e3d24300-85ff-4099-8b43-4b9edad8ab35 to disappear
May  6 07:41:14.905: INFO: Pod downwardapi-volume-e3d24300-85ff-4099-8b43-4b9edad8ab35 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  6 07:41:14.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7278" for this suite. 05/06/23 07:41:14.908
------------------------------
â€¢ [SLOW TEST] [5.070 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:41:09.843
    May  6 07:41:09.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename downward-api 05/06/23 07:41:09.844
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:41:10.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:41:10.863
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 05/06/23 07:41:10.866
    May  6 07:41:10.874: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3d24300-85ff-4099-8b43-4b9edad8ab35" in namespace "downward-api-7278" to be "Succeeded or Failed"
    May  6 07:41:10.877: INFO: Pod "downwardapi-volume-e3d24300-85ff-4099-8b43-4b9edad8ab35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.596134ms
    May  6 07:41:12.880: INFO: Pod "downwardapi-volume-e3d24300-85ff-4099-8b43-4b9edad8ab35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006030758s
    May  6 07:41:14.880: INFO: Pod "downwardapi-volume-e3d24300-85ff-4099-8b43-4b9edad8ab35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005887986s
    STEP: Saw pod success 05/06/23 07:41:14.88
    May  6 07:41:14.880: INFO: Pod "downwardapi-volume-e3d24300-85ff-4099-8b43-4b9edad8ab35" satisfied condition "Succeeded or Failed"
    May  6 07:41:14.883: INFO: Trying to get logs from node cncf-2 pod downwardapi-volume-e3d24300-85ff-4099-8b43-4b9edad8ab35 container client-container: <nil>
    STEP: delete the pod 05/06/23 07:41:14.888
    May  6 07:41:14.903: INFO: Waiting for pod downwardapi-volume-e3d24300-85ff-4099-8b43-4b9edad8ab35 to disappear
    May  6 07:41:14.905: INFO: Pod downwardapi-volume-e3d24300-85ff-4099-8b43-4b9edad8ab35 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  6 07:41:14.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7278" for this suite. 05/06/23 07:41:14.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:41:14.914
May  6 07:41:14.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubectl 05/06/23 07:41:14.915
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:41:15.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:41:15.934
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/06/23 07:41:15.936
May  6 07:41:15.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8946 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May  6 07:41:15.992: INFO: stderr: ""
May  6 07:41:15.992: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 05/06/23 07:41:15.992
STEP: verifying the pod e2e-test-httpd-pod was created 05/06/23 07:41:21.043
May  6 07:41:21.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8946 get pod e2e-test-httpd-pod -o json'
May  6 07:41:21.093: INFO: stderr: ""
May  6 07:41:21.093: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"703b08dd8d5d63ed0b5bbf021e61359d72b40b2e51350ed119bc938a156555d6\",\n            \"cni.projectcalico.org/podIP\": \"10.244.174.187/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.174.187/32\"\n        },\n        \"creationTimestamp\": \"2023-05-06T07:41:15Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8946\",\n        \"resourceVersion\": \"159057\",\n        \"uid\": \"602e33cb-d0ee-4cb8-ae8e-ba7fa43271b5\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-fjbmx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cncf-0\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-fjbmx\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-06T07:41:15Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-06T07:41:16Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-06T07:41:16Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-06T07:41:15Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://14fc30b27a7c5b9e7599ed7a3a49064a36d2515d929e0f275f3dd8a8635d00e9\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-06T07:41:16Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.0.134\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.174.187\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.174.187\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-06T07:41:15Z\"\n    }\n}\n"
STEP: replace the image in the pod 05/06/23 07:41:21.093
May  6 07:41:21.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8946 replace -f -'
May  6 07:41:21.536: INFO: stderr: ""
May  6 07:41:21.536: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 05/06/23 07:41:21.536
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
May  6 07:41:21.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8946 delete pods e2e-test-httpd-pod'
May  6 07:41:22.946: INFO: stderr: ""
May  6 07:41:22.947: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  6 07:41:22.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8946" for this suite. 05/06/23 07:41:22.95
------------------------------
â€¢ [SLOW TEST] [8.042 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:41:14.914
    May  6 07:41:14.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubectl 05/06/23 07:41:14.915
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:41:15.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:41:15.934
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/06/23 07:41:15.936
    May  6 07:41:15.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8946 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    May  6 07:41:15.992: INFO: stderr: ""
    May  6 07:41:15.992: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 05/06/23 07:41:15.992
    STEP: verifying the pod e2e-test-httpd-pod was created 05/06/23 07:41:21.043
    May  6 07:41:21.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8946 get pod e2e-test-httpd-pod -o json'
    May  6 07:41:21.093: INFO: stderr: ""
    May  6 07:41:21.093: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"703b08dd8d5d63ed0b5bbf021e61359d72b40b2e51350ed119bc938a156555d6\",\n            \"cni.projectcalico.org/podIP\": \"10.244.174.187/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.174.187/32\"\n        },\n        \"creationTimestamp\": \"2023-05-06T07:41:15Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8946\",\n        \"resourceVersion\": \"159057\",\n        \"uid\": \"602e33cb-d0ee-4cb8-ae8e-ba7fa43271b5\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-fjbmx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cncf-0\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-fjbmx\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-06T07:41:15Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-06T07:41:16Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-06T07:41:16Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-06T07:41:15Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://14fc30b27a7c5b9e7599ed7a3a49064a36d2515d929e0f275f3dd8a8635d00e9\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-06T07:41:16Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.0.134\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.174.187\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.174.187\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-06T07:41:15Z\"\n    }\n}\n"
    STEP: replace the image in the pod 05/06/23 07:41:21.093
    May  6 07:41:21.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8946 replace -f -'
    May  6 07:41:21.536: INFO: stderr: ""
    May  6 07:41:21.536: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 05/06/23 07:41:21.536
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    May  6 07:41:21.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8946 delete pods e2e-test-httpd-pod'
    May  6 07:41:22.946: INFO: stderr: ""
    May  6 07:41:22.947: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  6 07:41:22.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8946" for this suite. 05/06/23 07:41:22.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:41:22.956
May  6 07:41:22.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename pods 05/06/23 07:41:22.957
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:41:23.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:41:23.978
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 05/06/23 07:41:23.98
STEP: submitting the pod to kubernetes 05/06/23 07:41:23.98
STEP: verifying QOS class is set on the pod 05/06/23 07:41:23.988
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
May  6 07:41:23.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7239" for this suite. 05/06/23 07:41:23.995
------------------------------
â€¢ [1.045 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:41:22.956
    May  6 07:41:22.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename pods 05/06/23 07:41:22.957
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:41:23.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:41:23.978
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 05/06/23 07:41:23.98
    STEP: submitting the pod to kubernetes 05/06/23 07:41:23.98
    STEP: verifying QOS class is set on the pod 05/06/23 07:41:23.988
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    May  6 07:41:23.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7239" for this suite. 05/06/23 07:41:23.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:41:24.002
May  6 07:41:24.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename endpointslicemirroring 05/06/23 07:41:24.003
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:41:25.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:41:25.022
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 05/06/23 07:41:25.037
May  6 07:41:25.043: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 05/06/23 07:41:27.046
May  6 07:41:27.055: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 05/06/23 07:41:29.059
May  6 07:41:29.067: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
May  6 07:41:31.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-6785" for this suite. 05/06/23 07:41:31.073
------------------------------
â€¢ [SLOW TEST] [7.081 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:41:24.002
    May  6 07:41:24.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename endpointslicemirroring 05/06/23 07:41:24.003
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:41:25.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:41:25.022
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 05/06/23 07:41:25.037
    May  6 07:41:25.043: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 05/06/23 07:41:27.046
    May  6 07:41:27.055: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 05/06/23 07:41:29.059
    May  6 07:41:29.067: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    May  6 07:41:31.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-6785" for this suite. 05/06/23 07:41:31.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:41:31.084
May  6 07:41:31.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename runtimeclass 05/06/23 07:41:31.084
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:41:32.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:41:32.103
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-3391-delete-me 05/06/23 07:41:32.108
STEP: Waiting for the RuntimeClass to disappear 05/06/23 07:41:32.113
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May  6 07:41:32.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3391" for this suite. 05/06/23 07:41:32.123
------------------------------
â€¢ [1.046 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:41:31.084
    May  6 07:41:31.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename runtimeclass 05/06/23 07:41:31.084
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:41:32.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:41:32.103
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-3391-delete-me 05/06/23 07:41:32.108
    STEP: Waiting for the RuntimeClass to disappear 05/06/23 07:41:32.113
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May  6 07:41:32.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3391" for this suite. 05/06/23 07:41:32.123
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:41:32.13
May  6 07:41:32.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename crd-publish-openapi 05/06/23 07:41:32.131
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:41:33.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:41:33.149
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 05/06/23 07:41:33.151
May  6 07:41:33.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: rename a version 05/06/23 07:41:37.151
STEP: check the new version name is served 05/06/23 07:41:37.165
STEP: check the old version name is removed 05/06/23 07:41:38.633
STEP: check the other version is not changed 05/06/23 07:41:39.355
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:41:42.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9647" for this suite. 05/06/23 07:41:42.659
------------------------------
â€¢ [SLOW TEST] [10.536 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:41:32.13
    May  6 07:41:32.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename crd-publish-openapi 05/06/23 07:41:32.131
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:41:33.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:41:33.149
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 05/06/23 07:41:33.151
    May  6 07:41:33.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: rename a version 05/06/23 07:41:37.151
    STEP: check the new version name is served 05/06/23 07:41:37.165
    STEP: check the old version name is removed 05/06/23 07:41:38.633
    STEP: check the other version is not changed 05/06/23 07:41:39.355
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:41:42.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9647" for this suite. 05/06/23 07:41:42.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:41:42.667
May  6 07:41:42.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename job 05/06/23 07:41:42.668
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:41:42.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:41:42.686
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 05/06/23 07:41:42.688
STEP: Ensuring active pods == parallelism 05/06/23 07:41:42.694
STEP: delete a job 05/06/23 07:41:44.698
STEP: deleting Job.batch foo in namespace job-5073, will wait for the garbage collector to delete the pods 05/06/23 07:41:44.698
May  6 07:41:44.768: INFO: Deleting Job.batch foo took: 16.75849ms
May  6 07:41:44.869: INFO: Terminating Job.batch foo pods took: 101.179668ms
STEP: Ensuring job was deleted 05/06/23 07:42:17.469
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May  6 07:42:17.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5073" for this suite. 05/06/23 07:42:17.475
------------------------------
â€¢ [SLOW TEST] [34.818 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:41:42.667
    May  6 07:41:42.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename job 05/06/23 07:41:42.668
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:41:42.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:41:42.686
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 05/06/23 07:41:42.688
    STEP: Ensuring active pods == parallelism 05/06/23 07:41:42.694
    STEP: delete a job 05/06/23 07:41:44.698
    STEP: deleting Job.batch foo in namespace job-5073, will wait for the garbage collector to delete the pods 05/06/23 07:41:44.698
    May  6 07:41:44.768: INFO: Deleting Job.batch foo took: 16.75849ms
    May  6 07:41:44.869: INFO: Terminating Job.batch foo pods took: 101.179668ms
    STEP: Ensuring job was deleted 05/06/23 07:42:17.469
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May  6 07:42:17.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5073" for this suite. 05/06/23 07:42:17.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:42:17.486
May  6 07:42:17.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename resourcequota 05/06/23 07:42:17.486
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:42:17.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:42:17.503
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 05/06/23 07:42:17.505
STEP: Creating a ResourceQuota 05/06/23 07:42:22.508
STEP: Ensuring resource quota status is calculated 05/06/23 07:42:22.514
STEP: Creating a Service 05/06/23 07:42:24.517
STEP: Creating a NodePort Service 05/06/23 07:42:24.535
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 05/06/23 07:42:24.558
STEP: Ensuring resource quota status captures service creation 05/06/23 07:42:24.595
STEP: Deleting Services 05/06/23 07:42:26.599
STEP: Ensuring resource quota status released usage 05/06/23 07:42:26.635
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  6 07:42:28.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5454" for this suite. 05/06/23 07:42:28.643
------------------------------
â€¢ [SLOW TEST] [11.165 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:42:17.486
    May  6 07:42:17.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename resourcequota 05/06/23 07:42:17.486
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:42:17.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:42:17.503
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 05/06/23 07:42:17.505
    STEP: Creating a ResourceQuota 05/06/23 07:42:22.508
    STEP: Ensuring resource quota status is calculated 05/06/23 07:42:22.514
    STEP: Creating a Service 05/06/23 07:42:24.517
    STEP: Creating a NodePort Service 05/06/23 07:42:24.535
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 05/06/23 07:42:24.558
    STEP: Ensuring resource quota status captures service creation 05/06/23 07:42:24.595
    STEP: Deleting Services 05/06/23 07:42:26.599
    STEP: Ensuring resource quota status released usage 05/06/23 07:42:26.635
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  6 07:42:28.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5454" for this suite. 05/06/23 07:42:28.643
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:42:28.651
May  6 07:42:28.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:42:28.652
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:42:28.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:42:28.67
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-4a9c1c46-966c-4ea2-a99e-9fa8631ea36e 05/06/23 07:42:28.672
STEP: Creating a pod to test consume secrets 05/06/23 07:42:28.676
May  6 07:42:28.683: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-17f1c749-ba18-4ff5-b6a8-362fc6c9e601" in namespace "projected-1796" to be "Succeeded or Failed"
May  6 07:42:28.687: INFO: Pod "pod-projected-secrets-17f1c749-ba18-4ff5-b6a8-362fc6c9e601": Phase="Pending", Reason="", readiness=false. Elapsed: 3.37232ms
May  6 07:42:30.690: INFO: Pod "pod-projected-secrets-17f1c749-ba18-4ff5-b6a8-362fc6c9e601": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007198454s
May  6 07:42:32.691: INFO: Pod "pod-projected-secrets-17f1c749-ba18-4ff5-b6a8-362fc6c9e601": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007690622s
STEP: Saw pod success 05/06/23 07:42:32.691
May  6 07:42:32.691: INFO: Pod "pod-projected-secrets-17f1c749-ba18-4ff5-b6a8-362fc6c9e601" satisfied condition "Succeeded or Failed"
May  6 07:42:32.694: INFO: Trying to get logs from node cncf-2 pod pod-projected-secrets-17f1c749-ba18-4ff5-b6a8-362fc6c9e601 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/06/23 07:42:32.7
May  6 07:42:32.716: INFO: Waiting for pod pod-projected-secrets-17f1c749-ba18-4ff5-b6a8-362fc6c9e601 to disappear
May  6 07:42:32.720: INFO: Pod pod-projected-secrets-17f1c749-ba18-4ff5-b6a8-362fc6c9e601 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May  6 07:42:32.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1796" for this suite. 05/06/23 07:42:32.723
------------------------------
â€¢ [4.079 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:42:28.651
    May  6 07:42:28.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:42:28.652
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:42:28.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:42:28.67
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-4a9c1c46-966c-4ea2-a99e-9fa8631ea36e 05/06/23 07:42:28.672
    STEP: Creating a pod to test consume secrets 05/06/23 07:42:28.676
    May  6 07:42:28.683: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-17f1c749-ba18-4ff5-b6a8-362fc6c9e601" in namespace "projected-1796" to be "Succeeded or Failed"
    May  6 07:42:28.687: INFO: Pod "pod-projected-secrets-17f1c749-ba18-4ff5-b6a8-362fc6c9e601": Phase="Pending", Reason="", readiness=false. Elapsed: 3.37232ms
    May  6 07:42:30.690: INFO: Pod "pod-projected-secrets-17f1c749-ba18-4ff5-b6a8-362fc6c9e601": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007198454s
    May  6 07:42:32.691: INFO: Pod "pod-projected-secrets-17f1c749-ba18-4ff5-b6a8-362fc6c9e601": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007690622s
    STEP: Saw pod success 05/06/23 07:42:32.691
    May  6 07:42:32.691: INFO: Pod "pod-projected-secrets-17f1c749-ba18-4ff5-b6a8-362fc6c9e601" satisfied condition "Succeeded or Failed"
    May  6 07:42:32.694: INFO: Trying to get logs from node cncf-2 pod pod-projected-secrets-17f1c749-ba18-4ff5-b6a8-362fc6c9e601 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/06/23 07:42:32.7
    May  6 07:42:32.716: INFO: Waiting for pod pod-projected-secrets-17f1c749-ba18-4ff5-b6a8-362fc6c9e601 to disappear
    May  6 07:42:32.720: INFO: Pod pod-projected-secrets-17f1c749-ba18-4ff5-b6a8-362fc6c9e601 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May  6 07:42:32.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1796" for this suite. 05/06/23 07:42:32.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:42:32.73
May  6 07:42:32.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename container-probe 05/06/23 07:42:32.731
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:42:32.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:42:32.748
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May  6 07:43:32.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4902" for this suite. 05/06/23 07:43:32.762
------------------------------
â€¢ [SLOW TEST] [60.039 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:42:32.73
    May  6 07:42:32.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename container-probe 05/06/23 07:42:32.731
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:42:32.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:42:32.748
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May  6 07:43:32.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4902" for this suite. 05/06/23 07:43:32.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:43:32.771
May  6 07:43:32.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename deployment 05/06/23 07:43:32.771
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:43:32.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:43:32.788
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
May  6 07:43:32.790: INFO: Creating deployment "webserver-deployment"
May  6 07:43:32.795: INFO: Waiting for observed generation 1
May  6 07:43:34.800: INFO: Waiting for all required pods to come up
May  6 07:43:34.803: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 05/06/23 07:43:34.803
May  6 07:43:34.803: INFO: Waiting for deployment "webserver-deployment" to complete
May  6 07:43:34.808: INFO: Updating deployment "webserver-deployment" with a non-existent image
May  6 07:43:34.816: INFO: Updating deployment webserver-deployment
May  6 07:43:34.816: INFO: Waiting for observed generation 2
May  6 07:43:36.821: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May  6 07:43:36.823: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May  6 07:43:36.826: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May  6 07:43:36.832: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May  6 07:43:36.832: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May  6 07:43:36.835: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May  6 07:43:36.839: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May  6 07:43:36.839: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May  6 07:43:36.847: INFO: Updating deployment webserver-deployment
May  6 07:43:36.847: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May  6 07:43:36.855: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May  6 07:43:36.856: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  6 07:43:38.866: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1529  86e1c310-946d-4a98-975f-26556f6f86ae 160307 3 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d779f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:11,UnavailableReplicas:22,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-06 07:43:36 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-05-06 07:43:38 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,},},ReadyReplicas:11,CollisionCount:nil,},}

May  6 07:43:38.869: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-1529  bbc5e691-0467-4c39-8745-d2974d21a2f8 160138 3 2023-05-06 07:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 86e1c310-946d-4a98-975f-26556f6f86ae 0xc004a7fe67 0xc004a7fe68}] [] [{kube-controller-manager Update apps/v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86e1c310-946d-4a98-975f-26556f6f86ae\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a7ff08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 07:43:38.869: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May  6 07:43:38.869: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-1529  68c8623c-fd90-4360-9aab-4279faabce1b 160305 3 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 86e1c310-946d-4a98-975f-26556f6f86ae 0xc004a7fd77 0xc004a7fd78}] [] [{kube-controller-manager Update apps/v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86e1c310-946d-4a98-975f-26556f6f86ae\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a7fe08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:11,AvailableReplicas:11,Conditions:[]ReplicaSetCondition{},},}
May  6 07:43:38.874: INFO: Pod "webserver-deployment-7f5969cbc7-2crm9" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2crm9 webserver-deployment-7f5969cbc7- deployment-1529  e9a2309a-4b23-40c9-8594-1a26a5c02023 159918 0 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a393c1659da2767297c6c41f9dacb4140a6dcb9096186e70683e77feeb3f9b65 cni.projectcalico.org/podIP:10.244.245.122/32 cni.projectcalico.org/podIPs:10.244.245.122/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc005d77df7 0xc005d77df8}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.245.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f7ktt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f7ktt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:10.244.245.122,StartTime:2023-05-06 07:43:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://586474a2084134a20c0352ef6466bd46742825ff8ee40cfa75b366ad5b7d9b02,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.245.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.874: INFO: Pod "webserver-deployment-7f5969cbc7-7b6kz" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7b6kz webserver-deployment-7f5969cbc7- deployment-1529  affd5e1a-d52d-41a8-bcee-094cb6ae5e33 159936 0 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d984e3fca7c8fc99d5f44eb2ac59b3ce143ea520bda3147d0fee463e6fcacc7c cni.projectcalico.org/podIP:10.244.21.77/32 cni.projectcalico.org/podIPs:10.244.21.77/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248a020 0xc00248a021}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.21.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rpvzg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rpvzg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:10.244.21.77,StartTime:2023-05-06 07:43:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2fb8844bc70136972584de36a762a8691d57db4a6220d1fec5774b2b8d2c639b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.21.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.874: INFO: Pod "webserver-deployment-7f5969cbc7-7zf9t" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7zf9t webserver-deployment-7f5969cbc7- deployment-1529  ca0059e9-a060-49c9-816e-2462cf67936c 160293 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2d757506c18ce95ca7979489d1dfb630f3cfee6d89efbfb369654dca8d09ae3d cni.projectcalico.org/podIP:10.244.20.189/32 cni.projectcalico.org/podIPs:10.244.20.189/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248a250 0xc00248a251}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s92jx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s92jx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.875: INFO: Pod "webserver-deployment-7f5969cbc7-8p7jz" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8p7jz webserver-deployment-7f5969cbc7- deployment-1529  8406a342-76a6-48f7-8629-d37200f0dcc8 160268 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:aa2c5633591100622e215fb569f8f3cb66bdb72d260aa7431ffe4d1971cb3515 cni.projectcalico.org/podIP:10.244.174.151/32 cni.projectcalico.org/podIPs:10.244.174.151/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248a430 0xc00248a431}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pvjj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pvjj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.875: INFO: Pod "webserver-deployment-7f5969cbc7-9qwhp" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9qwhp webserver-deployment-7f5969cbc7- deployment-1529  c7e18a0d-fc09-4116-838a-411d9deb0ee3 159934 0 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:98017510da7bd5474ffce130ca083fc8c8eb2d7c9a5832a63d4c3bc91bc6476b cni.projectcalico.org/podIP:10.244.20.186/32 cni.projectcalico.org/podIPs:10.244.20.186/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248a630 0xc00248a631}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.20.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nxdtd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nxdtd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:10.244.20.186,StartTime:2023-05-06 07:43:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d69bd56212a0c903ae389ee64bd9a9f4fcaecd0e891c582d97b5ac4c723d5933,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.20.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.875: INFO: Pod "webserver-deployment-7f5969cbc7-bbf8k" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bbf8k webserver-deployment-7f5969cbc7- deployment-1529  dbcf49a4-48c4-420d-8541-4d8cb3c107bb 160171 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2d6e4befb709fb7cacd8b4d47573a8a8cf76180a38880daec2bcaaaa870698d4 cni.projectcalico.org/podIP:10.244.21.86/32 cni.projectcalico.org/podIPs:10.244.21.86/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248a850 0xc00248a851}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l8zbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l8zbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.875: INFO: Pod "webserver-deployment-7f5969cbc7-c7wxh" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c7wxh webserver-deployment-7f5969cbc7- deployment-1529  4cbc4be0-30e2-4432-aa96-506628138d50 160286 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5f71025023f3ed644b7487d72dcef5e09ace20cf9ba51f721975342b7ea2753f cni.projectcalico.org/podIP:10.244.21.78/32 cni.projectcalico.org/podIPs:10.244.21.78/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248aa50 0xc00248aa51}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.21.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4bm8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4bm8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:10.244.21.78,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bf233db79287ae0df5b0099ce1d2793e25279c5fe0b5a21fa700f0cda8a147a2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.21.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.875: INFO: Pod "webserver-deployment-7f5969cbc7-c9wgf" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c9wgf webserver-deployment-7f5969cbc7- deployment-1529  5f30f09b-891c-47b5-958f-3b701095c18a 160281 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9a1a0ee8816abb1d79fafec6515690a08dde37b1ef008070efbc13d6a50acbc3 cni.projectcalico.org/podIP:10.244.20.153/32 cni.projectcalico.org/podIPs:10.244.20.153/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248ac80 0xc00248ac81}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.20.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-75wv9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-75wv9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:10.244.20.153,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7024398317e87d5154d1bb7d79a00c46913b36dd4e6f301df47f0dd5ccd75aec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.20.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.875: INFO: Pod "webserver-deployment-7f5969cbc7-czmtk" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-czmtk webserver-deployment-7f5969cbc7- deployment-1529  44d473a8-27bd-4724-8648-facbf22422ad 159931 0 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:32cd875103aeb4392d71f6bc2352a92ca9a1e6e1524e27fe25d35a3b20ead166 cni.projectcalico.org/podIP:10.244.20.185/32 cni.projectcalico.org/podIPs:10.244.20.185/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248afb0 0xc00248afb1}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.20.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kjgbs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kjgbs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:10.244.20.185,StartTime:2023-05-06 07:43:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b001e5f2a5597d3da54899ce7b64375c40f43a07eabea9d1888c93b744f1b3ae,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.20.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.876: INFO: Pod "webserver-deployment-7f5969cbc7-d8fm5" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d8fm5 webserver-deployment-7f5969cbc7- deployment-1529  a1e93041-d1ab-410a-a20e-2209d246e753 160300 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:fef95470ce58ccf0991aefa746b9e05a50c9e4c8928a8f0c8c430fb62e57deff cni.projectcalico.org/podIP:10.244.174.146/32 cni.projectcalico.org/podIPs:10.244.174.146/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248b1c0 0xc00248b1c1}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z8twd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z8twd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.876: INFO: Pod "webserver-deployment-7f5969cbc7-gg4mp" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gg4mp webserver-deployment-7f5969cbc7- deployment-1529  96afcc19-4242-4781-be77-aff6739bbefa 160304 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ecd1cf785b4692e15172f4ad0c6484587ba24cf564cbdcc222e54b209524421b cni.projectcalico.org/podIP:10.244.21.80/32 cni.projectcalico.org/podIPs:10.244.21.80/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248b3c0 0xc00248b3c1}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.21.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l94j8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l94j8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:10.244.21.80,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2dcc0d34d2644e9ac21a1835c49e9ea8b91ccbc1703b43470f29693a15035e9e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.21.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.876: INFO: Pod "webserver-deployment-7f5969cbc7-h6fbk" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-h6fbk webserver-deployment-7f5969cbc7- deployment-1529  671102af-f06b-4970-be63-a3463ec7d8d7 160189 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8cdce859cbcf2baa523a939105a77790963aad77c520a2681566299a9793e188 cni.projectcalico.org/podIP:10.244.20.154/32 cni.projectcalico.org/podIPs:10.244.20.154/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248b600 0xc00248b601}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9xv6t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9xv6t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.876: INFO: Pod "webserver-deployment-7f5969cbc7-hfj64" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hfj64 webserver-deployment-7f5969cbc7- deployment-1529  386335c5-a27a-41f9-a975-d339bf6bbff7 159932 0 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:59fd65214be03ca774a9512224ab5e1e93024d71a0377573b9d5c990b3e23bf9 cni.projectcalico.org/podIP:10.244.21.76/32 cni.projectcalico.org/podIPs:10.244.21.76/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248b810 0xc00248b811}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.21.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qh6cf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qh6cf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:10.244.21.76,StartTime:2023-05-06 07:43:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://dac5e8f4880360ee5bda718b6dd5425579835ade7167ca27052b276c99e5f31a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.21.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.876: INFO: Pod "webserver-deployment-7f5969cbc7-m9pmx" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m9pmx webserver-deployment-7f5969cbc7- deployment-1529  12f90060-23f0-4e24-ad28-e52d2014e436 160220 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:593a36389128de8469e7c83482efa0cb80defb2c3b622ece7d012a4f84b6d0d4 cni.projectcalico.org/podIP:10.244.245.126/32 cni.projectcalico.org/podIPs:10.244.245.126/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248ba20 0xc00248ba21}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9bgw8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9bgw8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.876: INFO: Pod "webserver-deployment-7f5969cbc7-p6n9v" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-p6n9v webserver-deployment-7f5969cbc7- deployment-1529  ec29be5b-cef7-484e-970b-fb7f19802899 159927 0 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5dec4140322f4999631e28ac74ff1eb7fa765ebf4415a2bdcb376212687289a3 cni.projectcalico.org/podIP:10.244.174.189/32 cni.projectcalico.org/podIPs:10.244.174.189/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248bc20 0xc00248bc21}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.174.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4nxqj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4nxqj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:10.244.174.189,StartTime:2023-05-06 07:43:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8e115ba981e4ca2c089c40176b8c6309d1c436bf911dd00dc6bf8aa534481e1c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.174.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.877: INFO: Pod "webserver-deployment-7f5969cbc7-pgkjr" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pgkjr webserver-deployment-7f5969cbc7- deployment-1529  7f273b4b-6db9-4fbd-8c3d-7590edd82435 160228 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9a8495884da76db5fa8f23ca9e9176117d205a96ef6008f32520de9553db970e cni.projectcalico.org/podIP:10.244.174.152/32 cni.projectcalico.org/podIPs:10.244.174.152/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248be40 0xc00248be41}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-69x5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-69x5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.877: INFO: Pod "webserver-deployment-7f5969cbc7-s8nkq" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s8nkq webserver-deployment-7f5969cbc7- deployment-1529  b6730873-4c3a-4db5-97dc-2e416e40e11b 160195 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4a4bde4fa3f482e750ff1d6d42fc3ca1aa464d5c49961b246efcf8777b5bd7a4 cni.projectcalico.org/podIP:10.244.174.147/32 cni.projectcalico.org/podIPs:10.244.174.147/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc0052c0020 0xc0052c0021}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d9vfs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d9vfs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.877: INFO: Pod "webserver-deployment-7f5969cbc7-scvjn" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-scvjn webserver-deployment-7f5969cbc7- deployment-1529  bc1dafca-f5bb-4075-8990-7f2d349161d5 160298 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:989b2bd612f25976f1cbea9b12b404925f51d567e21b895dbd5266ab21e13f73 cni.projectcalico.org/podIP:10.244.245.67/32 cni.projectcalico.org/podIPs:10.244.245.67/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc0052c0220 0xc0052c0221}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nbxw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nbxw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.877: INFO: Pod "webserver-deployment-7f5969cbc7-vmkvh" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vmkvh webserver-deployment-7f5969cbc7- deployment-1529  de2989a3-161c-4b23-9d8d-9544f2a84de1 159914 0 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7c384d086d945772a286cdecf969d147a8c268da83b7a4dc83dcbda34b239ca4 cni.projectcalico.org/podIP:10.244.245.123/32 cni.projectcalico.org/podIPs:10.244.245.123/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc0052c0400 0xc0052c0401}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.245.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-48vjn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-48vjn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:10.244.245.123,StartTime:2023-05-06 07:43:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bac0e06fd6f00e93ac612278092121a77278193c138d8adbc539d8d92d42b022,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.245.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.877: INFO: Pod "webserver-deployment-7f5969cbc7-wwf6c" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wwf6c webserver-deployment-7f5969cbc7- deployment-1529  bdeae3e0-d84c-408a-8097-ef8b01d5c6a0 159912 0 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:787f97119a8480be80f7c8c2d5092a70eb8bc8fa375862774ed063dd8cf87d17 cni.projectcalico.org/podIP:10.244.245.121/32 cni.projectcalico.org/podIPs:10.244.245.121/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc0052c0600 0xc0052c0601}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.245.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x6vn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x6vn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:10.244.245.121,StartTime:2023-05-06 07:43:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6c425976b29bfc6c0df6ad6db02dbc1f9d02eb75d841e81baf1c9cd7e4fa6061,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.245.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.877: INFO: Pod "webserver-deployment-d9f79cb5-458qg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-458qg webserver-deployment-d9f79cb5- deployment-1529  6edd2a0e-9cf6-4462-8724-7c9daf2f779d 160045 0 2023-05-06 07:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:7455b57919805a939b7c4dfecbb45e2a648308ede0615ab6b1fcf0f72fc15207 cni.projectcalico.org/podIP:10.244.20.187/32 cni.projectcalico.org/podIPs:10.244.20.187/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c07ef 0xc0052c0820}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.20.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wdfgp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wdfgp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:10.244.20.187,StartTime:2023-05-06 07:43:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.20.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.878: INFO: Pod "webserver-deployment-d9f79cb5-7n6ds" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7n6ds webserver-deployment-d9f79cb5- deployment-1529  e052caa2-5652-4ab0-a48c-c69b2e54eb9d 160188 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:99627cffda429a68484964e1d1ef648784d33dc351640ef3ccd62e3236fd6fda cni.projectcalico.org/podIP:10.244.245.125/32 cni.projectcalico.org/podIPs:10.244.245.125/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c0a4f 0xc0052c0a60}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4rcw2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4rcw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.878: INFO: Pod "webserver-deployment-d9f79cb5-dd4x2" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dd4x2 webserver-deployment-d9f79cb5- deployment-1529  e46cb4e4-481d-4590-a276-f801b807ccfb 160276 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:0e381c69d4b4e33b115b70468a4208cf16d739c2a38f77628fb289c2ff214c27 cni.projectcalico.org/podIP:10.244.21.90/32 cni.projectcalico.org/podIPs:10.244.21.90/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c0c6f 0xc0052c0ca0}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m5rjr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m5rjr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.878: INFO: Pod "webserver-deployment-d9f79cb5-jgrlz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jgrlz webserver-deployment-d9f79cb5- deployment-1529  f341380c-6963-49d0-8c54-ce1404b68028 160271 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2468cbb32b9119fe0b935a26f6b4c6cce46ce3a19d752ca3463cb38d2ea12012 cni.projectcalico.org/podIP:10.244.245.65/32 cni.projectcalico.org/podIPs:10.244.245.65/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c0e9f 0xc0052c0ef0}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t4jkt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t4jkt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.878: INFO: Pod "webserver-deployment-d9f79cb5-k2lcc" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-k2lcc webserver-deployment-d9f79cb5- deployment-1529  1c277f9f-4e71-40bd-b5a8-fcfb46ee9c12 160013 0 2023-05-06 07:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:61b1eaafda7a3718121a2ac90e6b40b8308a33d37860db564bf2c43f8144be50 cni.projectcalico.org/podIP:10.244.174.132/32 cni.projectcalico.org/podIPs:10.244.174.132/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c10df 0xc0052c10f0}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mgs85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mgs85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:,StartTime:2023-05-06 07:43:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.878: INFO: Pod "webserver-deployment-d9f79cb5-kf6qh" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kf6qh webserver-deployment-d9f79cb5- deployment-1529  bb90b87d-e964-4907-bee9-30fb15365c89 160242 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:053725732e1e6d1b61e7d8c5716769e0b1153729035b5b05a9a0f4690991b7bb cni.projectcalico.org/podIP:10.244.20.139/32 cni.projectcalico.org/podIPs:10.244.20.139/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c12df 0xc0052c1310}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v5tfd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v5tfd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.878: INFO: Pod "webserver-deployment-d9f79cb5-mks45" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mks45 webserver-deployment-d9f79cb5- deployment-1529  46dd858c-0452-4c6f-8417-15acf6b3b8e0 160031 0 2023-05-06 07:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3bb40531f94057dfd291b90f1acec6c5247c98e91d8d5850761a0b73ae114f9b cni.projectcalico.org/podIP:10.244.174.138/32 cni.projectcalico.org/podIPs:10.244.174.138/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c150f 0xc0052c1520}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.174.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l5zhc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l5zhc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:10.244.174.138,StartTime:2023-05-06 07:43:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.174.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.878: INFO: Pod "webserver-deployment-d9f79cb5-p8krt" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-p8krt webserver-deployment-d9f79cb5- deployment-1529  385bda73-052b-4e28-8ac9-caf99b9514ca 160048 0 2023-05-06 07:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:9397d7dcd5b970beac0b0eb13b6bc688954643fc89caa1d58f34561d57df6de2 cni.projectcalico.org/podIP:10.244.21.89/32 cni.projectcalico.org/podIPs:10.244.21.89/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c174f 0xc0052c1780}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.21.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v48h4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v48h4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:10.244.21.89,StartTime:2023-05-06 07:43:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.21.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.879: INFO: Pod "webserver-deployment-d9f79cb5-r6kzs" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-r6kzs webserver-deployment-d9f79cb5- deployment-1529  7ca87424-6f4f-4772-b570-d35fd5466914 160244 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:854839a4cc393e067252b04bd0a68116aff7e466428fa343c056063d3595bfde cni.projectcalico.org/podIP:10.244.174.144/32 cni.projectcalico.org/podIPs:10.244.174.144/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c19af 0xc0052c19c0}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gg49m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gg49m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.879: INFO: Pod "webserver-deployment-d9f79cb5-svb74" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-svb74 webserver-deployment-d9f79cb5- deployment-1529  6168332c-af5b-41fb-a21e-5117a4405df6 160232 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:8a244cdcc45a408ba7a84c54bad6d2a7e53fca070dec675f1ad4cfe8322271c4 cni.projectcalico.org/podIP:10.244.245.127/32 cni.projectcalico.org/podIPs:10.244.245.127/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c1bcf 0xc0052c1be0}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lwpft,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lwpft,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.879: INFO: Pod "webserver-deployment-d9f79cb5-swcgj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-swcgj webserver-deployment-d9f79cb5- deployment-1529  0d9b31f3-63cc-4d26-b088-5869b46bcc0a 160250 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:1c47d1cc1db26e9f1a5a574e1004502c6c4905bdd604e71a8fe3e04d74780078 cni.projectcalico.org/podIP:10.244.21.87/32 cni.projectcalico.org/podIPs:10.244.21.87/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc004da40bf 0xc004da40f0}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-llndk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-llndk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.879: INFO: Pod "webserver-deployment-d9f79cb5-vbsh9" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vbsh9 webserver-deployment-d9f79cb5- deployment-1529  c7aa8d30-f221-4433-8011-36106f147cc8 160135 0 2023-05-06 07:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:497ab9c3137143804f15a37a5868d01a4d1ddff583ba049a31c0a356b05aa126 cni.projectcalico.org/podIP:10.244.245.124/32 cni.projectcalico.org/podIPs:10.244.245.124/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc004da42ff 0xc004da4310}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.245.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n6cdp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n6cdp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:10.244.245.124,StartTime:2023-05-06 07:43:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.245.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  6 07:43:38.879: INFO: Pod "webserver-deployment-d9f79cb5-whnsl" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-whnsl webserver-deployment-d9f79cb5- deployment-1529  d2665ec5-334a-4d8e-a3c3-1ea161fe0c1b 160169 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:4b84d6c8d36bf577272468c5b90bb995581d5527e599098c0d54c40bf36a5278 cni.projectcalico.org/podIP:10.244.20.149/32 cni.projectcalico.org/podIPs:10.244.20.149/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc004da453f 0xc004da4570}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bz844,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bz844,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May  6 07:43:38.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1529" for this suite. 05/06/23 07:43:38.882
------------------------------
â€¢ [SLOW TEST] [6.117 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:43:32.771
    May  6 07:43:32.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename deployment 05/06/23 07:43:32.771
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:43:32.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:43:32.788
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    May  6 07:43:32.790: INFO: Creating deployment "webserver-deployment"
    May  6 07:43:32.795: INFO: Waiting for observed generation 1
    May  6 07:43:34.800: INFO: Waiting for all required pods to come up
    May  6 07:43:34.803: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 05/06/23 07:43:34.803
    May  6 07:43:34.803: INFO: Waiting for deployment "webserver-deployment" to complete
    May  6 07:43:34.808: INFO: Updating deployment "webserver-deployment" with a non-existent image
    May  6 07:43:34.816: INFO: Updating deployment webserver-deployment
    May  6 07:43:34.816: INFO: Waiting for observed generation 2
    May  6 07:43:36.821: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    May  6 07:43:36.823: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    May  6 07:43:36.826: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    May  6 07:43:36.832: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    May  6 07:43:36.832: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    May  6 07:43:36.835: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    May  6 07:43:36.839: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    May  6 07:43:36.839: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    May  6 07:43:36.847: INFO: Updating deployment webserver-deployment
    May  6 07:43:36.847: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    May  6 07:43:36.855: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    May  6 07:43:36.856: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  6 07:43:38.866: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-1529  86e1c310-946d-4a98-975f-26556f6f86ae 160307 3 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d779f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:11,UnavailableReplicas:22,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-06 07:43:36 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-05-06 07:43:38 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,},},ReadyReplicas:11,CollisionCount:nil,},}

    May  6 07:43:38.869: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-1529  bbc5e691-0467-4c39-8745-d2974d21a2f8 160138 3 2023-05-06 07:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 86e1c310-946d-4a98-975f-26556f6f86ae 0xc004a7fe67 0xc004a7fe68}] [] [{kube-controller-manager Update apps/v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86e1c310-946d-4a98-975f-26556f6f86ae\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a7ff08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  6 07:43:38.869: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    May  6 07:43:38.869: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-1529  68c8623c-fd90-4360-9aab-4279faabce1b 160305 3 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 86e1c310-946d-4a98-975f-26556f6f86ae 0xc004a7fd77 0xc004a7fd78}] [] [{kube-controller-manager Update apps/v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86e1c310-946d-4a98-975f-26556f6f86ae\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a7fe08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:11,AvailableReplicas:11,Conditions:[]ReplicaSetCondition{},},}
    May  6 07:43:38.874: INFO: Pod "webserver-deployment-7f5969cbc7-2crm9" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2crm9 webserver-deployment-7f5969cbc7- deployment-1529  e9a2309a-4b23-40c9-8594-1a26a5c02023 159918 0 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a393c1659da2767297c6c41f9dacb4140a6dcb9096186e70683e77feeb3f9b65 cni.projectcalico.org/podIP:10.244.245.122/32 cni.projectcalico.org/podIPs:10.244.245.122/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc005d77df7 0xc005d77df8}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.245.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f7ktt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f7ktt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:10.244.245.122,StartTime:2023-05-06 07:43:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://586474a2084134a20c0352ef6466bd46742825ff8ee40cfa75b366ad5b7d9b02,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.245.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.874: INFO: Pod "webserver-deployment-7f5969cbc7-7b6kz" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7b6kz webserver-deployment-7f5969cbc7- deployment-1529  affd5e1a-d52d-41a8-bcee-094cb6ae5e33 159936 0 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d984e3fca7c8fc99d5f44eb2ac59b3ce143ea520bda3147d0fee463e6fcacc7c cni.projectcalico.org/podIP:10.244.21.77/32 cni.projectcalico.org/podIPs:10.244.21.77/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248a020 0xc00248a021}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.21.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rpvzg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rpvzg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:10.244.21.77,StartTime:2023-05-06 07:43:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2fb8844bc70136972584de36a762a8691d57db4a6220d1fec5774b2b8d2c639b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.21.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.874: INFO: Pod "webserver-deployment-7f5969cbc7-7zf9t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7zf9t webserver-deployment-7f5969cbc7- deployment-1529  ca0059e9-a060-49c9-816e-2462cf67936c 160293 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2d757506c18ce95ca7979489d1dfb630f3cfee6d89efbfb369654dca8d09ae3d cni.projectcalico.org/podIP:10.244.20.189/32 cni.projectcalico.org/podIPs:10.244.20.189/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248a250 0xc00248a251}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s92jx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s92jx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.875: INFO: Pod "webserver-deployment-7f5969cbc7-8p7jz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8p7jz webserver-deployment-7f5969cbc7- deployment-1529  8406a342-76a6-48f7-8629-d37200f0dcc8 160268 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:aa2c5633591100622e215fb569f8f3cb66bdb72d260aa7431ffe4d1971cb3515 cni.projectcalico.org/podIP:10.244.174.151/32 cni.projectcalico.org/podIPs:10.244.174.151/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248a430 0xc00248a431}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pvjj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pvjj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.875: INFO: Pod "webserver-deployment-7f5969cbc7-9qwhp" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9qwhp webserver-deployment-7f5969cbc7- deployment-1529  c7e18a0d-fc09-4116-838a-411d9deb0ee3 159934 0 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:98017510da7bd5474ffce130ca083fc8c8eb2d7c9a5832a63d4c3bc91bc6476b cni.projectcalico.org/podIP:10.244.20.186/32 cni.projectcalico.org/podIPs:10.244.20.186/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248a630 0xc00248a631}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.20.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nxdtd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nxdtd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:10.244.20.186,StartTime:2023-05-06 07:43:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d69bd56212a0c903ae389ee64bd9a9f4fcaecd0e891c582d97b5ac4c723d5933,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.20.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.875: INFO: Pod "webserver-deployment-7f5969cbc7-bbf8k" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bbf8k webserver-deployment-7f5969cbc7- deployment-1529  dbcf49a4-48c4-420d-8541-4d8cb3c107bb 160171 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2d6e4befb709fb7cacd8b4d47573a8a8cf76180a38880daec2bcaaaa870698d4 cni.projectcalico.org/podIP:10.244.21.86/32 cni.projectcalico.org/podIPs:10.244.21.86/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248a850 0xc00248a851}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l8zbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l8zbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.875: INFO: Pod "webserver-deployment-7f5969cbc7-c7wxh" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c7wxh webserver-deployment-7f5969cbc7- deployment-1529  4cbc4be0-30e2-4432-aa96-506628138d50 160286 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5f71025023f3ed644b7487d72dcef5e09ace20cf9ba51f721975342b7ea2753f cni.projectcalico.org/podIP:10.244.21.78/32 cni.projectcalico.org/podIPs:10.244.21.78/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248aa50 0xc00248aa51}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.21.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4bm8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4bm8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:10.244.21.78,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bf233db79287ae0df5b0099ce1d2793e25279c5fe0b5a21fa700f0cda8a147a2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.21.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.875: INFO: Pod "webserver-deployment-7f5969cbc7-c9wgf" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c9wgf webserver-deployment-7f5969cbc7- deployment-1529  5f30f09b-891c-47b5-958f-3b701095c18a 160281 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9a1a0ee8816abb1d79fafec6515690a08dde37b1ef008070efbc13d6a50acbc3 cni.projectcalico.org/podIP:10.244.20.153/32 cni.projectcalico.org/podIPs:10.244.20.153/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248ac80 0xc00248ac81}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.20.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-75wv9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-75wv9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:10.244.20.153,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7024398317e87d5154d1bb7d79a00c46913b36dd4e6f301df47f0dd5ccd75aec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.20.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.875: INFO: Pod "webserver-deployment-7f5969cbc7-czmtk" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-czmtk webserver-deployment-7f5969cbc7- deployment-1529  44d473a8-27bd-4724-8648-facbf22422ad 159931 0 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:32cd875103aeb4392d71f6bc2352a92ca9a1e6e1524e27fe25d35a3b20ead166 cni.projectcalico.org/podIP:10.244.20.185/32 cni.projectcalico.org/podIPs:10.244.20.185/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248afb0 0xc00248afb1}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.20.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kjgbs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kjgbs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:10.244.20.185,StartTime:2023-05-06 07:43:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b001e5f2a5597d3da54899ce7b64375c40f43a07eabea9d1888c93b744f1b3ae,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.20.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.876: INFO: Pod "webserver-deployment-7f5969cbc7-d8fm5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d8fm5 webserver-deployment-7f5969cbc7- deployment-1529  a1e93041-d1ab-410a-a20e-2209d246e753 160300 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:fef95470ce58ccf0991aefa746b9e05a50c9e4c8928a8f0c8c430fb62e57deff cni.projectcalico.org/podIP:10.244.174.146/32 cni.projectcalico.org/podIPs:10.244.174.146/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248b1c0 0xc00248b1c1}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z8twd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z8twd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.876: INFO: Pod "webserver-deployment-7f5969cbc7-gg4mp" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gg4mp webserver-deployment-7f5969cbc7- deployment-1529  96afcc19-4242-4781-be77-aff6739bbefa 160304 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ecd1cf785b4692e15172f4ad0c6484587ba24cf564cbdcc222e54b209524421b cni.projectcalico.org/podIP:10.244.21.80/32 cni.projectcalico.org/podIPs:10.244.21.80/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248b3c0 0xc00248b3c1}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.21.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l94j8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l94j8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:10.244.21.80,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2dcc0d34d2644e9ac21a1835c49e9ea8b91ccbc1703b43470f29693a15035e9e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.21.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.876: INFO: Pod "webserver-deployment-7f5969cbc7-h6fbk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-h6fbk webserver-deployment-7f5969cbc7- deployment-1529  671102af-f06b-4970-be63-a3463ec7d8d7 160189 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8cdce859cbcf2baa523a939105a77790963aad77c520a2681566299a9793e188 cni.projectcalico.org/podIP:10.244.20.154/32 cni.projectcalico.org/podIPs:10.244.20.154/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248b600 0xc00248b601}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9xv6t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9xv6t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.876: INFO: Pod "webserver-deployment-7f5969cbc7-hfj64" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hfj64 webserver-deployment-7f5969cbc7- deployment-1529  386335c5-a27a-41f9-a975-d339bf6bbff7 159932 0 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:59fd65214be03ca774a9512224ab5e1e93024d71a0377573b9d5c990b3e23bf9 cni.projectcalico.org/podIP:10.244.21.76/32 cni.projectcalico.org/podIPs:10.244.21.76/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248b810 0xc00248b811}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.21.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qh6cf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qh6cf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:10.244.21.76,StartTime:2023-05-06 07:43:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://dac5e8f4880360ee5bda718b6dd5425579835ade7167ca27052b276c99e5f31a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.21.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.876: INFO: Pod "webserver-deployment-7f5969cbc7-m9pmx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m9pmx webserver-deployment-7f5969cbc7- deployment-1529  12f90060-23f0-4e24-ad28-e52d2014e436 160220 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:593a36389128de8469e7c83482efa0cb80defb2c3b622ece7d012a4f84b6d0d4 cni.projectcalico.org/podIP:10.244.245.126/32 cni.projectcalico.org/podIPs:10.244.245.126/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248ba20 0xc00248ba21}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9bgw8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9bgw8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.876: INFO: Pod "webserver-deployment-7f5969cbc7-p6n9v" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-p6n9v webserver-deployment-7f5969cbc7- deployment-1529  ec29be5b-cef7-484e-970b-fb7f19802899 159927 0 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5dec4140322f4999631e28ac74ff1eb7fa765ebf4415a2bdcb376212687289a3 cni.projectcalico.org/podIP:10.244.174.189/32 cni.projectcalico.org/podIPs:10.244.174.189/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248bc20 0xc00248bc21}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.174.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4nxqj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4nxqj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:10.244.174.189,StartTime:2023-05-06 07:43:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8e115ba981e4ca2c089c40176b8c6309d1c436bf911dd00dc6bf8aa534481e1c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.174.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.877: INFO: Pod "webserver-deployment-7f5969cbc7-pgkjr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pgkjr webserver-deployment-7f5969cbc7- deployment-1529  7f273b4b-6db9-4fbd-8c3d-7590edd82435 160228 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9a8495884da76db5fa8f23ca9e9176117d205a96ef6008f32520de9553db970e cni.projectcalico.org/podIP:10.244.174.152/32 cni.projectcalico.org/podIPs:10.244.174.152/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc00248be40 0xc00248be41}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-69x5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-69x5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.877: INFO: Pod "webserver-deployment-7f5969cbc7-s8nkq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s8nkq webserver-deployment-7f5969cbc7- deployment-1529  b6730873-4c3a-4db5-97dc-2e416e40e11b 160195 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4a4bde4fa3f482e750ff1d6d42fc3ca1aa464d5c49961b246efcf8777b5bd7a4 cni.projectcalico.org/podIP:10.244.174.147/32 cni.projectcalico.org/podIPs:10.244.174.147/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc0052c0020 0xc0052c0021}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d9vfs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d9vfs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.877: INFO: Pod "webserver-deployment-7f5969cbc7-scvjn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-scvjn webserver-deployment-7f5969cbc7- deployment-1529  bc1dafca-f5bb-4075-8990-7f2d349161d5 160298 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:989b2bd612f25976f1cbea9b12b404925f51d567e21b895dbd5266ab21e13f73 cni.projectcalico.org/podIP:10.244.245.67/32 cni.projectcalico.org/podIPs:10.244.245.67/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc0052c0220 0xc0052c0221}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nbxw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nbxw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.877: INFO: Pod "webserver-deployment-7f5969cbc7-vmkvh" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vmkvh webserver-deployment-7f5969cbc7- deployment-1529  de2989a3-161c-4b23-9d8d-9544f2a84de1 159914 0 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7c384d086d945772a286cdecf969d147a8c268da83b7a4dc83dcbda34b239ca4 cni.projectcalico.org/podIP:10.244.245.123/32 cni.projectcalico.org/podIPs:10.244.245.123/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc0052c0400 0xc0052c0401}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.245.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-48vjn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-48vjn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:10.244.245.123,StartTime:2023-05-06 07:43:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bac0e06fd6f00e93ac612278092121a77278193c138d8adbc539d8d92d42b022,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.245.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.877: INFO: Pod "webserver-deployment-7f5969cbc7-wwf6c" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wwf6c webserver-deployment-7f5969cbc7- deployment-1529  bdeae3e0-d84c-408a-8097-ef8b01d5c6a0 159912 0 2023-05-06 07:43:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:787f97119a8480be80f7c8c2d5092a70eb8bc8fa375862774ed063dd8cf87d17 cni.projectcalico.org/podIP:10.244.245.121/32 cni.projectcalico.org/podIPs:10.244.245.121/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 68c8623c-fd90-4360-9aab-4279faabce1b 0xc0052c0600 0xc0052c0601}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c8623c-fd90-4360-9aab-4279faabce1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.245.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x6vn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x6vn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:10.244.245.121,StartTime:2023-05-06 07:43:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:43:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6c425976b29bfc6c0df6ad6db02dbc1f9d02eb75d841e81baf1c9cd7e4fa6061,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.245.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.877: INFO: Pod "webserver-deployment-d9f79cb5-458qg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-458qg webserver-deployment-d9f79cb5- deployment-1529  6edd2a0e-9cf6-4462-8724-7c9daf2f779d 160045 0 2023-05-06 07:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:7455b57919805a939b7c4dfecbb45e2a648308ede0615ab6b1fcf0f72fc15207 cni.projectcalico.org/podIP:10.244.20.187/32 cni.projectcalico.org/podIPs:10.244.20.187/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c07ef 0xc0052c0820}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.20.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wdfgp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wdfgp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:10.244.20.187,StartTime:2023-05-06 07:43:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.20.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.878: INFO: Pod "webserver-deployment-d9f79cb5-7n6ds" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7n6ds webserver-deployment-d9f79cb5- deployment-1529  e052caa2-5652-4ab0-a48c-c69b2e54eb9d 160188 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:99627cffda429a68484964e1d1ef648784d33dc351640ef3ccd62e3236fd6fda cni.projectcalico.org/podIP:10.244.245.125/32 cni.projectcalico.org/podIPs:10.244.245.125/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c0a4f 0xc0052c0a60}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4rcw2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4rcw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.878: INFO: Pod "webserver-deployment-d9f79cb5-dd4x2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dd4x2 webserver-deployment-d9f79cb5- deployment-1529  e46cb4e4-481d-4590-a276-f801b807ccfb 160276 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:0e381c69d4b4e33b115b70468a4208cf16d739c2a38f77628fb289c2ff214c27 cni.projectcalico.org/podIP:10.244.21.90/32 cni.projectcalico.org/podIPs:10.244.21.90/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c0c6f 0xc0052c0ca0}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m5rjr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m5rjr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.878: INFO: Pod "webserver-deployment-d9f79cb5-jgrlz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jgrlz webserver-deployment-d9f79cb5- deployment-1529  f341380c-6963-49d0-8c54-ce1404b68028 160271 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2468cbb32b9119fe0b935a26f6b4c6cce46ce3a19d752ca3463cb38d2ea12012 cni.projectcalico.org/podIP:10.244.245.65/32 cni.projectcalico.org/podIPs:10.244.245.65/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c0e9f 0xc0052c0ef0}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t4jkt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t4jkt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.878: INFO: Pod "webserver-deployment-d9f79cb5-k2lcc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-k2lcc webserver-deployment-d9f79cb5- deployment-1529  1c277f9f-4e71-40bd-b5a8-fcfb46ee9c12 160013 0 2023-05-06 07:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:61b1eaafda7a3718121a2ac90e6b40b8308a33d37860db564bf2c43f8144be50 cni.projectcalico.org/podIP:10.244.174.132/32 cni.projectcalico.org/podIPs:10.244.174.132/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c10df 0xc0052c10f0}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mgs85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mgs85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:,StartTime:2023-05-06 07:43:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.878: INFO: Pod "webserver-deployment-d9f79cb5-kf6qh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kf6qh webserver-deployment-d9f79cb5- deployment-1529  bb90b87d-e964-4907-bee9-30fb15365c89 160242 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:053725732e1e6d1b61e7d8c5716769e0b1153729035b5b05a9a0f4690991b7bb cni.projectcalico.org/podIP:10.244.20.139/32 cni.projectcalico.org/podIPs:10.244.20.139/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c12df 0xc0052c1310}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v5tfd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v5tfd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.878: INFO: Pod "webserver-deployment-d9f79cb5-mks45" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mks45 webserver-deployment-d9f79cb5- deployment-1529  46dd858c-0452-4c6f-8417-15acf6b3b8e0 160031 0 2023-05-06 07:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3bb40531f94057dfd291b90f1acec6c5247c98e91d8d5850761a0b73ae114f9b cni.projectcalico.org/podIP:10.244.174.138/32 cni.projectcalico.org/podIPs:10.244.174.138/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c150f 0xc0052c1520}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.174.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l5zhc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l5zhc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:10.244.174.138,StartTime:2023-05-06 07:43:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.174.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.878: INFO: Pod "webserver-deployment-d9f79cb5-p8krt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-p8krt webserver-deployment-d9f79cb5- deployment-1529  385bda73-052b-4e28-8ac9-caf99b9514ca 160048 0 2023-05-06 07:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:9397d7dcd5b970beac0b0eb13b6bc688954643fc89caa1d58f34561d57df6de2 cni.projectcalico.org/podIP:10.244.21.89/32 cni.projectcalico.org/podIPs:10.244.21.89/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c174f 0xc0052c1780}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.21.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v48h4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v48h4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:10.244.21.89,StartTime:2023-05-06 07:43:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.21.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.879: INFO: Pod "webserver-deployment-d9f79cb5-r6kzs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-r6kzs webserver-deployment-d9f79cb5- deployment-1529  7ca87424-6f4f-4772-b570-d35fd5466914 160244 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:854839a4cc393e067252b04bd0a68116aff7e466428fa343c056063d3595bfde cni.projectcalico.org/podIP:10.244.174.144/32 cni.projectcalico.org/podIPs:10.244.174.144/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c19af 0xc0052c19c0}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gg49m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gg49m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.879: INFO: Pod "webserver-deployment-d9f79cb5-svb74" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-svb74 webserver-deployment-d9f79cb5- deployment-1529  6168332c-af5b-41fb-a21e-5117a4405df6 160232 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:8a244cdcc45a408ba7a84c54bad6d2a7e53fca070dec675f1ad4cfe8322271c4 cni.projectcalico.org/podIP:10.244.245.127/32 cni.projectcalico.org/podIPs:10.244.245.127/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc0052c1bcf 0xc0052c1be0}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lwpft,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lwpft,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.879: INFO: Pod "webserver-deployment-d9f79cb5-swcgj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-swcgj webserver-deployment-d9f79cb5- deployment-1529  0d9b31f3-63cc-4d26-b088-5869b46bcc0a 160250 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:1c47d1cc1db26e9f1a5a574e1004502c6c4905bdd604e71a8fe3e04d74780078 cni.projectcalico.org/podIP:10.244.21.87/32 cni.projectcalico.org/podIPs:10.244.21.87/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc004da40bf 0xc004da40f0}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-llndk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-llndk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.879: INFO: Pod "webserver-deployment-d9f79cb5-vbsh9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vbsh9 webserver-deployment-d9f79cb5- deployment-1529  c7aa8d30-f221-4433-8011-36106f147cc8 160135 0 2023-05-06 07:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:497ab9c3137143804f15a37a5868d01a4d1ddff583ba049a31c0a356b05aa126 cni.projectcalico.org/podIP:10.244.245.124/32 cni.projectcalico.org/podIPs:10.244.245.124/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc004da42ff 0xc004da4310}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:43:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.245.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n6cdp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n6cdp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:10.244.245.124,StartTime:2023-05-06 07:43:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.245.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  6 07:43:38.879: INFO: Pod "webserver-deployment-d9f79cb5-whnsl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-whnsl webserver-deployment-d9f79cb5- deployment-1529  d2665ec5-334a-4d8e-a3c3-1ea161fe0c1b 160169 0 2023-05-06 07:43:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:4b84d6c8d36bf577272468c5b90bb995581d5527e599098c0d54c40bf36a5278 cni.projectcalico.org/podIP:10.244.20.149/32 cni.projectcalico.org/podIPs:10.244.20.149/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 bbc5e691-0467-4c39-8745-d2974d21a2f8 0xc004da453f 0xc004da4570}] [] [{kube-controller-manager Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc5e691-0467-4c39-8745-d2974d21a2f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 07:43:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-06 07:43:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bz844,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bz844,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:43:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:,StartTime:2023-05-06 07:43:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May  6 07:43:38.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1529" for this suite. 05/06/23 07:43:38.882
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:43:38.89
May  6 07:43:38.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename subpath 05/06/23 07:43:38.89
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:43:38.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:43:38.908
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/06/23 07:43:38.91
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-s8xb 05/06/23 07:43:38.919
STEP: Creating a pod to test atomic-volume-subpath 05/06/23 07:43:38.919
May  6 07:43:38.929: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-s8xb" in namespace "subpath-1448" to be "Succeeded or Failed"
May  6 07:43:38.931: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.307043ms
May  6 07:43:40.935: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006183132s
May  6 07:43:42.934: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 4.005454499s
May  6 07:43:44.936: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 6.007773248s
May  6 07:43:46.934: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 8.005024555s
May  6 07:43:48.933: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 10.004911734s
May  6 07:43:50.934: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 12.005189413s
May  6 07:43:52.937: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 14.008071863s
May  6 07:43:54.934: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 16.005167734s
May  6 07:43:56.935: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 18.006652898s
May  6 07:43:58.934: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 20.005188387s
May  6 07:44:00.935: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 22.006072978s
May  6 07:44:02.935: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=false. Elapsed: 24.006641051s
May  6 07:44:04.935: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.006101256s
STEP: Saw pod success 05/06/23 07:44:04.935
May  6 07:44:04.935: INFO: Pod "pod-subpath-test-configmap-s8xb" satisfied condition "Succeeded or Failed"
May  6 07:44:04.937: INFO: Trying to get logs from node cncf-3 pod pod-subpath-test-configmap-s8xb container test-container-subpath-configmap-s8xb: <nil>
STEP: delete the pod 05/06/23 07:44:04.947
May  6 07:44:04.960: INFO: Waiting for pod pod-subpath-test-configmap-s8xb to disappear
May  6 07:44:04.963: INFO: Pod pod-subpath-test-configmap-s8xb no longer exists
STEP: Deleting pod pod-subpath-test-configmap-s8xb 05/06/23 07:44:04.963
May  6 07:44:04.963: INFO: Deleting pod "pod-subpath-test-configmap-s8xb" in namespace "subpath-1448"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May  6 07:44:04.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-1448" for this suite. 05/06/23 07:44:04.968
------------------------------
â€¢ [SLOW TEST] [26.083 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:43:38.89
    May  6 07:43:38.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename subpath 05/06/23 07:43:38.89
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:43:38.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:43:38.908
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/06/23 07:43:38.91
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-s8xb 05/06/23 07:43:38.919
    STEP: Creating a pod to test atomic-volume-subpath 05/06/23 07:43:38.919
    May  6 07:43:38.929: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-s8xb" in namespace "subpath-1448" to be "Succeeded or Failed"
    May  6 07:43:38.931: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.307043ms
    May  6 07:43:40.935: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006183132s
    May  6 07:43:42.934: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 4.005454499s
    May  6 07:43:44.936: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 6.007773248s
    May  6 07:43:46.934: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 8.005024555s
    May  6 07:43:48.933: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 10.004911734s
    May  6 07:43:50.934: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 12.005189413s
    May  6 07:43:52.937: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 14.008071863s
    May  6 07:43:54.934: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 16.005167734s
    May  6 07:43:56.935: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 18.006652898s
    May  6 07:43:58.934: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 20.005188387s
    May  6 07:44:00.935: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=true. Elapsed: 22.006072978s
    May  6 07:44:02.935: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Running", Reason="", readiness=false. Elapsed: 24.006641051s
    May  6 07:44:04.935: INFO: Pod "pod-subpath-test-configmap-s8xb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.006101256s
    STEP: Saw pod success 05/06/23 07:44:04.935
    May  6 07:44:04.935: INFO: Pod "pod-subpath-test-configmap-s8xb" satisfied condition "Succeeded or Failed"
    May  6 07:44:04.937: INFO: Trying to get logs from node cncf-3 pod pod-subpath-test-configmap-s8xb container test-container-subpath-configmap-s8xb: <nil>
    STEP: delete the pod 05/06/23 07:44:04.947
    May  6 07:44:04.960: INFO: Waiting for pod pod-subpath-test-configmap-s8xb to disappear
    May  6 07:44:04.963: INFO: Pod pod-subpath-test-configmap-s8xb no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-s8xb 05/06/23 07:44:04.963
    May  6 07:44:04.963: INFO: Deleting pod "pod-subpath-test-configmap-s8xb" in namespace "subpath-1448"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May  6 07:44:04.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-1448" for this suite. 05/06/23 07:44:04.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:44:04.975
May  6 07:44:04.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename watch 05/06/23 07:44:04.976
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:04.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:04.992
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 05/06/23 07:44:04.994
STEP: creating a new configmap 05/06/23 07:44:04.995
STEP: modifying the configmap once 05/06/23 07:44:04.998
STEP: changing the label value of the configmap 05/06/23 07:44:05.004
STEP: Expecting to observe a delete notification for the watched object 05/06/23 07:44:05.009
May  6 07:44:05.009: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8343  062c331c-5ef8-4b10-93b8-467332bfe4b2 160827 0 2023-05-06 07:44:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-06 07:44:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  6 07:44:05.010: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8343  062c331c-5ef8-4b10-93b8-467332bfe4b2 160828 0 2023-05-06 07:44:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-06 07:44:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May  6 07:44:05.010: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8343  062c331c-5ef8-4b10-93b8-467332bfe4b2 160829 0 2023-05-06 07:44:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-06 07:44:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 05/06/23 07:44:05.01
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 05/06/23 07:44:05.016
STEP: changing the label value of the configmap back 05/06/23 07:44:15.017
STEP: modifying the configmap a third time 05/06/23 07:44:15.026
STEP: deleting the configmap 05/06/23 07:44:15.034
STEP: Expecting to observe an add notification for the watched object when the label value was restored 05/06/23 07:44:15.04
May  6 07:44:15.040: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8343  062c331c-5ef8-4b10-93b8-467332bfe4b2 160878 0 2023-05-06 07:44:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-06 07:44:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  6 07:44:15.040: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8343  062c331c-5ef8-4b10-93b8-467332bfe4b2 160879 0 2023-05-06 07:44:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-06 07:44:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May  6 07:44:15.040: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8343  062c331c-5ef8-4b10-93b8-467332bfe4b2 160881 0 2023-05-06 07:44:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-06 07:44:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May  6 07:44:15.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8343" for this suite. 05/06/23 07:44:15.043
------------------------------
â€¢ [SLOW TEST] [10.079 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:44:04.975
    May  6 07:44:04.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename watch 05/06/23 07:44:04.976
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:04.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:04.992
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 05/06/23 07:44:04.994
    STEP: creating a new configmap 05/06/23 07:44:04.995
    STEP: modifying the configmap once 05/06/23 07:44:04.998
    STEP: changing the label value of the configmap 05/06/23 07:44:05.004
    STEP: Expecting to observe a delete notification for the watched object 05/06/23 07:44:05.009
    May  6 07:44:05.009: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8343  062c331c-5ef8-4b10-93b8-467332bfe4b2 160827 0 2023-05-06 07:44:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-06 07:44:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  6 07:44:05.010: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8343  062c331c-5ef8-4b10-93b8-467332bfe4b2 160828 0 2023-05-06 07:44:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-06 07:44:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  6 07:44:05.010: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8343  062c331c-5ef8-4b10-93b8-467332bfe4b2 160829 0 2023-05-06 07:44:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-06 07:44:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 05/06/23 07:44:05.01
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 05/06/23 07:44:05.016
    STEP: changing the label value of the configmap back 05/06/23 07:44:15.017
    STEP: modifying the configmap a third time 05/06/23 07:44:15.026
    STEP: deleting the configmap 05/06/23 07:44:15.034
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 05/06/23 07:44:15.04
    May  6 07:44:15.040: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8343  062c331c-5ef8-4b10-93b8-467332bfe4b2 160878 0 2023-05-06 07:44:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-06 07:44:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  6 07:44:15.040: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8343  062c331c-5ef8-4b10-93b8-467332bfe4b2 160879 0 2023-05-06 07:44:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-06 07:44:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  6 07:44:15.040: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8343  062c331c-5ef8-4b10-93b8-467332bfe4b2 160881 0 2023-05-06 07:44:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-06 07:44:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May  6 07:44:15.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8343" for this suite. 05/06/23 07:44:15.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:44:15.056
May  6 07:44:15.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename pods 05/06/23 07:44:15.057
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:15.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:15.073
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 05/06/23 07:44:15.075
STEP: submitting the pod to kubernetes 05/06/23 07:44:15.075
May  6 07:44:15.084: INFO: Waiting up to 5m0s for pod "pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c" in namespace "pods-4422" to be "running and ready"
May  6 07:44:15.086: INFO: Pod "pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017023ms
May  6 07:44:15.086: INFO: The phase of Pod pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c is Pending, waiting for it to be Running (with Ready = true)
May  6 07:44:17.089: INFO: Pod "pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004848155s
May  6 07:44:17.089: INFO: The phase of Pod pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c is Running (Ready = true)
May  6 07:44:17.089: INFO: Pod "pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 05/06/23 07:44:17.092
STEP: updating the pod 05/06/23 07:44:17.094
May  6 07:44:17.604: INFO: Successfully updated pod "pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c"
May  6 07:44:17.604: INFO: Waiting up to 5m0s for pod "pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c" in namespace "pods-4422" to be "running"
May  6 07:44:17.607: INFO: Pod "pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c": Phase="Running", Reason="", readiness=true. Elapsed: 2.579142ms
May  6 07:44:17.607: INFO: Pod "pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 05/06/23 07:44:17.607
May  6 07:44:17.609: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  6 07:44:17.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4422" for this suite. 05/06/23 07:44:17.612
------------------------------
â€¢ [2.561 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:44:15.056
    May  6 07:44:15.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename pods 05/06/23 07:44:15.057
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:15.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:15.073
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 05/06/23 07:44:15.075
    STEP: submitting the pod to kubernetes 05/06/23 07:44:15.075
    May  6 07:44:15.084: INFO: Waiting up to 5m0s for pod "pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c" in namespace "pods-4422" to be "running and ready"
    May  6 07:44:15.086: INFO: Pod "pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017023ms
    May  6 07:44:15.086: INFO: The phase of Pod pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:44:17.089: INFO: Pod "pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004848155s
    May  6 07:44:17.089: INFO: The phase of Pod pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c is Running (Ready = true)
    May  6 07:44:17.089: INFO: Pod "pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 05/06/23 07:44:17.092
    STEP: updating the pod 05/06/23 07:44:17.094
    May  6 07:44:17.604: INFO: Successfully updated pod "pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c"
    May  6 07:44:17.604: INFO: Waiting up to 5m0s for pod "pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c" in namespace "pods-4422" to be "running"
    May  6 07:44:17.607: INFO: Pod "pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c": Phase="Running", Reason="", readiness=true. Elapsed: 2.579142ms
    May  6 07:44:17.607: INFO: Pod "pod-update-936e43a4-cb3a-4c3c-bd1b-9f8eb28ad47c" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 05/06/23 07:44:17.607
    May  6 07:44:17.609: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  6 07:44:17.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4422" for this suite. 05/06/23 07:44:17.612
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:44:17.617
May  6 07:44:17.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename container-lifecycle-hook 05/06/23 07:44:17.618
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:17.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:17.634
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 05/06/23 07:44:17.639
May  6 07:44:17.645: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6025" to be "running and ready"
May  6 07:44:17.648: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.533845ms
May  6 07:44:17.648: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  6 07:44:19.651: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005320583s
May  6 07:44:19.651: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  6 07:44:19.651: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 05/06/23 07:44:19.653
May  6 07:44:19.659: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6025" to be "running and ready"
May  6 07:44:19.661: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.433805ms
May  6 07:44:19.661: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
May  6 07:44:21.665: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006390495s
May  6 07:44:21.665: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
May  6 07:44:21.665: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 05/06/23 07:44:21.667
May  6 07:44:21.675: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  6 07:44:21.677: INFO: Pod pod-with-prestop-http-hook still exists
May  6 07:44:23.679: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  6 07:44:23.681: INFO: Pod pod-with-prestop-http-hook still exists
May  6 07:44:25.678: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  6 07:44:25.681: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 05/06/23 07:44:25.681
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
May  6 07:44:25.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6025" for this suite. 05/06/23 07:44:25.696
------------------------------
â€¢ [SLOW TEST] [8.085 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:44:17.617
    May  6 07:44:17.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/06/23 07:44:17.618
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:17.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:17.634
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 05/06/23 07:44:17.639
    May  6 07:44:17.645: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6025" to be "running and ready"
    May  6 07:44:17.648: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.533845ms
    May  6 07:44:17.648: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:44:19.651: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005320583s
    May  6 07:44:19.651: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  6 07:44:19.651: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 05/06/23 07:44:19.653
    May  6 07:44:19.659: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6025" to be "running and ready"
    May  6 07:44:19.661: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.433805ms
    May  6 07:44:19.661: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:44:21.665: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006390495s
    May  6 07:44:21.665: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    May  6 07:44:21.665: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 05/06/23 07:44:21.667
    May  6 07:44:21.675: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May  6 07:44:21.677: INFO: Pod pod-with-prestop-http-hook still exists
    May  6 07:44:23.679: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May  6 07:44:23.681: INFO: Pod pod-with-prestop-http-hook still exists
    May  6 07:44:25.678: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May  6 07:44:25.681: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 05/06/23 07:44:25.681
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    May  6 07:44:25.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6025" for this suite. 05/06/23 07:44:25.696
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:44:25.702
May  6 07:44:25.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename ephemeral-containers-test 05/06/23 07:44:25.703
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:25.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:25.722
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 05/06/23 07:44:25.724
May  6 07:44:25.733: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8096" to be "running and ready"
May  6 07:44:25.737: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.20912ms
May  6 07:44:25.737: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
May  6 07:44:27.740: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007447286s
May  6 07:44:27.740: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
May  6 07:44:27.740: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 05/06/23 07:44:27.743
May  6 07:44:27.754: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8096" to be "container debugger running"
May  6 07:44:27.757: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.7779ms
May  6 07:44:29.760: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005585818s
May  6 07:44:29.760: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 05/06/23 07:44:29.76
May  6 07:44:29.760: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-8096 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 07:44:29.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:44:29.761: INFO: ExecWithOptions: Clientset creation
May  6 07:44:29.761: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-8096/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
May  6 07:44:29.807: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May  6 07:44:29.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-8096" for this suite. 05/06/23 07:44:29.82
------------------------------
â€¢ [4.123 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:44:25.702
    May  6 07:44:25.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename ephemeral-containers-test 05/06/23 07:44:25.703
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:25.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:25.722
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 05/06/23 07:44:25.724
    May  6 07:44:25.733: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8096" to be "running and ready"
    May  6 07:44:25.737: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.20912ms
    May  6 07:44:25.737: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:44:27.740: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007447286s
    May  6 07:44:27.740: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    May  6 07:44:27.740: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 05/06/23 07:44:27.743
    May  6 07:44:27.754: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8096" to be "container debugger running"
    May  6 07:44:27.757: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.7779ms
    May  6 07:44:29.760: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005585818s
    May  6 07:44:29.760: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 05/06/23 07:44:29.76
    May  6 07:44:29.760: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-8096 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 07:44:29.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:44:29.761: INFO: ExecWithOptions: Clientset creation
    May  6 07:44:29.761: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-8096/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    May  6 07:44:29.807: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May  6 07:44:29.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-8096" for this suite. 05/06/23 07:44:29.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:44:29.826
May  6 07:44:29.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename downward-api 05/06/23 07:44:29.826
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:29.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:29.843
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 05/06/23 07:44:29.845
May  6 07:44:29.852: INFO: Waiting up to 5m0s for pod "annotationupdate956b2501-435e-4d60-85e2-1f9193fcf573" in namespace "downward-api-5627" to be "running and ready"
May  6 07:44:29.855: INFO: Pod "annotationupdate956b2501-435e-4d60-85e2-1f9193fcf573": Phase="Pending", Reason="", readiness=false. Elapsed: 3.055487ms
May  6 07:44:29.855: INFO: The phase of Pod annotationupdate956b2501-435e-4d60-85e2-1f9193fcf573 is Pending, waiting for it to be Running (with Ready = true)
May  6 07:44:31.858: INFO: Pod "annotationupdate956b2501-435e-4d60-85e2-1f9193fcf573": Phase="Running", Reason="", readiness=true. Elapsed: 2.006244467s
May  6 07:44:31.858: INFO: The phase of Pod annotationupdate956b2501-435e-4d60-85e2-1f9193fcf573 is Running (Ready = true)
May  6 07:44:31.858: INFO: Pod "annotationupdate956b2501-435e-4d60-85e2-1f9193fcf573" satisfied condition "running and ready"
May  6 07:44:32.377: INFO: Successfully updated pod "annotationupdate956b2501-435e-4d60-85e2-1f9193fcf573"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  6 07:44:36.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5627" for this suite. 05/06/23 07:44:36.399
------------------------------
â€¢ [SLOW TEST] [6.579 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:44:29.826
    May  6 07:44:29.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename downward-api 05/06/23 07:44:29.826
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:29.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:29.843
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 05/06/23 07:44:29.845
    May  6 07:44:29.852: INFO: Waiting up to 5m0s for pod "annotationupdate956b2501-435e-4d60-85e2-1f9193fcf573" in namespace "downward-api-5627" to be "running and ready"
    May  6 07:44:29.855: INFO: Pod "annotationupdate956b2501-435e-4d60-85e2-1f9193fcf573": Phase="Pending", Reason="", readiness=false. Elapsed: 3.055487ms
    May  6 07:44:29.855: INFO: The phase of Pod annotationupdate956b2501-435e-4d60-85e2-1f9193fcf573 is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:44:31.858: INFO: Pod "annotationupdate956b2501-435e-4d60-85e2-1f9193fcf573": Phase="Running", Reason="", readiness=true. Elapsed: 2.006244467s
    May  6 07:44:31.858: INFO: The phase of Pod annotationupdate956b2501-435e-4d60-85e2-1f9193fcf573 is Running (Ready = true)
    May  6 07:44:31.858: INFO: Pod "annotationupdate956b2501-435e-4d60-85e2-1f9193fcf573" satisfied condition "running and ready"
    May  6 07:44:32.377: INFO: Successfully updated pod "annotationupdate956b2501-435e-4d60-85e2-1f9193fcf573"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  6 07:44:36.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5627" for this suite. 05/06/23 07:44:36.399
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:44:36.405
May  6 07:44:36.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename discovery 05/06/23 07:44:36.406
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:36.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:36.424
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 05/06/23 07:44:36.426
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
May  6 07:44:36.601: INFO: Checking APIGroup: apiregistration.k8s.io
May  6 07:44:36.602: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May  6 07:44:36.602: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
May  6 07:44:36.602: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May  6 07:44:36.602: INFO: Checking APIGroup: apps
May  6 07:44:36.602: INFO: PreferredVersion.GroupVersion: apps/v1
May  6 07:44:36.602: INFO: Versions found [{apps/v1 v1}]
May  6 07:44:36.602: INFO: apps/v1 matches apps/v1
May  6 07:44:36.602: INFO: Checking APIGroup: events.k8s.io
May  6 07:44:36.603: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May  6 07:44:36.603: INFO: Versions found [{events.k8s.io/v1 v1}]
May  6 07:44:36.603: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May  6 07:44:36.603: INFO: Checking APIGroup: authentication.k8s.io
May  6 07:44:36.604: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May  6 07:44:36.604: INFO: Versions found [{authentication.k8s.io/v1 v1}]
May  6 07:44:36.604: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May  6 07:44:36.604: INFO: Checking APIGroup: authorization.k8s.io
May  6 07:44:36.605: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May  6 07:44:36.605: INFO: Versions found [{authorization.k8s.io/v1 v1}]
May  6 07:44:36.605: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May  6 07:44:36.605: INFO: Checking APIGroup: autoscaling
May  6 07:44:36.605: INFO: PreferredVersion.GroupVersion: autoscaling/v2
May  6 07:44:36.605: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
May  6 07:44:36.605: INFO: autoscaling/v2 matches autoscaling/v2
May  6 07:44:36.605: INFO: Checking APIGroup: batch
May  6 07:44:36.606: INFO: PreferredVersion.GroupVersion: batch/v1
May  6 07:44:36.606: INFO: Versions found [{batch/v1 v1}]
May  6 07:44:36.606: INFO: batch/v1 matches batch/v1
May  6 07:44:36.606: INFO: Checking APIGroup: certificates.k8s.io
May  6 07:44:36.607: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May  6 07:44:36.607: INFO: Versions found [{certificates.k8s.io/v1 v1}]
May  6 07:44:36.607: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May  6 07:44:36.607: INFO: Checking APIGroup: networking.k8s.io
May  6 07:44:36.608: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May  6 07:44:36.608: INFO: Versions found [{networking.k8s.io/v1 v1}]
May  6 07:44:36.608: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May  6 07:44:36.608: INFO: Checking APIGroup: policy
May  6 07:44:36.608: INFO: PreferredVersion.GroupVersion: policy/v1
May  6 07:44:36.608: INFO: Versions found [{policy/v1 v1}]
May  6 07:44:36.609: INFO: policy/v1 matches policy/v1
May  6 07:44:36.609: INFO: Checking APIGroup: rbac.authorization.k8s.io
May  6 07:44:36.609: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May  6 07:44:36.609: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
May  6 07:44:36.609: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May  6 07:44:36.609: INFO: Checking APIGroup: storage.k8s.io
May  6 07:44:36.610: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May  6 07:44:36.610: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May  6 07:44:36.610: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May  6 07:44:36.610: INFO: Checking APIGroup: admissionregistration.k8s.io
May  6 07:44:36.610: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May  6 07:44:36.610: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
May  6 07:44:36.610: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May  6 07:44:36.610: INFO: Checking APIGroup: apiextensions.k8s.io
May  6 07:44:36.611: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May  6 07:44:36.611: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
May  6 07:44:36.611: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May  6 07:44:36.611: INFO: Checking APIGroup: scheduling.k8s.io
May  6 07:44:36.612: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May  6 07:44:36.612: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
May  6 07:44:36.612: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May  6 07:44:36.612: INFO: Checking APIGroup: coordination.k8s.io
May  6 07:44:36.612: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May  6 07:44:36.612: INFO: Versions found [{coordination.k8s.io/v1 v1}]
May  6 07:44:36.612: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May  6 07:44:36.612: INFO: Checking APIGroup: node.k8s.io
May  6 07:44:36.613: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May  6 07:44:36.613: INFO: Versions found [{node.k8s.io/v1 v1}]
May  6 07:44:36.613: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May  6 07:44:36.613: INFO: Checking APIGroup: discovery.k8s.io
May  6 07:44:36.614: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
May  6 07:44:36.614: INFO: Versions found [{discovery.k8s.io/v1 v1}]
May  6 07:44:36.614: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
May  6 07:44:36.614: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May  6 07:44:36.615: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
May  6 07:44:36.615: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
May  6 07:44:36.615: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
May  6 07:44:36.615: INFO: Checking APIGroup: crd.projectcalico.org
May  6 07:44:36.615: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
May  6 07:44:36.615: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
May  6 07:44:36.615: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
May  6 07:44:36.615: INFO: Checking APIGroup: operator.tigera.io
May  6 07:44:36.616: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
May  6 07:44:36.616: INFO: Versions found [{operator.tigera.io/v1 v1}]
May  6 07:44:36.616: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
May  6 07:44:36.616: INFO: Checking APIGroup: cluster.rafay.dev
May  6 07:44:36.616: INFO: PreferredVersion.GroupVersion: cluster.rafay.dev/v2
May  6 07:44:36.616: INFO: Versions found [{cluster.rafay.dev/v2 v2}]
May  6 07:44:36.616: INFO: cluster.rafay.dev/v2 matches cluster.rafay.dev/v2
May  6 07:44:36.616: INFO: Checking APIGroup: system.k8smgmt.io
May  6 07:44:36.617: INFO: PreferredVersion.GroupVersion: system.k8smgmt.io/v3
May  6 07:44:36.617: INFO: Versions found [{system.k8smgmt.io/v3 v3}]
May  6 07:44:36.617: INFO: system.k8smgmt.io/v3 matches system.k8smgmt.io/v3
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
May  6 07:44:36.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-8811" for this suite. 05/06/23 07:44:36.62
------------------------------
â€¢ [0.220 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:44:36.405
    May  6 07:44:36.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename discovery 05/06/23 07:44:36.406
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:36.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:36.424
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 05/06/23 07:44:36.426
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    May  6 07:44:36.601: INFO: Checking APIGroup: apiregistration.k8s.io
    May  6 07:44:36.602: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    May  6 07:44:36.602: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    May  6 07:44:36.602: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    May  6 07:44:36.602: INFO: Checking APIGroup: apps
    May  6 07:44:36.602: INFO: PreferredVersion.GroupVersion: apps/v1
    May  6 07:44:36.602: INFO: Versions found [{apps/v1 v1}]
    May  6 07:44:36.602: INFO: apps/v1 matches apps/v1
    May  6 07:44:36.602: INFO: Checking APIGroup: events.k8s.io
    May  6 07:44:36.603: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    May  6 07:44:36.603: INFO: Versions found [{events.k8s.io/v1 v1}]
    May  6 07:44:36.603: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    May  6 07:44:36.603: INFO: Checking APIGroup: authentication.k8s.io
    May  6 07:44:36.604: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    May  6 07:44:36.604: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    May  6 07:44:36.604: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    May  6 07:44:36.604: INFO: Checking APIGroup: authorization.k8s.io
    May  6 07:44:36.605: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    May  6 07:44:36.605: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    May  6 07:44:36.605: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    May  6 07:44:36.605: INFO: Checking APIGroup: autoscaling
    May  6 07:44:36.605: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    May  6 07:44:36.605: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    May  6 07:44:36.605: INFO: autoscaling/v2 matches autoscaling/v2
    May  6 07:44:36.605: INFO: Checking APIGroup: batch
    May  6 07:44:36.606: INFO: PreferredVersion.GroupVersion: batch/v1
    May  6 07:44:36.606: INFO: Versions found [{batch/v1 v1}]
    May  6 07:44:36.606: INFO: batch/v1 matches batch/v1
    May  6 07:44:36.606: INFO: Checking APIGroup: certificates.k8s.io
    May  6 07:44:36.607: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    May  6 07:44:36.607: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    May  6 07:44:36.607: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    May  6 07:44:36.607: INFO: Checking APIGroup: networking.k8s.io
    May  6 07:44:36.608: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    May  6 07:44:36.608: INFO: Versions found [{networking.k8s.io/v1 v1}]
    May  6 07:44:36.608: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    May  6 07:44:36.608: INFO: Checking APIGroup: policy
    May  6 07:44:36.608: INFO: PreferredVersion.GroupVersion: policy/v1
    May  6 07:44:36.608: INFO: Versions found [{policy/v1 v1}]
    May  6 07:44:36.609: INFO: policy/v1 matches policy/v1
    May  6 07:44:36.609: INFO: Checking APIGroup: rbac.authorization.k8s.io
    May  6 07:44:36.609: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    May  6 07:44:36.609: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    May  6 07:44:36.609: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    May  6 07:44:36.609: INFO: Checking APIGroup: storage.k8s.io
    May  6 07:44:36.610: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    May  6 07:44:36.610: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    May  6 07:44:36.610: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    May  6 07:44:36.610: INFO: Checking APIGroup: admissionregistration.k8s.io
    May  6 07:44:36.610: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    May  6 07:44:36.610: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    May  6 07:44:36.610: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    May  6 07:44:36.610: INFO: Checking APIGroup: apiextensions.k8s.io
    May  6 07:44:36.611: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    May  6 07:44:36.611: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    May  6 07:44:36.611: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    May  6 07:44:36.611: INFO: Checking APIGroup: scheduling.k8s.io
    May  6 07:44:36.612: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    May  6 07:44:36.612: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    May  6 07:44:36.612: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    May  6 07:44:36.612: INFO: Checking APIGroup: coordination.k8s.io
    May  6 07:44:36.612: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    May  6 07:44:36.612: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    May  6 07:44:36.612: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    May  6 07:44:36.612: INFO: Checking APIGroup: node.k8s.io
    May  6 07:44:36.613: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    May  6 07:44:36.613: INFO: Versions found [{node.k8s.io/v1 v1}]
    May  6 07:44:36.613: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    May  6 07:44:36.613: INFO: Checking APIGroup: discovery.k8s.io
    May  6 07:44:36.614: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    May  6 07:44:36.614: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    May  6 07:44:36.614: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    May  6 07:44:36.614: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    May  6 07:44:36.615: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    May  6 07:44:36.615: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    May  6 07:44:36.615: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    May  6 07:44:36.615: INFO: Checking APIGroup: crd.projectcalico.org
    May  6 07:44:36.615: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    May  6 07:44:36.615: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    May  6 07:44:36.615: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    May  6 07:44:36.615: INFO: Checking APIGroup: operator.tigera.io
    May  6 07:44:36.616: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
    May  6 07:44:36.616: INFO: Versions found [{operator.tigera.io/v1 v1}]
    May  6 07:44:36.616: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
    May  6 07:44:36.616: INFO: Checking APIGroup: cluster.rafay.dev
    May  6 07:44:36.616: INFO: PreferredVersion.GroupVersion: cluster.rafay.dev/v2
    May  6 07:44:36.616: INFO: Versions found [{cluster.rafay.dev/v2 v2}]
    May  6 07:44:36.616: INFO: cluster.rafay.dev/v2 matches cluster.rafay.dev/v2
    May  6 07:44:36.616: INFO: Checking APIGroup: system.k8smgmt.io
    May  6 07:44:36.617: INFO: PreferredVersion.GroupVersion: system.k8smgmt.io/v3
    May  6 07:44:36.617: INFO: Versions found [{system.k8smgmt.io/v3 v3}]
    May  6 07:44:36.617: INFO: system.k8smgmt.io/v3 matches system.k8smgmt.io/v3
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    May  6 07:44:36.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-8811" for this suite. 05/06/23 07:44:36.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:44:36.626
May  6 07:44:36.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:44:36.627
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:36.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:36.644
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-d1ed7336-8237-46f7-ac37-f800d54d42cc 05/06/23 07:44:36.647
STEP: Creating a pod to test consume configMaps 05/06/23 07:44:36.651
May  6 07:44:36.660: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-08a42b72-f51b-4cde-b816-7e320a844eea" in namespace "projected-4220" to be "Succeeded or Failed"
May  6 07:44:36.662: INFO: Pod "pod-projected-configmaps-08a42b72-f51b-4cde-b816-7e320a844eea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073812ms
May  6 07:44:38.665: INFO: Pod "pod-projected-configmaps-08a42b72-f51b-4cde-b816-7e320a844eea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005199723s
May  6 07:44:40.666: INFO: Pod "pod-projected-configmaps-08a42b72-f51b-4cde-b816-7e320a844eea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005719591s
STEP: Saw pod success 05/06/23 07:44:40.666
May  6 07:44:40.666: INFO: Pod "pod-projected-configmaps-08a42b72-f51b-4cde-b816-7e320a844eea" satisfied condition "Succeeded or Failed"
May  6 07:44:40.668: INFO: Trying to get logs from node cncf-0 pod pod-projected-configmaps-08a42b72-f51b-4cde-b816-7e320a844eea container agnhost-container: <nil>
STEP: delete the pod 05/06/23 07:44:40.673
May  6 07:44:40.682: INFO: Waiting for pod pod-projected-configmaps-08a42b72-f51b-4cde-b816-7e320a844eea to disappear
May  6 07:44:40.685: INFO: Pod pod-projected-configmaps-08a42b72-f51b-4cde-b816-7e320a844eea no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May  6 07:44:40.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4220" for this suite. 05/06/23 07:44:40.688
------------------------------
â€¢ [4.067 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:44:36.626
    May  6 07:44:36.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:44:36.627
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:36.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:36.644
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-d1ed7336-8237-46f7-ac37-f800d54d42cc 05/06/23 07:44:36.647
    STEP: Creating a pod to test consume configMaps 05/06/23 07:44:36.651
    May  6 07:44:36.660: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-08a42b72-f51b-4cde-b816-7e320a844eea" in namespace "projected-4220" to be "Succeeded or Failed"
    May  6 07:44:36.662: INFO: Pod "pod-projected-configmaps-08a42b72-f51b-4cde-b816-7e320a844eea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073812ms
    May  6 07:44:38.665: INFO: Pod "pod-projected-configmaps-08a42b72-f51b-4cde-b816-7e320a844eea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005199723s
    May  6 07:44:40.666: INFO: Pod "pod-projected-configmaps-08a42b72-f51b-4cde-b816-7e320a844eea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005719591s
    STEP: Saw pod success 05/06/23 07:44:40.666
    May  6 07:44:40.666: INFO: Pod "pod-projected-configmaps-08a42b72-f51b-4cde-b816-7e320a844eea" satisfied condition "Succeeded or Failed"
    May  6 07:44:40.668: INFO: Trying to get logs from node cncf-0 pod pod-projected-configmaps-08a42b72-f51b-4cde-b816-7e320a844eea container agnhost-container: <nil>
    STEP: delete the pod 05/06/23 07:44:40.673
    May  6 07:44:40.682: INFO: Waiting for pod pod-projected-configmaps-08a42b72-f51b-4cde-b816-7e320a844eea to disappear
    May  6 07:44:40.685: INFO: Pod pod-projected-configmaps-08a42b72-f51b-4cde-b816-7e320a844eea no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May  6 07:44:40.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4220" for this suite. 05/06/23 07:44:40.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:44:40.693
May  6 07:44:40.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename configmap 05/06/23 07:44:40.694
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:40.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:40.711
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-07bf4d1b-6697-4cd0-949a-19c86071ef7e 05/06/23 07:44:40.715
STEP: Creating the pod 05/06/23 07:44:40.721
May  6 07:44:40.727: INFO: Waiting up to 5m0s for pod "pod-configmaps-dd4474ef-4fdb-4c06-9e51-18fc90198bdb" in namespace "configmap-724" to be "running"
May  6 07:44:40.730: INFO: Pod "pod-configmaps-dd4474ef-4fdb-4c06-9e51-18fc90198bdb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.172158ms
May  6 07:44:42.732: INFO: Pod "pod-configmaps-dd4474ef-4fdb-4c06-9e51-18fc90198bdb": Phase="Running", Reason="", readiness=false. Elapsed: 2.004938924s
May  6 07:44:42.732: INFO: Pod "pod-configmaps-dd4474ef-4fdb-4c06-9e51-18fc90198bdb" satisfied condition "running"
STEP: Waiting for pod with text data 05/06/23 07:44:42.733
STEP: Waiting for pod with binary data 05/06/23 07:44:42.738
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  6 07:44:42.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-724" for this suite. 05/06/23 07:44:42.747
------------------------------
â€¢ [2.060 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:44:40.693
    May  6 07:44:40.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename configmap 05/06/23 07:44:40.694
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:40.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:40.711
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-07bf4d1b-6697-4cd0-949a-19c86071ef7e 05/06/23 07:44:40.715
    STEP: Creating the pod 05/06/23 07:44:40.721
    May  6 07:44:40.727: INFO: Waiting up to 5m0s for pod "pod-configmaps-dd4474ef-4fdb-4c06-9e51-18fc90198bdb" in namespace "configmap-724" to be "running"
    May  6 07:44:40.730: INFO: Pod "pod-configmaps-dd4474ef-4fdb-4c06-9e51-18fc90198bdb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.172158ms
    May  6 07:44:42.732: INFO: Pod "pod-configmaps-dd4474ef-4fdb-4c06-9e51-18fc90198bdb": Phase="Running", Reason="", readiness=false. Elapsed: 2.004938924s
    May  6 07:44:42.732: INFO: Pod "pod-configmaps-dd4474ef-4fdb-4c06-9e51-18fc90198bdb" satisfied condition "running"
    STEP: Waiting for pod with text data 05/06/23 07:44:42.733
    STEP: Waiting for pod with binary data 05/06/23 07:44:42.738
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  6 07:44:42.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-724" for this suite. 05/06/23 07:44:42.747
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:44:42.754
May  6 07:44:42.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename job 05/06/23 07:44:42.754
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:42.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:42.771
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 05/06/23 07:44:42.775
STEP: Patching the Job 05/06/23 07:44:42.781
STEP: Watching for Job to be patched 05/06/23 07:44:42.796
May  6 07:44:42.797: INFO: Event ADDED observed for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking:]
May  6 07:44:42.797: INFO: Event MODIFIED observed for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking:]
May  6 07:44:42.797: INFO: Event MODIFIED found for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-8bh68:patched e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 05/06/23 07:44:42.797
STEP: Watching for Job to be updated 05/06/23 07:44:42.805
May  6 07:44:42.806: INFO: Event MODIFIED found for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-8bh68:patched e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  6 07:44:42.806: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 05/06/23 07:44:42.806
May  6 07:44:42.808: INFO: Job: e2e-8bh68 as labels: map[e2e-8bh68:patched e2e-job-label:e2e-8bh68]
STEP: Waiting for job to complete 05/06/23 07:44:42.808
STEP: Delete a job collection with a labelselector 05/06/23 07:44:50.814
STEP: Watching for Job to be deleted 05/06/23 07:44:50.821
May  6 07:44:50.822: INFO: Event MODIFIED observed for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-8bh68:patched e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  6 07:44:50.822: INFO: Event MODIFIED observed for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-8bh68:patched e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  6 07:44:50.822: INFO: Event MODIFIED observed for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-8bh68:patched e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  6 07:44:50.822: INFO: Event MODIFIED observed for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-8bh68:patched e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  6 07:44:50.822: INFO: Event MODIFIED observed for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-8bh68:patched e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  6 07:44:50.822: INFO: Event DELETED found for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-8bh68:patched e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 05/06/23 07:44:50.822
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May  6 07:44:50.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3350" for this suite. 05/06/23 07:44:50.827
------------------------------
â€¢ [SLOW TEST] [8.082 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:44:42.754
    May  6 07:44:42.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename job 05/06/23 07:44:42.754
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:42.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:42.771
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 05/06/23 07:44:42.775
    STEP: Patching the Job 05/06/23 07:44:42.781
    STEP: Watching for Job to be patched 05/06/23 07:44:42.796
    May  6 07:44:42.797: INFO: Event ADDED observed for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking:]
    May  6 07:44:42.797: INFO: Event MODIFIED observed for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking:]
    May  6 07:44:42.797: INFO: Event MODIFIED found for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-8bh68:patched e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 05/06/23 07:44:42.797
    STEP: Watching for Job to be updated 05/06/23 07:44:42.805
    May  6 07:44:42.806: INFO: Event MODIFIED found for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-8bh68:patched e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  6 07:44:42.806: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 05/06/23 07:44:42.806
    May  6 07:44:42.808: INFO: Job: e2e-8bh68 as labels: map[e2e-8bh68:patched e2e-job-label:e2e-8bh68]
    STEP: Waiting for job to complete 05/06/23 07:44:42.808
    STEP: Delete a job collection with a labelselector 05/06/23 07:44:50.814
    STEP: Watching for Job to be deleted 05/06/23 07:44:50.821
    May  6 07:44:50.822: INFO: Event MODIFIED observed for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-8bh68:patched e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  6 07:44:50.822: INFO: Event MODIFIED observed for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-8bh68:patched e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  6 07:44:50.822: INFO: Event MODIFIED observed for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-8bh68:patched e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  6 07:44:50.822: INFO: Event MODIFIED observed for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-8bh68:patched e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  6 07:44:50.822: INFO: Event MODIFIED observed for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-8bh68:patched e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  6 07:44:50.822: INFO: Event DELETED found for Job e2e-8bh68 in namespace job-3350 with labels: map[e2e-8bh68:patched e2e-job-label:e2e-8bh68] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 05/06/23 07:44:50.822
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May  6 07:44:50.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3350" for this suite. 05/06/23 07:44:50.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:44:50.836
May  6 07:44:50.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename limitrange 05/06/23 07:44:50.837
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:50.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:50.865
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 05/06/23 07:44:50.866
STEP: Setting up watch 05/06/23 07:44:50.867
STEP: Submitting a LimitRange 05/06/23 07:44:50.973
STEP: Verifying LimitRange creation was observed 05/06/23 07:44:50.978
STEP: Fetching the LimitRange to ensure it has proper values 05/06/23 07:44:50.978
May  6 07:44:50.981: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May  6 07:44:50.981: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 05/06/23 07:44:50.981
STEP: Ensuring Pod has resource requirements applied from LimitRange 05/06/23 07:44:50.987
May  6 07:44:50.990: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May  6 07:44:50.990: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 05/06/23 07:44:50.99
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 05/06/23 07:44:50.997
May  6 07:44:50.999: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May  6 07:44:51.000: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 05/06/23 07:44:51
STEP: Failing to create a Pod with more than max resources 05/06/23 07:44:51.002
STEP: Updating a LimitRange 05/06/23 07:44:51.003
STEP: Verifying LimitRange updating is effective 05/06/23 07:44:51.008
STEP: Creating a Pod with less than former min resources 05/06/23 07:44:53.012
STEP: Failing to create a Pod with more than max resources 05/06/23 07:44:53.018
STEP: Deleting a LimitRange 05/06/23 07:44:53.02
STEP: Verifying the LimitRange was deleted 05/06/23 07:44:53.026
May  6 07:44:58.031: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 05/06/23 07:44:58.031
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
May  6 07:44:58.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-873" for this suite. 05/06/23 07:44:58.043
------------------------------
â€¢ [SLOW TEST] [7.214 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:44:50.836
    May  6 07:44:50.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename limitrange 05/06/23 07:44:50.837
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:50.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:50.865
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 05/06/23 07:44:50.866
    STEP: Setting up watch 05/06/23 07:44:50.867
    STEP: Submitting a LimitRange 05/06/23 07:44:50.973
    STEP: Verifying LimitRange creation was observed 05/06/23 07:44:50.978
    STEP: Fetching the LimitRange to ensure it has proper values 05/06/23 07:44:50.978
    May  6 07:44:50.981: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    May  6 07:44:50.981: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 05/06/23 07:44:50.981
    STEP: Ensuring Pod has resource requirements applied from LimitRange 05/06/23 07:44:50.987
    May  6 07:44:50.990: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    May  6 07:44:50.990: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 05/06/23 07:44:50.99
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 05/06/23 07:44:50.997
    May  6 07:44:50.999: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    May  6 07:44:51.000: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 05/06/23 07:44:51
    STEP: Failing to create a Pod with more than max resources 05/06/23 07:44:51.002
    STEP: Updating a LimitRange 05/06/23 07:44:51.003
    STEP: Verifying LimitRange updating is effective 05/06/23 07:44:51.008
    STEP: Creating a Pod with less than former min resources 05/06/23 07:44:53.012
    STEP: Failing to create a Pod with more than max resources 05/06/23 07:44:53.018
    STEP: Deleting a LimitRange 05/06/23 07:44:53.02
    STEP: Verifying the LimitRange was deleted 05/06/23 07:44:53.026
    May  6 07:44:58.031: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 05/06/23 07:44:58.031
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    May  6 07:44:58.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-873" for this suite. 05/06/23 07:44:58.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:44:58.05
May  6 07:44:58.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename pod-network-test 05/06/23 07:44:58.05
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:58.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:58.065
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-3227 05/06/23 07:44:58.067
STEP: creating a selector 05/06/23 07:44:58.067
STEP: Creating the service pods in kubernetes 05/06/23 07:44:58.067
May  6 07:44:58.067: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  6 07:44:58.103: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3227" to be "running and ready"
May  6 07:44:58.107: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.228325ms
May  6 07:44:58.107: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  6 07:45:00.111: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008248001s
May  6 07:45:00.111: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 07:45:02.111: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.007853719s
May  6 07:45:02.111: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 07:45:04.111: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008414812s
May  6 07:45:04.111: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 07:45:06.111: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008029397s
May  6 07:45:06.111: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 07:45:08.111: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007825837s
May  6 07:45:08.111: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 07:45:10.110: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.006753775s
May  6 07:45:10.110: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  6 07:45:10.110: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  6 07:45:10.112: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3227" to be "running and ready"
May  6 07:45:10.114: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.954604ms
May  6 07:45:10.114: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  6 07:45:10.114: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  6 07:45:10.116: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3227" to be "running and ready"
May  6 07:45:10.118: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.858152ms
May  6 07:45:10.118: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  6 07:45:10.118: INFO: Pod "netserver-2" satisfied condition "running and ready"
May  6 07:45:10.124: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-3227" to be "running and ready"
May  6 07:45:10.126: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 1.987397ms
May  6 07:45:10.126: INFO: The phase of Pod netserver-3 is Running (Ready = true)
May  6 07:45:10.126: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 05/06/23 07:45:10.128
May  6 07:45:10.135: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3227" to be "running"
May  6 07:45:10.139: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.638885ms
May  6 07:45:12.143: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008066557s
May  6 07:45:12.143: INFO: Pod "test-container-pod" satisfied condition "running"
May  6 07:45:12.145: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
May  6 07:45:12.145: INFO: Breadth first check of 10.244.174.141 on host 10.0.0.134...
May  6 07:45:12.147: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.174.145:9080/dial?request=hostname&protocol=udp&host=10.244.174.141&port=8081&tries=1'] Namespace:pod-network-test-3227 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 07:45:12.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:45:12.148: INFO: ExecWithOptions: Clientset creation
May  6 07:45:12.148: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3227/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.174.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.174.141%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  6 07:45:12.206: INFO: Waiting for responses: map[]
May  6 07:45:12.207: INFO: reached 10.244.174.141 after 0/1 tries
May  6 07:45:12.207: INFO: Breadth first check of 10.244.21.92 on host 10.0.0.107...
May  6 07:45:12.209: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.174.145:9080/dial?request=hostname&protocol=udp&host=10.244.21.92&port=8081&tries=1'] Namespace:pod-network-test-3227 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 07:45:12.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:45:12.209: INFO: ExecWithOptions: Clientset creation
May  6 07:45:12.209: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3227/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.174.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.21.92%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  6 07:45:12.265: INFO: Waiting for responses: map[]
May  6 07:45:12.265: INFO: reached 10.244.21.92 after 0/1 tries
May  6 07:45:12.265: INFO: Breadth first check of 10.244.20.158 on host 10.0.0.180...
May  6 07:45:12.267: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.174.145:9080/dial?request=hostname&protocol=udp&host=10.244.20.158&port=8081&tries=1'] Namespace:pod-network-test-3227 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 07:45:12.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:45:12.268: INFO: ExecWithOptions: Clientset creation
May  6 07:45:12.268: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3227/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.174.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.20.158%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  6 07:45:12.300: INFO: Waiting for responses: map[]
May  6 07:45:12.300: INFO: reached 10.244.20.158 after 0/1 tries
May  6 07:45:12.300: INFO: Breadth first check of 10.244.245.77 on host 10.0.0.240...
May  6 07:45:12.303: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.174.145:9080/dial?request=hostname&protocol=udp&host=10.244.245.77&port=8081&tries=1'] Namespace:pod-network-test-3227 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 07:45:12.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:45:12.303: INFO: ExecWithOptions: Clientset creation
May  6 07:45:12.303: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3227/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.174.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.245.77%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  6 07:45:12.350: INFO: Waiting for responses: map[]
May  6 07:45:12.350: INFO: reached 10.244.245.77 after 0/1 tries
May  6 07:45:12.350: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
May  6 07:45:12.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3227" for this suite. 05/06/23 07:45:12.353
------------------------------
â€¢ [SLOW TEST] [14.308 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:44:58.05
    May  6 07:44:58.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename pod-network-test 05/06/23 07:44:58.05
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:44:58.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:44:58.065
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-3227 05/06/23 07:44:58.067
    STEP: creating a selector 05/06/23 07:44:58.067
    STEP: Creating the service pods in kubernetes 05/06/23 07:44:58.067
    May  6 07:44:58.067: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  6 07:44:58.103: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3227" to be "running and ready"
    May  6 07:44:58.107: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.228325ms
    May  6 07:44:58.107: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:45:00.111: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008248001s
    May  6 07:45:00.111: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 07:45:02.111: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.007853719s
    May  6 07:45:02.111: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 07:45:04.111: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008414812s
    May  6 07:45:04.111: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 07:45:06.111: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008029397s
    May  6 07:45:06.111: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 07:45:08.111: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007825837s
    May  6 07:45:08.111: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 07:45:10.110: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.006753775s
    May  6 07:45:10.110: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  6 07:45:10.110: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  6 07:45:10.112: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3227" to be "running and ready"
    May  6 07:45:10.114: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.954604ms
    May  6 07:45:10.114: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  6 07:45:10.114: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  6 07:45:10.116: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3227" to be "running and ready"
    May  6 07:45:10.118: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.858152ms
    May  6 07:45:10.118: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  6 07:45:10.118: INFO: Pod "netserver-2" satisfied condition "running and ready"
    May  6 07:45:10.124: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-3227" to be "running and ready"
    May  6 07:45:10.126: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 1.987397ms
    May  6 07:45:10.126: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    May  6 07:45:10.126: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 05/06/23 07:45:10.128
    May  6 07:45:10.135: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3227" to be "running"
    May  6 07:45:10.139: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.638885ms
    May  6 07:45:12.143: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008066557s
    May  6 07:45:12.143: INFO: Pod "test-container-pod" satisfied condition "running"
    May  6 07:45:12.145: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    May  6 07:45:12.145: INFO: Breadth first check of 10.244.174.141 on host 10.0.0.134...
    May  6 07:45:12.147: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.174.145:9080/dial?request=hostname&protocol=udp&host=10.244.174.141&port=8081&tries=1'] Namespace:pod-network-test-3227 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 07:45:12.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:45:12.148: INFO: ExecWithOptions: Clientset creation
    May  6 07:45:12.148: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3227/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.174.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.174.141%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  6 07:45:12.206: INFO: Waiting for responses: map[]
    May  6 07:45:12.207: INFO: reached 10.244.174.141 after 0/1 tries
    May  6 07:45:12.207: INFO: Breadth first check of 10.244.21.92 on host 10.0.0.107...
    May  6 07:45:12.209: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.174.145:9080/dial?request=hostname&protocol=udp&host=10.244.21.92&port=8081&tries=1'] Namespace:pod-network-test-3227 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 07:45:12.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:45:12.209: INFO: ExecWithOptions: Clientset creation
    May  6 07:45:12.209: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3227/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.174.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.21.92%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  6 07:45:12.265: INFO: Waiting for responses: map[]
    May  6 07:45:12.265: INFO: reached 10.244.21.92 after 0/1 tries
    May  6 07:45:12.265: INFO: Breadth first check of 10.244.20.158 on host 10.0.0.180...
    May  6 07:45:12.267: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.174.145:9080/dial?request=hostname&protocol=udp&host=10.244.20.158&port=8081&tries=1'] Namespace:pod-network-test-3227 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 07:45:12.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:45:12.268: INFO: ExecWithOptions: Clientset creation
    May  6 07:45:12.268: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3227/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.174.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.20.158%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  6 07:45:12.300: INFO: Waiting for responses: map[]
    May  6 07:45:12.300: INFO: reached 10.244.20.158 after 0/1 tries
    May  6 07:45:12.300: INFO: Breadth first check of 10.244.245.77 on host 10.0.0.240...
    May  6 07:45:12.303: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.174.145:9080/dial?request=hostname&protocol=udp&host=10.244.245.77&port=8081&tries=1'] Namespace:pod-network-test-3227 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 07:45:12.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:45:12.303: INFO: ExecWithOptions: Clientset creation
    May  6 07:45:12.303: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3227/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.174.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.245.77%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  6 07:45:12.350: INFO: Waiting for responses: map[]
    May  6 07:45:12.350: INFO: reached 10.244.245.77 after 0/1 tries
    May  6 07:45:12.350: INFO: Going to retry 0 out of 4 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    May  6 07:45:12.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3227" for this suite. 05/06/23 07:45:12.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:45:12.358
May  6 07:45:12.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename configmap 05/06/23 07:45:12.359
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:45:12.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:45:12.374
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-514a6c82-2186-4d92-8ea8-ad39d7cf3449 05/06/23 07:45:12.379
STEP: Creating the pod 05/06/23 07:45:12.383
May  6 07:45:12.389: INFO: Waiting up to 5m0s for pod "pod-configmaps-4ac676a9-0dba-43ba-8cb8-6d954106dbbb" in namespace "configmap-4522" to be "running and ready"
May  6 07:45:12.391: INFO: Pod "pod-configmaps-4ac676a9-0dba-43ba-8cb8-6d954106dbbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019598ms
May  6 07:45:12.391: INFO: The phase of Pod pod-configmaps-4ac676a9-0dba-43ba-8cb8-6d954106dbbb is Pending, waiting for it to be Running (with Ready = true)
May  6 07:45:14.394: INFO: Pod "pod-configmaps-4ac676a9-0dba-43ba-8cb8-6d954106dbbb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005007815s
May  6 07:45:14.394: INFO: The phase of Pod pod-configmaps-4ac676a9-0dba-43ba-8cb8-6d954106dbbb is Running (Ready = true)
May  6 07:45:14.394: INFO: Pod "pod-configmaps-4ac676a9-0dba-43ba-8cb8-6d954106dbbb" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-514a6c82-2186-4d92-8ea8-ad39d7cf3449 05/06/23 07:45:14.401
STEP: waiting to observe update in volume 05/06/23 07:45:14.411
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  6 07:45:16.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4522" for this suite. 05/06/23 07:45:16.425
------------------------------
â€¢ [4.071 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:45:12.358
    May  6 07:45:12.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename configmap 05/06/23 07:45:12.359
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:45:12.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:45:12.374
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-514a6c82-2186-4d92-8ea8-ad39d7cf3449 05/06/23 07:45:12.379
    STEP: Creating the pod 05/06/23 07:45:12.383
    May  6 07:45:12.389: INFO: Waiting up to 5m0s for pod "pod-configmaps-4ac676a9-0dba-43ba-8cb8-6d954106dbbb" in namespace "configmap-4522" to be "running and ready"
    May  6 07:45:12.391: INFO: Pod "pod-configmaps-4ac676a9-0dba-43ba-8cb8-6d954106dbbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019598ms
    May  6 07:45:12.391: INFO: The phase of Pod pod-configmaps-4ac676a9-0dba-43ba-8cb8-6d954106dbbb is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:45:14.394: INFO: Pod "pod-configmaps-4ac676a9-0dba-43ba-8cb8-6d954106dbbb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005007815s
    May  6 07:45:14.394: INFO: The phase of Pod pod-configmaps-4ac676a9-0dba-43ba-8cb8-6d954106dbbb is Running (Ready = true)
    May  6 07:45:14.394: INFO: Pod "pod-configmaps-4ac676a9-0dba-43ba-8cb8-6d954106dbbb" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-514a6c82-2186-4d92-8ea8-ad39d7cf3449 05/06/23 07:45:14.401
    STEP: waiting to observe update in volume 05/06/23 07:45:14.411
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  6 07:45:16.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4522" for this suite. 05/06/23 07:45:16.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:45:16.431
May  6 07:45:16.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename pods 05/06/23 07:45:16.431
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:45:16.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:45:16.449
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 05/06/23 07:45:16.451
STEP: submitting the pod to kubernetes 05/06/23 07:45:16.451
May  6 07:45:16.459: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f" in namespace "pods-4344" to be "running and ready"
May  6 07:45:16.460: INFO: Pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.699229ms
May  6 07:45:16.460: INFO: The phase of Pod pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f is Pending, waiting for it to be Running (with Ready = true)
May  6 07:45:18.465: INFO: Pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f": Phase="Running", Reason="", readiness=true. Elapsed: 2.006483008s
May  6 07:45:18.465: INFO: The phase of Pod pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f is Running (Ready = true)
May  6 07:45:18.465: INFO: Pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 05/06/23 07:45:18.467
STEP: updating the pod 05/06/23 07:45:18.47
May  6 07:45:18.983: INFO: Successfully updated pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f"
May  6 07:45:18.983: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f" in namespace "pods-4344" to be "terminated with reason DeadlineExceeded"
May  6 07:45:18.985: INFO: Pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f": Phase="Running", Reason="", readiness=true. Elapsed: 2.003788ms
May  6 07:45:20.990: INFO: Pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f": Phase="Running", Reason="", readiness=true. Elapsed: 2.006208067s
May  6 07:45:22.989: INFO: Pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.005566235s
May  6 07:45:22.989: INFO: Pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  6 07:45:22.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4344" for this suite. 05/06/23 07:45:22.992
------------------------------
â€¢ [SLOW TEST] [6.568 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:45:16.431
    May  6 07:45:16.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename pods 05/06/23 07:45:16.431
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:45:16.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:45:16.449
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 05/06/23 07:45:16.451
    STEP: submitting the pod to kubernetes 05/06/23 07:45:16.451
    May  6 07:45:16.459: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f" in namespace "pods-4344" to be "running and ready"
    May  6 07:45:16.460: INFO: Pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.699229ms
    May  6 07:45:16.460: INFO: The phase of Pod pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:45:18.465: INFO: Pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f": Phase="Running", Reason="", readiness=true. Elapsed: 2.006483008s
    May  6 07:45:18.465: INFO: The phase of Pod pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f is Running (Ready = true)
    May  6 07:45:18.465: INFO: Pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 05/06/23 07:45:18.467
    STEP: updating the pod 05/06/23 07:45:18.47
    May  6 07:45:18.983: INFO: Successfully updated pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f"
    May  6 07:45:18.983: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f" in namespace "pods-4344" to be "terminated with reason DeadlineExceeded"
    May  6 07:45:18.985: INFO: Pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f": Phase="Running", Reason="", readiness=true. Elapsed: 2.003788ms
    May  6 07:45:20.990: INFO: Pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f": Phase="Running", Reason="", readiness=true. Elapsed: 2.006208067s
    May  6 07:45:22.989: INFO: Pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.005566235s
    May  6 07:45:22.989: INFO: Pod "pod-update-activedeadlineseconds-302685ee-2191-4566-af08-7a06048ba75f" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  6 07:45:22.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4344" for this suite. 05/06/23 07:45:22.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:45:23
May  6 07:45:23.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename container-probe 05/06/23 07:45:23
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:45:23.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:45:23.016
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-e5432db0-358c-427f-9ac3-d69d18f2a7c1 in namespace container-probe-250 05/06/23 07:45:23.019
May  6 07:45:23.028: INFO: Waiting up to 5m0s for pod "liveness-e5432db0-358c-427f-9ac3-d69d18f2a7c1" in namespace "container-probe-250" to be "not pending"
May  6 07:45:23.031: INFO: Pod "liveness-e5432db0-358c-427f-9ac3-d69d18f2a7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.829347ms
May  6 07:45:25.034: INFO: Pod "liveness-e5432db0-358c-427f-9ac3-d69d18f2a7c1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005856549s
May  6 07:45:25.034: INFO: Pod "liveness-e5432db0-358c-427f-9ac3-d69d18f2a7c1" satisfied condition "not pending"
May  6 07:45:25.034: INFO: Started pod liveness-e5432db0-358c-427f-9ac3-d69d18f2a7c1 in namespace container-probe-250
STEP: checking the pod's current state and verifying that restartCount is present 05/06/23 07:45:25.034
May  6 07:45:25.037: INFO: Initial restart count of pod liveness-e5432db0-358c-427f-9ac3-d69d18f2a7c1 is 0
May  6 07:45:45.078: INFO: Restart count of pod container-probe-250/liveness-e5432db0-358c-427f-9ac3-d69d18f2a7c1 is now 1 (20.04123301s elapsed)
STEP: deleting the pod 05/06/23 07:45:45.078
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May  6 07:45:45.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-250" for this suite. 05/06/23 07:45:45.093
------------------------------
â€¢ [SLOW TEST] [22.099 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:45:23
    May  6 07:45:23.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename container-probe 05/06/23 07:45:23
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:45:23.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:45:23.016
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-e5432db0-358c-427f-9ac3-d69d18f2a7c1 in namespace container-probe-250 05/06/23 07:45:23.019
    May  6 07:45:23.028: INFO: Waiting up to 5m0s for pod "liveness-e5432db0-358c-427f-9ac3-d69d18f2a7c1" in namespace "container-probe-250" to be "not pending"
    May  6 07:45:23.031: INFO: Pod "liveness-e5432db0-358c-427f-9ac3-d69d18f2a7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.829347ms
    May  6 07:45:25.034: INFO: Pod "liveness-e5432db0-358c-427f-9ac3-d69d18f2a7c1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005856549s
    May  6 07:45:25.034: INFO: Pod "liveness-e5432db0-358c-427f-9ac3-d69d18f2a7c1" satisfied condition "not pending"
    May  6 07:45:25.034: INFO: Started pod liveness-e5432db0-358c-427f-9ac3-d69d18f2a7c1 in namespace container-probe-250
    STEP: checking the pod's current state and verifying that restartCount is present 05/06/23 07:45:25.034
    May  6 07:45:25.037: INFO: Initial restart count of pod liveness-e5432db0-358c-427f-9ac3-d69d18f2a7c1 is 0
    May  6 07:45:45.078: INFO: Restart count of pod container-probe-250/liveness-e5432db0-358c-427f-9ac3-d69d18f2a7c1 is now 1 (20.04123301s elapsed)
    STEP: deleting the pod 05/06/23 07:45:45.078
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May  6 07:45:45.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-250" for this suite. 05/06/23 07:45:45.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:45:45.1
May  6 07:45:45.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename namespaces 05/06/23 07:45:45.101
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:45:45.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:45:45.117
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 05/06/23 07:45:45.119
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:45:45.133
STEP: Creating a pod in the namespace 05/06/23 07:45:45.135
STEP: Waiting for the pod to have running status 05/06/23 07:45:45.141
May  6 07:45:45.141: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6474" to be "running"
May  6 07:45:45.145: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.754847ms
May  6 07:45:47.148: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007019091s
May  6 07:45:47.148: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 05/06/23 07:45:47.148
STEP: Waiting for the namespace to be removed. 05/06/23 07:45:47.154
STEP: Recreating the namespace 05/06/23 07:45:58.157
STEP: Verifying there are no pods in the namespace 05/06/23 07:45:58.174
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 07:45:58.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9640" for this suite. 05/06/23 07:45:58.179
STEP: Destroying namespace "nsdeletetest-6474" for this suite. 05/06/23 07:45:58.188
May  6 07:45:58.190: INFO: Namespace nsdeletetest-6474 was already deleted
STEP: Destroying namespace "nsdeletetest-8239" for this suite. 05/06/23 07:45:58.19
------------------------------
â€¢ [SLOW TEST] [13.095 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:45:45.1
    May  6 07:45:45.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename namespaces 05/06/23 07:45:45.101
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:45:45.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:45:45.117
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 05/06/23 07:45:45.119
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:45:45.133
    STEP: Creating a pod in the namespace 05/06/23 07:45:45.135
    STEP: Waiting for the pod to have running status 05/06/23 07:45:45.141
    May  6 07:45:45.141: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6474" to be "running"
    May  6 07:45:45.145: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.754847ms
    May  6 07:45:47.148: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007019091s
    May  6 07:45:47.148: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 05/06/23 07:45:47.148
    STEP: Waiting for the namespace to be removed. 05/06/23 07:45:47.154
    STEP: Recreating the namespace 05/06/23 07:45:58.157
    STEP: Verifying there are no pods in the namespace 05/06/23 07:45:58.174
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 07:45:58.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9640" for this suite. 05/06/23 07:45:58.179
    STEP: Destroying namespace "nsdeletetest-6474" for this suite. 05/06/23 07:45:58.188
    May  6 07:45:58.190: INFO: Namespace nsdeletetest-6474 was already deleted
    STEP: Destroying namespace "nsdeletetest-8239" for this suite. 05/06/23 07:45:58.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:45:58.196
May  6 07:45:58.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename downward-api 05/06/23 07:45:58.197
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:45:58.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:45:58.213
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 05/06/23 07:45:58.215
May  6 07:45:58.223: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34171df8-534d-4351-a111-2079ce06c4fd" in namespace "downward-api-9585" to be "Succeeded or Failed"
May  6 07:45:58.225: INFO: Pod "downwardapi-volume-34171df8-534d-4351-a111-2079ce06c4fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044045ms
May  6 07:46:00.228: INFO: Pod "downwardapi-volume-34171df8-534d-4351-a111-2079ce06c4fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005257531s
May  6 07:46:02.228: INFO: Pod "downwardapi-volume-34171df8-534d-4351-a111-2079ce06c4fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005349687s
STEP: Saw pod success 05/06/23 07:46:02.228
May  6 07:46:02.228: INFO: Pod "downwardapi-volume-34171df8-534d-4351-a111-2079ce06c4fd" satisfied condition "Succeeded or Failed"
May  6 07:46:02.230: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-34171df8-534d-4351-a111-2079ce06c4fd container client-container: <nil>
STEP: delete the pod 05/06/23 07:46:02.236
May  6 07:46:02.247: INFO: Waiting for pod downwardapi-volume-34171df8-534d-4351-a111-2079ce06c4fd to disappear
May  6 07:46:02.249: INFO: Pod downwardapi-volume-34171df8-534d-4351-a111-2079ce06c4fd no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  6 07:46:02.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9585" for this suite. 05/06/23 07:46:02.252
------------------------------
â€¢ [4.061 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:45:58.196
    May  6 07:45:58.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename downward-api 05/06/23 07:45:58.197
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:45:58.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:45:58.213
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 05/06/23 07:45:58.215
    May  6 07:45:58.223: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34171df8-534d-4351-a111-2079ce06c4fd" in namespace "downward-api-9585" to be "Succeeded or Failed"
    May  6 07:45:58.225: INFO: Pod "downwardapi-volume-34171df8-534d-4351-a111-2079ce06c4fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044045ms
    May  6 07:46:00.228: INFO: Pod "downwardapi-volume-34171df8-534d-4351-a111-2079ce06c4fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005257531s
    May  6 07:46:02.228: INFO: Pod "downwardapi-volume-34171df8-534d-4351-a111-2079ce06c4fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005349687s
    STEP: Saw pod success 05/06/23 07:46:02.228
    May  6 07:46:02.228: INFO: Pod "downwardapi-volume-34171df8-534d-4351-a111-2079ce06c4fd" satisfied condition "Succeeded or Failed"
    May  6 07:46:02.230: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-34171df8-534d-4351-a111-2079ce06c4fd container client-container: <nil>
    STEP: delete the pod 05/06/23 07:46:02.236
    May  6 07:46:02.247: INFO: Waiting for pod downwardapi-volume-34171df8-534d-4351-a111-2079ce06c4fd to disappear
    May  6 07:46:02.249: INFO: Pod downwardapi-volume-34171df8-534d-4351-a111-2079ce06c4fd no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  6 07:46:02.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9585" for this suite. 05/06/23 07:46:02.252
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:46:02.258
May  6 07:46:02.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename daemonsets 05/06/23 07:46:02.259
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:02.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:02.276
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 05/06/23 07:46:02.297
STEP: Check that daemon pods launch on every node of the cluster. 05/06/23 07:46:02.302
May  6 07:46:02.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 07:46:02.307: INFO: Node cncf-0 is running 0 daemon pod, expected 1
May  6 07:46:03.313: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  6 07:46:03.313: INFO: Node cncf-0 is running 0 daemon pod, expected 1
May  6 07:46:04.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
May  6 07:46:04.314: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Getting /status 05/06/23 07:46:04.317
May  6 07:46:04.319: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 05/06/23 07:46:04.319
May  6 07:46:04.328: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 05/06/23 07:46:04.328
May  6 07:46:04.329: INFO: Observed &DaemonSet event: ADDED
May  6 07:46:04.329: INFO: Observed &DaemonSet event: MODIFIED
May  6 07:46:04.329: INFO: Observed &DaemonSet event: MODIFIED
May  6 07:46:04.329: INFO: Observed &DaemonSet event: MODIFIED
May  6 07:46:04.329: INFO: Observed &DaemonSet event: MODIFIED
May  6 07:46:04.330: INFO: Observed &DaemonSet event: MODIFIED
May  6 07:46:04.330: INFO: Found daemon set daemon-set in namespace daemonsets-4980 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  6 07:46:04.330: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 05/06/23 07:46:04.33
STEP: watching for the daemon set status to be patched 05/06/23 07:46:04.337
May  6 07:46:04.338: INFO: Observed &DaemonSet event: ADDED
May  6 07:46:04.338: INFO: Observed &DaemonSet event: MODIFIED
May  6 07:46:04.338: INFO: Observed &DaemonSet event: MODIFIED
May  6 07:46:04.338: INFO: Observed &DaemonSet event: MODIFIED
May  6 07:46:04.338: INFO: Observed &DaemonSet event: MODIFIED
May  6 07:46:04.338: INFO: Observed &DaemonSet event: MODIFIED
May  6 07:46:04.338: INFO: Observed daemon set daemon-set in namespace daemonsets-4980 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  6 07:46:04.339: INFO: Observed &DaemonSet event: MODIFIED
May  6 07:46:04.339: INFO: Found daemon set daemon-set in namespace daemonsets-4980 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
May  6 07:46:04.339: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/06/23 07:46:04.341
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4980, will wait for the garbage collector to delete the pods 05/06/23 07:46:04.341
May  6 07:46:04.400: INFO: Deleting DaemonSet.extensions daemon-set took: 5.633475ms
May  6 07:46:04.500: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.189835ms
May  6 07:46:07.003: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 07:46:07.003: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  6 07:46:07.004: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"162092"},"items":null}

May  6 07:46:07.007: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"162092"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 07:46:07.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4980" for this suite. 05/06/23 07:46:07.023
------------------------------
â€¢ [4.771 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:46:02.258
    May  6 07:46:02.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename daemonsets 05/06/23 07:46:02.259
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:02.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:02.276
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 05/06/23 07:46:02.297
    STEP: Check that daemon pods launch on every node of the cluster. 05/06/23 07:46:02.302
    May  6 07:46:02.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 07:46:02.307: INFO: Node cncf-0 is running 0 daemon pod, expected 1
    May  6 07:46:03.313: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  6 07:46:03.313: INFO: Node cncf-0 is running 0 daemon pod, expected 1
    May  6 07:46:04.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    May  6 07:46:04.314: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Getting /status 05/06/23 07:46:04.317
    May  6 07:46:04.319: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 05/06/23 07:46:04.319
    May  6 07:46:04.328: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 05/06/23 07:46:04.328
    May  6 07:46:04.329: INFO: Observed &DaemonSet event: ADDED
    May  6 07:46:04.329: INFO: Observed &DaemonSet event: MODIFIED
    May  6 07:46:04.329: INFO: Observed &DaemonSet event: MODIFIED
    May  6 07:46:04.329: INFO: Observed &DaemonSet event: MODIFIED
    May  6 07:46:04.329: INFO: Observed &DaemonSet event: MODIFIED
    May  6 07:46:04.330: INFO: Observed &DaemonSet event: MODIFIED
    May  6 07:46:04.330: INFO: Found daemon set daemon-set in namespace daemonsets-4980 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  6 07:46:04.330: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 05/06/23 07:46:04.33
    STEP: watching for the daemon set status to be patched 05/06/23 07:46:04.337
    May  6 07:46:04.338: INFO: Observed &DaemonSet event: ADDED
    May  6 07:46:04.338: INFO: Observed &DaemonSet event: MODIFIED
    May  6 07:46:04.338: INFO: Observed &DaemonSet event: MODIFIED
    May  6 07:46:04.338: INFO: Observed &DaemonSet event: MODIFIED
    May  6 07:46:04.338: INFO: Observed &DaemonSet event: MODIFIED
    May  6 07:46:04.338: INFO: Observed &DaemonSet event: MODIFIED
    May  6 07:46:04.338: INFO: Observed daemon set daemon-set in namespace daemonsets-4980 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  6 07:46:04.339: INFO: Observed &DaemonSet event: MODIFIED
    May  6 07:46:04.339: INFO: Found daemon set daemon-set in namespace daemonsets-4980 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    May  6 07:46:04.339: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/06/23 07:46:04.341
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4980, will wait for the garbage collector to delete the pods 05/06/23 07:46:04.341
    May  6 07:46:04.400: INFO: Deleting DaemonSet.extensions daemon-set took: 5.633475ms
    May  6 07:46:04.500: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.189835ms
    May  6 07:46:07.003: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 07:46:07.003: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  6 07:46:07.004: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"162092"},"items":null}

    May  6 07:46:07.007: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"162092"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 07:46:07.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4980" for this suite. 05/06/23 07:46:07.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:46:07.029
May  6 07:46:07.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:46:07.03
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:07.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:07.046
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 05/06/23 07:46:07.048
May  6 07:46:07.056: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c0a15201-bed2-4f28-821a-eeee360cf696" in namespace "projected-5666" to be "Succeeded or Failed"
May  6 07:46:07.059: INFO: Pod "downwardapi-volume-c0a15201-bed2-4f28-821a-eeee360cf696": Phase="Pending", Reason="", readiness=false. Elapsed: 2.66729ms
May  6 07:46:09.063: INFO: Pod "downwardapi-volume-c0a15201-bed2-4f28-821a-eeee360cf696": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006922658s
May  6 07:46:11.061: INFO: Pod "downwardapi-volume-c0a15201-bed2-4f28-821a-eeee360cf696": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005394323s
STEP: Saw pod success 05/06/23 07:46:11.061
May  6 07:46:11.062: INFO: Pod "downwardapi-volume-c0a15201-bed2-4f28-821a-eeee360cf696" satisfied condition "Succeeded or Failed"
May  6 07:46:11.064: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-c0a15201-bed2-4f28-821a-eeee360cf696 container client-container: <nil>
STEP: delete the pod 05/06/23 07:46:11.069
May  6 07:46:11.087: INFO: Waiting for pod downwardapi-volume-c0a15201-bed2-4f28-821a-eeee360cf696 to disappear
May  6 07:46:11.089: INFO: Pod downwardapi-volume-c0a15201-bed2-4f28-821a-eeee360cf696 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  6 07:46:11.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5666" for this suite. 05/06/23 07:46:11.093
------------------------------
â€¢ [4.071 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:46:07.029
    May  6 07:46:07.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:46:07.03
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:07.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:07.046
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 05/06/23 07:46:07.048
    May  6 07:46:07.056: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c0a15201-bed2-4f28-821a-eeee360cf696" in namespace "projected-5666" to be "Succeeded or Failed"
    May  6 07:46:07.059: INFO: Pod "downwardapi-volume-c0a15201-bed2-4f28-821a-eeee360cf696": Phase="Pending", Reason="", readiness=false. Elapsed: 2.66729ms
    May  6 07:46:09.063: INFO: Pod "downwardapi-volume-c0a15201-bed2-4f28-821a-eeee360cf696": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006922658s
    May  6 07:46:11.061: INFO: Pod "downwardapi-volume-c0a15201-bed2-4f28-821a-eeee360cf696": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005394323s
    STEP: Saw pod success 05/06/23 07:46:11.061
    May  6 07:46:11.062: INFO: Pod "downwardapi-volume-c0a15201-bed2-4f28-821a-eeee360cf696" satisfied condition "Succeeded or Failed"
    May  6 07:46:11.064: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-c0a15201-bed2-4f28-821a-eeee360cf696 container client-container: <nil>
    STEP: delete the pod 05/06/23 07:46:11.069
    May  6 07:46:11.087: INFO: Waiting for pod downwardapi-volume-c0a15201-bed2-4f28-821a-eeee360cf696 to disappear
    May  6 07:46:11.089: INFO: Pod downwardapi-volume-c0a15201-bed2-4f28-821a-eeee360cf696 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  6 07:46:11.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5666" for this suite. 05/06/23 07:46:11.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:46:11.102
May  6 07:46:11.102: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubectl 05/06/23 07:46:11.102
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:11.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:11.122
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 05/06/23 07:46:11.125
May  6 07:46:11.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7193 create -f -'
May  6 07:46:11.561: INFO: stderr: ""
May  6 07:46:11.561: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 05/06/23 07:46:11.561
May  6 07:46:11.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7193 diff -f -'
May  6 07:46:12.009: INFO: rc: 1
May  6 07:46:12.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7193 delete -f -'
May  6 07:46:12.064: INFO: stderr: ""
May  6 07:46:12.064: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  6 07:46:12.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7193" for this suite. 05/06/23 07:46:12.067
------------------------------
â€¢ [0.970 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:46:11.102
    May  6 07:46:11.102: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubectl 05/06/23 07:46:11.102
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:11.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:11.122
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 05/06/23 07:46:11.125
    May  6 07:46:11.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7193 create -f -'
    May  6 07:46:11.561: INFO: stderr: ""
    May  6 07:46:11.561: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 05/06/23 07:46:11.561
    May  6 07:46:11.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7193 diff -f -'
    May  6 07:46:12.009: INFO: rc: 1
    May  6 07:46:12.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7193 delete -f -'
    May  6 07:46:12.064: INFO: stderr: ""
    May  6 07:46:12.064: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  6 07:46:12.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7193" for this suite. 05/06/23 07:46:12.067
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:46:12.072
May  6 07:46:12.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename custom-resource-definition 05/06/23 07:46:12.072
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:12.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:12.105
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 05/06/23 07:46:12.107
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 05/06/23 07:46:12.108
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 05/06/23 07:46:12.108
STEP: fetching the /apis/apiextensions.k8s.io discovery document 05/06/23 07:46:12.108
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 05/06/23 07:46:12.109
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 05/06/23 07:46:12.109
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 05/06/23 07:46:12.11
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:46:12.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-264" for this suite. 05/06/23 07:46:12.115
------------------------------
â€¢ [0.052 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:46:12.072
    May  6 07:46:12.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename custom-resource-definition 05/06/23 07:46:12.072
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:12.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:12.105
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 05/06/23 07:46:12.107
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 05/06/23 07:46:12.108
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 05/06/23 07:46:12.108
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 05/06/23 07:46:12.108
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 05/06/23 07:46:12.109
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 05/06/23 07:46:12.109
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 05/06/23 07:46:12.11
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:46:12.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-264" for this suite. 05/06/23 07:46:12.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:46:12.124
May  6 07:46:12.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename replicaset 05/06/23 07:46:12.124
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:12.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:12.142
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 05/06/23 07:46:12.144
STEP: Verify that the required pods have come up 05/06/23 07:46:12.149
May  6 07:46:12.151: INFO: Pod name sample-pod: Found 0 pods out of 3
May  6 07:46:17.154: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 05/06/23 07:46:17.154
May  6 07:46:17.157: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 05/06/23 07:46:17.157
STEP: DeleteCollection of the ReplicaSets 05/06/23 07:46:17.159
STEP: After DeleteCollection verify that ReplicaSets have been deleted 05/06/23 07:46:17.169
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May  6 07:46:17.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6248" for this suite. 05/06/23 07:46:17.174
------------------------------
â€¢ [SLOW TEST] [5.056 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:46:12.124
    May  6 07:46:12.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename replicaset 05/06/23 07:46:12.124
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:12.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:12.142
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 05/06/23 07:46:12.144
    STEP: Verify that the required pods have come up 05/06/23 07:46:12.149
    May  6 07:46:12.151: INFO: Pod name sample-pod: Found 0 pods out of 3
    May  6 07:46:17.154: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 05/06/23 07:46:17.154
    May  6 07:46:17.157: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 05/06/23 07:46:17.157
    STEP: DeleteCollection of the ReplicaSets 05/06/23 07:46:17.159
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 05/06/23 07:46:17.169
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May  6 07:46:17.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6248" for this suite. 05/06/23 07:46:17.174
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:46:17.18
May  6 07:46:17.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename webhook 05/06/23 07:46:17.18
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:17.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:17.212
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/06/23 07:46:17.25
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:46:17.587
STEP: Deploying the webhook pod 05/06/23 07:46:17.595
STEP: Wait for the deployment to be ready 05/06/23 07:46:17.606
May  6 07:46:17.613: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/06/23 07:46:19.621
STEP: Verifying the service has paired with the endpoint 05/06/23 07:46:19.632
May  6 07:46:20.632: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 05/06/23 07:46:20.684
STEP: Creating a configMap that does not comply to the validation webhook rules 05/06/23 07:46:21.731
STEP: Deleting the collection of validation webhooks 05/06/23 07:46:22.754
STEP: Creating a configMap that does not comply to the validation webhook rules 05/06/23 07:46:22.799
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:46:22.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5232" for this suite. 05/06/23 07:46:22.853
STEP: Destroying namespace "webhook-5232-markers" for this suite. 05/06/23 07:46:22.862
------------------------------
â€¢ [SLOW TEST] [5.690 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:46:17.18
    May  6 07:46:17.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename webhook 05/06/23 07:46:17.18
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:17.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:17.212
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/06/23 07:46:17.25
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:46:17.587
    STEP: Deploying the webhook pod 05/06/23 07:46:17.595
    STEP: Wait for the deployment to be ready 05/06/23 07:46:17.606
    May  6 07:46:17.613: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/06/23 07:46:19.621
    STEP: Verifying the service has paired with the endpoint 05/06/23 07:46:19.632
    May  6 07:46:20.632: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 05/06/23 07:46:20.684
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/06/23 07:46:21.731
    STEP: Deleting the collection of validation webhooks 05/06/23 07:46:22.754
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/06/23 07:46:22.799
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:46:22.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5232" for this suite. 05/06/23 07:46:22.853
    STEP: Destroying namespace "webhook-5232-markers" for this suite. 05/06/23 07:46:22.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:46:22.871
May  6 07:46:22.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename container-runtime 05/06/23 07:46:22.872
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:22.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:22.892
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 05/06/23 07:46:22.894
STEP: wait for the container to reach Succeeded 05/06/23 07:46:22.9
STEP: get the container status 05/06/23 07:46:26.914
STEP: the container should be terminated 05/06/23 07:46:26.916
STEP: the termination message should be set 05/06/23 07:46:26.916
May  6 07:46:26.916: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 05/06/23 07:46:26.916
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May  6 07:46:26.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5446" for this suite. 05/06/23 07:46:26.932
------------------------------
â€¢ [4.066 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:46:22.871
    May  6 07:46:22.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename container-runtime 05/06/23 07:46:22.872
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:22.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:22.892
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 05/06/23 07:46:22.894
    STEP: wait for the container to reach Succeeded 05/06/23 07:46:22.9
    STEP: get the container status 05/06/23 07:46:26.914
    STEP: the container should be terminated 05/06/23 07:46:26.916
    STEP: the termination message should be set 05/06/23 07:46:26.916
    May  6 07:46:26.916: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 05/06/23 07:46:26.916
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May  6 07:46:26.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5446" for this suite. 05/06/23 07:46:26.932
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:46:26.937
May  6 07:46:26.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename deployment 05/06/23 07:46:26.938
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:26.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:26.954
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
May  6 07:46:26.963: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May  6 07:46:31.967: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/06/23 07:46:31.967
May  6 07:46:31.967: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 05/06/23 07:46:31.975
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  6 07:46:31.985: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-975  4dbc8ae0-a017-4c6b-aa32-6f30b33ec93f 162509 1 2023-05-06 07:46:31 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-05-06 07:46:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a7eaa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

May  6 07:46:31.988: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
May  6 07:46:31.988: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May  6 07:46:31.988: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-975  35186be8-b874-49ea-b479-14cc53c84067 162510 1 2023-05-06 07:46:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 4dbc8ae0-a017-4c6b-aa32-6f30b33ec93f 0xc004a7ede7 0xc004a7ede8}] [] [{e2e.test Update apps/v1 2023-05-06 07:46:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:46:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-06 07:46:31 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"4dbc8ae0-a017-4c6b-aa32-6f30b33ec93f\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004a7eea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  6 07:46:31.991: INFO: Pod "test-cleanup-controller-z4k4p" is available:
&Pod{ObjectMeta:{test-cleanup-controller-z4k4p test-cleanup-controller- deployment-975  6990bea9-53ec-4829-8670-1a23b4db9599 162493 0 2023-05-06 07:46:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:f21151000835a521c7b021f71da9e4a28dcb99df1b28b7082a2df2b9aadc9dad cni.projectcalico.org/podIP:10.244.174.131/32 cni.projectcalico.org/podIPs:10.244.174.131/32] [{apps/v1 ReplicaSet test-cleanup-controller 35186be8-b874-49ea-b479-14cc53c84067 0xc004a7f197 0xc004a7f198}] [] [{kube-controller-manager Update v1 2023-05-06 07:46:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35186be8-b874-49ea-b479-14cc53c84067\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:46:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:46:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.174.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vmz8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vmz8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:46:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:46:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:46:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:46:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:10.244.174.131,StartTime:2023-05-06 07:46:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:46:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3ac9ebaa7ca85f5e17d8a5b1698f7f38ae43abbd94fa48c2b761462c59dd8e75,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.174.131,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May  6 07:46:31.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-975" for this suite. 05/06/23 07:46:31.999
------------------------------
â€¢ [SLOW TEST] [5.077 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:46:26.937
    May  6 07:46:26.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename deployment 05/06/23 07:46:26.938
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:26.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:26.954
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    May  6 07:46:26.963: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    May  6 07:46:31.967: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/06/23 07:46:31.967
    May  6 07:46:31.967: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 05/06/23 07:46:31.975
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  6 07:46:31.985: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-975  4dbc8ae0-a017-4c6b-aa32-6f30b33ec93f 162509 1 2023-05-06 07:46:31 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-05-06 07:46:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a7eaa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    May  6 07:46:31.988: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    May  6 07:46:31.988: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    May  6 07:46:31.988: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-975  35186be8-b874-49ea-b479-14cc53c84067 162510 1 2023-05-06 07:46:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 4dbc8ae0-a017-4c6b-aa32-6f30b33ec93f 0xc004a7ede7 0xc004a7ede8}] [] [{e2e.test Update apps/v1 2023-05-06 07:46:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:46:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-06 07:46:31 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"4dbc8ae0-a017-4c6b-aa32-6f30b33ec93f\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004a7eea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  6 07:46:31.991: INFO: Pod "test-cleanup-controller-z4k4p" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-z4k4p test-cleanup-controller- deployment-975  6990bea9-53ec-4829-8670-1a23b4db9599 162493 0 2023-05-06 07:46:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:f21151000835a521c7b021f71da9e4a28dcb99df1b28b7082a2df2b9aadc9dad cni.projectcalico.org/podIP:10.244.174.131/32 cni.projectcalico.org/podIPs:10.244.174.131/32] [{apps/v1 ReplicaSet test-cleanup-controller 35186be8-b874-49ea-b479-14cc53c84067 0xc004a7f197 0xc004a7f198}] [] [{kube-controller-manager Update v1 2023-05-06 07:46:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35186be8-b874-49ea-b479-14cc53c84067\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:46:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:46:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.174.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vmz8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vmz8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:46:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:46:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:46:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:46:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:10.244.174.131,StartTime:2023-05-06 07:46:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:46:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3ac9ebaa7ca85f5e17d8a5b1698f7f38ae43abbd94fa48c2b761462c59dd8e75,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.174.131,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May  6 07:46:31.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-975" for this suite. 05/06/23 07:46:31.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:46:32.015
May  6 07:46:32.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubectl 05/06/23 07:46:32.016
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:32.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:32.037
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 05/06/23 07:46:32.039
May  6 07:46:32.039: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May  6 07:46:32.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 create -f -'
May  6 07:46:32.203: INFO: stderr: ""
May  6 07:46:32.204: INFO: stdout: "service/agnhost-replica created\n"
May  6 07:46:32.204: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May  6 07:46:32.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 create -f -'
May  6 07:46:32.375: INFO: stderr: ""
May  6 07:46:32.375: INFO: stdout: "service/agnhost-primary created\n"
May  6 07:46:32.375: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May  6 07:46:32.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 create -f -'
May  6 07:46:32.534: INFO: stderr: ""
May  6 07:46:32.534: INFO: stdout: "service/frontend created\n"
May  6 07:46:32.534: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May  6 07:46:32.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 create -f -'
May  6 07:46:32.975: INFO: stderr: ""
May  6 07:46:32.975: INFO: stdout: "deployment.apps/frontend created\n"
May  6 07:46:32.975: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May  6 07:46:32.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 create -f -'
May  6 07:46:33.134: INFO: stderr: ""
May  6 07:46:33.134: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May  6 07:46:33.134: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May  6 07:46:33.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 create -f -'
May  6 07:46:33.291: INFO: stderr: ""
May  6 07:46:33.291: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 05/06/23 07:46:33.291
May  6 07:46:33.291: INFO: Waiting for all frontend pods to be Running.
May  6 07:46:38.342: INFO: Waiting for frontend to serve content.
May  6 07:46:38.349: INFO: Trying to add a new entry to the guestbook.
May  6 07:46:38.359: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 05/06/23 07:46:38.364
May  6 07:46:38.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 delete --grace-period=0 --force -f -'
May  6 07:46:38.433: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 07:46:38.433: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 05/06/23 07:46:38.433
May  6 07:46:38.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 delete --grace-period=0 --force -f -'
May  6 07:46:38.507: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 07:46:38.507: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 05/06/23 07:46:38.507
May  6 07:46:38.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 delete --grace-period=0 --force -f -'
May  6 07:46:38.571: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 07:46:38.571: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 05/06/23 07:46:38.571
May  6 07:46:38.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 delete --grace-period=0 --force -f -'
May  6 07:46:38.622: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 07:46:38.622: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 05/06/23 07:46:38.622
May  6 07:46:38.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 delete --grace-period=0 --force -f -'
May  6 07:46:38.693: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 07:46:38.693: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 05/06/23 07:46:38.694
May  6 07:46:38.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 delete --grace-period=0 --force -f -'
May  6 07:46:38.787: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 07:46:38.787: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  6 07:46:38.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8804" for this suite. 05/06/23 07:46:38.791
------------------------------
â€¢ [SLOW TEST] [6.785 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:46:32.015
    May  6 07:46:32.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubectl 05/06/23 07:46:32.016
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:32.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:32.037
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 05/06/23 07:46:32.039
    May  6 07:46:32.039: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    May  6 07:46:32.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 create -f -'
    May  6 07:46:32.203: INFO: stderr: ""
    May  6 07:46:32.204: INFO: stdout: "service/agnhost-replica created\n"
    May  6 07:46:32.204: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    May  6 07:46:32.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 create -f -'
    May  6 07:46:32.375: INFO: stderr: ""
    May  6 07:46:32.375: INFO: stdout: "service/agnhost-primary created\n"
    May  6 07:46:32.375: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    May  6 07:46:32.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 create -f -'
    May  6 07:46:32.534: INFO: stderr: ""
    May  6 07:46:32.534: INFO: stdout: "service/frontend created\n"
    May  6 07:46:32.534: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    May  6 07:46:32.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 create -f -'
    May  6 07:46:32.975: INFO: stderr: ""
    May  6 07:46:32.975: INFO: stdout: "deployment.apps/frontend created\n"
    May  6 07:46:32.975: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    May  6 07:46:32.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 create -f -'
    May  6 07:46:33.134: INFO: stderr: ""
    May  6 07:46:33.134: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    May  6 07:46:33.134: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    May  6 07:46:33.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 create -f -'
    May  6 07:46:33.291: INFO: stderr: ""
    May  6 07:46:33.291: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 05/06/23 07:46:33.291
    May  6 07:46:33.291: INFO: Waiting for all frontend pods to be Running.
    May  6 07:46:38.342: INFO: Waiting for frontend to serve content.
    May  6 07:46:38.349: INFO: Trying to add a new entry to the guestbook.
    May  6 07:46:38.359: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 05/06/23 07:46:38.364
    May  6 07:46:38.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 delete --grace-period=0 --force -f -'
    May  6 07:46:38.433: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  6 07:46:38.433: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 05/06/23 07:46:38.433
    May  6 07:46:38.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 delete --grace-period=0 --force -f -'
    May  6 07:46:38.507: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  6 07:46:38.507: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 05/06/23 07:46:38.507
    May  6 07:46:38.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 delete --grace-period=0 --force -f -'
    May  6 07:46:38.571: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  6 07:46:38.571: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 05/06/23 07:46:38.571
    May  6 07:46:38.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 delete --grace-period=0 --force -f -'
    May  6 07:46:38.622: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  6 07:46:38.622: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 05/06/23 07:46:38.622
    May  6 07:46:38.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 delete --grace-period=0 --force -f -'
    May  6 07:46:38.693: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  6 07:46:38.693: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 05/06/23 07:46:38.694
    May  6 07:46:38.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8804 delete --grace-period=0 --force -f -'
    May  6 07:46:38.787: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  6 07:46:38.787: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  6 07:46:38.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8804" for this suite. 05/06/23 07:46:38.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:46:38.8
May  6 07:46:38.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename pod-network-test 05/06/23 07:46:38.801
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:38.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:38.819
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-910 05/06/23 07:46:38.821
STEP: creating a selector 05/06/23 07:46:38.821
STEP: Creating the service pods in kubernetes 05/06/23 07:46:38.822
May  6 07:46:38.822: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  6 07:46:38.872: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-910" to be "running and ready"
May  6 07:46:38.880: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.806125ms
May  6 07:46:38.880: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  6 07:46:40.883: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011430004s
May  6 07:46:40.883: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 07:46:42.883: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011365754s
May  6 07:46:42.883: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 07:46:44.884: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011790654s
May  6 07:46:44.884: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 07:46:46.884: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012264076s
May  6 07:46:46.884: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 07:46:48.883: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01093417s
May  6 07:46:48.883: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 07:46:50.884: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.012169761s
May  6 07:46:50.884: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  6 07:46:50.884: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  6 07:46:50.888: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-910" to be "running and ready"
May  6 07:46:50.890: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006633ms
May  6 07:46:50.890: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  6 07:46:50.890: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  6 07:46:50.892: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-910" to be "running and ready"
May  6 07:46:50.894: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.773079ms
May  6 07:46:50.894: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  6 07:46:50.894: INFO: Pod "netserver-2" satisfied condition "running and ready"
May  6 07:46:50.896: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-910" to be "running and ready"
May  6 07:46:50.898: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.010591ms
May  6 07:46:50.898: INFO: The phase of Pod netserver-3 is Running (Ready = true)
May  6 07:46:50.898: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 05/06/23 07:46:50.901
May  6 07:46:50.912: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-910" to be "running"
May  6 07:46:50.915: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.627805ms
May  6 07:46:52.919: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006269757s
May  6 07:46:52.919: INFO: Pod "test-container-pod" satisfied condition "running"
May  6 07:46:52.921: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
May  6 07:46:52.921: INFO: Breadth first check of 10.244.174.169 on host 10.0.0.134...
May  6 07:46:52.923: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.174.166:9080/dial?request=hostname&protocol=http&host=10.244.174.169&port=8083&tries=1'] Namespace:pod-network-test-910 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 07:46:52.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:46:52.924: INFO: ExecWithOptions: Clientset creation
May  6 07:46:52.924: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-910/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.174.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.174.169%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  6 07:46:53.000: INFO: Waiting for responses: map[]
May  6 07:46:53.000: INFO: reached 10.244.174.169 after 0/1 tries
May  6 07:46:53.000: INFO: Breadth first check of 10.244.21.101 on host 10.0.0.107...
May  6 07:46:53.002: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.174.166:9080/dial?request=hostname&protocol=http&host=10.244.21.101&port=8083&tries=1'] Namespace:pod-network-test-910 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 07:46:53.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:46:53.002: INFO: ExecWithOptions: Clientset creation
May  6 07:46:53.002: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-910/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.174.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.21.101%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  6 07:46:53.065: INFO: Waiting for responses: map[]
May  6 07:46:53.065: INFO: reached 10.244.21.101 after 0/1 tries
May  6 07:46:53.065: INFO: Breadth first check of 10.244.20.138 on host 10.0.0.180...
May  6 07:46:53.067: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.174.166:9080/dial?request=hostname&protocol=http&host=10.244.20.138&port=8083&tries=1'] Namespace:pod-network-test-910 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 07:46:53.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:46:53.068: INFO: ExecWithOptions: Clientset creation
May  6 07:46:53.068: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-910/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.174.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.20.138%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  6 07:46:53.118: INFO: Waiting for responses: map[]
May  6 07:46:53.118: INFO: reached 10.244.20.138 after 0/1 tries
May  6 07:46:53.118: INFO: Breadth first check of 10.244.245.99 on host 10.0.0.240...
May  6 07:46:53.120: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.174.166:9080/dial?request=hostname&protocol=http&host=10.244.245.99&port=8083&tries=1'] Namespace:pod-network-test-910 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 07:46:53.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:46:53.120: INFO: ExecWithOptions: Clientset creation
May  6 07:46:53.120: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-910/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.174.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.245.99%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  6 07:46:53.198: INFO: Waiting for responses: map[]
May  6 07:46:53.198: INFO: reached 10.244.245.99 after 0/1 tries
May  6 07:46:53.198: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
May  6 07:46:53.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-910" for this suite. 05/06/23 07:46:53.201
------------------------------
â€¢ [SLOW TEST] [14.407 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:46:38.8
    May  6 07:46:38.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename pod-network-test 05/06/23 07:46:38.801
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:38.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:38.819
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-910 05/06/23 07:46:38.821
    STEP: creating a selector 05/06/23 07:46:38.821
    STEP: Creating the service pods in kubernetes 05/06/23 07:46:38.822
    May  6 07:46:38.822: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  6 07:46:38.872: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-910" to be "running and ready"
    May  6 07:46:38.880: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.806125ms
    May  6 07:46:38.880: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:46:40.883: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011430004s
    May  6 07:46:40.883: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 07:46:42.883: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011365754s
    May  6 07:46:42.883: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 07:46:44.884: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011790654s
    May  6 07:46:44.884: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 07:46:46.884: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012264076s
    May  6 07:46:46.884: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 07:46:48.883: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01093417s
    May  6 07:46:48.883: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 07:46:50.884: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.012169761s
    May  6 07:46:50.884: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  6 07:46:50.884: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  6 07:46:50.888: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-910" to be "running and ready"
    May  6 07:46:50.890: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006633ms
    May  6 07:46:50.890: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  6 07:46:50.890: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  6 07:46:50.892: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-910" to be "running and ready"
    May  6 07:46:50.894: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.773079ms
    May  6 07:46:50.894: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  6 07:46:50.894: INFO: Pod "netserver-2" satisfied condition "running and ready"
    May  6 07:46:50.896: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-910" to be "running and ready"
    May  6 07:46:50.898: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.010591ms
    May  6 07:46:50.898: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    May  6 07:46:50.898: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 05/06/23 07:46:50.901
    May  6 07:46:50.912: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-910" to be "running"
    May  6 07:46:50.915: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.627805ms
    May  6 07:46:52.919: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006269757s
    May  6 07:46:52.919: INFO: Pod "test-container-pod" satisfied condition "running"
    May  6 07:46:52.921: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    May  6 07:46:52.921: INFO: Breadth first check of 10.244.174.169 on host 10.0.0.134...
    May  6 07:46:52.923: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.174.166:9080/dial?request=hostname&protocol=http&host=10.244.174.169&port=8083&tries=1'] Namespace:pod-network-test-910 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 07:46:52.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:46:52.924: INFO: ExecWithOptions: Clientset creation
    May  6 07:46:52.924: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-910/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.174.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.174.169%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  6 07:46:53.000: INFO: Waiting for responses: map[]
    May  6 07:46:53.000: INFO: reached 10.244.174.169 after 0/1 tries
    May  6 07:46:53.000: INFO: Breadth first check of 10.244.21.101 on host 10.0.0.107...
    May  6 07:46:53.002: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.174.166:9080/dial?request=hostname&protocol=http&host=10.244.21.101&port=8083&tries=1'] Namespace:pod-network-test-910 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 07:46:53.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:46:53.002: INFO: ExecWithOptions: Clientset creation
    May  6 07:46:53.002: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-910/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.174.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.21.101%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  6 07:46:53.065: INFO: Waiting for responses: map[]
    May  6 07:46:53.065: INFO: reached 10.244.21.101 after 0/1 tries
    May  6 07:46:53.065: INFO: Breadth first check of 10.244.20.138 on host 10.0.0.180...
    May  6 07:46:53.067: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.174.166:9080/dial?request=hostname&protocol=http&host=10.244.20.138&port=8083&tries=1'] Namespace:pod-network-test-910 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 07:46:53.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:46:53.068: INFO: ExecWithOptions: Clientset creation
    May  6 07:46:53.068: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-910/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.174.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.20.138%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  6 07:46:53.118: INFO: Waiting for responses: map[]
    May  6 07:46:53.118: INFO: reached 10.244.20.138 after 0/1 tries
    May  6 07:46:53.118: INFO: Breadth first check of 10.244.245.99 on host 10.0.0.240...
    May  6 07:46:53.120: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.174.166:9080/dial?request=hostname&protocol=http&host=10.244.245.99&port=8083&tries=1'] Namespace:pod-network-test-910 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 07:46:53.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:46:53.120: INFO: ExecWithOptions: Clientset creation
    May  6 07:46:53.120: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-910/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.174.166%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.245.99%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  6 07:46:53.198: INFO: Waiting for responses: map[]
    May  6 07:46:53.198: INFO: reached 10.244.245.99 after 0/1 tries
    May  6 07:46:53.198: INFO: Going to retry 0 out of 4 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    May  6 07:46:53.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-910" for this suite. 05/06/23 07:46:53.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:46:53.208
May  6 07:46:53.208: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename crd-webhook 05/06/23 07:46:53.209
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:53.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:53.227
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 05/06/23 07:46:53.229
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/06/23 07:46:53.459
STEP: Deploying the custom resource conversion webhook pod 05/06/23 07:46:53.468
STEP: Wait for the deployment to be ready 05/06/23 07:46:53.478
May  6 07:46:53.484: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/06/23 07:46:55.493
STEP: Verifying the service has paired with the endpoint 05/06/23 07:46:55.504
May  6 07:46:56.504: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
May  6 07:46:56.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Creating a v1 custom resource 05/06/23 07:46:59.071
STEP: Create a v2 custom resource 05/06/23 07:46:59.089
STEP: List CRs in v1 05/06/23 07:46:59.127
STEP: List CRs in v2 05/06/23 07:46:59.13
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:46:59.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-7548" for this suite. 05/06/23 07:46:59.72
------------------------------
â€¢ [SLOW TEST] [6.526 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:46:53.208
    May  6 07:46:53.208: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename crd-webhook 05/06/23 07:46:53.209
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:53.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:53.227
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 05/06/23 07:46:53.229
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/06/23 07:46:53.459
    STEP: Deploying the custom resource conversion webhook pod 05/06/23 07:46:53.468
    STEP: Wait for the deployment to be ready 05/06/23 07:46:53.478
    May  6 07:46:53.484: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/06/23 07:46:55.493
    STEP: Verifying the service has paired with the endpoint 05/06/23 07:46:55.504
    May  6 07:46:56.504: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    May  6 07:46:56.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Creating a v1 custom resource 05/06/23 07:46:59.071
    STEP: Create a v2 custom resource 05/06/23 07:46:59.089
    STEP: List CRs in v1 05/06/23 07:46:59.127
    STEP: List CRs in v2 05/06/23 07:46:59.13
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:46:59.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-7548" for this suite. 05/06/23 07:46:59.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:46:59.736
May  6 07:46:59.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename downward-api 05/06/23 07:46:59.737
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:59.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:59.77
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 05/06/23 07:46:59.774
May  6 07:46:59.785: INFO: Waiting up to 5m0s for pod "downward-api-a2c57c73-78a1-4789-8099-96d2b009df7e" in namespace "downward-api-9705" to be "Succeeded or Failed"
May  6 07:46:59.788: INFO: Pod "downward-api-a2c57c73-78a1-4789-8099-96d2b009df7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.907074ms
May  6 07:47:01.791: INFO: Pod "downward-api-a2c57c73-78a1-4789-8099-96d2b009df7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006352023s
May  6 07:47:03.792: INFO: Pod "downward-api-a2c57c73-78a1-4789-8099-96d2b009df7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006723131s
STEP: Saw pod success 05/06/23 07:47:03.792
May  6 07:47:03.792: INFO: Pod "downward-api-a2c57c73-78a1-4789-8099-96d2b009df7e" satisfied condition "Succeeded or Failed"
May  6 07:47:03.794: INFO: Trying to get logs from node cncf-3 pod downward-api-a2c57c73-78a1-4789-8099-96d2b009df7e container dapi-container: <nil>
STEP: delete the pod 05/06/23 07:47:03.804
May  6 07:47:03.814: INFO: Waiting for pod downward-api-a2c57c73-78a1-4789-8099-96d2b009df7e to disappear
May  6 07:47:03.817: INFO: Pod downward-api-a2c57c73-78a1-4789-8099-96d2b009df7e no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May  6 07:47:03.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9705" for this suite. 05/06/23 07:47:03.82
------------------------------
â€¢ [4.088 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:46:59.736
    May  6 07:46:59.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename downward-api 05/06/23 07:46:59.737
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:46:59.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:46:59.77
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 05/06/23 07:46:59.774
    May  6 07:46:59.785: INFO: Waiting up to 5m0s for pod "downward-api-a2c57c73-78a1-4789-8099-96d2b009df7e" in namespace "downward-api-9705" to be "Succeeded or Failed"
    May  6 07:46:59.788: INFO: Pod "downward-api-a2c57c73-78a1-4789-8099-96d2b009df7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.907074ms
    May  6 07:47:01.791: INFO: Pod "downward-api-a2c57c73-78a1-4789-8099-96d2b009df7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006352023s
    May  6 07:47:03.792: INFO: Pod "downward-api-a2c57c73-78a1-4789-8099-96d2b009df7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006723131s
    STEP: Saw pod success 05/06/23 07:47:03.792
    May  6 07:47:03.792: INFO: Pod "downward-api-a2c57c73-78a1-4789-8099-96d2b009df7e" satisfied condition "Succeeded or Failed"
    May  6 07:47:03.794: INFO: Trying to get logs from node cncf-3 pod downward-api-a2c57c73-78a1-4789-8099-96d2b009df7e container dapi-container: <nil>
    STEP: delete the pod 05/06/23 07:47:03.804
    May  6 07:47:03.814: INFO: Waiting for pod downward-api-a2c57c73-78a1-4789-8099-96d2b009df7e to disappear
    May  6 07:47:03.817: INFO: Pod downward-api-a2c57c73-78a1-4789-8099-96d2b009df7e no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May  6 07:47:03.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9705" for this suite. 05/06/23 07:47:03.82
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:47:03.824
May  6 07:47:03.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename daemonsets 05/06/23 07:47:03.825
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:47:03.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:47:03.84
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
May  6 07:47:03.857: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 05/06/23 07:47:03.862
May  6 07:47:03.867: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 07:47:03.867: INFO: Node cncf-0 is running 0 daemon pod, expected 1
May  6 07:47:04.877: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  6 07:47:04.877: INFO: Node cncf-1 is running 0 daemon pod, expected 1
May  6 07:47:05.874: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
May  6 07:47:05.874: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Update daemon pods image. 05/06/23 07:47:05.883
STEP: Check that daemon pods images are updated. 05/06/23 07:47:05.894
May  6 07:47:05.899: INFO: Wrong image for pod: daemon-set-2whcd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  6 07:47:05.899: INFO: Wrong image for pod: daemon-set-4d4qm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  6 07:47:05.899: INFO: Wrong image for pod: daemon-set-7qrwf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  6 07:47:05.899: INFO: Wrong image for pod: daemon-set-ccl6l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  6 07:47:06.906: INFO: Wrong image for pod: daemon-set-2whcd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  6 07:47:06.906: INFO: Wrong image for pod: daemon-set-4d4qm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  6 07:47:06.906: INFO: Wrong image for pod: daemon-set-7qrwf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  6 07:47:07.906: INFO: Wrong image for pod: daemon-set-2whcd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  6 07:47:07.906: INFO: Wrong image for pod: daemon-set-4d4qm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  6 07:47:07.906: INFO: Wrong image for pod: daemon-set-7qrwf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  6 07:47:08.906: INFO: Pod daemon-set-26plf is not available
May  6 07:47:08.906: INFO: Wrong image for pod: daemon-set-2whcd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  6 07:47:08.906: INFO: Wrong image for pod: daemon-set-4d4qm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  6 07:47:08.906: INFO: Wrong image for pod: daemon-set-7qrwf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  6 07:47:09.905: INFO: Wrong image for pod: daemon-set-2whcd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  6 07:47:09.905: INFO: Wrong image for pod: daemon-set-7qrwf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  6 07:47:09.905: INFO: Pod daemon-set-dd9gh is not available
May  6 07:47:10.906: INFO: Wrong image for pod: daemon-set-7qrwf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  6 07:47:11.907: INFO: Wrong image for pod: daemon-set-7qrwf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May  6 07:47:11.907: INFO: Pod daemon-set-7wqjg is not available
May  6 07:47:12.905: INFO: Pod daemon-set-cqc29 is not available
STEP: Check that daemon pods are still running on every node of the cluster. 05/06/23 07:47:12.907
May  6 07:47:12.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  6 07:47:12.913: INFO: Node cncf-0 is running 0 daemon pod, expected 1
May  6 07:47:13.919: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
May  6 07:47:13.919: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/06/23 07:47:13.93
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4767, will wait for the garbage collector to delete the pods 05/06/23 07:47:13.93
May  6 07:47:13.988: INFO: Deleting DaemonSet.extensions daemon-set took: 5.772431ms
May  6 07:47:14.088: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.385557ms
May  6 07:47:16.791: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 07:47:16.791: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  6 07:47:16.793: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"163397"},"items":null}

May  6 07:47:16.795: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"163397"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 07:47:16.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4767" for this suite. 05/06/23 07:47:16.809
------------------------------
â€¢ [SLOW TEST] [12.990 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:47:03.824
    May  6 07:47:03.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename daemonsets 05/06/23 07:47:03.825
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:47:03.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:47:03.84
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    May  6 07:47:03.857: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 05/06/23 07:47:03.862
    May  6 07:47:03.867: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 07:47:03.867: INFO: Node cncf-0 is running 0 daemon pod, expected 1
    May  6 07:47:04.877: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  6 07:47:04.877: INFO: Node cncf-1 is running 0 daemon pod, expected 1
    May  6 07:47:05.874: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    May  6 07:47:05.874: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Update daemon pods image. 05/06/23 07:47:05.883
    STEP: Check that daemon pods images are updated. 05/06/23 07:47:05.894
    May  6 07:47:05.899: INFO: Wrong image for pod: daemon-set-2whcd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  6 07:47:05.899: INFO: Wrong image for pod: daemon-set-4d4qm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  6 07:47:05.899: INFO: Wrong image for pod: daemon-set-7qrwf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  6 07:47:05.899: INFO: Wrong image for pod: daemon-set-ccl6l. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  6 07:47:06.906: INFO: Wrong image for pod: daemon-set-2whcd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  6 07:47:06.906: INFO: Wrong image for pod: daemon-set-4d4qm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  6 07:47:06.906: INFO: Wrong image for pod: daemon-set-7qrwf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  6 07:47:07.906: INFO: Wrong image for pod: daemon-set-2whcd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  6 07:47:07.906: INFO: Wrong image for pod: daemon-set-4d4qm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  6 07:47:07.906: INFO: Wrong image for pod: daemon-set-7qrwf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  6 07:47:08.906: INFO: Pod daemon-set-26plf is not available
    May  6 07:47:08.906: INFO: Wrong image for pod: daemon-set-2whcd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  6 07:47:08.906: INFO: Wrong image for pod: daemon-set-4d4qm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  6 07:47:08.906: INFO: Wrong image for pod: daemon-set-7qrwf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  6 07:47:09.905: INFO: Wrong image for pod: daemon-set-2whcd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  6 07:47:09.905: INFO: Wrong image for pod: daemon-set-7qrwf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  6 07:47:09.905: INFO: Pod daemon-set-dd9gh is not available
    May  6 07:47:10.906: INFO: Wrong image for pod: daemon-set-7qrwf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  6 07:47:11.907: INFO: Wrong image for pod: daemon-set-7qrwf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May  6 07:47:11.907: INFO: Pod daemon-set-7wqjg is not available
    May  6 07:47:12.905: INFO: Pod daemon-set-cqc29 is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 05/06/23 07:47:12.907
    May  6 07:47:12.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  6 07:47:12.913: INFO: Node cncf-0 is running 0 daemon pod, expected 1
    May  6 07:47:13.919: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    May  6 07:47:13.919: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/06/23 07:47:13.93
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4767, will wait for the garbage collector to delete the pods 05/06/23 07:47:13.93
    May  6 07:47:13.988: INFO: Deleting DaemonSet.extensions daemon-set took: 5.772431ms
    May  6 07:47:14.088: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.385557ms
    May  6 07:47:16.791: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 07:47:16.791: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  6 07:47:16.793: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"163397"},"items":null}

    May  6 07:47:16.795: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"163397"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 07:47:16.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4767" for this suite. 05/06/23 07:47:16.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:47:16.815
May  6 07:47:16.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename services 05/06/23 07:47:16.816
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:47:16.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:47:16.832
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-385 05/06/23 07:47:16.834
STEP: creating replication controller nodeport-test in namespace services-385 05/06/23 07:47:16.849
I0506 07:47:16.856815      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-385, replica count: 2
I0506 07:47:19.908293      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 07:47:19.908: INFO: Creating new exec pod
May  6 07:47:19.915: INFO: Waiting up to 5m0s for pod "execpodd5wm7" in namespace "services-385" to be "running"
May  6 07:47:19.917: INFO: Pod "execpodd5wm7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.588038ms
May  6 07:47:21.921: INFO: Pod "execpodd5wm7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006433748s
May  6 07:47:21.921: INFO: Pod "execpodd5wm7" satisfied condition "running"
May  6 07:47:22.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-385 exec execpodd5wm7 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
May  6 07:47:23.041: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May  6 07:47:23.041: INFO: stdout: ""
May  6 07:47:23.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-385 exec execpodd5wm7 -- /bin/sh -x -c nc -v -z -w 2 10.107.180.241 80'
May  6 07:47:23.150: INFO: stderr: "+ nc -v -z -w 2 10.107.180.241 80\nConnection to 10.107.180.241 80 port [tcp/http] succeeded!\n"
May  6 07:47:23.150: INFO: stdout: ""
May  6 07:47:23.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-385 exec execpodd5wm7 -- /bin/sh -x -c nc -v -z -w 2 10.0.0.240 31045'
May  6 07:47:23.253: INFO: stderr: "+ nc -v -z -w 2 10.0.0.240 31045\nConnection to 10.0.0.240 31045 port [tcp/*] succeeded!\n"
May  6 07:47:23.253: INFO: stdout: ""
May  6 07:47:23.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-385 exec execpodd5wm7 -- /bin/sh -x -c nc -v -z -w 2 10.0.0.134 31045'
May  6 07:47:23.354: INFO: stderr: "+ nc -v -z -w 2 10.0.0.134 31045\nConnection to 10.0.0.134 31045 port [tcp/*] succeeded!\n"
May  6 07:47:23.354: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  6 07:47:23.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-385" for this suite. 05/06/23 07:47:23.357
------------------------------
â€¢ [SLOW TEST] [6.549 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:47:16.815
    May  6 07:47:16.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename services 05/06/23 07:47:16.816
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:47:16.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:47:16.832
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-385 05/06/23 07:47:16.834
    STEP: creating replication controller nodeport-test in namespace services-385 05/06/23 07:47:16.849
    I0506 07:47:16.856815      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-385, replica count: 2
    I0506 07:47:19.908293      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  6 07:47:19.908: INFO: Creating new exec pod
    May  6 07:47:19.915: INFO: Waiting up to 5m0s for pod "execpodd5wm7" in namespace "services-385" to be "running"
    May  6 07:47:19.917: INFO: Pod "execpodd5wm7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.588038ms
    May  6 07:47:21.921: INFO: Pod "execpodd5wm7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006433748s
    May  6 07:47:21.921: INFO: Pod "execpodd5wm7" satisfied condition "running"
    May  6 07:47:22.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-385 exec execpodd5wm7 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    May  6 07:47:23.041: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    May  6 07:47:23.041: INFO: stdout: ""
    May  6 07:47:23.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-385 exec execpodd5wm7 -- /bin/sh -x -c nc -v -z -w 2 10.107.180.241 80'
    May  6 07:47:23.150: INFO: stderr: "+ nc -v -z -w 2 10.107.180.241 80\nConnection to 10.107.180.241 80 port [tcp/http] succeeded!\n"
    May  6 07:47:23.150: INFO: stdout: ""
    May  6 07:47:23.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-385 exec execpodd5wm7 -- /bin/sh -x -c nc -v -z -w 2 10.0.0.240 31045'
    May  6 07:47:23.253: INFO: stderr: "+ nc -v -z -w 2 10.0.0.240 31045\nConnection to 10.0.0.240 31045 port [tcp/*] succeeded!\n"
    May  6 07:47:23.253: INFO: stdout: ""
    May  6 07:47:23.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-385 exec execpodd5wm7 -- /bin/sh -x -c nc -v -z -w 2 10.0.0.134 31045'
    May  6 07:47:23.354: INFO: stderr: "+ nc -v -z -w 2 10.0.0.134 31045\nConnection to 10.0.0.134 31045 port [tcp/*] succeeded!\n"
    May  6 07:47:23.354: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  6 07:47:23.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-385" for this suite. 05/06/23 07:47:23.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:47:23.366
May  6 07:47:23.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename proxy 05/06/23 07:47:23.367
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:47:23.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:47:23.382
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
May  6 07:47:23.384: INFO: Creating pod...
May  6 07:47:23.394: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6650" to be "running"
May  6 07:47:23.396: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.465745ms
May  6 07:47:25.401: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.006733287s
May  6 07:47:25.401: INFO: Pod "agnhost" satisfied condition "running"
May  6 07:47:25.401: INFO: Creating service...
May  6 07:47:25.414: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/pods/agnhost/proxy?method=DELETE
May  6 07:47:25.420: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  6 07:47:25.420: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/pods/agnhost/proxy?method=OPTIONS
May  6 07:47:25.424: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  6 07:47:25.424: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/pods/agnhost/proxy?method=PATCH
May  6 07:47:25.428: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  6 07:47:25.428: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/pods/agnhost/proxy?method=POST
May  6 07:47:25.431: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  6 07:47:25.431: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/pods/agnhost/proxy?method=PUT
May  6 07:47:25.434: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May  6 07:47:25.434: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/services/e2e-proxy-test-service/proxy?method=DELETE
May  6 07:47:25.439: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  6 07:47:25.439: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/services/e2e-proxy-test-service/proxy?method=OPTIONS
May  6 07:47:25.443: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  6 07:47:25.443: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/services/e2e-proxy-test-service/proxy?method=PATCH
May  6 07:47:25.449: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  6 07:47:25.449: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/services/e2e-proxy-test-service/proxy?method=POST
May  6 07:47:25.453: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  6 07:47:25.453: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/services/e2e-proxy-test-service/proxy?method=PUT
May  6 07:47:25.459: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May  6 07:47:25.459: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/pods/agnhost/proxy?method=GET
May  6 07:47:25.461: INFO: http.Client request:GET StatusCode:301
May  6 07:47:25.461: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/services/e2e-proxy-test-service/proxy?method=GET
May  6 07:47:25.466: INFO: http.Client request:GET StatusCode:301
May  6 07:47:25.466: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/pods/agnhost/proxy?method=HEAD
May  6 07:47:25.469: INFO: http.Client request:HEAD StatusCode:301
May  6 07:47:25.469: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/services/e2e-proxy-test-service/proxy?method=HEAD
May  6 07:47:25.473: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
May  6 07:47:25.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6650" for this suite. 05/06/23 07:47:25.477
------------------------------
â€¢ [2.117 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:47:23.366
    May  6 07:47:23.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename proxy 05/06/23 07:47:23.367
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:47:23.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:47:23.382
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    May  6 07:47:23.384: INFO: Creating pod...
    May  6 07:47:23.394: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6650" to be "running"
    May  6 07:47:23.396: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.465745ms
    May  6 07:47:25.401: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.006733287s
    May  6 07:47:25.401: INFO: Pod "agnhost" satisfied condition "running"
    May  6 07:47:25.401: INFO: Creating service...
    May  6 07:47:25.414: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/pods/agnhost/proxy?method=DELETE
    May  6 07:47:25.420: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  6 07:47:25.420: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/pods/agnhost/proxy?method=OPTIONS
    May  6 07:47:25.424: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  6 07:47:25.424: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/pods/agnhost/proxy?method=PATCH
    May  6 07:47:25.428: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  6 07:47:25.428: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/pods/agnhost/proxy?method=POST
    May  6 07:47:25.431: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  6 07:47:25.431: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/pods/agnhost/proxy?method=PUT
    May  6 07:47:25.434: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May  6 07:47:25.434: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/services/e2e-proxy-test-service/proxy?method=DELETE
    May  6 07:47:25.439: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  6 07:47:25.439: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/services/e2e-proxy-test-service/proxy?method=OPTIONS
    May  6 07:47:25.443: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  6 07:47:25.443: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/services/e2e-proxy-test-service/proxy?method=PATCH
    May  6 07:47:25.449: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  6 07:47:25.449: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/services/e2e-proxy-test-service/proxy?method=POST
    May  6 07:47:25.453: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  6 07:47:25.453: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/services/e2e-proxy-test-service/proxy?method=PUT
    May  6 07:47:25.459: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May  6 07:47:25.459: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/pods/agnhost/proxy?method=GET
    May  6 07:47:25.461: INFO: http.Client request:GET StatusCode:301
    May  6 07:47:25.461: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/services/e2e-proxy-test-service/proxy?method=GET
    May  6 07:47:25.466: INFO: http.Client request:GET StatusCode:301
    May  6 07:47:25.466: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/pods/agnhost/proxy?method=HEAD
    May  6 07:47:25.469: INFO: http.Client request:HEAD StatusCode:301
    May  6 07:47:25.469: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6650/services/e2e-proxy-test-service/proxy?method=HEAD
    May  6 07:47:25.473: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    May  6 07:47:25.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6650" for this suite. 05/06/23 07:47:25.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:47:25.484
May  6 07:47:25.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename var-expansion 05/06/23 07:47:25.484
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:47:25.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:47:25.502
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 05/06/23 07:47:25.504
May  6 07:47:25.510: INFO: Waiting up to 5m0s for pod "var-expansion-ce1385ab-27bb-4c9c-885f-e07c9f8938d5" in namespace "var-expansion-8623" to be "Succeeded or Failed"
May  6 07:47:25.515: INFO: Pod "var-expansion-ce1385ab-27bb-4c9c-885f-e07c9f8938d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.426031ms
May  6 07:47:27.518: INFO: Pod "var-expansion-ce1385ab-27bb-4c9c-885f-e07c9f8938d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007787242s
May  6 07:47:29.517: INFO: Pod "var-expansion-ce1385ab-27bb-4c9c-885f-e07c9f8938d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006860353s
STEP: Saw pod success 05/06/23 07:47:29.517
May  6 07:47:29.517: INFO: Pod "var-expansion-ce1385ab-27bb-4c9c-885f-e07c9f8938d5" satisfied condition "Succeeded or Failed"
May  6 07:47:29.519: INFO: Trying to get logs from node cncf-2 pod var-expansion-ce1385ab-27bb-4c9c-885f-e07c9f8938d5 container dapi-container: <nil>
STEP: delete the pod 05/06/23 07:47:29.529
May  6 07:47:29.542: INFO: Waiting for pod var-expansion-ce1385ab-27bb-4c9c-885f-e07c9f8938d5 to disappear
May  6 07:47:29.545: INFO: Pod var-expansion-ce1385ab-27bb-4c9c-885f-e07c9f8938d5 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May  6 07:47:29.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8623" for this suite. 05/06/23 07:47:29.548
------------------------------
â€¢ [4.070 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:47:25.484
    May  6 07:47:25.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename var-expansion 05/06/23 07:47:25.484
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:47:25.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:47:25.502
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 05/06/23 07:47:25.504
    May  6 07:47:25.510: INFO: Waiting up to 5m0s for pod "var-expansion-ce1385ab-27bb-4c9c-885f-e07c9f8938d5" in namespace "var-expansion-8623" to be "Succeeded or Failed"
    May  6 07:47:25.515: INFO: Pod "var-expansion-ce1385ab-27bb-4c9c-885f-e07c9f8938d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.426031ms
    May  6 07:47:27.518: INFO: Pod "var-expansion-ce1385ab-27bb-4c9c-885f-e07c9f8938d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007787242s
    May  6 07:47:29.517: INFO: Pod "var-expansion-ce1385ab-27bb-4c9c-885f-e07c9f8938d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006860353s
    STEP: Saw pod success 05/06/23 07:47:29.517
    May  6 07:47:29.517: INFO: Pod "var-expansion-ce1385ab-27bb-4c9c-885f-e07c9f8938d5" satisfied condition "Succeeded or Failed"
    May  6 07:47:29.519: INFO: Trying to get logs from node cncf-2 pod var-expansion-ce1385ab-27bb-4c9c-885f-e07c9f8938d5 container dapi-container: <nil>
    STEP: delete the pod 05/06/23 07:47:29.529
    May  6 07:47:29.542: INFO: Waiting for pod var-expansion-ce1385ab-27bb-4c9c-885f-e07c9f8938d5 to disappear
    May  6 07:47:29.545: INFO: Pod var-expansion-ce1385ab-27bb-4c9c-885f-e07c9f8938d5 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May  6 07:47:29.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8623" for this suite. 05/06/23 07:47:29.548
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:47:29.554
May  6 07:47:29.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename disruption 05/06/23 07:47:29.555
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:47:29.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:47:29.576
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 05/06/23 07:47:29.578
STEP: Waiting for the pdb to be processed 05/06/23 07:47:29.583
STEP: updating the pdb 05/06/23 07:47:31.589
STEP: Waiting for the pdb to be processed 05/06/23 07:47:31.596
STEP: patching the pdb 05/06/23 07:47:33.602
STEP: Waiting for the pdb to be processed 05/06/23 07:47:33.609
STEP: Waiting for the pdb to be deleted 05/06/23 07:47:35.621
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May  6 07:47:35.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5642" for this suite. 05/06/23 07:47:35.627
------------------------------
â€¢ [SLOW TEST] [6.078 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:47:29.554
    May  6 07:47:29.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename disruption 05/06/23 07:47:29.555
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:47:29.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:47:29.576
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 05/06/23 07:47:29.578
    STEP: Waiting for the pdb to be processed 05/06/23 07:47:29.583
    STEP: updating the pdb 05/06/23 07:47:31.589
    STEP: Waiting for the pdb to be processed 05/06/23 07:47:31.596
    STEP: patching the pdb 05/06/23 07:47:33.602
    STEP: Waiting for the pdb to be processed 05/06/23 07:47:33.609
    STEP: Waiting for the pdb to be deleted 05/06/23 07:47:35.621
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May  6 07:47:35.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5642" for this suite. 05/06/23 07:47:35.627
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:47:35.633
May  6 07:47:35.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubectl 05/06/23 07:47:35.633
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:47:35.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:47:35.65
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 05/06/23 07:47:35.652
May  6 07:47:35.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-6077 create -f -'
May  6 07:47:36.399: INFO: stderr: ""
May  6 07:47:36.399: INFO: stdout: "pod/pause created\n"
May  6 07:47:36.399: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May  6 07:47:36.399: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6077" to be "running and ready"
May  6 07:47:36.401: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.581986ms
May  6 07:47:36.401: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'cncf-0' to be 'Running' but was 'Pending'
May  6 07:47:38.404: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.005862303s
May  6 07:47:38.404: INFO: Pod "pause" satisfied condition "running and ready"
May  6 07:47:38.404: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 05/06/23 07:47:38.404
May  6 07:47:38.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-6077 label pods pause testing-label=testing-label-value'
May  6 07:47:38.465: INFO: stderr: ""
May  6 07:47:38.465: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 05/06/23 07:47:38.465
May  6 07:47:38.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-6077 get pod pause -L testing-label'
May  6 07:47:38.515: INFO: stderr: ""
May  6 07:47:38.515: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 05/06/23 07:47:38.515
May  6 07:47:38.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-6077 label pods pause testing-label-'
May  6 07:47:38.576: INFO: stderr: ""
May  6 07:47:38.576: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 05/06/23 07:47:38.576
May  6 07:47:38.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-6077 get pod pause -L testing-label'
May  6 07:47:38.627: INFO: stderr: ""
May  6 07:47:38.627: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 05/06/23 07:47:38.627
May  6 07:47:38.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-6077 delete --grace-period=0 --force -f -'
May  6 07:47:38.685: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 07:47:38.685: INFO: stdout: "pod \"pause\" force deleted\n"
May  6 07:47:38.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-6077 get rc,svc -l name=pause --no-headers'
May  6 07:47:38.742: INFO: stderr: "No resources found in kubectl-6077 namespace.\n"
May  6 07:47:38.742: INFO: stdout: ""
May  6 07:47:38.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-6077 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  6 07:47:38.792: INFO: stderr: ""
May  6 07:47:38.792: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  6 07:47:38.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6077" for this suite. 05/06/23 07:47:38.796
------------------------------
â€¢ [3.171 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:47:35.633
    May  6 07:47:35.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubectl 05/06/23 07:47:35.633
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:47:35.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:47:35.65
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 05/06/23 07:47:35.652
    May  6 07:47:35.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-6077 create -f -'
    May  6 07:47:36.399: INFO: stderr: ""
    May  6 07:47:36.399: INFO: stdout: "pod/pause created\n"
    May  6 07:47:36.399: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    May  6 07:47:36.399: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6077" to be "running and ready"
    May  6 07:47:36.401: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.581986ms
    May  6 07:47:36.401: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'cncf-0' to be 'Running' but was 'Pending'
    May  6 07:47:38.404: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.005862303s
    May  6 07:47:38.404: INFO: Pod "pause" satisfied condition "running and ready"
    May  6 07:47:38.404: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 05/06/23 07:47:38.404
    May  6 07:47:38.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-6077 label pods pause testing-label=testing-label-value'
    May  6 07:47:38.465: INFO: stderr: ""
    May  6 07:47:38.465: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 05/06/23 07:47:38.465
    May  6 07:47:38.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-6077 get pod pause -L testing-label'
    May  6 07:47:38.515: INFO: stderr: ""
    May  6 07:47:38.515: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 05/06/23 07:47:38.515
    May  6 07:47:38.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-6077 label pods pause testing-label-'
    May  6 07:47:38.576: INFO: stderr: ""
    May  6 07:47:38.576: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 05/06/23 07:47:38.576
    May  6 07:47:38.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-6077 get pod pause -L testing-label'
    May  6 07:47:38.627: INFO: stderr: ""
    May  6 07:47:38.627: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 05/06/23 07:47:38.627
    May  6 07:47:38.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-6077 delete --grace-period=0 --force -f -'
    May  6 07:47:38.685: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  6 07:47:38.685: INFO: stdout: "pod \"pause\" force deleted\n"
    May  6 07:47:38.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-6077 get rc,svc -l name=pause --no-headers'
    May  6 07:47:38.742: INFO: stderr: "No resources found in kubectl-6077 namespace.\n"
    May  6 07:47:38.742: INFO: stdout: ""
    May  6 07:47:38.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-6077 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May  6 07:47:38.792: INFO: stderr: ""
    May  6 07:47:38.792: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  6 07:47:38.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6077" for this suite. 05/06/23 07:47:38.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:47:38.804
May  6 07:47:38.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename crd-publish-openapi 05/06/23 07:47:38.804
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:47:38.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:47:38.82
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 05/06/23 07:47:38.822
May  6 07:47:38.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 05/06/23 07:47:45.566
May  6 07:47:45.567: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:47:47.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:47:54.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2160" for this suite. 05/06/23 07:47:54.426
------------------------------
â€¢ [SLOW TEST] [15.628 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:47:38.804
    May  6 07:47:38.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename crd-publish-openapi 05/06/23 07:47:38.804
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:47:38.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:47:38.82
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 05/06/23 07:47:38.822
    May  6 07:47:38.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 05/06/23 07:47:45.566
    May  6 07:47:45.567: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:47:47.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:47:54.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2160" for this suite. 05/06/23 07:47:54.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:47:54.432
May  6 07:47:54.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename endpointslice 05/06/23 07:47:54.433
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:47:55.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:47:55.451
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 05/06/23 07:47:55.453
STEP: getting /apis/discovery.k8s.io 05/06/23 07:47:55.455
STEP: getting /apis/discovery.k8s.iov1 05/06/23 07:47:55.456
STEP: creating 05/06/23 07:47:55.457
STEP: getting 05/06/23 07:47:55.472
STEP: listing 05/06/23 07:47:55.475
STEP: watching 05/06/23 07:47:55.477
May  6 07:47:55.477: INFO: starting watch
STEP: cluster-wide listing 05/06/23 07:47:55.478
STEP: cluster-wide watching 05/06/23 07:47:55.481
May  6 07:47:55.482: INFO: starting watch
STEP: patching 05/06/23 07:47:55.482
STEP: updating 05/06/23 07:47:55.487
May  6 07:47:55.493: INFO: waiting for watch events with expected annotations
May  6 07:47:55.493: INFO: saw patched and updated annotations
STEP: deleting 05/06/23 07:47:55.494
STEP: deleting a collection 05/06/23 07:47:55.503
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
May  6 07:47:55.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-6126" for this suite. 05/06/23 07:47:55.519
------------------------------
â€¢ [1.094 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:47:54.432
    May  6 07:47:54.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename endpointslice 05/06/23 07:47:54.433
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:47:55.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:47:55.451
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 05/06/23 07:47:55.453
    STEP: getting /apis/discovery.k8s.io 05/06/23 07:47:55.455
    STEP: getting /apis/discovery.k8s.iov1 05/06/23 07:47:55.456
    STEP: creating 05/06/23 07:47:55.457
    STEP: getting 05/06/23 07:47:55.472
    STEP: listing 05/06/23 07:47:55.475
    STEP: watching 05/06/23 07:47:55.477
    May  6 07:47:55.477: INFO: starting watch
    STEP: cluster-wide listing 05/06/23 07:47:55.478
    STEP: cluster-wide watching 05/06/23 07:47:55.481
    May  6 07:47:55.482: INFO: starting watch
    STEP: patching 05/06/23 07:47:55.482
    STEP: updating 05/06/23 07:47:55.487
    May  6 07:47:55.493: INFO: waiting for watch events with expected annotations
    May  6 07:47:55.493: INFO: saw patched and updated annotations
    STEP: deleting 05/06/23 07:47:55.494
    STEP: deleting a collection 05/06/23 07:47:55.503
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    May  6 07:47:55.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-6126" for this suite. 05/06/23 07:47:55.519
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:47:55.526
May  6 07:47:55.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename services 05/06/23 07:47:55.527
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:47:56.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:47:56.547
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5013 05/06/23 07:47:56.549
STEP: changing the ExternalName service to type=ClusterIP 05/06/23 07:47:56.553
STEP: creating replication controller externalname-service in namespace services-5013 05/06/23 07:47:56.569
I0506 07:47:56.575637      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5013, replica count: 2
I0506 07:47:59.627360      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 07:47:59.627: INFO: Creating new exec pod
May  6 07:47:59.637: INFO: Waiting up to 5m0s for pod "execpodkq78z" in namespace "services-5013" to be "running"
May  6 07:47:59.639: INFO: Pod "execpodkq78z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.158323ms
May  6 07:48:01.643: INFO: Pod "execpodkq78z": Phase="Running", Reason="", readiness=true. Elapsed: 2.006341406s
May  6 07:48:01.643: INFO: Pod "execpodkq78z" satisfied condition "running"
May  6 07:48:02.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-5013 exec execpodkq78z -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
May  6 07:48:02.751: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  6 07:48:02.751: INFO: stdout: ""
May  6 07:48:02.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-5013 exec execpodkq78z -- /bin/sh -x -c nc -v -z -w 2 10.109.192.32 80'
May  6 07:48:02.849: INFO: stderr: "+ nc -v -z -w 2 10.109.192.32 80\nConnection to 10.109.192.32 80 port [tcp/http] succeeded!\n"
May  6 07:48:02.849: INFO: stdout: ""
May  6 07:48:02.849: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  6 07:48:02.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5013" for this suite. 05/06/23 07:48:02.879
------------------------------
â€¢ [SLOW TEST] [7.361 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:47:55.526
    May  6 07:47:55.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename services 05/06/23 07:47:55.527
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:47:56.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:47:56.547
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-5013 05/06/23 07:47:56.549
    STEP: changing the ExternalName service to type=ClusterIP 05/06/23 07:47:56.553
    STEP: creating replication controller externalname-service in namespace services-5013 05/06/23 07:47:56.569
    I0506 07:47:56.575637      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5013, replica count: 2
    I0506 07:47:59.627360      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  6 07:47:59.627: INFO: Creating new exec pod
    May  6 07:47:59.637: INFO: Waiting up to 5m0s for pod "execpodkq78z" in namespace "services-5013" to be "running"
    May  6 07:47:59.639: INFO: Pod "execpodkq78z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.158323ms
    May  6 07:48:01.643: INFO: Pod "execpodkq78z": Phase="Running", Reason="", readiness=true. Elapsed: 2.006341406s
    May  6 07:48:01.643: INFO: Pod "execpodkq78z" satisfied condition "running"
    May  6 07:48:02.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-5013 exec execpodkq78z -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    May  6 07:48:02.751: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    May  6 07:48:02.751: INFO: stdout: ""
    May  6 07:48:02.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-5013 exec execpodkq78z -- /bin/sh -x -c nc -v -z -w 2 10.109.192.32 80'
    May  6 07:48:02.849: INFO: stderr: "+ nc -v -z -w 2 10.109.192.32 80\nConnection to 10.109.192.32 80 port [tcp/http] succeeded!\n"
    May  6 07:48:02.849: INFO: stdout: ""
    May  6 07:48:02.849: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  6 07:48:02.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5013" for this suite. 05/06/23 07:48:02.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:48:02.887
May  6 07:48:02.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename var-expansion 05/06/23 07:48:02.888
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:48:03.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:48:03.907
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 05/06/23 07:48:03.909
STEP: waiting for pod running 05/06/23 07:48:03.928
May  6 07:48:03.928: INFO: Waiting up to 2m0s for pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736" in namespace "var-expansion-4488" to be "running"
May  6 07:48:03.930: INFO: Pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736": Phase="Pending", Reason="", readiness=false. Elapsed: 2.32026ms
May  6 07:48:05.933: INFO: Pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736": Phase="Running", Reason="", readiness=true. Elapsed: 2.005326165s
May  6 07:48:05.933: INFO: Pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736" satisfied condition "running"
STEP: creating a file in subpath 05/06/23 07:48:05.933
May  6 07:48:05.936: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4488 PodName:var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 07:48:05.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:48:05.937: INFO: ExecWithOptions: Clientset creation
May  6 07:48:05.937: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-4488/pods/var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 05/06/23 07:48:05.995
May  6 07:48:05.998: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4488 PodName:var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 07:48:05.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 07:48:05.999: INFO: ExecWithOptions: Clientset creation
May  6 07:48:05.999: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-4488/pods/var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 05/06/23 07:48:06.056
May  6 07:48:06.567: INFO: Successfully updated pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736"
STEP: waiting for annotated pod running 05/06/23 07:48:06.568
May  6 07:48:06.568: INFO: Waiting up to 2m0s for pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736" in namespace "var-expansion-4488" to be "running"
May  6 07:48:06.570: INFO: Pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736": Phase="Running", Reason="", readiness=true. Elapsed: 2.234236ms
May  6 07:48:06.570: INFO: Pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736" satisfied condition "running"
STEP: deleting the pod gracefully 05/06/23 07:48:06.57
May  6 07:48:06.570: INFO: Deleting pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736" in namespace "var-expansion-4488"
May  6 07:48:06.576: INFO: Wait up to 5m0s for pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May  6 07:48:40.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4488" for this suite. 05/06/23 07:48:40.585
------------------------------
â€¢ [SLOW TEST] [37.704 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:48:02.887
    May  6 07:48:02.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename var-expansion 05/06/23 07:48:02.888
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:48:03.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:48:03.907
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 05/06/23 07:48:03.909
    STEP: waiting for pod running 05/06/23 07:48:03.928
    May  6 07:48:03.928: INFO: Waiting up to 2m0s for pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736" in namespace "var-expansion-4488" to be "running"
    May  6 07:48:03.930: INFO: Pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736": Phase="Pending", Reason="", readiness=false. Elapsed: 2.32026ms
    May  6 07:48:05.933: INFO: Pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736": Phase="Running", Reason="", readiness=true. Elapsed: 2.005326165s
    May  6 07:48:05.933: INFO: Pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736" satisfied condition "running"
    STEP: creating a file in subpath 05/06/23 07:48:05.933
    May  6 07:48:05.936: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4488 PodName:var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 07:48:05.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:48:05.937: INFO: ExecWithOptions: Clientset creation
    May  6 07:48:05.937: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-4488/pods/var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 05/06/23 07:48:05.995
    May  6 07:48:05.998: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4488 PodName:var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 07:48:05.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 07:48:05.999: INFO: ExecWithOptions: Clientset creation
    May  6 07:48:05.999: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-4488/pods/var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 05/06/23 07:48:06.056
    May  6 07:48:06.567: INFO: Successfully updated pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736"
    STEP: waiting for annotated pod running 05/06/23 07:48:06.568
    May  6 07:48:06.568: INFO: Waiting up to 2m0s for pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736" in namespace "var-expansion-4488" to be "running"
    May  6 07:48:06.570: INFO: Pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736": Phase="Running", Reason="", readiness=true. Elapsed: 2.234236ms
    May  6 07:48:06.570: INFO: Pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736" satisfied condition "running"
    STEP: deleting the pod gracefully 05/06/23 07:48:06.57
    May  6 07:48:06.570: INFO: Deleting pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736" in namespace "var-expansion-4488"
    May  6 07:48:06.576: INFO: Wait up to 5m0s for pod "var-expansion-223c26d2-04ed-45fc-a7ff-5614f382e736" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May  6 07:48:40.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4488" for this suite. 05/06/23 07:48:40.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:48:40.592
May  6 07:48:40.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename replication-controller 05/06/23 07:48:40.593
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:48:41.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:48:41.614
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 05/06/23 07:48:41.617
May  6 07:48:41.623: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7206" to be "running and ready"
May  6 07:48:41.625: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340016ms
May  6 07:48:41.625: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
May  6 07:48:43.629: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.005628181s
May  6 07:48:43.629: INFO: The phase of Pod pod-adoption is Running (Ready = true)
May  6 07:48:43.629: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 05/06/23 07:48:43.632
STEP: Then the orphan pod is adopted 05/06/23 07:48:43.636
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May  6 07:48:44.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7206" for this suite. 05/06/23 07:48:44.647
------------------------------
â€¢ [4.061 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:48:40.592
    May  6 07:48:40.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename replication-controller 05/06/23 07:48:40.593
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:48:41.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:48:41.614
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 05/06/23 07:48:41.617
    May  6 07:48:41.623: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7206" to be "running and ready"
    May  6 07:48:41.625: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340016ms
    May  6 07:48:41.625: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:48:43.629: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.005628181s
    May  6 07:48:43.629: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    May  6 07:48:43.629: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 05/06/23 07:48:43.632
    STEP: Then the orphan pod is adopted 05/06/23 07:48:43.636
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May  6 07:48:44.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7206" for this suite. 05/06/23 07:48:44.647
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:48:44.653
May  6 07:48:44.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename var-expansion 05/06/23 07:48:44.654
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:48:45.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:48:45.671
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
May  6 07:48:45.681: INFO: Waiting up to 2m0s for pod "var-expansion-1ab1a6fe-5563-49a1-a2e7-74d6a7de5fb2" in namespace "var-expansion-3838" to be "container 0 failed with reason CreateContainerConfigError"
May  6 07:48:45.683: INFO: Pod "var-expansion-1ab1a6fe-5563-49a1-a2e7-74d6a7de5fb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.374954ms
May  6 07:48:47.687: INFO: Pod "var-expansion-1ab1a6fe-5563-49a1-a2e7-74d6a7de5fb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005978987s
May  6 07:48:47.687: INFO: Pod "var-expansion-1ab1a6fe-5563-49a1-a2e7-74d6a7de5fb2" satisfied condition "container 0 failed with reason CreateContainerConfigError"
May  6 07:48:47.687: INFO: Deleting pod "var-expansion-1ab1a6fe-5563-49a1-a2e7-74d6a7de5fb2" in namespace "var-expansion-3838"
May  6 07:48:47.696: INFO: Wait up to 5m0s for pod "var-expansion-1ab1a6fe-5563-49a1-a2e7-74d6a7de5fb2" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May  6 07:48:51.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3838" for this suite. 05/06/23 07:48:51.705
------------------------------
â€¢ [SLOW TEST] [7.059 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:48:44.653
    May  6 07:48:44.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename var-expansion 05/06/23 07:48:44.654
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:48:45.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:48:45.671
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    May  6 07:48:45.681: INFO: Waiting up to 2m0s for pod "var-expansion-1ab1a6fe-5563-49a1-a2e7-74d6a7de5fb2" in namespace "var-expansion-3838" to be "container 0 failed with reason CreateContainerConfigError"
    May  6 07:48:45.683: INFO: Pod "var-expansion-1ab1a6fe-5563-49a1-a2e7-74d6a7de5fb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.374954ms
    May  6 07:48:47.687: INFO: Pod "var-expansion-1ab1a6fe-5563-49a1-a2e7-74d6a7de5fb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005978987s
    May  6 07:48:47.687: INFO: Pod "var-expansion-1ab1a6fe-5563-49a1-a2e7-74d6a7de5fb2" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    May  6 07:48:47.687: INFO: Deleting pod "var-expansion-1ab1a6fe-5563-49a1-a2e7-74d6a7de5fb2" in namespace "var-expansion-3838"
    May  6 07:48:47.696: INFO: Wait up to 5m0s for pod "var-expansion-1ab1a6fe-5563-49a1-a2e7-74d6a7de5fb2" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May  6 07:48:51.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3838" for this suite. 05/06/23 07:48:51.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:48:51.713
May  6 07:48:51.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename services 05/06/23 07:48:51.714
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:48:52.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:48:52.735
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-7525 05/06/23 07:48:52.737
STEP: creating service affinity-nodeport in namespace services-7525 05/06/23 07:48:52.737
STEP: creating replication controller affinity-nodeport in namespace services-7525 05/06/23 07:48:52.753
I0506 07:48:52.760898      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7525, replica count: 3
I0506 07:48:55.811986      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 07:48:55.820: INFO: Creating new exec pod
May  6 07:48:55.828: INFO: Waiting up to 5m0s for pod "execpod-affinity5xhpr" in namespace "services-7525" to be "running"
May  6 07:48:55.830: INFO: Pod "execpod-affinity5xhpr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.4311ms
May  6 07:48:57.833: INFO: Pod "execpod-affinity5xhpr": Phase="Running", Reason="", readiness=true. Elapsed: 2.005301312s
May  6 07:48:57.833: INFO: Pod "execpod-affinity5xhpr" satisfied condition "running"
May  6 07:48:58.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7525 exec execpod-affinity5xhpr -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
May  6 07:48:58.943: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May  6 07:48:58.943: INFO: stdout: ""
May  6 07:48:58.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7525 exec execpod-affinity5xhpr -- /bin/sh -x -c nc -v -z -w 2 10.104.189.114 80'
May  6 07:48:59.046: INFO: stderr: "+ nc -v -z -w 2 10.104.189.114 80\nConnection to 10.104.189.114 80 port [tcp/http] succeeded!\n"
May  6 07:48:59.046: INFO: stdout: ""
May  6 07:48:59.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7525 exec execpod-affinity5xhpr -- /bin/sh -x -c nc -v -z -w 2 10.0.0.180 32013'
May  6 07:48:59.149: INFO: stderr: "+ nc -v -z -w 2 10.0.0.180 32013\nConnection to 10.0.0.180 32013 port [tcp/*] succeeded!\n"
May  6 07:48:59.149: INFO: stdout: ""
May  6 07:48:59.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7525 exec execpod-affinity5xhpr -- /bin/sh -x -c nc -v -z -w 2 10.0.0.107 32013'
May  6 07:48:59.249: INFO: stderr: "+ nc -v -z -w 2 10.0.0.107 32013\nConnection to 10.0.0.107 32013 port [tcp/*] succeeded!\n"
May  6 07:48:59.249: INFO: stdout: ""
May  6 07:48:59.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7525 exec execpod-affinity5xhpr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.134:32013/ ; done'
May  6 07:48:59.389: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n"
May  6 07:48:59.389: INFO: stdout: "\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4"
May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
May  6 07:48:59.389: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7525, will wait for the garbage collector to delete the pods 05/06/23 07:48:59.406
May  6 07:48:59.466: INFO: Deleting ReplicationController affinity-nodeport took: 5.759656ms
May  6 07:48:59.566: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.580247ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  6 07:49:01.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7525" for this suite. 05/06/23 07:49:01.692
------------------------------
â€¢ [SLOW TEST] [9.985 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:48:51.713
    May  6 07:48:51.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename services 05/06/23 07:48:51.714
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:48:52.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:48:52.735
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-7525 05/06/23 07:48:52.737
    STEP: creating service affinity-nodeport in namespace services-7525 05/06/23 07:48:52.737
    STEP: creating replication controller affinity-nodeport in namespace services-7525 05/06/23 07:48:52.753
    I0506 07:48:52.760898      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7525, replica count: 3
    I0506 07:48:55.811986      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  6 07:48:55.820: INFO: Creating new exec pod
    May  6 07:48:55.828: INFO: Waiting up to 5m0s for pod "execpod-affinity5xhpr" in namespace "services-7525" to be "running"
    May  6 07:48:55.830: INFO: Pod "execpod-affinity5xhpr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.4311ms
    May  6 07:48:57.833: INFO: Pod "execpod-affinity5xhpr": Phase="Running", Reason="", readiness=true. Elapsed: 2.005301312s
    May  6 07:48:57.833: INFO: Pod "execpod-affinity5xhpr" satisfied condition "running"
    May  6 07:48:58.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7525 exec execpod-affinity5xhpr -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    May  6 07:48:58.943: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    May  6 07:48:58.943: INFO: stdout: ""
    May  6 07:48:58.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7525 exec execpod-affinity5xhpr -- /bin/sh -x -c nc -v -z -w 2 10.104.189.114 80'
    May  6 07:48:59.046: INFO: stderr: "+ nc -v -z -w 2 10.104.189.114 80\nConnection to 10.104.189.114 80 port [tcp/http] succeeded!\n"
    May  6 07:48:59.046: INFO: stdout: ""
    May  6 07:48:59.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7525 exec execpod-affinity5xhpr -- /bin/sh -x -c nc -v -z -w 2 10.0.0.180 32013'
    May  6 07:48:59.149: INFO: stderr: "+ nc -v -z -w 2 10.0.0.180 32013\nConnection to 10.0.0.180 32013 port [tcp/*] succeeded!\n"
    May  6 07:48:59.149: INFO: stdout: ""
    May  6 07:48:59.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7525 exec execpod-affinity5xhpr -- /bin/sh -x -c nc -v -z -w 2 10.0.0.107 32013'
    May  6 07:48:59.249: INFO: stderr: "+ nc -v -z -w 2 10.0.0.107 32013\nConnection to 10.0.0.107 32013 port [tcp/*] succeeded!\n"
    May  6 07:48:59.249: INFO: stdout: ""
    May  6 07:48:59.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7525 exec execpod-affinity5xhpr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.134:32013/ ; done'
    May  6 07:48:59.389: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:32013/\n"
    May  6 07:48:59.389: INFO: stdout: "\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4\naffinity-nodeport-xvll4"
    May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
    May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
    May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
    May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
    May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
    May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
    May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
    May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
    May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
    May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
    May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
    May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
    May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
    May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
    May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
    May  6 07:48:59.389: INFO: Received response from host: affinity-nodeport-xvll4
    May  6 07:48:59.389: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-7525, will wait for the garbage collector to delete the pods 05/06/23 07:48:59.406
    May  6 07:48:59.466: INFO: Deleting ReplicationController affinity-nodeport took: 5.759656ms
    May  6 07:48:59.566: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.580247ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  6 07:49:01.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7525" for this suite. 05/06/23 07:49:01.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:49:01.699
May  6 07:49:01.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:49:01.7
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:49:02.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:49:02.719
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 05/06/23 07:49:02.721
May  6 07:49:02.730: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7dcdb97f-c726-486d-ade4-02efb14937ad" in namespace "projected-7876" to be "Succeeded or Failed"
May  6 07:49:02.732: INFO: Pod "downwardapi-volume-7dcdb97f-c726-486d-ade4-02efb14937ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.633855ms
May  6 07:49:04.735: INFO: Pod "downwardapi-volume-7dcdb97f-c726-486d-ade4-02efb14937ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005710229s
May  6 07:49:06.735: INFO: Pod "downwardapi-volume-7dcdb97f-c726-486d-ade4-02efb14937ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005519271s
STEP: Saw pod success 05/06/23 07:49:06.735
May  6 07:49:06.735: INFO: Pod "downwardapi-volume-7dcdb97f-c726-486d-ade4-02efb14937ad" satisfied condition "Succeeded or Failed"
May  6 07:49:06.738: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-7dcdb97f-c726-486d-ade4-02efb14937ad container client-container: <nil>
STEP: delete the pod 05/06/23 07:49:06.747
May  6 07:49:06.762: INFO: Waiting for pod downwardapi-volume-7dcdb97f-c726-486d-ade4-02efb14937ad to disappear
May  6 07:49:06.765: INFO: Pod downwardapi-volume-7dcdb97f-c726-486d-ade4-02efb14937ad no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  6 07:49:06.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7876" for this suite. 05/06/23 07:49:06.769
------------------------------
â€¢ [SLOW TEST] [5.075 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:49:01.699
    May  6 07:49:01.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:49:01.7
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:49:02.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:49:02.719
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 05/06/23 07:49:02.721
    May  6 07:49:02.730: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7dcdb97f-c726-486d-ade4-02efb14937ad" in namespace "projected-7876" to be "Succeeded or Failed"
    May  6 07:49:02.732: INFO: Pod "downwardapi-volume-7dcdb97f-c726-486d-ade4-02efb14937ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.633855ms
    May  6 07:49:04.735: INFO: Pod "downwardapi-volume-7dcdb97f-c726-486d-ade4-02efb14937ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005710229s
    May  6 07:49:06.735: INFO: Pod "downwardapi-volume-7dcdb97f-c726-486d-ade4-02efb14937ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005519271s
    STEP: Saw pod success 05/06/23 07:49:06.735
    May  6 07:49:06.735: INFO: Pod "downwardapi-volume-7dcdb97f-c726-486d-ade4-02efb14937ad" satisfied condition "Succeeded or Failed"
    May  6 07:49:06.738: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-7dcdb97f-c726-486d-ade4-02efb14937ad container client-container: <nil>
    STEP: delete the pod 05/06/23 07:49:06.747
    May  6 07:49:06.762: INFO: Waiting for pod downwardapi-volume-7dcdb97f-c726-486d-ade4-02efb14937ad to disappear
    May  6 07:49:06.765: INFO: Pod downwardapi-volume-7dcdb97f-c726-486d-ade4-02efb14937ad no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  6 07:49:06.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7876" for this suite. 05/06/23 07:49:06.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:49:06.774
May  6 07:49:06.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename subpath 05/06/23 07:49:06.775
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:49:07.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:49:07.794
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/06/23 07:49:07.796
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-9cj5 05/06/23 07:49:07.804
STEP: Creating a pod to test atomic-volume-subpath 05/06/23 07:49:07.804
May  6 07:49:07.813: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-9cj5" in namespace "subpath-7169" to be "Succeeded or Failed"
May  6 07:49:07.816: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.871427ms
May  6 07:49:09.819: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 2.005627951s
May  6 07:49:11.820: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 4.006901136s
May  6 07:49:13.820: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 6.006554243s
May  6 07:49:15.819: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 8.005724543s
May  6 07:49:17.820: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 10.006772384s
May  6 07:49:19.820: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 12.00682017s
May  6 07:49:21.821: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 14.007418584s
May  6 07:49:23.821: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 16.007614093s
May  6 07:49:25.826: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 18.012414543s
May  6 07:49:27.820: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 20.006508246s
May  6 07:49:29.820: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=false. Elapsed: 22.006680281s
May  6 07:49:31.820: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006310184s
STEP: Saw pod success 05/06/23 07:49:31.82
May  6 07:49:31.820: INFO: Pod "pod-subpath-test-downwardapi-9cj5" satisfied condition "Succeeded or Failed"
May  6 07:49:31.822: INFO: Trying to get logs from node cncf-0 pod pod-subpath-test-downwardapi-9cj5 container test-container-subpath-downwardapi-9cj5: <nil>
STEP: delete the pod 05/06/23 07:49:31.829
May  6 07:49:31.840: INFO: Waiting for pod pod-subpath-test-downwardapi-9cj5 to disappear
May  6 07:49:31.843: INFO: Pod pod-subpath-test-downwardapi-9cj5 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-9cj5 05/06/23 07:49:31.843
May  6 07:49:31.843: INFO: Deleting pod "pod-subpath-test-downwardapi-9cj5" in namespace "subpath-7169"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May  6 07:49:31.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7169" for this suite. 05/06/23 07:49:31.848
------------------------------
â€¢ [SLOW TEST] [25.081 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:49:06.774
    May  6 07:49:06.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename subpath 05/06/23 07:49:06.775
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:49:07.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:49:07.794
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/06/23 07:49:07.796
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-9cj5 05/06/23 07:49:07.804
    STEP: Creating a pod to test atomic-volume-subpath 05/06/23 07:49:07.804
    May  6 07:49:07.813: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-9cj5" in namespace "subpath-7169" to be "Succeeded or Failed"
    May  6 07:49:07.816: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.871427ms
    May  6 07:49:09.819: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 2.005627951s
    May  6 07:49:11.820: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 4.006901136s
    May  6 07:49:13.820: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 6.006554243s
    May  6 07:49:15.819: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 8.005724543s
    May  6 07:49:17.820: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 10.006772384s
    May  6 07:49:19.820: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 12.00682017s
    May  6 07:49:21.821: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 14.007418584s
    May  6 07:49:23.821: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 16.007614093s
    May  6 07:49:25.826: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 18.012414543s
    May  6 07:49:27.820: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=true. Elapsed: 20.006508246s
    May  6 07:49:29.820: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Running", Reason="", readiness=false. Elapsed: 22.006680281s
    May  6 07:49:31.820: INFO: Pod "pod-subpath-test-downwardapi-9cj5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006310184s
    STEP: Saw pod success 05/06/23 07:49:31.82
    May  6 07:49:31.820: INFO: Pod "pod-subpath-test-downwardapi-9cj5" satisfied condition "Succeeded or Failed"
    May  6 07:49:31.822: INFO: Trying to get logs from node cncf-0 pod pod-subpath-test-downwardapi-9cj5 container test-container-subpath-downwardapi-9cj5: <nil>
    STEP: delete the pod 05/06/23 07:49:31.829
    May  6 07:49:31.840: INFO: Waiting for pod pod-subpath-test-downwardapi-9cj5 to disappear
    May  6 07:49:31.843: INFO: Pod pod-subpath-test-downwardapi-9cj5 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-9cj5 05/06/23 07:49:31.843
    May  6 07:49:31.843: INFO: Deleting pod "pod-subpath-test-downwardapi-9cj5" in namespace "subpath-7169"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May  6 07:49:31.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7169" for this suite. 05/06/23 07:49:31.848
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:49:31.856
May  6 07:49:31.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename secrets 05/06/23 07:49:31.857
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:49:32.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:49:32.875
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 05/06/23 07:49:32.876
STEP: listing secrets in all namespaces to ensure that there are more than zero 05/06/23 07:49:32.88
STEP: patching the secret 05/06/23 07:49:32.911
STEP: deleting the secret using a LabelSelector 05/06/23 07:49:32.916
STEP: listing secrets in all namespaces, searching for label name and value in patch 05/06/23 07:49:32.922
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
May  6 07:49:32.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8092" for this suite. 05/06/23 07:49:32.928
------------------------------
â€¢ [1.077 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:49:31.856
    May  6 07:49:31.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename secrets 05/06/23 07:49:31.857
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:49:32.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:49:32.875
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 05/06/23 07:49:32.876
    STEP: listing secrets in all namespaces to ensure that there are more than zero 05/06/23 07:49:32.88
    STEP: patching the secret 05/06/23 07:49:32.911
    STEP: deleting the secret using a LabelSelector 05/06/23 07:49:32.916
    STEP: listing secrets in all namespaces, searching for label name and value in patch 05/06/23 07:49:32.922
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    May  6 07:49:32.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8092" for this suite. 05/06/23 07:49:32.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:49:32.935
May  6 07:49:32.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:49:32.935
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:49:33.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:49:33.953
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-33e039ff-5f8a-4467-b6ae-82407b175e14 05/06/23 07:49:33.958
STEP: Creating secret with name s-test-opt-upd-8b2077b5-729c-4b0f-830d-8b8d794265bc 05/06/23 07:49:33.962
STEP: Creating the pod 05/06/23 07:49:33.966
May  6 07:49:33.974: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5ad92ac4-0054-4f96-a500-3df083e308d3" in namespace "projected-6986" to be "running and ready"
May  6 07:49:33.976: INFO: Pod "pod-projected-secrets-5ad92ac4-0054-4f96-a500-3df083e308d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031491ms
May  6 07:49:33.976: INFO: The phase of Pod pod-projected-secrets-5ad92ac4-0054-4f96-a500-3df083e308d3 is Pending, waiting for it to be Running (with Ready = true)
May  6 07:49:35.979: INFO: Pod "pod-projected-secrets-5ad92ac4-0054-4f96-a500-3df083e308d3": Phase="Running", Reason="", readiness=true. Elapsed: 2.005773449s
May  6 07:49:35.979: INFO: The phase of Pod pod-projected-secrets-5ad92ac4-0054-4f96-a500-3df083e308d3 is Running (Ready = true)
May  6 07:49:35.979: INFO: Pod "pod-projected-secrets-5ad92ac4-0054-4f96-a500-3df083e308d3" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-33e039ff-5f8a-4467-b6ae-82407b175e14 05/06/23 07:49:35.995
STEP: Updating secret s-test-opt-upd-8b2077b5-729c-4b0f-830d-8b8d794265bc 05/06/23 07:49:36.001
STEP: Creating secret with name s-test-opt-create-276b4e56-20b2-4cd3-8792-58346c5efbcf 05/06/23 07:49:36.006
STEP: waiting to observe update in volume 05/06/23 07:49:36.01
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May  6 07:49:38.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6986" for this suite. 05/06/23 07:49:38.033
------------------------------
â€¢ [SLOW TEST] [5.104 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:49:32.935
    May  6 07:49:32.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:49:32.935
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:49:33.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:49:33.953
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-33e039ff-5f8a-4467-b6ae-82407b175e14 05/06/23 07:49:33.958
    STEP: Creating secret with name s-test-opt-upd-8b2077b5-729c-4b0f-830d-8b8d794265bc 05/06/23 07:49:33.962
    STEP: Creating the pod 05/06/23 07:49:33.966
    May  6 07:49:33.974: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5ad92ac4-0054-4f96-a500-3df083e308d3" in namespace "projected-6986" to be "running and ready"
    May  6 07:49:33.976: INFO: Pod "pod-projected-secrets-5ad92ac4-0054-4f96-a500-3df083e308d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031491ms
    May  6 07:49:33.976: INFO: The phase of Pod pod-projected-secrets-5ad92ac4-0054-4f96-a500-3df083e308d3 is Pending, waiting for it to be Running (with Ready = true)
    May  6 07:49:35.979: INFO: Pod "pod-projected-secrets-5ad92ac4-0054-4f96-a500-3df083e308d3": Phase="Running", Reason="", readiness=true. Elapsed: 2.005773449s
    May  6 07:49:35.979: INFO: The phase of Pod pod-projected-secrets-5ad92ac4-0054-4f96-a500-3df083e308d3 is Running (Ready = true)
    May  6 07:49:35.979: INFO: Pod "pod-projected-secrets-5ad92ac4-0054-4f96-a500-3df083e308d3" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-33e039ff-5f8a-4467-b6ae-82407b175e14 05/06/23 07:49:35.995
    STEP: Updating secret s-test-opt-upd-8b2077b5-729c-4b0f-830d-8b8d794265bc 05/06/23 07:49:36.001
    STEP: Creating secret with name s-test-opt-create-276b4e56-20b2-4cd3-8792-58346c5efbcf 05/06/23 07:49:36.006
    STEP: waiting to observe update in volume 05/06/23 07:49:36.01
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May  6 07:49:38.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6986" for this suite. 05/06/23 07:49:38.033
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:49:38.039
May  6 07:49:38.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename webhook 05/06/23 07:49:38.039
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:49:39.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:49:39.057
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/06/23 07:49:41.073
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:49:41.187
STEP: Deploying the webhook pod 05/06/23 07:49:41.194
STEP: Wait for the deployment to be ready 05/06/23 07:49:41.204
May  6 07:49:41.210: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/06/23 07:49:43.219
STEP: Verifying the service has paired with the endpoint 05/06/23 07:49:43.234
May  6 07:49:44.234: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
May  6 07:49:44.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4710-crds.webhook.example.com via the AdmissionRegistration API 05/06/23 07:49:44.745
STEP: Creating a custom resource while v1 is storage version 05/06/23 07:49:45.778
STEP: Patching Custom Resource Definition to set v2 as storage 05/06/23 07:49:48.847
STEP: Patching the custom resource while v2 is storage version 05/06/23 07:49:48.863
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:49:49.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3792" for this suite. 05/06/23 07:49:49.436
STEP: Destroying namespace "webhook-3792-markers" for this suite. 05/06/23 07:49:49.447
------------------------------
â€¢ [SLOW TEST] [11.416 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:49:38.039
    May  6 07:49:38.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename webhook 05/06/23 07:49:38.039
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:49:39.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:49:39.057
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/06/23 07:49:41.073
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:49:41.187
    STEP: Deploying the webhook pod 05/06/23 07:49:41.194
    STEP: Wait for the deployment to be ready 05/06/23 07:49:41.204
    May  6 07:49:41.210: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/06/23 07:49:43.219
    STEP: Verifying the service has paired with the endpoint 05/06/23 07:49:43.234
    May  6 07:49:44.234: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    May  6 07:49:44.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4710-crds.webhook.example.com via the AdmissionRegistration API 05/06/23 07:49:44.745
    STEP: Creating a custom resource while v1 is storage version 05/06/23 07:49:45.778
    STEP: Patching Custom Resource Definition to set v2 as storage 05/06/23 07:49:48.847
    STEP: Patching the custom resource while v2 is storage version 05/06/23 07:49:48.863
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:49:49.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3792" for this suite. 05/06/23 07:49:49.436
    STEP: Destroying namespace "webhook-3792-markers" for this suite. 05/06/23 07:49:49.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:49:49.457
May  6 07:49:49.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename emptydir 05/06/23 07:49:49.458
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:49:50.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:49:50.48
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 05/06/23 07:49:50.482
May  6 07:49:50.491: INFO: Waiting up to 5m0s for pod "pod-d7ad8f60-e12b-4b74-9e80-9ba396483276" in namespace "emptydir-4758" to be "Succeeded or Failed"
May  6 07:49:50.493: INFO: Pod "pod-d7ad8f60-e12b-4b74-9e80-9ba396483276": Phase="Pending", Reason="", readiness=false. Elapsed: 2.130318ms
May  6 07:49:52.497: INFO: Pod "pod-d7ad8f60-e12b-4b74-9e80-9ba396483276": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005477287s
May  6 07:49:54.497: INFO: Pod "pod-d7ad8f60-e12b-4b74-9e80-9ba396483276": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006259543s
STEP: Saw pod success 05/06/23 07:49:54.497
May  6 07:49:54.498: INFO: Pod "pod-d7ad8f60-e12b-4b74-9e80-9ba396483276" satisfied condition "Succeeded or Failed"
May  6 07:49:54.500: INFO: Trying to get logs from node cncf-0 pod pod-d7ad8f60-e12b-4b74-9e80-9ba396483276 container test-container: <nil>
STEP: delete the pod 05/06/23 07:49:54.505
May  6 07:49:54.518: INFO: Waiting for pod pod-d7ad8f60-e12b-4b74-9e80-9ba396483276 to disappear
May  6 07:49:54.520: INFO: Pod pod-d7ad8f60-e12b-4b74-9e80-9ba396483276 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  6 07:49:54.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4758" for this suite. 05/06/23 07:49:54.524
------------------------------
â€¢ [SLOW TEST] [5.074 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:49:49.457
    May  6 07:49:49.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename emptydir 05/06/23 07:49:49.458
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:49:50.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:49:50.48
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 05/06/23 07:49:50.482
    May  6 07:49:50.491: INFO: Waiting up to 5m0s for pod "pod-d7ad8f60-e12b-4b74-9e80-9ba396483276" in namespace "emptydir-4758" to be "Succeeded or Failed"
    May  6 07:49:50.493: INFO: Pod "pod-d7ad8f60-e12b-4b74-9e80-9ba396483276": Phase="Pending", Reason="", readiness=false. Elapsed: 2.130318ms
    May  6 07:49:52.497: INFO: Pod "pod-d7ad8f60-e12b-4b74-9e80-9ba396483276": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005477287s
    May  6 07:49:54.497: INFO: Pod "pod-d7ad8f60-e12b-4b74-9e80-9ba396483276": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006259543s
    STEP: Saw pod success 05/06/23 07:49:54.497
    May  6 07:49:54.498: INFO: Pod "pod-d7ad8f60-e12b-4b74-9e80-9ba396483276" satisfied condition "Succeeded or Failed"
    May  6 07:49:54.500: INFO: Trying to get logs from node cncf-0 pod pod-d7ad8f60-e12b-4b74-9e80-9ba396483276 container test-container: <nil>
    STEP: delete the pod 05/06/23 07:49:54.505
    May  6 07:49:54.518: INFO: Waiting for pod pod-d7ad8f60-e12b-4b74-9e80-9ba396483276 to disappear
    May  6 07:49:54.520: INFO: Pod pod-d7ad8f60-e12b-4b74-9e80-9ba396483276 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  6 07:49:54.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4758" for this suite. 05/06/23 07:49:54.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:49:54.531
May  6 07:49:54.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename cronjob 05/06/23 07:49:54.532
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:49:55.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:49:55.554
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 05/06/23 07:49:55.556
STEP: Ensuring no jobs are scheduled 05/06/23 07:49:55.561
STEP: Ensuring no job exists by listing jobs explicitly 05/06/23 07:54:55.567
STEP: Removing cronjob 05/06/23 07:54:55.569
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May  6 07:54:55.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2407" for this suite. 05/06/23 07:54:55.578
------------------------------
â€¢ [SLOW TEST] [301.051 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:49:54.531
    May  6 07:49:54.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename cronjob 05/06/23 07:49:54.532
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:49:55.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:49:55.554
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 05/06/23 07:49:55.556
    STEP: Ensuring no jobs are scheduled 05/06/23 07:49:55.561
    STEP: Ensuring no job exists by listing jobs explicitly 05/06/23 07:54:55.567
    STEP: Removing cronjob 05/06/23 07:54:55.569
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May  6 07:54:55.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2407" for this suite. 05/06/23 07:54:55.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:54:55.584
May  6 07:54:55.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename sched-pred 05/06/23 07:54:55.584
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:54:56.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:54:56.601
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
May  6 07:54:56.603: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  6 07:54:56.609: INFO: Waiting for terminating namespaces to be deleted...
May  6 07:54:56.611: INFO: 
Logging pods the apiserver thinks is on node cncf-0 before test
May  6 07:54:56.617: INFO: calico-node-vvl9m from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.617: INFO: 	Container calico-node ready: true, restart count 0
May  6 07:54:56.617: INFO: kube-apiserver-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.617: INFO: 	Container kube-apiserver ready: true, restart count 1
May  6 07:54:56.617: INFO: kube-controller-manager-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.617: INFO: 	Container kube-controller-manager ready: true, restart count 0
May  6 07:54:56.617: INFO: kube-proxy-cq72q from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.617: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 07:54:56.617: INFO: kube-scheduler-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.617: INFO: 	Container kube-scheduler ready: true, restart count 1
May  6 07:54:56.617: INFO: kubelet-rubber-stamp-c6b74568-5mtx7 from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.617: INFO: 	Container kubelet-rubber-stamp ready: true, restart count 1
May  6 07:54:56.617: INFO: tigera-operator-585fc94df6-xnxld from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.617: INFO: 	Container tigera-operator ready: true, restart count 0
May  6 07:54:56.617: INFO: externalsvc-hjgnp from services-2585 started at 2023-05-06 06:58:05 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.617: INFO: 	Container externalsvc ready: true, restart count 0
May  6 07:54:56.617: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-np4v2 from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 07:54:56.617: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 07:54:56.617: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 07:54:56.617: INFO: 
Logging pods the apiserver thinks is on node cncf-1 before test
May  6 07:54:56.623: INFO: calico-node-kz2jp from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.623: INFO: 	Container calico-node ready: true, restart count 0
May  6 07:54:56.623: INFO: calico-typha-7d85b4d7df-79mt7 from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.623: INFO: 	Container calico-typha ready: true, restart count 0
May  6 07:54:56.623: INFO: dns-test-6ffeff72-54ba-4fa1-9bbb-359bba68b89e from dns-3718 started at 2023-05-06 06:44:04 +0000 UTC (3 container statuses recorded)
May  6 07:54:56.623: INFO: 	Container jessie-querier ready: true, restart count 6
May  6 07:54:56.623: INFO: 	Container querier ready: true, restart count 5
May  6 07:54:56.623: INFO: 	Container webserver ready: true, restart count 0
May  6 07:54:56.623: INFO: coredns-5b98b988fd-8gbpp from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.623: INFO: 	Container coredns ready: true, restart count 0
May  6 07:54:56.623: INFO: kube-apiserver-cncf-1 from kube-system started at 2023-05-05 20:24:38 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.623: INFO: 	Container kube-apiserver ready: true, restart count 1
May  6 07:54:56.623: INFO: kube-controller-manager-cncf-1 from kube-system started at 2023-05-05 20:24:14 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.623: INFO: 	Container kube-controller-manager ready: true, restart count 0
May  6 07:54:56.623: INFO: kube-proxy-4wtf5 from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.623: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 07:54:56.623: INFO: kube-scheduler-cncf-1 from kube-system started at 2023-05-05 20:23:17 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.623: INFO: 	Container kube-scheduler ready: true, restart count 1
May  6 07:54:56.623: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-5lxlb from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 07:54:56.623: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 07:54:56.623: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 07:54:56.623: INFO: 
Logging pods the apiserver thinks is on node cncf-2 before test
May  6 07:54:56.628: INFO: calico-kube-controllers-574d8db6c-nnb5w from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.628: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  6 07:54:56.628: INFO: calico-node-q42q6 from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.628: INFO: 	Container calico-node ready: true, restart count 0
May  6 07:54:56.628: INFO: calico-typha-7d85b4d7df-6s76x from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.628: INFO: 	Container calico-typha ready: true, restart count 0
May  6 07:54:56.628: INFO: coredns-5b98b988fd-7r5vm from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.628: INFO: 	Container coredns ready: true, restart count 0
May  6 07:54:56.628: INFO: kube-apiserver-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.628: INFO: 	Container kube-apiserver ready: true, restart count 1
May  6 07:54:56.628: INFO: kube-controller-manager-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.628: INFO: 	Container kube-controller-manager ready: true, restart count 0
May  6 07:54:56.628: INFO: kube-proxy-jl76p from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.628: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 07:54:56.628: INFO: kube-scheduler-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.628: INFO: 	Container kube-scheduler ready: true, restart count 1
May  6 07:54:56.628: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-mjqth from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 07:54:56.628: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 07:54:56.628: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 07:54:56.628: INFO: 
Logging pods the apiserver thinks is on node cncf-3 before test
May  6 07:54:56.633: INFO: calico-node-sv4fh from calico-system started at 2023-05-05 20:28:36 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.633: INFO: 	Container calico-node ready: true, restart count 0
May  6 07:54:56.633: INFO: kube-proxy-n8v5r from kube-system started at 2023-05-05 20:28:36 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.633: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 07:54:56.633: INFO: controller-manager-v3-6b6b89c4f6-skqkm from rafay-system started at 2023-05-05 20:32:05 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.633: INFO: 	Container manager ready: true, restart count 0
May  6 07:54:56.633: INFO: edge-client-59448698df-gxb8q from rafay-system started at 2023-05-05 20:31:45 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.633: INFO: 	Container edge-client ready: true, restart count 0
May  6 07:54:56.633: INFO: rafay-connector-v3-79f986b9c6-rxjdn from rafay-system started at 2023-05-05 20:32:04 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.633: INFO: 	Container connector ready: true, restart count 0
May  6 07:54:56.633: INFO: v2-relay-agent-6b76bc6c6f-m6hgg from rafay-system started at 2023-05-05 20:32:06 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.633: INFO: 	Container relay-agent ready: true, restart count 0
May  6 07:54:56.633: INFO: externalsvc-45jwj from services-2585 started at 2023-05-06 06:58:05 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.633: INFO: 	Container externalsvc ready: true, restart count 0
May  6 07:54:56.633: INFO: sonobuoy from sonobuoy started at 2023-05-06 06:59:12 +0000 UTC (1 container statuses recorded)
May  6 07:54:56.633: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  6 07:54:56.633: INFO: sonobuoy-e2e-job-47910262329e4558 from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 07:54:56.633: INFO: 	Container e2e ready: true, restart count 0
May  6 07:54:56.633: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 07:54:56.633: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-ts2xw from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 07:54:56.633: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 07:54:56.633: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/06/23 07:54:56.633
May  6 07:54:56.643: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2239" to be "running"
May  6 07:54:56.646: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.804138ms
May  6 07:54:58.649: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005833355s
May  6 07:54:58.649: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/06/23 07:54:58.651
STEP: Trying to apply a random label on the found node. 05/06/23 07:54:58.664
STEP: verifying the node has the label kubernetes.io/e2e-b77e9823-e4ea-47ab-9642-544fb047f189 42 05/06/23 07:54:58.674
STEP: Trying to relaunch the pod, now with labels. 05/06/23 07:54:58.677
May  6 07:54:58.681: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2239" to be "not pending"
May  6 07:54:58.684: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.315831ms
May  6 07:55:00.687: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.005078032s
May  6 07:55:00.687: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-b77e9823-e4ea-47ab-9642-544fb047f189 off the node cncf-0 05/06/23 07:55:00.689
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b77e9823-e4ea-47ab-9642-544fb047f189 05/06/23 07:55:00.702
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 07:55:00.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2239" for this suite. 05/06/23 07:55:00.708
------------------------------
â€¢ [SLOW TEST] [5.130 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:54:55.584
    May  6 07:54:55.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename sched-pred 05/06/23 07:54:55.584
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:54:56.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:54:56.601
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    May  6 07:54:56.603: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  6 07:54:56.609: INFO: Waiting for terminating namespaces to be deleted...
    May  6 07:54:56.611: INFO: 
    Logging pods the apiserver thinks is on node cncf-0 before test
    May  6 07:54:56.617: INFO: calico-node-vvl9m from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.617: INFO: 	Container calico-node ready: true, restart count 0
    May  6 07:54:56.617: INFO: kube-apiserver-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.617: INFO: 	Container kube-apiserver ready: true, restart count 1
    May  6 07:54:56.617: INFO: kube-controller-manager-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.617: INFO: 	Container kube-controller-manager ready: true, restart count 0
    May  6 07:54:56.617: INFO: kube-proxy-cq72q from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.617: INFO: 	Container kube-proxy ready: true, restart count 0
    May  6 07:54:56.617: INFO: kube-scheduler-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.617: INFO: 	Container kube-scheduler ready: true, restart count 1
    May  6 07:54:56.617: INFO: kubelet-rubber-stamp-c6b74568-5mtx7 from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.617: INFO: 	Container kubelet-rubber-stamp ready: true, restart count 1
    May  6 07:54:56.617: INFO: tigera-operator-585fc94df6-xnxld from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.617: INFO: 	Container tigera-operator ready: true, restart count 0
    May  6 07:54:56.617: INFO: externalsvc-hjgnp from services-2585 started at 2023-05-06 06:58:05 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.617: INFO: 	Container externalsvc ready: true, restart count 0
    May  6 07:54:56.617: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-np4v2 from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 07:54:56.617: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 07:54:56.617: INFO: 	Container systemd-logs ready: true, restart count 0
    May  6 07:54:56.617: INFO: 
    Logging pods the apiserver thinks is on node cncf-1 before test
    May  6 07:54:56.623: INFO: calico-node-kz2jp from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.623: INFO: 	Container calico-node ready: true, restart count 0
    May  6 07:54:56.623: INFO: calico-typha-7d85b4d7df-79mt7 from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.623: INFO: 	Container calico-typha ready: true, restart count 0
    May  6 07:54:56.623: INFO: dns-test-6ffeff72-54ba-4fa1-9bbb-359bba68b89e from dns-3718 started at 2023-05-06 06:44:04 +0000 UTC (3 container statuses recorded)
    May  6 07:54:56.623: INFO: 	Container jessie-querier ready: true, restart count 6
    May  6 07:54:56.623: INFO: 	Container querier ready: true, restart count 5
    May  6 07:54:56.623: INFO: 	Container webserver ready: true, restart count 0
    May  6 07:54:56.623: INFO: coredns-5b98b988fd-8gbpp from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.623: INFO: 	Container coredns ready: true, restart count 0
    May  6 07:54:56.623: INFO: kube-apiserver-cncf-1 from kube-system started at 2023-05-05 20:24:38 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.623: INFO: 	Container kube-apiserver ready: true, restart count 1
    May  6 07:54:56.623: INFO: kube-controller-manager-cncf-1 from kube-system started at 2023-05-05 20:24:14 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.623: INFO: 	Container kube-controller-manager ready: true, restart count 0
    May  6 07:54:56.623: INFO: kube-proxy-4wtf5 from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.623: INFO: 	Container kube-proxy ready: true, restart count 0
    May  6 07:54:56.623: INFO: kube-scheduler-cncf-1 from kube-system started at 2023-05-05 20:23:17 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.623: INFO: 	Container kube-scheduler ready: true, restart count 1
    May  6 07:54:56.623: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-5lxlb from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 07:54:56.623: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 07:54:56.623: INFO: 	Container systemd-logs ready: true, restart count 0
    May  6 07:54:56.623: INFO: 
    Logging pods the apiserver thinks is on node cncf-2 before test
    May  6 07:54:56.628: INFO: calico-kube-controllers-574d8db6c-nnb5w from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.628: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  6 07:54:56.628: INFO: calico-node-q42q6 from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.628: INFO: 	Container calico-node ready: true, restart count 0
    May  6 07:54:56.628: INFO: calico-typha-7d85b4d7df-6s76x from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.628: INFO: 	Container calico-typha ready: true, restart count 0
    May  6 07:54:56.628: INFO: coredns-5b98b988fd-7r5vm from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.628: INFO: 	Container coredns ready: true, restart count 0
    May  6 07:54:56.628: INFO: kube-apiserver-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.628: INFO: 	Container kube-apiserver ready: true, restart count 1
    May  6 07:54:56.628: INFO: kube-controller-manager-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.628: INFO: 	Container kube-controller-manager ready: true, restart count 0
    May  6 07:54:56.628: INFO: kube-proxy-jl76p from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.628: INFO: 	Container kube-proxy ready: true, restart count 0
    May  6 07:54:56.628: INFO: kube-scheduler-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.628: INFO: 	Container kube-scheduler ready: true, restart count 1
    May  6 07:54:56.628: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-mjqth from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 07:54:56.628: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 07:54:56.628: INFO: 	Container systemd-logs ready: true, restart count 0
    May  6 07:54:56.628: INFO: 
    Logging pods the apiserver thinks is on node cncf-3 before test
    May  6 07:54:56.633: INFO: calico-node-sv4fh from calico-system started at 2023-05-05 20:28:36 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.633: INFO: 	Container calico-node ready: true, restart count 0
    May  6 07:54:56.633: INFO: kube-proxy-n8v5r from kube-system started at 2023-05-05 20:28:36 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.633: INFO: 	Container kube-proxy ready: true, restart count 0
    May  6 07:54:56.633: INFO: controller-manager-v3-6b6b89c4f6-skqkm from rafay-system started at 2023-05-05 20:32:05 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.633: INFO: 	Container manager ready: true, restart count 0
    May  6 07:54:56.633: INFO: edge-client-59448698df-gxb8q from rafay-system started at 2023-05-05 20:31:45 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.633: INFO: 	Container edge-client ready: true, restart count 0
    May  6 07:54:56.633: INFO: rafay-connector-v3-79f986b9c6-rxjdn from rafay-system started at 2023-05-05 20:32:04 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.633: INFO: 	Container connector ready: true, restart count 0
    May  6 07:54:56.633: INFO: v2-relay-agent-6b76bc6c6f-m6hgg from rafay-system started at 2023-05-05 20:32:06 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.633: INFO: 	Container relay-agent ready: true, restart count 0
    May  6 07:54:56.633: INFO: externalsvc-45jwj from services-2585 started at 2023-05-06 06:58:05 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.633: INFO: 	Container externalsvc ready: true, restart count 0
    May  6 07:54:56.633: INFO: sonobuoy from sonobuoy started at 2023-05-06 06:59:12 +0000 UTC (1 container statuses recorded)
    May  6 07:54:56.633: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  6 07:54:56.633: INFO: sonobuoy-e2e-job-47910262329e4558 from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 07:54:56.633: INFO: 	Container e2e ready: true, restart count 0
    May  6 07:54:56.633: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 07:54:56.633: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-ts2xw from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 07:54:56.633: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 07:54:56.633: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/06/23 07:54:56.633
    May  6 07:54:56.643: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2239" to be "running"
    May  6 07:54:56.646: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.804138ms
    May  6 07:54:58.649: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005833355s
    May  6 07:54:58.649: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/06/23 07:54:58.651
    STEP: Trying to apply a random label on the found node. 05/06/23 07:54:58.664
    STEP: verifying the node has the label kubernetes.io/e2e-b77e9823-e4ea-47ab-9642-544fb047f189 42 05/06/23 07:54:58.674
    STEP: Trying to relaunch the pod, now with labels. 05/06/23 07:54:58.677
    May  6 07:54:58.681: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2239" to be "not pending"
    May  6 07:54:58.684: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.315831ms
    May  6 07:55:00.687: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.005078032s
    May  6 07:55:00.687: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-b77e9823-e4ea-47ab-9642-544fb047f189 off the node cncf-0 05/06/23 07:55:00.689
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-b77e9823-e4ea-47ab-9642-544fb047f189 05/06/23 07:55:00.702
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 07:55:00.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2239" for this suite. 05/06/23 07:55:00.708
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:55:00.714
May  6 07:55:00.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename cronjob 05/06/23 07:55:00.715
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:55:01.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:55:01.733
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 05/06/23 07:55:01.735
STEP: Ensuring more than one job is running at a time 05/06/23 07:55:01.742
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 05/06/23 07:57:01.745
STEP: Removing cronjob 05/06/23 07:57:01.747
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May  6 07:57:01.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7149" for this suite. 05/06/23 07:57:01.757
------------------------------
â€¢ [SLOW TEST] [121.051 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:55:00.714
    May  6 07:55:00.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename cronjob 05/06/23 07:55:00.715
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:55:01.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:55:01.733
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 05/06/23 07:55:01.735
    STEP: Ensuring more than one job is running at a time 05/06/23 07:55:01.742
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 05/06/23 07:57:01.745
    STEP: Removing cronjob 05/06/23 07:57:01.747
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May  6 07:57:01.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7149" for this suite. 05/06/23 07:57:01.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:57:01.767
May  6 07:57:01.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename crd-publish-openapi 05/06/23 07:57:01.767
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:57:02.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:57:02.786
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 05/06/23 07:57:02.788
May  6 07:57:02.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: mark a version not serverd 05/06/23 07:57:07.337
STEP: check the unserved version gets removed 05/06/23 07:57:07.356
STEP: check the other version is not changed 05/06/23 07:57:09.224
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:57:12.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6337" for this suite. 05/06/23 07:57:12.581
------------------------------
â€¢ [SLOW TEST] [10.820 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:57:01.767
    May  6 07:57:01.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename crd-publish-openapi 05/06/23 07:57:01.767
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:57:02.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:57:02.786
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 05/06/23 07:57:02.788
    May  6 07:57:02.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: mark a version not serverd 05/06/23 07:57:07.337
    STEP: check the unserved version gets removed 05/06/23 07:57:07.356
    STEP: check the other version is not changed 05/06/23 07:57:09.224
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:57:12.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6337" for this suite. 05/06/23 07:57:12.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:57:12.588
May  6 07:57:12.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename watch 05/06/23 07:57:12.589
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:57:13.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:57:13.607
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 05/06/23 07:57:13.609
STEP: creating a watch on configmaps with label B 05/06/23 07:57:13.61
STEP: creating a watch on configmaps with label A or B 05/06/23 07:57:13.611
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 05/06/23 07:57:13.611
May  6 07:57:13.615: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6474  2d60e2cc-8b5f-4fdc-904f-b693ce11691f 166560 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  6 07:57:13.615: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6474  2d60e2cc-8b5f-4fdc-904f-b693ce11691f 166560 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 05/06/23 07:57:13.615
May  6 07:57:13.621: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6474  2d60e2cc-8b5f-4fdc-904f-b693ce11691f 166561 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May  6 07:57:13.621: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6474  2d60e2cc-8b5f-4fdc-904f-b693ce11691f 166561 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 05/06/23 07:57:13.621
May  6 07:57:13.628: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6474  2d60e2cc-8b5f-4fdc-904f-b693ce11691f 166562 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  6 07:57:13.628: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6474  2d60e2cc-8b5f-4fdc-904f-b693ce11691f 166562 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 05/06/23 07:57:13.628
May  6 07:57:13.633: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6474  2d60e2cc-8b5f-4fdc-904f-b693ce11691f 166563 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  6 07:57:13.633: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6474  2d60e2cc-8b5f-4fdc-904f-b693ce11691f 166563 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 05/06/23 07:57:13.633
May  6 07:57:13.637: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6474  fe246513-4376-411d-8697-804a1938ee0c 166564 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  6 07:57:13.637: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6474  fe246513-4376-411d-8697-804a1938ee0c 166564 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 05/06/23 07:57:23.637
May  6 07:57:23.643: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6474  fe246513-4376-411d-8697-804a1938ee0c 166603 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  6 07:57:23.643: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6474  fe246513-4376-411d-8697-804a1938ee0c 166603 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May  6 07:57:33.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6474" for this suite. 05/06/23 07:57:33.651
------------------------------
â€¢ [SLOW TEST] [21.069 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:57:12.588
    May  6 07:57:12.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename watch 05/06/23 07:57:12.589
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:57:13.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:57:13.607
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 05/06/23 07:57:13.609
    STEP: creating a watch on configmaps with label B 05/06/23 07:57:13.61
    STEP: creating a watch on configmaps with label A or B 05/06/23 07:57:13.611
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 05/06/23 07:57:13.611
    May  6 07:57:13.615: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6474  2d60e2cc-8b5f-4fdc-904f-b693ce11691f 166560 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  6 07:57:13.615: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6474  2d60e2cc-8b5f-4fdc-904f-b693ce11691f 166560 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 05/06/23 07:57:13.615
    May  6 07:57:13.621: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6474  2d60e2cc-8b5f-4fdc-904f-b693ce11691f 166561 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  6 07:57:13.621: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6474  2d60e2cc-8b5f-4fdc-904f-b693ce11691f 166561 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 05/06/23 07:57:13.621
    May  6 07:57:13.628: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6474  2d60e2cc-8b5f-4fdc-904f-b693ce11691f 166562 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  6 07:57:13.628: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6474  2d60e2cc-8b5f-4fdc-904f-b693ce11691f 166562 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 05/06/23 07:57:13.628
    May  6 07:57:13.633: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6474  2d60e2cc-8b5f-4fdc-904f-b693ce11691f 166563 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  6 07:57:13.633: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6474  2d60e2cc-8b5f-4fdc-904f-b693ce11691f 166563 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 05/06/23 07:57:13.633
    May  6 07:57:13.637: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6474  fe246513-4376-411d-8697-804a1938ee0c 166564 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  6 07:57:13.637: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6474  fe246513-4376-411d-8697-804a1938ee0c 166564 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 05/06/23 07:57:23.637
    May  6 07:57:23.643: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6474  fe246513-4376-411d-8697-804a1938ee0c 166603 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  6 07:57:23.643: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6474  fe246513-4376-411d-8697-804a1938ee0c 166603 0 2023-05-06 07:57:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-06 07:57:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May  6 07:57:33.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6474" for this suite. 05/06/23 07:57:33.651
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:57:33.658
May  6 07:57:33.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubectl 05/06/23 07:57:33.658
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:57:34.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:57:34.674
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 05/06/23 07:57:34.679
May  6 07:57:34.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 create -f -'
May  6 07:57:35.162: INFO: stderr: ""
May  6 07:57:35.162: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/06/23 07:57:35.162
May  6 07:57:35.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  6 07:57:35.215: INFO: stderr: ""
May  6 07:57:35.215: INFO: stdout: "update-demo-nautilus-qd8nx update-demo-nautilus-xzlmp "
May  6 07:57:35.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 get pods update-demo-nautilus-qd8nx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  6 07:57:35.266: INFO: stderr: ""
May  6 07:57:35.266: INFO: stdout: ""
May  6 07:57:35.266: INFO: update-demo-nautilus-qd8nx is created but not running
May  6 07:57:40.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  6 07:57:40.318: INFO: stderr: ""
May  6 07:57:40.318: INFO: stdout: "update-demo-nautilus-qd8nx update-demo-nautilus-xzlmp "
May  6 07:57:40.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 get pods update-demo-nautilus-qd8nx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  6 07:57:40.365: INFO: stderr: ""
May  6 07:57:40.365: INFO: stdout: "true"
May  6 07:57:40.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 get pods update-demo-nautilus-qd8nx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  6 07:57:40.415: INFO: stderr: ""
May  6 07:57:40.415: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May  6 07:57:40.415: INFO: validating pod update-demo-nautilus-qd8nx
May  6 07:57:40.419: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 07:57:40.420: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 07:57:40.420: INFO: update-demo-nautilus-qd8nx is verified up and running
May  6 07:57:40.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 get pods update-demo-nautilus-xzlmp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  6 07:57:40.471: INFO: stderr: ""
May  6 07:57:40.471: INFO: stdout: "true"
May  6 07:57:40.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 get pods update-demo-nautilus-xzlmp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  6 07:57:40.520: INFO: stderr: ""
May  6 07:57:40.520: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May  6 07:57:40.520: INFO: validating pod update-demo-nautilus-xzlmp
May  6 07:57:40.524: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 07:57:40.524: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 07:57:40.524: INFO: update-demo-nautilus-xzlmp is verified up and running
STEP: using delete to clean up resources 05/06/23 07:57:40.524
May  6 07:57:40.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 delete --grace-period=0 --force -f -'
May  6 07:57:40.575: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 07:57:40.575: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  6 07:57:40.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 get rc,svc -l name=update-demo --no-headers'
May  6 07:57:40.631: INFO: stderr: "No resources found in kubectl-8991 namespace.\n"
May  6 07:57:40.631: INFO: stdout: ""
May  6 07:57:40.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  6 07:57:40.682: INFO: stderr: ""
May  6 07:57:40.682: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  6 07:57:40.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8991" for this suite. 05/06/23 07:57:40.685
------------------------------
â€¢ [SLOW TEST] [7.036 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:57:33.658
    May  6 07:57:33.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubectl 05/06/23 07:57:33.658
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:57:34.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:57:34.674
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 05/06/23 07:57:34.679
    May  6 07:57:34.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 create -f -'
    May  6 07:57:35.162: INFO: stderr: ""
    May  6 07:57:35.162: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/06/23 07:57:35.162
    May  6 07:57:35.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  6 07:57:35.215: INFO: stderr: ""
    May  6 07:57:35.215: INFO: stdout: "update-demo-nautilus-qd8nx update-demo-nautilus-xzlmp "
    May  6 07:57:35.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 get pods update-demo-nautilus-qd8nx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  6 07:57:35.266: INFO: stderr: ""
    May  6 07:57:35.266: INFO: stdout: ""
    May  6 07:57:35.266: INFO: update-demo-nautilus-qd8nx is created but not running
    May  6 07:57:40.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  6 07:57:40.318: INFO: stderr: ""
    May  6 07:57:40.318: INFO: stdout: "update-demo-nautilus-qd8nx update-demo-nautilus-xzlmp "
    May  6 07:57:40.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 get pods update-demo-nautilus-qd8nx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  6 07:57:40.365: INFO: stderr: ""
    May  6 07:57:40.365: INFO: stdout: "true"
    May  6 07:57:40.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 get pods update-demo-nautilus-qd8nx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  6 07:57:40.415: INFO: stderr: ""
    May  6 07:57:40.415: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May  6 07:57:40.415: INFO: validating pod update-demo-nautilus-qd8nx
    May  6 07:57:40.419: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  6 07:57:40.420: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  6 07:57:40.420: INFO: update-demo-nautilus-qd8nx is verified up and running
    May  6 07:57:40.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 get pods update-demo-nautilus-xzlmp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  6 07:57:40.471: INFO: stderr: ""
    May  6 07:57:40.471: INFO: stdout: "true"
    May  6 07:57:40.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 get pods update-demo-nautilus-xzlmp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  6 07:57:40.520: INFO: stderr: ""
    May  6 07:57:40.520: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May  6 07:57:40.520: INFO: validating pod update-demo-nautilus-xzlmp
    May  6 07:57:40.524: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  6 07:57:40.524: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  6 07:57:40.524: INFO: update-demo-nautilus-xzlmp is verified up and running
    STEP: using delete to clean up resources 05/06/23 07:57:40.524
    May  6 07:57:40.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 delete --grace-period=0 --force -f -'
    May  6 07:57:40.575: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  6 07:57:40.575: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    May  6 07:57:40.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 get rc,svc -l name=update-demo --no-headers'
    May  6 07:57:40.631: INFO: stderr: "No resources found in kubectl-8991 namespace.\n"
    May  6 07:57:40.631: INFO: stdout: ""
    May  6 07:57:40.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8991 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May  6 07:57:40.682: INFO: stderr: ""
    May  6 07:57:40.682: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  6 07:57:40.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8991" for this suite. 05/06/23 07:57:40.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:57:40.695
May  6 07:57:40.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename webhook 05/06/23 07:57:40.696
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:57:42.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:57:42.008
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/06/23 07:57:44.027
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:57:44.456
STEP: Deploying the webhook pod 05/06/23 07:57:44.462
STEP: Wait for the deployment to be ready 05/06/23 07:57:44.473
May  6 07:57:44.477: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/06/23 07:57:46.485
STEP: Verifying the service has paired with the endpoint 05/06/23 07:57:46.497
May  6 07:57:47.498: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/06/23 07:57:47.5
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/06/23 07:57:47.512
STEP: Creating a dummy validating-webhook-configuration object 05/06/23 07:57:47.531
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 05/06/23 07:57:47.538
STEP: Creating a dummy mutating-webhook-configuration object 05/06/23 07:57:47.542
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 05/06/23 07:57:47.548
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:57:47.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9382" for this suite. 05/06/23 07:57:47.611
STEP: Destroying namespace "webhook-9382-markers" for this suite. 05/06/23 07:57:47.619
------------------------------
â€¢ [SLOW TEST] [6.930 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:57:40.695
    May  6 07:57:40.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename webhook 05/06/23 07:57:40.696
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:57:42.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:57:42.008
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/06/23 07:57:44.027
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:57:44.456
    STEP: Deploying the webhook pod 05/06/23 07:57:44.462
    STEP: Wait for the deployment to be ready 05/06/23 07:57:44.473
    May  6 07:57:44.477: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/06/23 07:57:46.485
    STEP: Verifying the service has paired with the endpoint 05/06/23 07:57:46.497
    May  6 07:57:47.498: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/06/23 07:57:47.5
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/06/23 07:57:47.512
    STEP: Creating a dummy validating-webhook-configuration object 05/06/23 07:57:47.531
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 05/06/23 07:57:47.538
    STEP: Creating a dummy mutating-webhook-configuration object 05/06/23 07:57:47.542
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 05/06/23 07:57:47.548
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:57:47.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9382" for this suite. 05/06/23 07:57:47.611
    STEP: Destroying namespace "webhook-9382-markers" for this suite. 05/06/23 07:57:47.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:57:47.626
May  6 07:57:47.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename services 05/06/23 07:57:47.627
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:57:48.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:57:48.645
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 05/06/23 07:57:48.65
STEP: watching for the Service to be added 05/06/23 07:57:48.662
May  6 07:57:48.663: INFO: Found Service test-service-d5pbh in namespace services-5799 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
May  6 07:57:48.663: INFO: Service test-service-d5pbh created
STEP: Getting /status 05/06/23 07:57:48.663
May  6 07:57:48.665: INFO: Service test-service-d5pbh has LoadBalancer: {[]}
STEP: patching the ServiceStatus 05/06/23 07:57:48.665
STEP: watching for the Service to be patched 05/06/23 07:57:48.67
May  6 07:57:48.672: INFO: observed Service test-service-d5pbh in namespace services-5799 with annotations: map[] & LoadBalancer: {[]}
May  6 07:57:48.672: INFO: Found Service test-service-d5pbh in namespace services-5799 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
May  6 07:57:48.672: INFO: Service test-service-d5pbh has service status patched
STEP: updating the ServiceStatus 05/06/23 07:57:48.672
May  6 07:57:48.680: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 05/06/23 07:57:48.68
May  6 07:57:48.681: INFO: Observed Service test-service-d5pbh in namespace services-5799 with annotations: map[] & Conditions: {[]}
May  6 07:57:48.681: INFO: Observed event: &Service{ObjectMeta:{test-service-d5pbh  services-5799  21b2b80f-2679-4cb4-b263-27a2e50fc885 166843 0 2023-05-06 07:57:48 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-06 07:57:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-06 07:57:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.98.11.170,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.98.11.170],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
May  6 07:57:48.681: INFO: Found Service test-service-d5pbh in namespace services-5799 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  6 07:57:48.681: INFO: Service test-service-d5pbh has service status updated
STEP: patching the service 05/06/23 07:57:48.681
STEP: watching for the Service to be patched 05/06/23 07:57:48.69
May  6 07:57:48.692: INFO: observed Service test-service-d5pbh in namespace services-5799 with labels: map[test-service-static:true]
May  6 07:57:48.692: INFO: observed Service test-service-d5pbh in namespace services-5799 with labels: map[test-service-static:true]
May  6 07:57:48.692: INFO: observed Service test-service-d5pbh in namespace services-5799 with labels: map[test-service-static:true]
May  6 07:57:48.692: INFO: Found Service test-service-d5pbh in namespace services-5799 with labels: map[test-service:patched test-service-static:true]
May  6 07:57:48.692: INFO: Service test-service-d5pbh patched
STEP: deleting the service 05/06/23 07:57:48.692
STEP: watching for the Service to be deleted 05/06/23 07:57:48.705
May  6 07:57:48.706: INFO: Observed event: ADDED
May  6 07:57:48.706: INFO: Observed event: MODIFIED
May  6 07:57:48.706: INFO: Observed event: MODIFIED
May  6 07:57:48.706: INFO: Observed event: MODIFIED
May  6 07:57:48.706: INFO: Found Service test-service-d5pbh in namespace services-5799 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
May  6 07:57:48.706: INFO: Service test-service-d5pbh deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  6 07:57:48.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5799" for this suite. 05/06/23 07:57:48.71
------------------------------
â€¢ [1.089 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:57:47.626
    May  6 07:57:47.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename services 05/06/23 07:57:47.627
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:57:48.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:57:48.645
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 05/06/23 07:57:48.65
    STEP: watching for the Service to be added 05/06/23 07:57:48.662
    May  6 07:57:48.663: INFO: Found Service test-service-d5pbh in namespace services-5799 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    May  6 07:57:48.663: INFO: Service test-service-d5pbh created
    STEP: Getting /status 05/06/23 07:57:48.663
    May  6 07:57:48.665: INFO: Service test-service-d5pbh has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 05/06/23 07:57:48.665
    STEP: watching for the Service to be patched 05/06/23 07:57:48.67
    May  6 07:57:48.672: INFO: observed Service test-service-d5pbh in namespace services-5799 with annotations: map[] & LoadBalancer: {[]}
    May  6 07:57:48.672: INFO: Found Service test-service-d5pbh in namespace services-5799 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    May  6 07:57:48.672: INFO: Service test-service-d5pbh has service status patched
    STEP: updating the ServiceStatus 05/06/23 07:57:48.672
    May  6 07:57:48.680: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 05/06/23 07:57:48.68
    May  6 07:57:48.681: INFO: Observed Service test-service-d5pbh in namespace services-5799 with annotations: map[] & Conditions: {[]}
    May  6 07:57:48.681: INFO: Observed event: &Service{ObjectMeta:{test-service-d5pbh  services-5799  21b2b80f-2679-4cb4-b263-27a2e50fc885 166843 0 2023-05-06 07:57:48 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-06 07:57:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-06 07:57:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.98.11.170,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.98.11.170],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    May  6 07:57:48.681: INFO: Found Service test-service-d5pbh in namespace services-5799 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  6 07:57:48.681: INFO: Service test-service-d5pbh has service status updated
    STEP: patching the service 05/06/23 07:57:48.681
    STEP: watching for the Service to be patched 05/06/23 07:57:48.69
    May  6 07:57:48.692: INFO: observed Service test-service-d5pbh in namespace services-5799 with labels: map[test-service-static:true]
    May  6 07:57:48.692: INFO: observed Service test-service-d5pbh in namespace services-5799 with labels: map[test-service-static:true]
    May  6 07:57:48.692: INFO: observed Service test-service-d5pbh in namespace services-5799 with labels: map[test-service-static:true]
    May  6 07:57:48.692: INFO: Found Service test-service-d5pbh in namespace services-5799 with labels: map[test-service:patched test-service-static:true]
    May  6 07:57:48.692: INFO: Service test-service-d5pbh patched
    STEP: deleting the service 05/06/23 07:57:48.692
    STEP: watching for the Service to be deleted 05/06/23 07:57:48.705
    May  6 07:57:48.706: INFO: Observed event: ADDED
    May  6 07:57:48.706: INFO: Observed event: MODIFIED
    May  6 07:57:48.706: INFO: Observed event: MODIFIED
    May  6 07:57:48.706: INFO: Observed event: MODIFIED
    May  6 07:57:48.706: INFO: Found Service test-service-d5pbh in namespace services-5799 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    May  6 07:57:48.706: INFO: Service test-service-d5pbh deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  6 07:57:48.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5799" for this suite. 05/06/23 07:57:48.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:57:48.716
May  6 07:57:48.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:57:48.717
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:57:49.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:57:49.734
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 05/06/23 07:57:49.736
May  6 07:57:49.744: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9fb42c34-3df4-4321-bf0f-d41c6287e174" in namespace "projected-9856" to be "Succeeded or Failed"
May  6 07:57:49.747: INFO: Pod "downwardapi-volume-9fb42c34-3df4-4321-bf0f-d41c6287e174": Phase="Pending", Reason="", readiness=false. Elapsed: 2.395904ms
May  6 07:57:51.749: INFO: Pod "downwardapi-volume-9fb42c34-3df4-4321-bf0f-d41c6287e174": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004908536s
May  6 07:57:53.750: INFO: Pod "downwardapi-volume-9fb42c34-3df4-4321-bf0f-d41c6287e174": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005298923s
STEP: Saw pod success 05/06/23 07:57:53.75
May  6 07:57:53.750: INFO: Pod "downwardapi-volume-9fb42c34-3df4-4321-bf0f-d41c6287e174" satisfied condition "Succeeded or Failed"
May  6 07:57:53.752: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-9fb42c34-3df4-4321-bf0f-d41c6287e174 container client-container: <nil>
STEP: delete the pod 05/06/23 07:57:53.763
May  6 07:57:53.774: INFO: Waiting for pod downwardapi-volume-9fb42c34-3df4-4321-bf0f-d41c6287e174 to disappear
May  6 07:57:53.777: INFO: Pod downwardapi-volume-9fb42c34-3df4-4321-bf0f-d41c6287e174 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  6 07:57:53.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9856" for this suite. 05/06/23 07:57:53.78
------------------------------
â€¢ [SLOW TEST] [5.071 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:57:48.716
    May  6 07:57:48.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:57:48.717
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:57:49.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:57:49.734
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 05/06/23 07:57:49.736
    May  6 07:57:49.744: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9fb42c34-3df4-4321-bf0f-d41c6287e174" in namespace "projected-9856" to be "Succeeded or Failed"
    May  6 07:57:49.747: INFO: Pod "downwardapi-volume-9fb42c34-3df4-4321-bf0f-d41c6287e174": Phase="Pending", Reason="", readiness=false. Elapsed: 2.395904ms
    May  6 07:57:51.749: INFO: Pod "downwardapi-volume-9fb42c34-3df4-4321-bf0f-d41c6287e174": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004908536s
    May  6 07:57:53.750: INFO: Pod "downwardapi-volume-9fb42c34-3df4-4321-bf0f-d41c6287e174": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005298923s
    STEP: Saw pod success 05/06/23 07:57:53.75
    May  6 07:57:53.750: INFO: Pod "downwardapi-volume-9fb42c34-3df4-4321-bf0f-d41c6287e174" satisfied condition "Succeeded or Failed"
    May  6 07:57:53.752: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-9fb42c34-3df4-4321-bf0f-d41c6287e174 container client-container: <nil>
    STEP: delete the pod 05/06/23 07:57:53.763
    May  6 07:57:53.774: INFO: Waiting for pod downwardapi-volume-9fb42c34-3df4-4321-bf0f-d41c6287e174 to disappear
    May  6 07:57:53.777: INFO: Pod downwardapi-volume-9fb42c34-3df4-4321-bf0f-d41c6287e174 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  6 07:57:53.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9856" for this suite. 05/06/23 07:57:53.78
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:57:53.787
May  6 07:57:53.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubectl 05/06/23 07:57:53.788
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:57:54.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:57:54.805
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/06/23 07:57:54.806
May  6 07:57:54.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-1583 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
May  6 07:57:54.863: INFO: stderr: ""
May  6 07:57:54.863: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 05/06/23 07:57:54.863
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
May  6 07:57:54.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-1583 delete pods e2e-test-httpd-pod'
May  6 07:57:56.895: INFO: stderr: ""
May  6 07:57:56.895: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  6 07:57:56.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1583" for this suite. 05/06/23 07:57:56.898
------------------------------
â€¢ [3.116 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:57:53.787
    May  6 07:57:53.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubectl 05/06/23 07:57:53.788
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:57:54.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:57:54.805
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/06/23 07:57:54.806
    May  6 07:57:54.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-1583 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    May  6 07:57:54.863: INFO: stderr: ""
    May  6 07:57:54.863: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 05/06/23 07:57:54.863
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    May  6 07:57:54.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-1583 delete pods e2e-test-httpd-pod'
    May  6 07:57:56.895: INFO: stderr: ""
    May  6 07:57:56.895: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  6 07:57:56.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1583" for this suite. 05/06/23 07:57:56.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:57:56.904
May  6 07:57:56.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename configmap 05/06/23 07:57:56.905
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:57:57.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:57:57.921
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  6 07:57:57.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3528" for this suite. 05/06/23 07:57:57.956
------------------------------
â€¢ [1.055 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:57:56.904
    May  6 07:57:56.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename configmap 05/06/23 07:57:56.905
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:57:57.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:57:57.921
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  6 07:57:57.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3528" for this suite. 05/06/23 07:57:57.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:57:57.96
May  6 07:57:57.960: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename security-context-test 05/06/23 07:57:57.961
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:57:58.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:57:58.979
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
May  6 07:57:58.987: INFO: Waiting up to 5m0s for pod "busybox-user-65534-0fd9764c-8869-40d2-a517-aea5635abb0d" in namespace "security-context-test-4421" to be "Succeeded or Failed"
May  6 07:57:58.990: INFO: Pod "busybox-user-65534-0fd9764c-8869-40d2-a517-aea5635abb0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.461168ms
May  6 07:58:00.993: INFO: Pod "busybox-user-65534-0fd9764c-8869-40d2-a517-aea5635abb0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005614626s
May  6 07:58:02.994: INFO: Pod "busybox-user-65534-0fd9764c-8869-40d2-a517-aea5635abb0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006572395s
May  6 07:58:02.994: INFO: Pod "busybox-user-65534-0fd9764c-8869-40d2-a517-aea5635abb0d" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May  6 07:58:02.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4421" for this suite. 05/06/23 07:58:02.997
------------------------------
â€¢ [SLOW TEST] [5.043 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:57:57.96
    May  6 07:57:57.960: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename security-context-test 05/06/23 07:57:57.961
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:57:58.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:57:58.979
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    May  6 07:57:58.987: INFO: Waiting up to 5m0s for pod "busybox-user-65534-0fd9764c-8869-40d2-a517-aea5635abb0d" in namespace "security-context-test-4421" to be "Succeeded or Failed"
    May  6 07:57:58.990: INFO: Pod "busybox-user-65534-0fd9764c-8869-40d2-a517-aea5635abb0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.461168ms
    May  6 07:58:00.993: INFO: Pod "busybox-user-65534-0fd9764c-8869-40d2-a517-aea5635abb0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005614626s
    May  6 07:58:02.994: INFO: Pod "busybox-user-65534-0fd9764c-8869-40d2-a517-aea5635abb0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006572395s
    May  6 07:58:02.994: INFO: Pod "busybox-user-65534-0fd9764c-8869-40d2-a517-aea5635abb0d" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May  6 07:58:02.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4421" for this suite. 05/06/23 07:58:02.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:58:03.004
May  6 07:58:03.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename controllerrevisions 05/06/23 07:58:03.004
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:04.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:04.025
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-r7sgv-daemon-set" 05/06/23 07:58:05.046
STEP: Check that daemon pods launch on every node of the cluster. 05/06/23 07:58:05.051
May  6 07:58:05.063: INFO: Number of nodes with available pods controlled by daemonset e2e-r7sgv-daemon-set: 0
May  6 07:58:05.063: INFO: Node cncf-0 is running 0 daemon pod, expected 1
May  6 07:58:06.070: INFO: Number of nodes with available pods controlled by daemonset e2e-r7sgv-daemon-set: 1
May  6 07:58:06.070: INFO: Node cncf-1 is running 0 daemon pod, expected 1
May  6 07:58:07.069: INFO: Number of nodes with available pods controlled by daemonset e2e-r7sgv-daemon-set: 4
May  6 07:58:07.069: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset e2e-r7sgv-daemon-set
STEP: Confirm DaemonSet "e2e-r7sgv-daemon-set" successfully created with "daemonset-name=e2e-r7sgv-daemon-set" label 05/06/23 07:58:07.071
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-r7sgv-daemon-set" 05/06/23 07:58:07.077
May  6 07:58:07.081: INFO: Located ControllerRevision: "e2e-r7sgv-daemon-set-77758b7d94"
STEP: Patching ControllerRevision "e2e-r7sgv-daemon-set-77758b7d94" 05/06/23 07:58:07.083
May  6 07:58:07.091: INFO: e2e-r7sgv-daemon-set-77758b7d94 has been patched
STEP: Create a new ControllerRevision 05/06/23 07:58:07.091
May  6 07:58:07.096: INFO: Created ControllerRevision: e2e-r7sgv-daemon-set-758b89cd8d
STEP: Confirm that there are two ControllerRevisions 05/06/23 07:58:07.096
May  6 07:58:07.096: INFO: Requesting list of ControllerRevisions to confirm quantity
May  6 07:58:07.098: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-r7sgv-daemon-set-77758b7d94" 05/06/23 07:58:07.098
STEP: Confirm that there is only one ControllerRevision 05/06/23 07:58:07.104
May  6 07:58:07.104: INFO: Requesting list of ControllerRevisions to confirm quantity
May  6 07:58:07.107: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-r7sgv-daemon-set-758b89cd8d" 05/06/23 07:58:07.11
May  6 07:58:07.118: INFO: e2e-r7sgv-daemon-set-758b89cd8d has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 05/06/23 07:58:07.118
W0506 07:58:07.129461      21 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 05/06/23 07:58:07.129
May  6 07:58:07.129: INFO: Requesting list of ControllerRevisions to confirm quantity
May  6 07:58:08.132: INFO: Requesting list of ControllerRevisions to confirm quantity
May  6 07:58:08.135: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-r7sgv-daemon-set-758b89cd8d=updated" 05/06/23 07:58:08.135
STEP: Confirm that there is only one ControllerRevision 05/06/23 07:58:08.144
May  6 07:58:08.145: INFO: Requesting list of ControllerRevisions to confirm quantity
May  6 07:58:08.147: INFO: Found 1 ControllerRevisions
May  6 07:58:08.149: INFO: ControllerRevision "e2e-r7sgv-daemon-set-665dfcc9b5" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-r7sgv-daemon-set" 05/06/23 07:58:08.152
STEP: deleting DaemonSet.extensions e2e-r7sgv-daemon-set in namespace controllerrevisions-963, will wait for the garbage collector to delete the pods 05/06/23 07:58:08.152
May  6 07:58:08.211: INFO: Deleting DaemonSet.extensions e2e-r7sgv-daemon-set took: 6.114391ms
May  6 07:58:08.311: INFO: Terminating DaemonSet.extensions e2e-r7sgv-daemon-set pods took: 100.579857ms
May  6 07:58:09.515: INFO: Number of nodes with available pods controlled by daemonset e2e-r7sgv-daemon-set: 0
May  6 07:58:09.515: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-r7sgv-daemon-set
May  6 07:58:09.517: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"167161"},"items":null}

May  6 07:58:09.519: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"167161"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 07:58:09.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-963" for this suite. 05/06/23 07:58:09.534
------------------------------
â€¢ [SLOW TEST] [6.537 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:58:03.004
    May  6 07:58:03.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename controllerrevisions 05/06/23 07:58:03.004
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:04.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:04.025
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-r7sgv-daemon-set" 05/06/23 07:58:05.046
    STEP: Check that daemon pods launch on every node of the cluster. 05/06/23 07:58:05.051
    May  6 07:58:05.063: INFO: Number of nodes with available pods controlled by daemonset e2e-r7sgv-daemon-set: 0
    May  6 07:58:05.063: INFO: Node cncf-0 is running 0 daemon pod, expected 1
    May  6 07:58:06.070: INFO: Number of nodes with available pods controlled by daemonset e2e-r7sgv-daemon-set: 1
    May  6 07:58:06.070: INFO: Node cncf-1 is running 0 daemon pod, expected 1
    May  6 07:58:07.069: INFO: Number of nodes with available pods controlled by daemonset e2e-r7sgv-daemon-set: 4
    May  6 07:58:07.069: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset e2e-r7sgv-daemon-set
    STEP: Confirm DaemonSet "e2e-r7sgv-daemon-set" successfully created with "daemonset-name=e2e-r7sgv-daemon-set" label 05/06/23 07:58:07.071
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-r7sgv-daemon-set" 05/06/23 07:58:07.077
    May  6 07:58:07.081: INFO: Located ControllerRevision: "e2e-r7sgv-daemon-set-77758b7d94"
    STEP: Patching ControllerRevision "e2e-r7sgv-daemon-set-77758b7d94" 05/06/23 07:58:07.083
    May  6 07:58:07.091: INFO: e2e-r7sgv-daemon-set-77758b7d94 has been patched
    STEP: Create a new ControllerRevision 05/06/23 07:58:07.091
    May  6 07:58:07.096: INFO: Created ControllerRevision: e2e-r7sgv-daemon-set-758b89cd8d
    STEP: Confirm that there are two ControllerRevisions 05/06/23 07:58:07.096
    May  6 07:58:07.096: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  6 07:58:07.098: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-r7sgv-daemon-set-77758b7d94" 05/06/23 07:58:07.098
    STEP: Confirm that there is only one ControllerRevision 05/06/23 07:58:07.104
    May  6 07:58:07.104: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  6 07:58:07.107: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-r7sgv-daemon-set-758b89cd8d" 05/06/23 07:58:07.11
    May  6 07:58:07.118: INFO: e2e-r7sgv-daemon-set-758b89cd8d has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 05/06/23 07:58:07.118
    W0506 07:58:07.129461      21 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 05/06/23 07:58:07.129
    May  6 07:58:07.129: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  6 07:58:08.132: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  6 07:58:08.135: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-r7sgv-daemon-set-758b89cd8d=updated" 05/06/23 07:58:08.135
    STEP: Confirm that there is only one ControllerRevision 05/06/23 07:58:08.144
    May  6 07:58:08.145: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  6 07:58:08.147: INFO: Found 1 ControllerRevisions
    May  6 07:58:08.149: INFO: ControllerRevision "e2e-r7sgv-daemon-set-665dfcc9b5" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-r7sgv-daemon-set" 05/06/23 07:58:08.152
    STEP: deleting DaemonSet.extensions e2e-r7sgv-daemon-set in namespace controllerrevisions-963, will wait for the garbage collector to delete the pods 05/06/23 07:58:08.152
    May  6 07:58:08.211: INFO: Deleting DaemonSet.extensions e2e-r7sgv-daemon-set took: 6.114391ms
    May  6 07:58:08.311: INFO: Terminating DaemonSet.extensions e2e-r7sgv-daemon-set pods took: 100.579857ms
    May  6 07:58:09.515: INFO: Number of nodes with available pods controlled by daemonset e2e-r7sgv-daemon-set: 0
    May  6 07:58:09.515: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-r7sgv-daemon-set
    May  6 07:58:09.517: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"167161"},"items":null}

    May  6 07:58:09.519: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"167161"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 07:58:09.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-963" for this suite. 05/06/23 07:58:09.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:58:09.541
May  6 07:58:09.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename svcaccounts 05/06/23 07:58:09.542
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:10.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:10.559
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
May  6 07:58:10.564: INFO: Got root ca configmap in namespace "svcaccounts-1984"
May  6 07:58:10.569: INFO: Deleted root ca configmap in namespace "svcaccounts-1984"
STEP: waiting for a new root ca configmap created 05/06/23 07:58:11.07
May  6 07:58:11.073: INFO: Recreated root ca configmap in namespace "svcaccounts-1984"
May  6 07:58:11.078: INFO: Updated root ca configmap in namespace "svcaccounts-1984"
STEP: waiting for the root ca configmap reconciled 05/06/23 07:58:11.579
May  6 07:58:11.582: INFO: Reconciled root ca configmap in namespace "svcaccounts-1984"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May  6 07:58:11.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1984" for this suite. 05/06/23 07:58:11.585
------------------------------
â€¢ [2.049 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:58:09.541
    May  6 07:58:09.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename svcaccounts 05/06/23 07:58:09.542
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:10.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:10.559
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    May  6 07:58:10.564: INFO: Got root ca configmap in namespace "svcaccounts-1984"
    May  6 07:58:10.569: INFO: Deleted root ca configmap in namespace "svcaccounts-1984"
    STEP: waiting for a new root ca configmap created 05/06/23 07:58:11.07
    May  6 07:58:11.073: INFO: Recreated root ca configmap in namespace "svcaccounts-1984"
    May  6 07:58:11.078: INFO: Updated root ca configmap in namespace "svcaccounts-1984"
    STEP: waiting for the root ca configmap reconciled 05/06/23 07:58:11.579
    May  6 07:58:11.582: INFO: Reconciled root ca configmap in namespace "svcaccounts-1984"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May  6 07:58:11.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1984" for this suite. 05/06/23 07:58:11.585
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:58:11.591
May  6 07:58:11.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename deployment 05/06/23 07:58:11.592
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:12.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:12.612
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
May  6 07:58:12.622: INFO: Pod name rollover-pod: Found 0 pods out of 1
May  6 07:58:17.628: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/06/23 07:58:17.628
May  6 07:58:17.628: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May  6 07:58:19.631: INFO: Creating deployment "test-rollover-deployment"
May  6 07:58:19.639: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May  6 07:58:21.644: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May  6 07:58:21.649: INFO: Ensure that both replica sets have 1 created replica
May  6 07:58:21.653: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May  6 07:58:21.662: INFO: Updating deployment test-rollover-deployment
May  6 07:58:21.662: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May  6 07:58:23.668: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May  6 07:58:23.673: INFO: Make sure deployment "test-rollover-deployment" is complete
May  6 07:58:23.678: INFO: all replica sets need to contain the pod-template-hash label
May  6 07:58:23.678: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 07:58:25.683: INFO: all replica sets need to contain the pod-template-hash label
May  6 07:58:25.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 07:58:27.685: INFO: all replica sets need to contain the pod-template-hash label
May  6 07:58:27.685: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 07:58:29.683: INFO: all replica sets need to contain the pod-template-hash label
May  6 07:58:29.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 07:58:31.684: INFO: all replica sets need to contain the pod-template-hash label
May  6 07:58:31.684: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  6 07:58:33.684: INFO: 
May  6 07:58:33.684: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  6 07:58:33.690: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8934  c6f00376-702f-4100-8064-17976dd43d8f 167375 2 2023-05-06 07:58:19 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-06 07:58:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:58:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00600eb98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-06 07:58:19 +0000 UTC,LastTransitionTime:2023-05-06 07:58:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-05-06 07:58:33 +0000 UTC,LastTransitionTime:2023-05-06 07:58:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  6 07:58:33.693: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8934  80eaaf8f-bba4-4c1a-9501-15f3f581e1c1 167365 2 2023-05-06 07:58:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment c6f00376-702f-4100-8064-17976dd43d8f 0xc0060c6817 0xc0060c6818}] [] [{kube-controller-manager Update apps/v1 2023-05-06 07:58:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6f00376-702f-4100-8064-17976dd43d8f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:58:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0060c68c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  6 07:58:33.693: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May  6 07:58:33.693: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8934  4ae71f48-af7a-4d3b-b5dd-dc342108de35 167374 2 2023-05-06 07:58:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment c6f00376-702f-4100-8064-17976dd43d8f 0xc0060c66e7 0xc0060c66e8}] [] [{e2e.test Update apps/v1 2023-05-06 07:58:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:58:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6f00376-702f-4100-8064-17976dd43d8f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:58:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0060c67a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 07:58:33.693: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8934  2bf59b2f-7cac-41c2-8b7f-67b7a154dcda 167311 2 2023-05-06 07:58:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment c6f00376-702f-4100-8064-17976dd43d8f 0xc0060c6937 0xc0060c6938}] [] [{kube-controller-manager Update apps/v1 2023-05-06 07:58:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6f00376-702f-4100-8064-17976dd43d8f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:58:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0060c69e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 07:58:33.695: INFO: Pod "test-rollover-deployment-6c6df9974f-cqmj8" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-cqmj8 test-rollover-deployment-6c6df9974f- deployment-8934  964eb438-5c8f-4e3c-be32-79c550879389 167326 0 2023-05-06 07:58:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:681011b105f075eaa48fbcfb54ad1a460201ae757ed407084e3e9235dc84d2ee cni.projectcalico.org/podIP:10.244.20.146/32 cni.projectcalico.org/podIPs:10.244.20.146/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 80eaaf8f-bba4-4c1a-9501-15f3f581e1c1 0xc00600ef47 0xc00600ef48}] [] [{kube-controller-manager Update v1 2023-05-06 07:58:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"80eaaf8f-bba4-4c1a-9501-15f3f581e1c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:58:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:58:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.20.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5rjxk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5rjxk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:58:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:58:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:58:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:58:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:10.244.20.146,StartTime:2023-05-06 07:58:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:58:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://04683879eaff629e0d5dfd8476c3a86f113e079af06efeac8c36d72ded4af15a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.20.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May  6 07:58:33.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8934" for this suite. 05/06/23 07:58:33.698
------------------------------
â€¢ [SLOW TEST] [22.113 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:58:11.591
    May  6 07:58:11.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename deployment 05/06/23 07:58:11.592
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:12.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:12.612
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    May  6 07:58:12.622: INFO: Pod name rollover-pod: Found 0 pods out of 1
    May  6 07:58:17.628: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/06/23 07:58:17.628
    May  6 07:58:17.628: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    May  6 07:58:19.631: INFO: Creating deployment "test-rollover-deployment"
    May  6 07:58:19.639: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    May  6 07:58:21.644: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    May  6 07:58:21.649: INFO: Ensure that both replica sets have 1 created replica
    May  6 07:58:21.653: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    May  6 07:58:21.662: INFO: Updating deployment test-rollover-deployment
    May  6 07:58:21.662: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    May  6 07:58:23.668: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    May  6 07:58:23.673: INFO: Make sure deployment "test-rollover-deployment" is complete
    May  6 07:58:23.678: INFO: all replica sets need to contain the pod-template-hash label
    May  6 07:58:23.678: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  6 07:58:25.683: INFO: all replica sets need to contain the pod-template-hash label
    May  6 07:58:25.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  6 07:58:27.685: INFO: all replica sets need to contain the pod-template-hash label
    May  6 07:58:27.685: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  6 07:58:29.683: INFO: all replica sets need to contain the pod-template-hash label
    May  6 07:58:29.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  6 07:58:31.684: INFO: all replica sets need to contain the pod-template-hash label
    May  6 07:58:31.684: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 7, 58, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 7, 58, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  6 07:58:33.684: INFO: 
    May  6 07:58:33.684: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  6 07:58:33.690: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-8934  c6f00376-702f-4100-8064-17976dd43d8f 167375 2 2023-05-06 07:58:19 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-06 07:58:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:58:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00600eb98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-06 07:58:19 +0000 UTC,LastTransitionTime:2023-05-06 07:58:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-05-06 07:58:33 +0000 UTC,LastTransitionTime:2023-05-06 07:58:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  6 07:58:33.693: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8934  80eaaf8f-bba4-4c1a-9501-15f3f581e1c1 167365 2 2023-05-06 07:58:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment c6f00376-702f-4100-8064-17976dd43d8f 0xc0060c6817 0xc0060c6818}] [] [{kube-controller-manager Update apps/v1 2023-05-06 07:58:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6f00376-702f-4100-8064-17976dd43d8f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:58:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0060c68c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  6 07:58:33.693: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    May  6 07:58:33.693: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8934  4ae71f48-af7a-4d3b-b5dd-dc342108de35 167374 2 2023-05-06 07:58:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment c6f00376-702f-4100-8064-17976dd43d8f 0xc0060c66e7 0xc0060c66e8}] [] [{e2e.test Update apps/v1 2023-05-06 07:58:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:58:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6f00376-702f-4100-8064-17976dd43d8f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:58:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0060c67a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  6 07:58:33.693: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8934  2bf59b2f-7cac-41c2-8b7f-67b7a154dcda 167311 2 2023-05-06 07:58:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment c6f00376-702f-4100-8064-17976dd43d8f 0xc0060c6937 0xc0060c6938}] [] [{kube-controller-manager Update apps/v1 2023-05-06 07:58:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6f00376-702f-4100-8064-17976dd43d8f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 07:58:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0060c69e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  6 07:58:33.695: INFO: Pod "test-rollover-deployment-6c6df9974f-cqmj8" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-cqmj8 test-rollover-deployment-6c6df9974f- deployment-8934  964eb438-5c8f-4e3c-be32-79c550879389 167326 0 2023-05-06 07:58:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:681011b105f075eaa48fbcfb54ad1a460201ae757ed407084e3e9235dc84d2ee cni.projectcalico.org/podIP:10.244.20.146/32 cni.projectcalico.org/podIPs:10.244.20.146/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 80eaaf8f-bba4-4c1a-9501-15f3f581e1c1 0xc00600ef47 0xc00600ef48}] [] [{kube-controller-manager Update v1 2023-05-06 07:58:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"80eaaf8f-bba4-4c1a-9501-15f3f581e1c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-06 07:58:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-06 07:58:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.20.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5rjxk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5rjxk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:58:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:58:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:58:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 07:58:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:10.244.20.146,StartTime:2023-05-06 07:58:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 07:58:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://04683879eaff629e0d5dfd8476c3a86f113e079af06efeac8c36d72ded4af15a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.20.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May  6 07:58:33.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8934" for this suite. 05/06/23 07:58:33.698
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:58:33.704
May  6 07:58:33.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename disruption 05/06/23 07:58:33.705
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:34.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:34.72
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:58:34.721
May  6 07:58:34.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename disruption-2 05/06/23 07:58:34.722
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:35.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:35.739
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 05/06/23 07:58:35.745
STEP: Waiting for the pdb to be processed 05/06/23 07:58:37.755
STEP: Waiting for the pdb to be processed 05/06/23 07:58:39.765
STEP: listing a collection of PDBs across all namespaces 05/06/23 07:58:41.772
STEP: listing a collection of PDBs in namespace disruption-8039 05/06/23 07:58:41.774
STEP: deleting a collection of PDBs 05/06/23 07:58:41.776
STEP: Waiting for the PDB collection to be deleted 05/06/23 07:58:41.785
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
May  6 07:58:41.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May  6 07:58:41.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-4050" for this suite. 05/06/23 07:58:41.793
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8039" for this suite. 05/06/23 07:58:41.798
------------------------------
â€¢ [SLOW TEST] [8.100 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:58:33.704
    May  6 07:58:33.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename disruption 05/06/23 07:58:33.705
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:34.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:34.72
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:58:34.721
    May  6 07:58:34.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename disruption-2 05/06/23 07:58:34.722
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:35.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:35.739
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 05/06/23 07:58:35.745
    STEP: Waiting for the pdb to be processed 05/06/23 07:58:37.755
    STEP: Waiting for the pdb to be processed 05/06/23 07:58:39.765
    STEP: listing a collection of PDBs across all namespaces 05/06/23 07:58:41.772
    STEP: listing a collection of PDBs in namespace disruption-8039 05/06/23 07:58:41.774
    STEP: deleting a collection of PDBs 05/06/23 07:58:41.776
    STEP: Waiting for the PDB collection to be deleted 05/06/23 07:58:41.785
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    May  6 07:58:41.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May  6 07:58:41.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-4050" for this suite. 05/06/23 07:58:41.793
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8039" for this suite. 05/06/23 07:58:41.798
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:58:41.804
May  6 07:58:41.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 07:58:41.805
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:42.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:42.819
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-ec214a99-c28c-48db-8517-e653ceb3db73 05/06/23 07:58:42.821
STEP: Creating a pod to test consume secrets 05/06/23 07:58:42.825
May  6 07:58:42.833: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-10a419ca-9716-43c9-b4f5-e1cdddf2958a" in namespace "projected-8672" to be "Succeeded or Failed"
May  6 07:58:42.835: INFO: Pod "pod-projected-secrets-10a419ca-9716-43c9-b4f5-e1cdddf2958a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016281ms
May  6 07:58:44.838: INFO: Pod "pod-projected-secrets-10a419ca-9716-43c9-b4f5-e1cdddf2958a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005243014s
May  6 07:58:46.838: INFO: Pod "pod-projected-secrets-10a419ca-9716-43c9-b4f5-e1cdddf2958a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004811622s
STEP: Saw pod success 05/06/23 07:58:46.838
May  6 07:58:46.838: INFO: Pod "pod-projected-secrets-10a419ca-9716-43c9-b4f5-e1cdddf2958a" satisfied condition "Succeeded or Failed"
May  6 07:58:46.841: INFO: Trying to get logs from node cncf-0 pod pod-projected-secrets-10a419ca-9716-43c9-b4f5-e1cdddf2958a container projected-secret-volume-test: <nil>
STEP: delete the pod 05/06/23 07:58:46.847
May  6 07:58:46.858: INFO: Waiting for pod pod-projected-secrets-10a419ca-9716-43c9-b4f5-e1cdddf2958a to disappear
May  6 07:58:46.860: INFO: Pod pod-projected-secrets-10a419ca-9716-43c9-b4f5-e1cdddf2958a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May  6 07:58:46.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8672" for this suite. 05/06/23 07:58:46.864
------------------------------
â€¢ [SLOW TEST] [5.066 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:58:41.804
    May  6 07:58:41.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 07:58:41.805
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:42.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:42.819
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-ec214a99-c28c-48db-8517-e653ceb3db73 05/06/23 07:58:42.821
    STEP: Creating a pod to test consume secrets 05/06/23 07:58:42.825
    May  6 07:58:42.833: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-10a419ca-9716-43c9-b4f5-e1cdddf2958a" in namespace "projected-8672" to be "Succeeded or Failed"
    May  6 07:58:42.835: INFO: Pod "pod-projected-secrets-10a419ca-9716-43c9-b4f5-e1cdddf2958a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016281ms
    May  6 07:58:44.838: INFO: Pod "pod-projected-secrets-10a419ca-9716-43c9-b4f5-e1cdddf2958a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005243014s
    May  6 07:58:46.838: INFO: Pod "pod-projected-secrets-10a419ca-9716-43c9-b4f5-e1cdddf2958a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004811622s
    STEP: Saw pod success 05/06/23 07:58:46.838
    May  6 07:58:46.838: INFO: Pod "pod-projected-secrets-10a419ca-9716-43c9-b4f5-e1cdddf2958a" satisfied condition "Succeeded or Failed"
    May  6 07:58:46.841: INFO: Trying to get logs from node cncf-0 pod pod-projected-secrets-10a419ca-9716-43c9-b4f5-e1cdddf2958a container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/06/23 07:58:46.847
    May  6 07:58:46.858: INFO: Waiting for pod pod-projected-secrets-10a419ca-9716-43c9-b4f5-e1cdddf2958a to disappear
    May  6 07:58:46.860: INFO: Pod pod-projected-secrets-10a419ca-9716-43c9-b4f5-e1cdddf2958a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May  6 07:58:46.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8672" for this suite. 05/06/23 07:58:46.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:58:46.872
May  6 07:58:46.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename podtemplate 05/06/23 07:58:46.873
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:47.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:47.889
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 05/06/23 07:58:47.89
STEP: Replace a pod template 05/06/23 07:58:47.895
May  6 07:58:47.902: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
May  6 07:58:47.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-7026" for this suite. 05/06/23 07:58:47.905
------------------------------
â€¢ [1.037 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:58:46.872
    May  6 07:58:46.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename podtemplate 05/06/23 07:58:46.873
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:47.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:47.889
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 05/06/23 07:58:47.89
    STEP: Replace a pod template 05/06/23 07:58:47.895
    May  6 07:58:47.902: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    May  6 07:58:47.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-7026" for this suite. 05/06/23 07:58:47.905
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:58:47.91
May  6 07:58:47.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename namespaces 05/06/23 07:58:47.91
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:48.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:48.928
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-cjvql" 05/06/23 07:58:48.93
May  6 07:58:49.944: INFO: Namespace "e2e-ns-cjvql-3490" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-cjvql-3490" 05/06/23 07:58:49.944
May  6 07:58:49.950: INFO: Namespace "e2e-ns-cjvql-3490" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-cjvql-3490" 05/06/23 07:58:49.95
May  6 07:58:49.957: INFO: Namespace "e2e-ns-cjvql-3490" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 07:58:49.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6917" for this suite. 05/06/23 07:58:49.96
STEP: Destroying namespace "e2e-ns-cjvql-3490" for this suite. 05/06/23 07:58:49.965
------------------------------
â€¢ [2.062 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:58:47.91
    May  6 07:58:47.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename namespaces 05/06/23 07:58:47.91
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:48.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:48.928
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-cjvql" 05/06/23 07:58:48.93
    May  6 07:58:49.944: INFO: Namespace "e2e-ns-cjvql-3490" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-cjvql-3490" 05/06/23 07:58:49.944
    May  6 07:58:49.950: INFO: Namespace "e2e-ns-cjvql-3490" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-cjvql-3490" 05/06/23 07:58:49.95
    May  6 07:58:49.957: INFO: Namespace "e2e-ns-cjvql-3490" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 07:58:49.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6917" for this suite. 05/06/23 07:58:49.96
    STEP: Destroying namespace "e2e-ns-cjvql-3490" for this suite. 05/06/23 07:58:49.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:58:49.972
May  6 07:58:49.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename emptydir 05/06/23 07:58:49.973
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:50.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:50.988
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 05/06/23 07:58:50.99
May  6 07:58:50.996: INFO: Waiting up to 5m0s for pod "pod-cd44423d-7281-45d3-9253-fad27aff040c" in namespace "emptydir-5123" to be "Succeeded or Failed"
May  6 07:58:50.998: INFO: Pod "pod-cd44423d-7281-45d3-9253-fad27aff040c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.91044ms
May  6 07:58:53.002: INFO: Pod "pod-cd44423d-7281-45d3-9253-fad27aff040c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005979443s
May  6 07:58:55.003: INFO: Pod "pod-cd44423d-7281-45d3-9253-fad27aff040c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006603046s
STEP: Saw pod success 05/06/23 07:58:55.003
May  6 07:58:55.003: INFO: Pod "pod-cd44423d-7281-45d3-9253-fad27aff040c" satisfied condition "Succeeded or Failed"
May  6 07:58:55.005: INFO: Trying to get logs from node cncf-0 pod pod-cd44423d-7281-45d3-9253-fad27aff040c container test-container: <nil>
STEP: delete the pod 05/06/23 07:58:55.014
May  6 07:58:55.027: INFO: Waiting for pod pod-cd44423d-7281-45d3-9253-fad27aff040c to disappear
May  6 07:58:55.030: INFO: Pod pod-cd44423d-7281-45d3-9253-fad27aff040c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  6 07:58:55.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5123" for this suite. 05/06/23 07:58:55.034
------------------------------
â€¢ [SLOW TEST] [5.067 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:58:49.972
    May  6 07:58:49.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename emptydir 05/06/23 07:58:49.973
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:50.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:50.988
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 05/06/23 07:58:50.99
    May  6 07:58:50.996: INFO: Waiting up to 5m0s for pod "pod-cd44423d-7281-45d3-9253-fad27aff040c" in namespace "emptydir-5123" to be "Succeeded or Failed"
    May  6 07:58:50.998: INFO: Pod "pod-cd44423d-7281-45d3-9253-fad27aff040c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.91044ms
    May  6 07:58:53.002: INFO: Pod "pod-cd44423d-7281-45d3-9253-fad27aff040c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005979443s
    May  6 07:58:55.003: INFO: Pod "pod-cd44423d-7281-45d3-9253-fad27aff040c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006603046s
    STEP: Saw pod success 05/06/23 07:58:55.003
    May  6 07:58:55.003: INFO: Pod "pod-cd44423d-7281-45d3-9253-fad27aff040c" satisfied condition "Succeeded or Failed"
    May  6 07:58:55.005: INFO: Trying to get logs from node cncf-0 pod pod-cd44423d-7281-45d3-9253-fad27aff040c container test-container: <nil>
    STEP: delete the pod 05/06/23 07:58:55.014
    May  6 07:58:55.027: INFO: Waiting for pod pod-cd44423d-7281-45d3-9253-fad27aff040c to disappear
    May  6 07:58:55.030: INFO: Pod pod-cd44423d-7281-45d3-9253-fad27aff040c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  6 07:58:55.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5123" for this suite. 05/06/23 07:58:55.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:58:55.039
May  6 07:58:55.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename webhook 05/06/23 07:58:55.04
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:56.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:56.055
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/06/23 07:58:58.073
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:58:58.383
STEP: Deploying the webhook pod 05/06/23 07:58:58.39
STEP: Wait for the deployment to be ready 05/06/23 07:58:58.399
May  6 07:58:58.405: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/06/23 07:59:00.412
STEP: Verifying the service has paired with the endpoint 05/06/23 07:59:00.423
May  6 07:59:01.424: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 05/06/23 07:59:01.427
STEP: create a pod that should be denied by the webhook 05/06/23 07:59:01.438
STEP: create a pod that causes the webhook to hang 05/06/23 07:59:01.446
STEP: create a configmap that should be denied by the webhook 05/06/23 07:59:11.453
STEP: create a configmap that should be admitted by the webhook 05/06/23 07:59:11.471
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 05/06/23 07:59:11.484
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 05/06/23 07:59:11.491
STEP: create a namespace that bypass the webhook 05/06/23 07:59:11.495
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 05/06/23 07:59:12.504
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 07:59:12.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1914" for this suite. 05/06/23 07:59:12.571
STEP: Destroying namespace "webhook-1914-markers" for this suite. 05/06/23 07:59:12.58
------------------------------
â€¢ [SLOW TEST] [17.551 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:58:55.039
    May  6 07:58:55.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename webhook 05/06/23 07:58:55.04
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:58:56.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:58:56.055
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/06/23 07:58:58.073
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 07:58:58.383
    STEP: Deploying the webhook pod 05/06/23 07:58:58.39
    STEP: Wait for the deployment to be ready 05/06/23 07:58:58.399
    May  6 07:58:58.405: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/06/23 07:59:00.412
    STEP: Verifying the service has paired with the endpoint 05/06/23 07:59:00.423
    May  6 07:59:01.424: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 05/06/23 07:59:01.427
    STEP: create a pod that should be denied by the webhook 05/06/23 07:59:01.438
    STEP: create a pod that causes the webhook to hang 05/06/23 07:59:01.446
    STEP: create a configmap that should be denied by the webhook 05/06/23 07:59:11.453
    STEP: create a configmap that should be admitted by the webhook 05/06/23 07:59:11.471
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 05/06/23 07:59:11.484
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 05/06/23 07:59:11.491
    STEP: create a namespace that bypass the webhook 05/06/23 07:59:11.495
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 05/06/23 07:59:12.504
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 07:59:12.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1914" for this suite. 05/06/23 07:59:12.571
    STEP: Destroying namespace "webhook-1914-markers" for this suite. 05/06/23 07:59:12.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:59:12.593
May  6 07:59:12.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename gc 05/06/23 07:59:12.594
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:59:13.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:59:13.611
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 05/06/23 07:59:13.613
STEP: Wait for the Deployment to create new ReplicaSet 05/06/23 07:59:13.619
STEP: delete the deployment 05/06/23 07:59:14.13
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 05/06/23 07:59:14.141
STEP: Gathering metrics 05/06/23 07:59:14.657
May  6 07:59:14.672: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-2" in namespace "kube-system" to be "running and ready"
May  6 07:59:14.675: INFO: Pod "kube-controller-manager-cncf-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.258524ms
May  6 07:59:14.675: INFO: The phase of Pod kube-controller-manager-cncf-2 is Running (Ready = true)
May  6 07:59:14.675: INFO: Pod "kube-controller-manager-cncf-2" satisfied condition "running and ready"
May  6 07:59:14.711: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May  6 07:59:14.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7640" for this suite. 05/06/23 07:59:14.714
------------------------------
â€¢ [2.127 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:59:12.593
    May  6 07:59:12.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename gc 05/06/23 07:59:12.594
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:59:13.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:59:13.611
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 05/06/23 07:59:13.613
    STEP: Wait for the Deployment to create new ReplicaSet 05/06/23 07:59:13.619
    STEP: delete the deployment 05/06/23 07:59:14.13
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 05/06/23 07:59:14.141
    STEP: Gathering metrics 05/06/23 07:59:14.657
    May  6 07:59:14.672: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-2" in namespace "kube-system" to be "running and ready"
    May  6 07:59:14.675: INFO: Pod "kube-controller-manager-cncf-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.258524ms
    May  6 07:59:14.675: INFO: The phase of Pod kube-controller-manager-cncf-2 is Running (Ready = true)
    May  6 07:59:14.675: INFO: Pod "kube-controller-manager-cncf-2" satisfied condition "running and ready"
    May  6 07:59:14.711: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May  6 07:59:14.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7640" for this suite. 05/06/23 07:59:14.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:59:14.722
May  6 07:59:14.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename resourcequota 05/06/23 07:59:14.722
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:59:15.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:59:15.739
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 05/06/23 07:59:15.741
STEP: Ensuring ResourceQuota status is calculated 05/06/23 07:59:15.746
STEP: Creating a ResourceQuota with not best effort scope 05/06/23 07:59:17.749
STEP: Ensuring ResourceQuota status is calculated 05/06/23 07:59:17.753
STEP: Creating a best-effort pod 05/06/23 07:59:19.757
STEP: Ensuring resource quota with best effort scope captures the pod usage 05/06/23 07:59:19.77
STEP: Ensuring resource quota with not best effort ignored the pod usage 05/06/23 07:59:21.773
STEP: Deleting the pod 05/06/23 07:59:23.776
STEP: Ensuring resource quota status released the pod usage 05/06/23 07:59:23.787
STEP: Creating a not best-effort pod 05/06/23 07:59:25.791
STEP: Ensuring resource quota with not best effort scope captures the pod usage 05/06/23 07:59:25.807
STEP: Ensuring resource quota with best effort scope ignored the pod usage 05/06/23 07:59:27.811
STEP: Deleting the pod 05/06/23 07:59:29.814
STEP: Ensuring resource quota status released the pod usage 05/06/23 07:59:29.825
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  6 07:59:31.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5526" for this suite. 05/06/23 07:59:31.832
------------------------------
â€¢ [SLOW TEST] [17.116 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:59:14.722
    May  6 07:59:14.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename resourcequota 05/06/23 07:59:14.722
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:59:15.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:59:15.739
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 05/06/23 07:59:15.741
    STEP: Ensuring ResourceQuota status is calculated 05/06/23 07:59:15.746
    STEP: Creating a ResourceQuota with not best effort scope 05/06/23 07:59:17.749
    STEP: Ensuring ResourceQuota status is calculated 05/06/23 07:59:17.753
    STEP: Creating a best-effort pod 05/06/23 07:59:19.757
    STEP: Ensuring resource quota with best effort scope captures the pod usage 05/06/23 07:59:19.77
    STEP: Ensuring resource quota with not best effort ignored the pod usage 05/06/23 07:59:21.773
    STEP: Deleting the pod 05/06/23 07:59:23.776
    STEP: Ensuring resource quota status released the pod usage 05/06/23 07:59:23.787
    STEP: Creating a not best-effort pod 05/06/23 07:59:25.791
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 05/06/23 07:59:25.807
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 05/06/23 07:59:27.811
    STEP: Deleting the pod 05/06/23 07:59:29.814
    STEP: Ensuring resource quota status released the pod usage 05/06/23 07:59:29.825
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  6 07:59:31.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5526" for this suite. 05/06/23 07:59:31.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 07:59:31.839
May  6 07:59:31.839: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename container-probe 05/06/23 07:59:31.84
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:59:32.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:59:32.856
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-d4c73cd2-e5be-4170-b65a-45d8c15a6cfa in namespace container-probe-7803 05/06/23 07:59:32.858
May  6 07:59:32.865: INFO: Waiting up to 5m0s for pod "busybox-d4c73cd2-e5be-4170-b65a-45d8c15a6cfa" in namespace "container-probe-7803" to be "not pending"
May  6 07:59:32.867: INFO: Pod "busybox-d4c73cd2-e5be-4170-b65a-45d8c15a6cfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.271506ms
May  6 07:59:34.870: INFO: Pod "busybox-d4c73cd2-e5be-4170-b65a-45d8c15a6cfa": Phase="Running", Reason="", readiness=true. Elapsed: 2.005176338s
May  6 07:59:34.870: INFO: Pod "busybox-d4c73cd2-e5be-4170-b65a-45d8c15a6cfa" satisfied condition "not pending"
May  6 07:59:34.870: INFO: Started pod busybox-d4c73cd2-e5be-4170-b65a-45d8c15a6cfa in namespace container-probe-7803
STEP: checking the pod's current state and verifying that restartCount is present 05/06/23 07:59:34.87
May  6 07:59:34.873: INFO: Initial restart count of pod busybox-d4c73cd2-e5be-4170-b65a-45d8c15a6cfa is 0
STEP: deleting the pod 05/06/23 08:03:35.274
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May  6 08:03:35.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7803" for this suite. 05/06/23 08:03:35.294
------------------------------
â€¢ [SLOW TEST] [243.462 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 07:59:31.839
    May  6 07:59:31.839: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename container-probe 05/06/23 07:59:31.84
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 07:59:32.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 07:59:32.856
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-d4c73cd2-e5be-4170-b65a-45d8c15a6cfa in namespace container-probe-7803 05/06/23 07:59:32.858
    May  6 07:59:32.865: INFO: Waiting up to 5m0s for pod "busybox-d4c73cd2-e5be-4170-b65a-45d8c15a6cfa" in namespace "container-probe-7803" to be "not pending"
    May  6 07:59:32.867: INFO: Pod "busybox-d4c73cd2-e5be-4170-b65a-45d8c15a6cfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.271506ms
    May  6 07:59:34.870: INFO: Pod "busybox-d4c73cd2-e5be-4170-b65a-45d8c15a6cfa": Phase="Running", Reason="", readiness=true. Elapsed: 2.005176338s
    May  6 07:59:34.870: INFO: Pod "busybox-d4c73cd2-e5be-4170-b65a-45d8c15a6cfa" satisfied condition "not pending"
    May  6 07:59:34.870: INFO: Started pod busybox-d4c73cd2-e5be-4170-b65a-45d8c15a6cfa in namespace container-probe-7803
    STEP: checking the pod's current state and verifying that restartCount is present 05/06/23 07:59:34.87
    May  6 07:59:34.873: INFO: Initial restart count of pod busybox-d4c73cd2-e5be-4170-b65a-45d8c15a6cfa is 0
    STEP: deleting the pod 05/06/23 08:03:35.274
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May  6 08:03:35.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7803" for this suite. 05/06/23 08:03:35.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:03:35.302
May  6 08:03:35.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename statefulset 05/06/23 08:03:35.302
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:03:36.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:03:36.32
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8517 05/06/23 08:03:36.322
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 05/06/23 08:03:36.326
STEP: Creating stateful set ss in namespace statefulset-8517 05/06/23 08:03:36.328
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8517 05/06/23 08:03:36.334
May  6 08:03:36.337: INFO: Found 0 stateful pods, waiting for 1
May  6 08:03:46.340: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 05/06/23 08:03:46.34
May  6 08:03:46.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-8517 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 08:03:46.489: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 08:03:46.489: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 08:03:46.489: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 08:03:46.492: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  6 08:03:56.496: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  6 08:03:56.496: INFO: Waiting for statefulset status.replicas updated to 0
May  6 08:03:56.508: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999985s
May  6 08:03:57.511: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997365585s
May  6 08:03:58.515: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994563552s
May  6 08:03:59.520: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.990155666s
May  6 08:04:00.523: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985613494s
May  6 08:04:01.526: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.982388567s
May  6 08:04:02.529: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.979725036s
May  6 08:04:03.532: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.976422048s
May  6 08:04:04.535: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.97312882s
May  6 08:04:05.538: INFO: Verifying statefulset ss doesn't scale past 1 for another 970.45042ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8517 05/06/23 08:04:06.538
May  6 08:04:06.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-8517 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 08:04:06.654: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 08:04:06.654: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 08:04:06.654: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 08:04:06.658: INFO: Found 1 stateful pods, waiting for 3
May  6 08:04:16.662: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  6 08:04:16.662: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  6 08:04:16.662: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 05/06/23 08:04:16.662
STEP: Scale down will halt with unhealthy stateful pod 05/06/23 08:04:16.662
May  6 08:04:16.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-8517 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 08:04:16.782: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 08:04:16.782: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 08:04:16.782: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 08:04:16.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-8517 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 08:04:16.892: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 08:04:16.892: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 08:04:16.892: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 08:04:16.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-8517 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 08:04:16.993: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 08:04:16.993: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 08:04:16.993: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 08:04:16.993: INFO: Waiting for statefulset status.replicas updated to 0
May  6 08:04:16.995: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May  6 08:04:27.001: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  6 08:04:27.001: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  6 08:04:27.001: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  6 08:04:27.011: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999851s
May  6 08:04:28.014: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997288075s
May  6 08:04:29.018: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993713829s
May  6 08:04:30.022: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988851778s
May  6 08:04:31.025: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985620744s
May  6 08:04:32.028: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982377158s
May  6 08:04:33.031: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.979412872s
May  6 08:04:34.036: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974755489s
May  6 08:04:35.039: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.971428684s
May  6 08:04:36.042: INFO: Verifying statefulset ss doesn't scale past 3 for another 968.296157ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8517 05/06/23 08:04:37.043
May  6 08:04:37.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-8517 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 08:04:37.145: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 08:04:37.145: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 08:04:37.145: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 08:04:37.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-8517 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 08:04:37.245: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 08:04:37.245: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 08:04:37.246: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 08:04:37.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-8517 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 08:04:37.343: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 08:04:37.343: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 08:04:37.343: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 08:04:37.343: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 05/06/23 08:04:47.355
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May  6 08:04:47.355: INFO: Deleting all statefulset in ns statefulset-8517
May  6 08:04:47.357: INFO: Scaling statefulset ss to 0
May  6 08:04:47.366: INFO: Waiting for statefulset status.replicas updated to 0
May  6 08:04:47.368: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May  6 08:04:47.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8517" for this suite. 05/06/23 08:04:47.383
------------------------------
â€¢ [SLOW TEST] [72.089 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:03:35.302
    May  6 08:03:35.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename statefulset 05/06/23 08:03:35.302
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:03:36.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:03:36.32
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8517 05/06/23 08:03:36.322
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 05/06/23 08:03:36.326
    STEP: Creating stateful set ss in namespace statefulset-8517 05/06/23 08:03:36.328
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8517 05/06/23 08:03:36.334
    May  6 08:03:36.337: INFO: Found 0 stateful pods, waiting for 1
    May  6 08:03:46.340: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 05/06/23 08:03:46.34
    May  6 08:03:46.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-8517 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  6 08:03:46.489: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  6 08:03:46.489: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  6 08:03:46.489: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  6 08:03:46.492: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    May  6 08:03:56.496: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  6 08:03:56.496: INFO: Waiting for statefulset status.replicas updated to 0
    May  6 08:03:56.508: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999985s
    May  6 08:03:57.511: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997365585s
    May  6 08:03:58.515: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994563552s
    May  6 08:03:59.520: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.990155666s
    May  6 08:04:00.523: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985613494s
    May  6 08:04:01.526: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.982388567s
    May  6 08:04:02.529: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.979725036s
    May  6 08:04:03.532: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.976422048s
    May  6 08:04:04.535: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.97312882s
    May  6 08:04:05.538: INFO: Verifying statefulset ss doesn't scale past 1 for another 970.45042ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8517 05/06/23 08:04:06.538
    May  6 08:04:06.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-8517 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  6 08:04:06.654: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  6 08:04:06.654: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  6 08:04:06.654: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  6 08:04:06.658: INFO: Found 1 stateful pods, waiting for 3
    May  6 08:04:16.662: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May  6 08:04:16.662: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    May  6 08:04:16.662: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 05/06/23 08:04:16.662
    STEP: Scale down will halt with unhealthy stateful pod 05/06/23 08:04:16.662
    May  6 08:04:16.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-8517 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  6 08:04:16.782: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  6 08:04:16.782: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  6 08:04:16.782: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  6 08:04:16.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-8517 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  6 08:04:16.892: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  6 08:04:16.892: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  6 08:04:16.892: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  6 08:04:16.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-8517 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  6 08:04:16.993: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  6 08:04:16.993: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  6 08:04:16.993: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  6 08:04:16.993: INFO: Waiting for statefulset status.replicas updated to 0
    May  6 08:04:16.995: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    May  6 08:04:27.001: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  6 08:04:27.001: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    May  6 08:04:27.001: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    May  6 08:04:27.011: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999851s
    May  6 08:04:28.014: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997288075s
    May  6 08:04:29.018: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993713829s
    May  6 08:04:30.022: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988851778s
    May  6 08:04:31.025: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985620744s
    May  6 08:04:32.028: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982377158s
    May  6 08:04:33.031: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.979412872s
    May  6 08:04:34.036: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974755489s
    May  6 08:04:35.039: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.971428684s
    May  6 08:04:36.042: INFO: Verifying statefulset ss doesn't scale past 3 for another 968.296157ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8517 05/06/23 08:04:37.043
    May  6 08:04:37.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-8517 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  6 08:04:37.145: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  6 08:04:37.145: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  6 08:04:37.145: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  6 08:04:37.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-8517 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  6 08:04:37.245: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  6 08:04:37.245: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  6 08:04:37.246: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  6 08:04:37.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-8517 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  6 08:04:37.343: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  6 08:04:37.343: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  6 08:04:37.343: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  6 08:04:37.343: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 05/06/23 08:04:47.355
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May  6 08:04:47.355: INFO: Deleting all statefulset in ns statefulset-8517
    May  6 08:04:47.357: INFO: Scaling statefulset ss to 0
    May  6 08:04:47.366: INFO: Waiting for statefulset status.replicas updated to 0
    May  6 08:04:47.368: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May  6 08:04:47.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8517" for this suite. 05/06/23 08:04:47.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:04:47.391
May  6 08:04:47.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 08:04:47.391
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:04:48.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:04:48.405
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 05/06/23 08:04:48.408
May  6 08:04:48.415: INFO: Waiting up to 5m0s for pod "labelsupdate9e4ca65d-00d9-4a78-94ad-3b472c49f006" in namespace "projected-711" to be "running and ready"
May  6 08:04:48.418: INFO: Pod "labelsupdate9e4ca65d-00d9-4a78-94ad-3b472c49f006": Phase="Pending", Reason="", readiness=false. Elapsed: 3.076126ms
May  6 08:04:48.418: INFO: The phase of Pod labelsupdate9e4ca65d-00d9-4a78-94ad-3b472c49f006 is Pending, waiting for it to be Running (with Ready = true)
May  6 08:04:50.420: INFO: Pod "labelsupdate9e4ca65d-00d9-4a78-94ad-3b472c49f006": Phase="Running", Reason="", readiness=true. Elapsed: 2.005515026s
May  6 08:04:50.420: INFO: The phase of Pod labelsupdate9e4ca65d-00d9-4a78-94ad-3b472c49f006 is Running (Ready = true)
May  6 08:04:50.420: INFO: Pod "labelsupdate9e4ca65d-00d9-4a78-94ad-3b472c49f006" satisfied condition "running and ready"
May  6 08:04:50.947: INFO: Successfully updated pod "labelsupdate9e4ca65d-00d9-4a78-94ad-3b472c49f006"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  6 08:04:54.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-711" for this suite. 05/06/23 08:04:54.968
------------------------------
â€¢ [SLOW TEST] [7.583 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:04:47.391
    May  6 08:04:47.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 08:04:47.391
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:04:48.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:04:48.405
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 05/06/23 08:04:48.408
    May  6 08:04:48.415: INFO: Waiting up to 5m0s for pod "labelsupdate9e4ca65d-00d9-4a78-94ad-3b472c49f006" in namespace "projected-711" to be "running and ready"
    May  6 08:04:48.418: INFO: Pod "labelsupdate9e4ca65d-00d9-4a78-94ad-3b472c49f006": Phase="Pending", Reason="", readiness=false. Elapsed: 3.076126ms
    May  6 08:04:48.418: INFO: The phase of Pod labelsupdate9e4ca65d-00d9-4a78-94ad-3b472c49f006 is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:04:50.420: INFO: Pod "labelsupdate9e4ca65d-00d9-4a78-94ad-3b472c49f006": Phase="Running", Reason="", readiness=true. Elapsed: 2.005515026s
    May  6 08:04:50.420: INFO: The phase of Pod labelsupdate9e4ca65d-00d9-4a78-94ad-3b472c49f006 is Running (Ready = true)
    May  6 08:04:50.420: INFO: Pod "labelsupdate9e4ca65d-00d9-4a78-94ad-3b472c49f006" satisfied condition "running and ready"
    May  6 08:04:50.947: INFO: Successfully updated pod "labelsupdate9e4ca65d-00d9-4a78-94ad-3b472c49f006"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  6 08:04:54.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-711" for this suite. 05/06/23 08:04:54.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:04:54.974
May  6 08:04:54.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename webhook 05/06/23 08:04:54.975
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:04:55.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:04:55.991
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/06/23 08:04:58.009
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 08:04:58.187
STEP: Deploying the webhook pod 05/06/23 08:04:58.195
STEP: Wait for the deployment to be ready 05/06/23 08:04:58.213
May  6 08:04:58.220: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/06/23 08:05:00.229
STEP: Verifying the service has paired with the endpoint 05/06/23 08:05:00.242
May  6 08:05:01.243: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 05/06/23 08:05:01.246
STEP: create a namespace for the webhook 05/06/23 08:05:02.276
STEP: create a configmap should be unconditionally rejected by the webhook 05/06/23 08:05:03.285
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 08:05:04.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1406" for this suite. 05/06/23 08:05:04.373
STEP: Destroying namespace "webhook-1406-markers" for this suite. 05/06/23 08:05:04.379
------------------------------
â€¢ [SLOW TEST] [9.419 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:04:54.974
    May  6 08:04:54.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename webhook 05/06/23 08:04:54.975
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:04:55.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:04:55.991
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/06/23 08:04:58.009
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 08:04:58.187
    STEP: Deploying the webhook pod 05/06/23 08:04:58.195
    STEP: Wait for the deployment to be ready 05/06/23 08:04:58.213
    May  6 08:04:58.220: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/06/23 08:05:00.229
    STEP: Verifying the service has paired with the endpoint 05/06/23 08:05:00.242
    May  6 08:05:01.243: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 05/06/23 08:05:01.246
    STEP: create a namespace for the webhook 05/06/23 08:05:02.276
    STEP: create a configmap should be unconditionally rejected by the webhook 05/06/23 08:05:03.285
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 08:05:04.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1406" for this suite. 05/06/23 08:05:04.373
    STEP: Destroying namespace "webhook-1406-markers" for this suite. 05/06/23 08:05:04.379
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:05:04.394
May  6 08:05:04.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename lease-test 05/06/23 08:05:04.395
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:05:05.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:05:05.411
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
May  6 08:05:05.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-6420" for this suite. 05/06/23 08:05:05.458
------------------------------
â€¢ [1.069 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:05:04.394
    May  6 08:05:04.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename lease-test 05/06/23 08:05:04.395
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:05:05.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:05:05.411
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    May  6 08:05:05.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-6420" for this suite. 05/06/23 08:05:05.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:05:05.464
May  6 08:05:05.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename configmap 05/06/23 08:05:05.465
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:05:06.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:05:06.482
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 05/06/23 08:05:06.483
STEP: fetching the ConfigMap 05/06/23 08:05:06.487
STEP: patching the ConfigMap 05/06/23 08:05:06.489
STEP: listing all ConfigMaps in all namespaces with a label selector 05/06/23 08:05:06.493
STEP: deleting the ConfigMap by collection with a label selector 05/06/23 08:05:06.496
STEP: listing all ConfigMaps in test namespace 05/06/23 08:05:06.502
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  6 08:05:06.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7738" for this suite. 05/06/23 08:05:06.507
------------------------------
â€¢ [1.049 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:05:05.464
    May  6 08:05:05.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename configmap 05/06/23 08:05:05.465
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:05:06.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:05:06.482
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 05/06/23 08:05:06.483
    STEP: fetching the ConfigMap 05/06/23 08:05:06.487
    STEP: patching the ConfigMap 05/06/23 08:05:06.489
    STEP: listing all ConfigMaps in all namespaces with a label selector 05/06/23 08:05:06.493
    STEP: deleting the ConfigMap by collection with a label selector 05/06/23 08:05:06.496
    STEP: listing all ConfigMaps in test namespace 05/06/23 08:05:06.502
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  6 08:05:06.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7738" for this suite. 05/06/23 08:05:06.507
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:05:06.513
May  6 08:05:06.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename watch 05/06/23 08:05:06.514
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:05:07.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:05:07.531
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 05/06/23 08:05:07.533
STEP: starting a background goroutine to produce watch events 05/06/23 08:05:07.536
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 05/06/23 08:05:07.536
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May  6 08:05:10.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6961" for this suite. 05/06/23 08:05:10.326
------------------------------
â€¢ [3.866 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:05:06.513
    May  6 08:05:06.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename watch 05/06/23 08:05:06.514
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:05:07.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:05:07.531
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 05/06/23 08:05:07.533
    STEP: starting a background goroutine to produce watch events 05/06/23 08:05:07.536
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 05/06/23 08:05:07.536
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May  6 08:05:10.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6961" for this suite. 05/06/23 08:05:10.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:05:10.38
May  6 08:05:10.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename cronjob 05/06/23 08:05:10.381
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:05:11.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:05:11.396
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 05/06/23 08:05:11.398
STEP: Ensuring a job is scheduled 05/06/23 08:05:11.405
STEP: Ensuring exactly one is scheduled 05/06/23 08:06:01.408
STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/06/23 08:06:01.411
STEP: Ensuring no more jobs are scheduled 05/06/23 08:06:01.414
STEP: Removing cronjob 05/06/23 08:11:01.421
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May  6 08:11:01.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2670" for this suite. 05/06/23 08:11:01.429
------------------------------
â€¢ [SLOW TEST] [351.058 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:05:10.38
    May  6 08:05:10.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename cronjob 05/06/23 08:05:10.381
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:05:11.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:05:11.396
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 05/06/23 08:05:11.398
    STEP: Ensuring a job is scheduled 05/06/23 08:05:11.405
    STEP: Ensuring exactly one is scheduled 05/06/23 08:06:01.408
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/06/23 08:06:01.411
    STEP: Ensuring no more jobs are scheduled 05/06/23 08:06:01.414
    STEP: Removing cronjob 05/06/23 08:11:01.421
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May  6 08:11:01.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2670" for this suite. 05/06/23 08:11:01.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:11:01.439
May  6 08:11:01.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename container-lifecycle-hook 05/06/23 08:11:01.44
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:11:02.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:11:02.456
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 05/06/23 08:11:02.462
May  6 08:11:02.469: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2453" to be "running and ready"
May  6 08:11:02.472: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.470866ms
May  6 08:11:02.472: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  6 08:11:04.475: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005773447s
May  6 08:11:04.475: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  6 08:11:04.475: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 05/06/23 08:11:04.478
May  6 08:11:04.484: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-2453" to be "running and ready"
May  6 08:11:04.488: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.3097ms
May  6 08:11:04.488: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May  6 08:11:06.491: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006493418s
May  6 08:11:06.491: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
May  6 08:11:06.491: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 05/06/23 08:11:06.494
May  6 08:11:06.499: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  6 08:11:06.502: INFO: Pod pod-with-prestop-exec-hook still exists
May  6 08:11:08.502: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  6 08:11:08.505: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 05/06/23 08:11:08.505
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
May  6 08:11:08.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2453" for this suite. 05/06/23 08:11:08.544
------------------------------
â€¢ [SLOW TEST] [7.111 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:11:01.439
    May  6 08:11:01.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/06/23 08:11:01.44
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:11:02.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:11:02.456
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 05/06/23 08:11:02.462
    May  6 08:11:02.469: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2453" to be "running and ready"
    May  6 08:11:02.472: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.470866ms
    May  6 08:11:02.472: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:11:04.475: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005773447s
    May  6 08:11:04.475: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  6 08:11:04.475: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 05/06/23 08:11:04.478
    May  6 08:11:04.484: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-2453" to be "running and ready"
    May  6 08:11:04.488: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.3097ms
    May  6 08:11:04.488: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:11:06.491: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006493418s
    May  6 08:11:06.491: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    May  6 08:11:06.491: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 05/06/23 08:11:06.494
    May  6 08:11:06.499: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May  6 08:11:06.502: INFO: Pod pod-with-prestop-exec-hook still exists
    May  6 08:11:08.502: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May  6 08:11:08.505: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 05/06/23 08:11:08.505
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    May  6 08:11:08.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2453" for this suite. 05/06/23 08:11:08.544
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:11:08.55
May  6 08:11:08.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename var-expansion 05/06/23 08:11:08.551
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:11:09.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:11:09.566
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 05/06/23 08:11:09.568
May  6 08:11:09.578: INFO: Waiting up to 5m0s for pod "var-expansion-335eb7d5-da68-480e-bbb7-b8d3c2a29834" in namespace "var-expansion-257" to be "Succeeded or Failed"
May  6 08:11:09.580: INFO: Pod "var-expansion-335eb7d5-da68-480e-bbb7-b8d3c2a29834": Phase="Pending", Reason="", readiness=false. Elapsed: 1.991134ms
May  6 08:11:11.583: INFO: Pod "var-expansion-335eb7d5-da68-480e-bbb7-b8d3c2a29834": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005219085s
STEP: Saw pod success 05/06/23 08:11:11.583
May  6 08:11:11.583: INFO: Pod "var-expansion-335eb7d5-da68-480e-bbb7-b8d3c2a29834" satisfied condition "Succeeded or Failed"
May  6 08:11:11.586: INFO: Trying to get logs from node cncf-0 pod var-expansion-335eb7d5-da68-480e-bbb7-b8d3c2a29834 container dapi-container: <nil>
STEP: delete the pod 05/06/23 08:11:11.598
May  6 08:11:11.610: INFO: Waiting for pod var-expansion-335eb7d5-da68-480e-bbb7-b8d3c2a29834 to disappear
May  6 08:11:11.613: INFO: Pod var-expansion-335eb7d5-da68-480e-bbb7-b8d3c2a29834 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May  6 08:11:11.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-257" for this suite. 05/06/23 08:11:11.616
------------------------------
â€¢ [3.071 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:11:08.55
    May  6 08:11:08.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename var-expansion 05/06/23 08:11:08.551
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:11:09.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:11:09.566
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 05/06/23 08:11:09.568
    May  6 08:11:09.578: INFO: Waiting up to 5m0s for pod "var-expansion-335eb7d5-da68-480e-bbb7-b8d3c2a29834" in namespace "var-expansion-257" to be "Succeeded or Failed"
    May  6 08:11:09.580: INFO: Pod "var-expansion-335eb7d5-da68-480e-bbb7-b8d3c2a29834": Phase="Pending", Reason="", readiness=false. Elapsed: 1.991134ms
    May  6 08:11:11.583: INFO: Pod "var-expansion-335eb7d5-da68-480e-bbb7-b8d3c2a29834": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005219085s
    STEP: Saw pod success 05/06/23 08:11:11.583
    May  6 08:11:11.583: INFO: Pod "var-expansion-335eb7d5-da68-480e-bbb7-b8d3c2a29834" satisfied condition "Succeeded or Failed"
    May  6 08:11:11.586: INFO: Trying to get logs from node cncf-0 pod var-expansion-335eb7d5-da68-480e-bbb7-b8d3c2a29834 container dapi-container: <nil>
    STEP: delete the pod 05/06/23 08:11:11.598
    May  6 08:11:11.610: INFO: Waiting for pod var-expansion-335eb7d5-da68-480e-bbb7-b8d3c2a29834 to disappear
    May  6 08:11:11.613: INFO: Pod var-expansion-335eb7d5-da68-480e-bbb7-b8d3c2a29834 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May  6 08:11:11.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-257" for this suite. 05/06/23 08:11:11.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:11:11.621
May  6 08:11:11.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename container-lifecycle-hook 05/06/23 08:11:11.622
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:11:12.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:11:12.642
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 05/06/23 08:11:12.647
May  6 08:11:12.655: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9718" to be "running and ready"
May  6 08:11:12.658: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.042431ms
May  6 08:11:12.658: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  6 08:11:14.662: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007021408s
May  6 08:11:14.662: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  6 08:11:14.662: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 05/06/23 08:11:14.664
May  6 08:11:14.669: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9718" to be "running and ready"
May  6 08:11:14.673: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.138886ms
May  6 08:11:14.673: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May  6 08:11:16.677: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007422883s
May  6 08:11:16.677: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
May  6 08:11:16.677: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 05/06/23 08:11:16.679
STEP: delete the pod with lifecycle hook 05/06/23 08:11:16.684
May  6 08:11:16.692: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  6 08:11:16.694: INFO: Pod pod-with-poststart-exec-hook still exists
May  6 08:11:18.695: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  6 08:11:18.698: INFO: Pod pod-with-poststart-exec-hook still exists
May  6 08:11:20.695: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  6 08:11:20.698: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
May  6 08:11:20.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9718" for this suite. 05/06/23 08:11:20.701
------------------------------
â€¢ [SLOW TEST] [9.085 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:11:11.621
    May  6 08:11:11.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/06/23 08:11:11.622
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:11:12.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:11:12.642
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 05/06/23 08:11:12.647
    May  6 08:11:12.655: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9718" to be "running and ready"
    May  6 08:11:12.658: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.042431ms
    May  6 08:11:12.658: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:11:14.662: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007021408s
    May  6 08:11:14.662: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  6 08:11:14.662: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 05/06/23 08:11:14.664
    May  6 08:11:14.669: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9718" to be "running and ready"
    May  6 08:11:14.673: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.138886ms
    May  6 08:11:14.673: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:11:16.677: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007422883s
    May  6 08:11:16.677: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    May  6 08:11:16.677: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 05/06/23 08:11:16.679
    STEP: delete the pod with lifecycle hook 05/06/23 08:11:16.684
    May  6 08:11:16.692: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May  6 08:11:16.694: INFO: Pod pod-with-poststart-exec-hook still exists
    May  6 08:11:18.695: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May  6 08:11:18.698: INFO: Pod pod-with-poststart-exec-hook still exists
    May  6 08:11:20.695: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May  6 08:11:20.698: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    May  6 08:11:20.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9718" for this suite. 05/06/23 08:11:20.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:11:20.707
May  6 08:11:20.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename resourcequota 05/06/23 08:11:20.708
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:11:21.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:11:21.722
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 05/06/23 08:11:21.724
STEP: Ensuring ResourceQuota status is calculated 05/06/23 08:11:21.729
STEP: Creating a ResourceQuota with not terminating scope 05/06/23 08:11:23.732
STEP: Ensuring ResourceQuota status is calculated 05/06/23 08:11:23.738
STEP: Creating a long running pod 05/06/23 08:11:25.74
STEP: Ensuring resource quota with not terminating scope captures the pod usage 05/06/23 08:11:25.755
STEP: Ensuring resource quota with terminating scope ignored the pod usage 05/06/23 08:11:27.758
STEP: Deleting the pod 05/06/23 08:11:29.761
STEP: Ensuring resource quota status released the pod usage 05/06/23 08:11:29.772
STEP: Creating a terminating pod 05/06/23 08:11:31.776
STEP: Ensuring resource quota with terminating scope captures the pod usage 05/06/23 08:11:31.784
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 05/06/23 08:11:33.788
STEP: Deleting the pod 05/06/23 08:11:35.791
STEP: Ensuring resource quota status released the pod usage 05/06/23 08:11:35.801
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  6 08:11:37.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2106" for this suite. 05/06/23 08:11:37.808
------------------------------
â€¢ [SLOW TEST] [17.107 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:11:20.707
    May  6 08:11:20.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename resourcequota 05/06/23 08:11:20.708
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:11:21.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:11:21.722
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 05/06/23 08:11:21.724
    STEP: Ensuring ResourceQuota status is calculated 05/06/23 08:11:21.729
    STEP: Creating a ResourceQuota with not terminating scope 05/06/23 08:11:23.732
    STEP: Ensuring ResourceQuota status is calculated 05/06/23 08:11:23.738
    STEP: Creating a long running pod 05/06/23 08:11:25.74
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 05/06/23 08:11:25.755
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 05/06/23 08:11:27.758
    STEP: Deleting the pod 05/06/23 08:11:29.761
    STEP: Ensuring resource quota status released the pod usage 05/06/23 08:11:29.772
    STEP: Creating a terminating pod 05/06/23 08:11:31.776
    STEP: Ensuring resource quota with terminating scope captures the pod usage 05/06/23 08:11:31.784
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 05/06/23 08:11:33.788
    STEP: Deleting the pod 05/06/23 08:11:35.791
    STEP: Ensuring resource quota status released the pod usage 05/06/23 08:11:35.801
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  6 08:11:37.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2106" for this suite. 05/06/23 08:11:37.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:11:37.816
May  6 08:11:37.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename replication-controller 05/06/23 08:11:37.816
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:11:38.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:11:38.836
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 05/06/23 08:11:38.837
STEP: When the matched label of one of its pods change 05/06/23 08:11:38.841
May  6 08:11:38.844: INFO: Pod name pod-release: Found 0 pods out of 1
May  6 08:11:43.848: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 05/06/23 08:11:43.858
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May  6 08:11:44.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-120" for this suite. 05/06/23 08:11:44.869
------------------------------
â€¢ [SLOW TEST] [7.059 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:11:37.816
    May  6 08:11:37.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename replication-controller 05/06/23 08:11:37.816
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:11:38.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:11:38.836
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 05/06/23 08:11:38.837
    STEP: When the matched label of one of its pods change 05/06/23 08:11:38.841
    May  6 08:11:38.844: INFO: Pod name pod-release: Found 0 pods out of 1
    May  6 08:11:43.848: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 05/06/23 08:11:43.858
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May  6 08:11:44.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-120" for this suite. 05/06/23 08:11:44.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:11:44.875
May  6 08:11:44.875: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename downward-api 05/06/23 08:11:44.876
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:11:45.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:11:45.892
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 05/06/23 08:11:45.894
May  6 08:11:45.901: INFO: Waiting up to 5m0s for pod "downward-api-2fdb0ed8-f3ae-424c-8a34-d76786a7c11a" in namespace "downward-api-2548" to be "Succeeded or Failed"
May  6 08:11:45.903: INFO: Pod "downward-api-2fdb0ed8-f3ae-424c-8a34-d76786a7c11a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.461297ms
May  6 08:11:47.907: INFO: Pod "downward-api-2fdb0ed8-f3ae-424c-8a34-d76786a7c11a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006086677s
May  6 08:11:49.908: INFO: Pod "downward-api-2fdb0ed8-f3ae-424c-8a34-d76786a7c11a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007082721s
STEP: Saw pod success 05/06/23 08:11:49.908
May  6 08:11:49.908: INFO: Pod "downward-api-2fdb0ed8-f3ae-424c-8a34-d76786a7c11a" satisfied condition "Succeeded or Failed"
May  6 08:11:49.913: INFO: Trying to get logs from node cncf-3 pod downward-api-2fdb0ed8-f3ae-424c-8a34-d76786a7c11a container dapi-container: <nil>
STEP: delete the pod 05/06/23 08:11:49.928
May  6 08:11:49.941: INFO: Waiting for pod downward-api-2fdb0ed8-f3ae-424c-8a34-d76786a7c11a to disappear
May  6 08:11:49.943: INFO: Pod downward-api-2fdb0ed8-f3ae-424c-8a34-d76786a7c11a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May  6 08:11:49.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2548" for this suite. 05/06/23 08:11:49.946
------------------------------
â€¢ [SLOW TEST] [5.080 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:11:44.875
    May  6 08:11:44.875: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename downward-api 05/06/23 08:11:44.876
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:11:45.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:11:45.892
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 05/06/23 08:11:45.894
    May  6 08:11:45.901: INFO: Waiting up to 5m0s for pod "downward-api-2fdb0ed8-f3ae-424c-8a34-d76786a7c11a" in namespace "downward-api-2548" to be "Succeeded or Failed"
    May  6 08:11:45.903: INFO: Pod "downward-api-2fdb0ed8-f3ae-424c-8a34-d76786a7c11a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.461297ms
    May  6 08:11:47.907: INFO: Pod "downward-api-2fdb0ed8-f3ae-424c-8a34-d76786a7c11a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006086677s
    May  6 08:11:49.908: INFO: Pod "downward-api-2fdb0ed8-f3ae-424c-8a34-d76786a7c11a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007082721s
    STEP: Saw pod success 05/06/23 08:11:49.908
    May  6 08:11:49.908: INFO: Pod "downward-api-2fdb0ed8-f3ae-424c-8a34-d76786a7c11a" satisfied condition "Succeeded or Failed"
    May  6 08:11:49.913: INFO: Trying to get logs from node cncf-3 pod downward-api-2fdb0ed8-f3ae-424c-8a34-d76786a7c11a container dapi-container: <nil>
    STEP: delete the pod 05/06/23 08:11:49.928
    May  6 08:11:49.941: INFO: Waiting for pod downward-api-2fdb0ed8-f3ae-424c-8a34-d76786a7c11a to disappear
    May  6 08:11:49.943: INFO: Pod downward-api-2fdb0ed8-f3ae-424c-8a34-d76786a7c11a no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May  6 08:11:49.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2548" for this suite. 05/06/23 08:11:49.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:11:49.956
May  6 08:11:49.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename replicaset 05/06/23 08:11:49.957
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:11:50.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:11:50.974
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
May  6 08:11:50.976: INFO: Creating ReplicaSet my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e
May  6 08:11:50.989: INFO: Pod name my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e: Found 0 pods out of 1
May  6 08:11:55.992: INFO: Pod name my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e: Found 1 pods out of 1
May  6 08:11:55.992: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e" is running
May  6 08:11:55.992: INFO: Waiting up to 5m0s for pod "my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e-gdfqd" in namespace "replicaset-9773" to be "running"
May  6 08:11:55.993: INFO: Pod "my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e-gdfqd": Phase="Running", Reason="", readiness=true. Elapsed: 1.901474ms
May  6 08:11:55.993: INFO: Pod "my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e-gdfqd" satisfied condition "running"
May  6 08:11:55.994: INFO: Pod "my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e-gdfqd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-06 08:11:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-06 08:11:52 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-06 08:11:52 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-06 08:11:50 +0000 UTC Reason: Message:}])
May  6 08:11:55.994: INFO: Trying to dial the pod
May  6 08:12:01.003: INFO: Controller my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e: Got expected result from replica 1 [my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e-gdfqd]: "my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e-gdfqd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May  6 08:12:01.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9773" for this suite. 05/06/23 08:12:01.007
------------------------------
â€¢ [SLOW TEST] [11.056 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:11:49.956
    May  6 08:11:49.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename replicaset 05/06/23 08:11:49.957
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:11:50.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:11:50.974
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    May  6 08:11:50.976: INFO: Creating ReplicaSet my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e
    May  6 08:11:50.989: INFO: Pod name my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e: Found 0 pods out of 1
    May  6 08:11:55.992: INFO: Pod name my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e: Found 1 pods out of 1
    May  6 08:11:55.992: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e" is running
    May  6 08:11:55.992: INFO: Waiting up to 5m0s for pod "my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e-gdfqd" in namespace "replicaset-9773" to be "running"
    May  6 08:11:55.993: INFO: Pod "my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e-gdfqd": Phase="Running", Reason="", readiness=true. Elapsed: 1.901474ms
    May  6 08:11:55.993: INFO: Pod "my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e-gdfqd" satisfied condition "running"
    May  6 08:11:55.994: INFO: Pod "my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e-gdfqd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-06 08:11:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-06 08:11:52 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-06 08:11:52 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-06 08:11:50 +0000 UTC Reason: Message:}])
    May  6 08:11:55.994: INFO: Trying to dial the pod
    May  6 08:12:01.003: INFO: Controller my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e: Got expected result from replica 1 [my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e-gdfqd]: "my-hostname-basic-33c3e5f1-f550-4a78-a44c-9154a581b54e-gdfqd", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May  6 08:12:01.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9773" for this suite. 05/06/23 08:12:01.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:12:01.013
May  6 08:12:01.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename configmap 05/06/23 08:12:01.014
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:02.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:02.032
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-1344/configmap-test-51a74474-4630-4743-a3f3-18c465e34720 05/06/23 08:12:02.034
STEP: Creating a pod to test consume configMaps 05/06/23 08:12:02.037
May  6 08:12:02.044: INFO: Waiting up to 5m0s for pod "pod-configmaps-7b3bc3a7-6a4b-4e45-bb26-04bc29f72987" in namespace "configmap-1344" to be "Succeeded or Failed"
May  6 08:12:02.046: INFO: Pod "pod-configmaps-7b3bc3a7-6a4b-4e45-bb26-04bc29f72987": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059063ms
May  6 08:12:04.050: INFO: Pod "pod-configmaps-7b3bc3a7-6a4b-4e45-bb26-04bc29f72987": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005688791s
May  6 08:12:06.049: INFO: Pod "pod-configmaps-7b3bc3a7-6a4b-4e45-bb26-04bc29f72987": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005080615s
STEP: Saw pod success 05/06/23 08:12:06.049
May  6 08:12:06.049: INFO: Pod "pod-configmaps-7b3bc3a7-6a4b-4e45-bb26-04bc29f72987" satisfied condition "Succeeded or Failed"
May  6 08:12:06.052: INFO: Trying to get logs from node cncf-2 pod pod-configmaps-7b3bc3a7-6a4b-4e45-bb26-04bc29f72987 container env-test: <nil>
STEP: delete the pod 05/06/23 08:12:06.065
May  6 08:12:06.079: INFO: Waiting for pod pod-configmaps-7b3bc3a7-6a4b-4e45-bb26-04bc29f72987 to disappear
May  6 08:12:06.081: INFO: Pod pod-configmaps-7b3bc3a7-6a4b-4e45-bb26-04bc29f72987 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  6 08:12:06.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1344" for this suite. 05/06/23 08:12:06.084
------------------------------
â€¢ [SLOW TEST] [5.077 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:12:01.013
    May  6 08:12:01.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename configmap 05/06/23 08:12:01.014
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:02.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:02.032
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-1344/configmap-test-51a74474-4630-4743-a3f3-18c465e34720 05/06/23 08:12:02.034
    STEP: Creating a pod to test consume configMaps 05/06/23 08:12:02.037
    May  6 08:12:02.044: INFO: Waiting up to 5m0s for pod "pod-configmaps-7b3bc3a7-6a4b-4e45-bb26-04bc29f72987" in namespace "configmap-1344" to be "Succeeded or Failed"
    May  6 08:12:02.046: INFO: Pod "pod-configmaps-7b3bc3a7-6a4b-4e45-bb26-04bc29f72987": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059063ms
    May  6 08:12:04.050: INFO: Pod "pod-configmaps-7b3bc3a7-6a4b-4e45-bb26-04bc29f72987": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005688791s
    May  6 08:12:06.049: INFO: Pod "pod-configmaps-7b3bc3a7-6a4b-4e45-bb26-04bc29f72987": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005080615s
    STEP: Saw pod success 05/06/23 08:12:06.049
    May  6 08:12:06.049: INFO: Pod "pod-configmaps-7b3bc3a7-6a4b-4e45-bb26-04bc29f72987" satisfied condition "Succeeded or Failed"
    May  6 08:12:06.052: INFO: Trying to get logs from node cncf-2 pod pod-configmaps-7b3bc3a7-6a4b-4e45-bb26-04bc29f72987 container env-test: <nil>
    STEP: delete the pod 05/06/23 08:12:06.065
    May  6 08:12:06.079: INFO: Waiting for pod pod-configmaps-7b3bc3a7-6a4b-4e45-bb26-04bc29f72987 to disappear
    May  6 08:12:06.081: INFO: Pod pod-configmaps-7b3bc3a7-6a4b-4e45-bb26-04bc29f72987 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  6 08:12:06.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1344" for this suite. 05/06/23 08:12:06.084
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:12:06.093
May  6 08:12:06.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename svcaccounts 05/06/23 08:12:06.093
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:07.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:07.107
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  05/06/23 08:12:07.109
May  6 08:12:07.117: INFO: Waiting up to 5m0s for pod "test-pod-967bfc7a-d518-4698-8c02-ad64cd913fee" in namespace "svcaccounts-6817" to be "Succeeded or Failed"
May  6 08:12:07.120: INFO: Pod "test-pod-967bfc7a-d518-4698-8c02-ad64cd913fee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.511323ms
May  6 08:12:09.124: INFO: Pod "test-pod-967bfc7a-d518-4698-8c02-ad64cd913fee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00646681s
May  6 08:12:11.123: INFO: Pod "test-pod-967bfc7a-d518-4698-8c02-ad64cd913fee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00556119s
STEP: Saw pod success 05/06/23 08:12:11.123
May  6 08:12:11.123: INFO: Pod "test-pod-967bfc7a-d518-4698-8c02-ad64cd913fee" satisfied condition "Succeeded or Failed"
May  6 08:12:11.125: INFO: Trying to get logs from node cncf-0 pod test-pod-967bfc7a-d518-4698-8c02-ad64cd913fee container agnhost-container: <nil>
STEP: delete the pod 05/06/23 08:12:11.13
May  6 08:12:11.142: INFO: Waiting for pod test-pod-967bfc7a-d518-4698-8c02-ad64cd913fee to disappear
May  6 08:12:11.145: INFO: Pod test-pod-967bfc7a-d518-4698-8c02-ad64cd913fee no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May  6 08:12:11.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6817" for this suite. 05/06/23 08:12:11.148
------------------------------
â€¢ [SLOW TEST] [5.063 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:12:06.093
    May  6 08:12:06.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename svcaccounts 05/06/23 08:12:06.093
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:07.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:07.107
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  05/06/23 08:12:07.109
    May  6 08:12:07.117: INFO: Waiting up to 5m0s for pod "test-pod-967bfc7a-d518-4698-8c02-ad64cd913fee" in namespace "svcaccounts-6817" to be "Succeeded or Failed"
    May  6 08:12:07.120: INFO: Pod "test-pod-967bfc7a-d518-4698-8c02-ad64cd913fee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.511323ms
    May  6 08:12:09.124: INFO: Pod "test-pod-967bfc7a-d518-4698-8c02-ad64cd913fee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00646681s
    May  6 08:12:11.123: INFO: Pod "test-pod-967bfc7a-d518-4698-8c02-ad64cd913fee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00556119s
    STEP: Saw pod success 05/06/23 08:12:11.123
    May  6 08:12:11.123: INFO: Pod "test-pod-967bfc7a-d518-4698-8c02-ad64cd913fee" satisfied condition "Succeeded or Failed"
    May  6 08:12:11.125: INFO: Trying to get logs from node cncf-0 pod test-pod-967bfc7a-d518-4698-8c02-ad64cd913fee container agnhost-container: <nil>
    STEP: delete the pod 05/06/23 08:12:11.13
    May  6 08:12:11.142: INFO: Waiting for pod test-pod-967bfc7a-d518-4698-8c02-ad64cd913fee to disappear
    May  6 08:12:11.145: INFO: Pod test-pod-967bfc7a-d518-4698-8c02-ad64cd913fee no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May  6 08:12:11.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6817" for this suite. 05/06/23 08:12:11.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:12:11.157
May  6 08:12:11.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename server-version 05/06/23 08:12:11.157
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:12.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:12.174
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 05/06/23 08:12:12.176
STEP: Confirm major version 05/06/23 08:12:12.176
May  6 08:12:12.176: INFO: Major version: 1
STEP: Confirm minor version 05/06/23 08:12:12.176
May  6 08:12:12.176: INFO: cleanMinorVersion: 26
May  6 08:12:12.176: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
May  6 08:12:12.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-5344" for this suite. 05/06/23 08:12:12.179
------------------------------
â€¢ [1.027 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:12:11.157
    May  6 08:12:11.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename server-version 05/06/23 08:12:11.157
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:12.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:12.174
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 05/06/23 08:12:12.176
    STEP: Confirm major version 05/06/23 08:12:12.176
    May  6 08:12:12.176: INFO: Major version: 1
    STEP: Confirm minor version 05/06/23 08:12:12.176
    May  6 08:12:12.176: INFO: cleanMinorVersion: 26
    May  6 08:12:12.176: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    May  6 08:12:12.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-5344" for this suite. 05/06/23 08:12:12.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:12:12.184
May  6 08:12:12.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 08:12:12.184
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:12.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:12.202
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-4344d0c3-2323-40ff-8588-1c8a322e9c28 05/06/23 08:12:12.203
STEP: Creating a pod to test consume configMaps 05/06/23 08:12:12.207
May  6 08:12:12.214: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c2aa93b4-f1b7-42d4-a23f-499e35e49b91" in namespace "projected-5886" to be "Succeeded or Failed"
May  6 08:12:12.216: INFO: Pod "pod-projected-configmaps-c2aa93b4-f1b7-42d4-a23f-499e35e49b91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.150226ms
May  6 08:12:14.220: INFO: Pod "pod-projected-configmaps-c2aa93b4-f1b7-42d4-a23f-499e35e49b91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005548994s
May  6 08:12:16.219: INFO: Pod "pod-projected-configmaps-c2aa93b4-f1b7-42d4-a23f-499e35e49b91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005157401s
STEP: Saw pod success 05/06/23 08:12:16.219
May  6 08:12:16.219: INFO: Pod "pod-projected-configmaps-c2aa93b4-f1b7-42d4-a23f-499e35e49b91" satisfied condition "Succeeded or Failed"
May  6 08:12:16.222: INFO: Trying to get logs from node cncf-0 pod pod-projected-configmaps-c2aa93b4-f1b7-42d4-a23f-499e35e49b91 container agnhost-container: <nil>
STEP: delete the pod 05/06/23 08:12:16.227
May  6 08:12:16.241: INFO: Waiting for pod pod-projected-configmaps-c2aa93b4-f1b7-42d4-a23f-499e35e49b91 to disappear
May  6 08:12:16.244: INFO: Pod pod-projected-configmaps-c2aa93b4-f1b7-42d4-a23f-499e35e49b91 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May  6 08:12:16.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5886" for this suite. 05/06/23 08:12:16.247
------------------------------
â€¢ [4.068 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:12:12.184
    May  6 08:12:12.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 08:12:12.184
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:12.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:12.202
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-4344d0c3-2323-40ff-8588-1c8a322e9c28 05/06/23 08:12:12.203
    STEP: Creating a pod to test consume configMaps 05/06/23 08:12:12.207
    May  6 08:12:12.214: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c2aa93b4-f1b7-42d4-a23f-499e35e49b91" in namespace "projected-5886" to be "Succeeded or Failed"
    May  6 08:12:12.216: INFO: Pod "pod-projected-configmaps-c2aa93b4-f1b7-42d4-a23f-499e35e49b91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.150226ms
    May  6 08:12:14.220: INFO: Pod "pod-projected-configmaps-c2aa93b4-f1b7-42d4-a23f-499e35e49b91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005548994s
    May  6 08:12:16.219: INFO: Pod "pod-projected-configmaps-c2aa93b4-f1b7-42d4-a23f-499e35e49b91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005157401s
    STEP: Saw pod success 05/06/23 08:12:16.219
    May  6 08:12:16.219: INFO: Pod "pod-projected-configmaps-c2aa93b4-f1b7-42d4-a23f-499e35e49b91" satisfied condition "Succeeded or Failed"
    May  6 08:12:16.222: INFO: Trying to get logs from node cncf-0 pod pod-projected-configmaps-c2aa93b4-f1b7-42d4-a23f-499e35e49b91 container agnhost-container: <nil>
    STEP: delete the pod 05/06/23 08:12:16.227
    May  6 08:12:16.241: INFO: Waiting for pod pod-projected-configmaps-c2aa93b4-f1b7-42d4-a23f-499e35e49b91 to disappear
    May  6 08:12:16.244: INFO: Pod pod-projected-configmaps-c2aa93b4-f1b7-42d4-a23f-499e35e49b91 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May  6 08:12:16.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5886" for this suite. 05/06/23 08:12:16.247
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:12:16.252
May  6 08:12:16.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename gc 05/06/23 08:12:16.253
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:16.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:16.268
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
May  6 08:12:16.308: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"f832e2de-bd03-4d65-9ba8-bdb790135a64", Controller:(*bool)(0xc00484644a), BlockOwnerDeletion:(*bool)(0xc00484644b)}}
May  6 08:12:16.318: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1b68b235-60b1-4caf-aa5c-69b72403af44", Controller:(*bool)(0xc002e7f6da), BlockOwnerDeletion:(*bool)(0xc002e7f6db)}}
May  6 08:12:16.326: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"257a4496-4d10-4821-b47f-cbc0e562f104", Controller:(*bool)(0xc0048466d2), BlockOwnerDeletion:(*bool)(0xc0048466d3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May  6 08:12:21.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6888" for this suite. 05/06/23 08:12:21.339
------------------------------
â€¢ [SLOW TEST] [5.094 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:12:16.252
    May  6 08:12:16.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename gc 05/06/23 08:12:16.253
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:16.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:16.268
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    May  6 08:12:16.308: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"f832e2de-bd03-4d65-9ba8-bdb790135a64", Controller:(*bool)(0xc00484644a), BlockOwnerDeletion:(*bool)(0xc00484644b)}}
    May  6 08:12:16.318: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1b68b235-60b1-4caf-aa5c-69b72403af44", Controller:(*bool)(0xc002e7f6da), BlockOwnerDeletion:(*bool)(0xc002e7f6db)}}
    May  6 08:12:16.326: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"257a4496-4d10-4821-b47f-cbc0e562f104", Controller:(*bool)(0xc0048466d2), BlockOwnerDeletion:(*bool)(0xc0048466d3)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May  6 08:12:21.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6888" for this suite. 05/06/23 08:12:21.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:12:21.347
May  6 08:12:21.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubelet-test 05/06/23 08:12:21.348
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:21.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:21.362
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 05/06/23 08:12:21.372
May  6 08:12:21.372: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases70847ca5-4c60-490a-9588-c7db252b2a03" in namespace "kubelet-test-1229" to be "completed"
May  6 08:12:21.376: INFO: Pod "agnhost-host-aliases70847ca5-4c60-490a-9588-c7db252b2a03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059124ms
May  6 08:12:23.380: INFO: Pod "agnhost-host-aliases70847ca5-4c60-490a-9588-c7db252b2a03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007677963s
May  6 08:12:25.380: INFO: Pod "agnhost-host-aliases70847ca5-4c60-490a-9588-c7db252b2a03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007596881s
May  6 08:12:25.380: INFO: Pod "agnhost-host-aliases70847ca5-4c60-490a-9588-c7db252b2a03" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May  6 08:12:25.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1229" for this suite. 05/06/23 08:12:25.388
------------------------------
â€¢ [4.047 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:12:21.347
    May  6 08:12:21.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubelet-test 05/06/23 08:12:21.348
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:21.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:21.362
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 05/06/23 08:12:21.372
    May  6 08:12:21.372: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases70847ca5-4c60-490a-9588-c7db252b2a03" in namespace "kubelet-test-1229" to be "completed"
    May  6 08:12:21.376: INFO: Pod "agnhost-host-aliases70847ca5-4c60-490a-9588-c7db252b2a03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059124ms
    May  6 08:12:23.380: INFO: Pod "agnhost-host-aliases70847ca5-4c60-490a-9588-c7db252b2a03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007677963s
    May  6 08:12:25.380: INFO: Pod "agnhost-host-aliases70847ca5-4c60-490a-9588-c7db252b2a03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007596881s
    May  6 08:12:25.380: INFO: Pod "agnhost-host-aliases70847ca5-4c60-490a-9588-c7db252b2a03" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May  6 08:12:25.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1229" for this suite. 05/06/23 08:12:25.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:12:25.395
May  6 08:12:25.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename dns 05/06/23 08:12:25.395
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:25.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:25.411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 05/06/23 08:12:25.413
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-515.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-515.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local;sleep 1; done
 05/06/23 08:12:25.418
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-515.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-515.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-515.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local;sleep 1; done
 05/06/23 08:12:25.418
STEP: creating a pod to probe DNS 05/06/23 08:12:25.418
STEP: submitting the pod to kubernetes 05/06/23 08:12:25.418
May  6 08:12:25.434: INFO: Waiting up to 15m0s for pod "dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4" in namespace "dns-515" to be "running"
May  6 08:12:25.436: INFO: Pod "dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.523035ms
May  6 08:12:27.440: INFO: Pod "dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005882922s
May  6 08:12:27.440: INFO: Pod "dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4" satisfied condition "running"
STEP: retrieving the pod 05/06/23 08:12:27.44
STEP: looking for the results for each expected name from probers 05/06/23 08:12:27.442
May  6 08:12:27.446: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:27.449: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:27.452: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:27.455: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:27.458: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:27.460: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:27.463: INFO: Unable to read jessie_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:27.466: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:27.466: INFO: Lookups using dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local]

May  6 08:12:32.469: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:32.472: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:32.475: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:32.479: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:32.482: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:32.485: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:32.487: INFO: Unable to read jessie_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:32.490: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:32.490: INFO: Lookups using dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local]

May  6 08:12:37.469: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:37.472: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:37.475: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:37.478: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:37.480: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:37.483: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:37.486: INFO: Unable to read jessie_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:37.489: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:37.489: INFO: Lookups using dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local]

May  6 08:12:42.470: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:42.473: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:42.476: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:42.479: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:42.482: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:42.485: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:42.487: INFO: Unable to read jessie_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:42.490: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:42.490: INFO: Lookups using dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local]

May  6 08:12:47.470: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:47.473: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:47.477: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:47.479: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:47.483: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:47.485: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:47.488: INFO: Unable to read jessie_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:47.491: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:47.491: INFO: Lookups using dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local]

May  6 08:12:52.469: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:52.473: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:52.476: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:52.479: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:52.482: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:52.485: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:52.487: INFO: Unable to read jessie_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:52.490: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
May  6 08:12:52.490: INFO: Lookups using dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local]

May  6 08:12:57.490: INFO: DNS probes using dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4 succeeded

STEP: deleting the pod 05/06/23 08:12:57.49
STEP: deleting the test headless service 05/06/23 08:12:57.508
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May  6 08:12:57.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-515" for this suite. 05/06/23 08:12:57.528
------------------------------
â€¢ [SLOW TEST] [32.142 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:12:25.395
    May  6 08:12:25.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename dns 05/06/23 08:12:25.395
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:25.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:25.411
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 05/06/23 08:12:25.413
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-515.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-515.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local;sleep 1; done
     05/06/23 08:12:25.418
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-515.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-515.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-515.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local;sleep 1; done
     05/06/23 08:12:25.418
    STEP: creating a pod to probe DNS 05/06/23 08:12:25.418
    STEP: submitting the pod to kubernetes 05/06/23 08:12:25.418
    May  6 08:12:25.434: INFO: Waiting up to 15m0s for pod "dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4" in namespace "dns-515" to be "running"
    May  6 08:12:25.436: INFO: Pod "dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.523035ms
    May  6 08:12:27.440: INFO: Pod "dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005882922s
    May  6 08:12:27.440: INFO: Pod "dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4" satisfied condition "running"
    STEP: retrieving the pod 05/06/23 08:12:27.44
    STEP: looking for the results for each expected name from probers 05/06/23 08:12:27.442
    May  6 08:12:27.446: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:27.449: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:27.452: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:27.455: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:27.458: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:27.460: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:27.463: INFO: Unable to read jessie_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:27.466: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:27.466: INFO: Lookups using dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local]

    May  6 08:12:32.469: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:32.472: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:32.475: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:32.479: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:32.482: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:32.485: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:32.487: INFO: Unable to read jessie_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:32.490: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:32.490: INFO: Lookups using dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local]

    May  6 08:12:37.469: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:37.472: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:37.475: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:37.478: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:37.480: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:37.483: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:37.486: INFO: Unable to read jessie_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:37.489: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:37.489: INFO: Lookups using dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local]

    May  6 08:12:42.470: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:42.473: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:42.476: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:42.479: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:42.482: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:42.485: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:42.487: INFO: Unable to read jessie_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:42.490: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:42.490: INFO: Lookups using dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local]

    May  6 08:12:47.470: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:47.473: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:47.477: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:47.479: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:47.483: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:47.485: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:47.488: INFO: Unable to read jessie_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:47.491: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:47.491: INFO: Lookups using dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local]

    May  6 08:12:52.469: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:52.473: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:52.476: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:52.479: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:52.482: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:52.485: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:52.487: INFO: Unable to read jessie_udp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:52.490: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local from pod dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4: the server could not find the requested resource (get pods dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4)
    May  6 08:12:52.490: INFO: Lookups using dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local wheezy_udp@dns-test-service-2.dns-515.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-515.svc.cluster.local jessie_udp@dns-test-service-2.dns-515.svc.cluster.local jessie_tcp@dns-test-service-2.dns-515.svc.cluster.local]

    May  6 08:12:57.490: INFO: DNS probes using dns-515/dns-test-b7973e0a-448e-405a-b78a-4e77e5caa1f4 succeeded

    STEP: deleting the pod 05/06/23 08:12:57.49
    STEP: deleting the test headless service 05/06/23 08:12:57.508
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May  6 08:12:57.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-515" for this suite. 05/06/23 08:12:57.528
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:12:57.539
May  6 08:12:57.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename runtimeclass 05/06/23 08:12:57.539
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:57.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:57.559
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May  6 08:12:57.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5026" for this suite. 05/06/23 08:12:57.568
------------------------------
â€¢ [0.037 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:12:57.539
    May  6 08:12:57.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename runtimeclass 05/06/23 08:12:57.539
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:57.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:57.559
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May  6 08:12:57.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5026" for this suite. 05/06/23 08:12:57.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:12:57.577
May  6 08:12:57.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename svcaccounts 05/06/23 08:12:57.578
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:57.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:57.591
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
May  6 08:12:57.603: INFO: Waiting up to 5m0s for pod "pod-service-account-ad1b166f-0307-4884-8051-70d604b5b0d4" in namespace "svcaccounts-7376" to be "running"
May  6 08:12:57.606: INFO: Pod "pod-service-account-ad1b166f-0307-4884-8051-70d604b5b0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.276455ms
May  6 08:12:59.609: INFO: Pod "pod-service-account-ad1b166f-0307-4884-8051-70d604b5b0d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005770427s
May  6 08:12:59.609: INFO: Pod "pod-service-account-ad1b166f-0307-4884-8051-70d604b5b0d4" satisfied condition "running"
STEP: reading a file in the container 05/06/23 08:12:59.609
May  6 08:12:59.609: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7376 pod-service-account-ad1b166f-0307-4884-8051-70d604b5b0d4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 05/06/23 08:12:59.739
May  6 08:12:59.739: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7376 pod-service-account-ad1b166f-0307-4884-8051-70d604b5b0d4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 05/06/23 08:12:59.845
May  6 08:12:59.845: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7376 pod-service-account-ad1b166f-0307-4884-8051-70d604b5b0d4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
May  6 08:12:59.947: INFO: Got root ca configmap in namespace "svcaccounts-7376"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May  6 08:12:59.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7376" for this suite. 05/06/23 08:12:59.953
------------------------------
â€¢ [2.384 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:12:57.577
    May  6 08:12:57.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename svcaccounts 05/06/23 08:12:57.578
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:57.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:57.591
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    May  6 08:12:57.603: INFO: Waiting up to 5m0s for pod "pod-service-account-ad1b166f-0307-4884-8051-70d604b5b0d4" in namespace "svcaccounts-7376" to be "running"
    May  6 08:12:57.606: INFO: Pod "pod-service-account-ad1b166f-0307-4884-8051-70d604b5b0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.276455ms
    May  6 08:12:59.609: INFO: Pod "pod-service-account-ad1b166f-0307-4884-8051-70d604b5b0d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005770427s
    May  6 08:12:59.609: INFO: Pod "pod-service-account-ad1b166f-0307-4884-8051-70d604b5b0d4" satisfied condition "running"
    STEP: reading a file in the container 05/06/23 08:12:59.609
    May  6 08:12:59.609: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7376 pod-service-account-ad1b166f-0307-4884-8051-70d604b5b0d4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 05/06/23 08:12:59.739
    May  6 08:12:59.739: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7376 pod-service-account-ad1b166f-0307-4884-8051-70d604b5b0d4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 05/06/23 08:12:59.845
    May  6 08:12:59.845: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7376 pod-service-account-ad1b166f-0307-4884-8051-70d604b5b0d4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    May  6 08:12:59.947: INFO: Got root ca configmap in namespace "svcaccounts-7376"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May  6 08:12:59.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7376" for this suite. 05/06/23 08:12:59.953
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:12:59.961
May  6 08:12:59.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename webhook 05/06/23 08:12:59.962
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:59.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:59.976
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/06/23 08:12:59.991
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 08:13:00.434
STEP: Deploying the webhook pod 05/06/23 08:13:00.44
STEP: Wait for the deployment to be ready 05/06/23 08:13:00.453
May  6 08:13:00.458: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  6 08:13:02.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 8, 13, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 8, 13, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 8, 13, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 8, 13, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/06/23 08:13:04.469
STEP: Verifying the service has paired with the endpoint 05/06/23 08:13:04.482
May  6 08:13:05.483: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 05/06/23 08:13:05.486
STEP: Updating a mutating webhook configuration's rules to not include the create operation 05/06/23 08:13:06.501
STEP: Creating a configMap that should not be mutated 05/06/23 08:13:06.507
STEP: Patching a mutating webhook configuration's rules to include the create operation 05/06/23 08:13:06.516
STEP: Creating a configMap that should be mutated 05/06/23 08:13:06.526
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 08:13:07.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9853" for this suite. 05/06/23 08:13:07.606
STEP: Destroying namespace "webhook-9853-markers" for this suite. 05/06/23 08:13:07.615
------------------------------
â€¢ [SLOW TEST] [7.666 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:12:59.961
    May  6 08:12:59.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename webhook 05/06/23 08:12:59.962
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:12:59.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:12:59.976
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/06/23 08:12:59.991
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 08:13:00.434
    STEP: Deploying the webhook pod 05/06/23 08:13:00.44
    STEP: Wait for the deployment to be ready 05/06/23 08:13:00.453
    May  6 08:13:00.458: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May  6 08:13:02.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 6, 8, 13, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 8, 13, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 8, 13, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 8, 13, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/06/23 08:13:04.469
    STEP: Verifying the service has paired with the endpoint 05/06/23 08:13:04.482
    May  6 08:13:05.483: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 05/06/23 08:13:05.486
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 05/06/23 08:13:06.501
    STEP: Creating a configMap that should not be mutated 05/06/23 08:13:06.507
    STEP: Patching a mutating webhook configuration's rules to include the create operation 05/06/23 08:13:06.516
    STEP: Creating a configMap that should be mutated 05/06/23 08:13:06.526
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 08:13:07.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9853" for this suite. 05/06/23 08:13:07.606
    STEP: Destroying namespace "webhook-9853-markers" for this suite. 05/06/23 08:13:07.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:13:07.628
May  6 08:13:07.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename downward-api 05/06/23 08:13:07.629
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:13:07.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:13:07.647
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 05/06/23 08:13:07.649
May  6 08:13:07.661: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8455d9d4-ef4b-4bbb-bca9-05bc16fc311d" in namespace "downward-api-3378" to be "Succeeded or Failed"
May  6 08:13:07.664: INFO: Pod "downwardapi-volume-8455d9d4-ef4b-4bbb-bca9-05bc16fc311d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.463952ms
May  6 08:13:09.667: INFO: Pod "downwardapi-volume-8455d9d4-ef4b-4bbb-bca9-05bc16fc311d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005623239s
May  6 08:13:11.667: INFO: Pod "downwardapi-volume-8455d9d4-ef4b-4bbb-bca9-05bc16fc311d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006252962s
STEP: Saw pod success 05/06/23 08:13:11.667
May  6 08:13:11.668: INFO: Pod "downwardapi-volume-8455d9d4-ef4b-4bbb-bca9-05bc16fc311d" satisfied condition "Succeeded or Failed"
May  6 08:13:11.670: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-8455d9d4-ef4b-4bbb-bca9-05bc16fc311d container client-container: <nil>
STEP: delete the pod 05/06/23 08:13:11.675
May  6 08:13:11.690: INFO: Waiting for pod downwardapi-volume-8455d9d4-ef4b-4bbb-bca9-05bc16fc311d to disappear
May  6 08:13:11.693: INFO: Pod downwardapi-volume-8455d9d4-ef4b-4bbb-bca9-05bc16fc311d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  6 08:13:11.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3378" for this suite. 05/06/23 08:13:11.696
------------------------------
â€¢ [4.074 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:13:07.628
    May  6 08:13:07.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename downward-api 05/06/23 08:13:07.629
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:13:07.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:13:07.647
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 05/06/23 08:13:07.649
    May  6 08:13:07.661: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8455d9d4-ef4b-4bbb-bca9-05bc16fc311d" in namespace "downward-api-3378" to be "Succeeded or Failed"
    May  6 08:13:07.664: INFO: Pod "downwardapi-volume-8455d9d4-ef4b-4bbb-bca9-05bc16fc311d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.463952ms
    May  6 08:13:09.667: INFO: Pod "downwardapi-volume-8455d9d4-ef4b-4bbb-bca9-05bc16fc311d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005623239s
    May  6 08:13:11.667: INFO: Pod "downwardapi-volume-8455d9d4-ef4b-4bbb-bca9-05bc16fc311d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006252962s
    STEP: Saw pod success 05/06/23 08:13:11.667
    May  6 08:13:11.668: INFO: Pod "downwardapi-volume-8455d9d4-ef4b-4bbb-bca9-05bc16fc311d" satisfied condition "Succeeded or Failed"
    May  6 08:13:11.670: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-8455d9d4-ef4b-4bbb-bca9-05bc16fc311d container client-container: <nil>
    STEP: delete the pod 05/06/23 08:13:11.675
    May  6 08:13:11.690: INFO: Waiting for pod downwardapi-volume-8455d9d4-ef4b-4bbb-bca9-05bc16fc311d to disappear
    May  6 08:13:11.693: INFO: Pod downwardapi-volume-8455d9d4-ef4b-4bbb-bca9-05bc16fc311d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  6 08:13:11.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3378" for this suite. 05/06/23 08:13:11.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:13:11.702
May  6 08:13:11.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename podtemplate 05/06/23 08:13:11.703
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:13:11.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:13:11.719
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
May  6 08:13:11.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-1563" for this suite. 05/06/23 08:13:11.755
------------------------------
â€¢ [0.058 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:13:11.702
    May  6 08:13:11.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename podtemplate 05/06/23 08:13:11.703
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:13:11.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:13:11.719
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    May  6 08:13:11.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-1563" for this suite. 05/06/23 08:13:11.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:13:11.762
May  6 08:13:11.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename taint-single-pod 05/06/23 08:13:11.762
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:13:11.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:13:11.777
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
May  6 08:13:11.778: INFO: Waiting up to 1m0s for all nodes to be ready
May  6 08:14:11.801: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
May  6 08:14:11.803: INFO: Starting informer...
STEP: Starting pod... 05/06/23 08:14:11.803
May  6 08:14:12.020: INFO: Pod is running on cncf-0. Tainting Node
STEP: Trying to apply a taint on the Node 05/06/23 08:14:12.02
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/06/23 08:14:12.03
STEP: Waiting short time to make sure Pod is queued for deletion 05/06/23 08:14:12.033
May  6 08:14:12.033: INFO: Pod wasn't evicted. Proceeding
May  6 08:14:12.033: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/06/23 08:14:12.047
STEP: Waiting some time to make sure that toleration time passed. 05/06/23 08:14:12.05
May  6 08:15:27.050: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 08:15:27.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-1568" for this suite. 05/06/23 08:15:27.054
------------------------------
â€¢ [SLOW TEST] [135.298 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:13:11.762
    May  6 08:13:11.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename taint-single-pod 05/06/23 08:13:11.762
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:13:11.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:13:11.777
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    May  6 08:13:11.778: INFO: Waiting up to 1m0s for all nodes to be ready
    May  6 08:14:11.801: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    May  6 08:14:11.803: INFO: Starting informer...
    STEP: Starting pod... 05/06/23 08:14:11.803
    May  6 08:14:12.020: INFO: Pod is running on cncf-0. Tainting Node
    STEP: Trying to apply a taint on the Node 05/06/23 08:14:12.02
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/06/23 08:14:12.03
    STEP: Waiting short time to make sure Pod is queued for deletion 05/06/23 08:14:12.033
    May  6 08:14:12.033: INFO: Pod wasn't evicted. Proceeding
    May  6 08:14:12.033: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/06/23 08:14:12.047
    STEP: Waiting some time to make sure that toleration time passed. 05/06/23 08:14:12.05
    May  6 08:15:27.050: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 08:15:27.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-1568" for this suite. 05/06/23 08:15:27.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:15:27.061
May  6 08:15:27.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename secrets 05/06/23 08:15:27.061
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:15:28.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:15:28.078
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May  6 08:15:28.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1392" for this suite. 05/06/23 08:15:28.123
------------------------------
â€¢ [1.069 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:15:27.061
    May  6 08:15:27.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename secrets 05/06/23 08:15:27.061
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:15:28.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:15:28.078
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May  6 08:15:28.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1392" for this suite. 05/06/23 08:15:28.123
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:15:28.13
May  6 08:15:28.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename endpointslice 05/06/23 08:15:28.13
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:15:29.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:15:29.148
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
May  6 08:15:31.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-305" for this suite. 05/06/23 08:15:31.213
------------------------------
â€¢ [3.089 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:15:28.13
    May  6 08:15:28.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename endpointslice 05/06/23 08:15:28.13
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:15:29.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:15:29.148
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    May  6 08:15:31.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-305" for this suite. 05/06/23 08:15:31.213
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:15:31.219
May  6 08:15:31.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename emptydir 05/06/23 08:15:31.22
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:15:32.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:15:32.244
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 05/06/23 08:15:32.247
May  6 08:15:32.255: INFO: Waiting up to 5m0s for pod "pod-5c9619ec-3f5a-4360-a536-33ef3efbf132" in namespace "emptydir-3881" to be "Succeeded or Failed"
May  6 08:15:32.257: INFO: Pod "pod-5c9619ec-3f5a-4360-a536-33ef3efbf132": Phase="Pending", Reason="", readiness=false. Elapsed: 2.752942ms
May  6 08:15:34.261: INFO: Pod "pod-5c9619ec-3f5a-4360-a536-33ef3efbf132": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006390263s
May  6 08:15:36.260: INFO: Pod "pod-5c9619ec-3f5a-4360-a536-33ef3efbf132": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005247095s
STEP: Saw pod success 05/06/23 08:15:36.26
May  6 08:15:36.260: INFO: Pod "pod-5c9619ec-3f5a-4360-a536-33ef3efbf132" satisfied condition "Succeeded or Failed"
May  6 08:15:36.262: INFO: Trying to get logs from node cncf-0 pod pod-5c9619ec-3f5a-4360-a536-33ef3efbf132 container test-container: <nil>
STEP: delete the pod 05/06/23 08:15:36.277
May  6 08:15:36.290: INFO: Waiting for pod pod-5c9619ec-3f5a-4360-a536-33ef3efbf132 to disappear
May  6 08:15:36.293: INFO: Pod pod-5c9619ec-3f5a-4360-a536-33ef3efbf132 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  6 08:15:36.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3881" for this suite. 05/06/23 08:15:36.297
------------------------------
â€¢ [SLOW TEST] [5.084 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:15:31.219
    May  6 08:15:31.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename emptydir 05/06/23 08:15:31.22
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:15:32.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:15:32.244
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 05/06/23 08:15:32.247
    May  6 08:15:32.255: INFO: Waiting up to 5m0s for pod "pod-5c9619ec-3f5a-4360-a536-33ef3efbf132" in namespace "emptydir-3881" to be "Succeeded or Failed"
    May  6 08:15:32.257: INFO: Pod "pod-5c9619ec-3f5a-4360-a536-33ef3efbf132": Phase="Pending", Reason="", readiness=false. Elapsed: 2.752942ms
    May  6 08:15:34.261: INFO: Pod "pod-5c9619ec-3f5a-4360-a536-33ef3efbf132": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006390263s
    May  6 08:15:36.260: INFO: Pod "pod-5c9619ec-3f5a-4360-a536-33ef3efbf132": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005247095s
    STEP: Saw pod success 05/06/23 08:15:36.26
    May  6 08:15:36.260: INFO: Pod "pod-5c9619ec-3f5a-4360-a536-33ef3efbf132" satisfied condition "Succeeded or Failed"
    May  6 08:15:36.262: INFO: Trying to get logs from node cncf-0 pod pod-5c9619ec-3f5a-4360-a536-33ef3efbf132 container test-container: <nil>
    STEP: delete the pod 05/06/23 08:15:36.277
    May  6 08:15:36.290: INFO: Waiting for pod pod-5c9619ec-3f5a-4360-a536-33ef3efbf132 to disappear
    May  6 08:15:36.293: INFO: Pod pod-5c9619ec-3f5a-4360-a536-33ef3efbf132 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  6 08:15:36.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3881" for this suite. 05/06/23 08:15:36.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:15:36.304
May  6 08:15:36.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename pod-network-test 05/06/23 08:15:36.304
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:15:37.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:15:37.322
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-5565 05/06/23 08:15:37.324
STEP: creating a selector 05/06/23 08:15:37.324
STEP: Creating the service pods in kubernetes 05/06/23 08:15:37.324
May  6 08:15:37.324: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  6 08:15:37.373: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5565" to be "running and ready"
May  6 08:15:37.384: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.669527ms
May  6 08:15:37.384: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  6 08:15:39.387: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013820763s
May  6 08:15:39.387: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 08:15:41.388: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015088422s
May  6 08:15:41.388: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 08:15:43.387: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01368006s
May  6 08:15:43.387: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 08:15:45.387: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013865213s
May  6 08:15:45.387: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 08:15:47.387: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013923451s
May  6 08:15:47.387: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 08:15:49.387: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.013557472s
May  6 08:15:49.387: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 08:15:51.388: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.015206979s
May  6 08:15:51.388: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 08:15:53.388: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014828652s
May  6 08:15:53.388: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 08:15:55.387: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.013577694s
May  6 08:15:55.387: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 08:15:57.387: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013671939s
May  6 08:15:57.387: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  6 08:15:59.387: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.013428684s
May  6 08:15:59.387: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  6 08:15:59.387: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  6 08:15:59.389: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5565" to be "running and ready"
May  6 08:15:59.391: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.443314ms
May  6 08:15:59.391: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  6 08:15:59.391: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  6 08:15:59.393: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5565" to be "running and ready"
May  6 08:15:59.396: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.841641ms
May  6 08:15:59.396: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  6 08:15:59.396: INFO: Pod "netserver-2" satisfied condition "running and ready"
May  6 08:15:59.398: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-5565" to be "running and ready"
May  6 08:15:59.400: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.04692ms
May  6 08:15:59.400: INFO: The phase of Pod netserver-3 is Running (Ready = true)
May  6 08:15:59.400: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 05/06/23 08:15:59.402
May  6 08:15:59.412: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5565" to be "running"
May  6 08:15:59.416: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.510383ms
May  6 08:16:01.419: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007089891s
May  6 08:16:01.420: INFO: Pod "test-container-pod" satisfied condition "running"
May  6 08:16:01.422: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5565" to be "running"
May  6 08:16:01.424: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.162009ms
May  6 08:16:01.424: INFO: Pod "host-test-container-pod" satisfied condition "running"
May  6 08:16:01.426: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
May  6 08:16:01.426: INFO: Going to poll 10.244.174.141 on port 8081 at least 0 times, with a maximum of 46 tries before failing
May  6 08:16:01.428: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.174.141 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5565 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 08:16:01.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 08:16:01.428: INFO: ExecWithOptions: Clientset creation
May  6 08:16:01.428: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5565/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.174.141+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  6 08:16:02.487: INFO: Found all 1 expected endpoints: [netserver-0]
May  6 08:16:02.487: INFO: Going to poll 10.244.21.83 on port 8081 at least 0 times, with a maximum of 46 tries before failing
May  6 08:16:02.489: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.21.83 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5565 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 08:16:02.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 08:16:02.490: INFO: ExecWithOptions: Clientset creation
May  6 08:16:02.490: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5565/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.21.83+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  6 08:16:03.541: INFO: Found all 1 expected endpoints: [netserver-1]
May  6 08:16:03.541: INFO: Going to poll 10.244.20.164 on port 8081 at least 0 times, with a maximum of 46 tries before failing
May  6 08:16:03.543: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.20.164 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5565 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 08:16:03.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 08:16:03.544: INFO: ExecWithOptions: Clientset creation
May  6 08:16:03.544: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5565/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.20.164+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  6 08:16:04.597: INFO: Found all 1 expected endpoints: [netserver-2]
May  6 08:16:04.597: INFO: Going to poll 10.244.245.100 on port 8081 at least 0 times, with a maximum of 46 tries before failing
May  6 08:16:04.599: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.245.100 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5565 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 08:16:04.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 08:16:04.600: INFO: ExecWithOptions: Clientset creation
May  6 08:16:04.600: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5565/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.245.100+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  6 08:16:05.664: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
May  6 08:16:05.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-5565" for this suite. 05/06/23 08:16:05.668
------------------------------
â€¢ [SLOW TEST] [29.370 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:15:36.304
    May  6 08:15:36.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename pod-network-test 05/06/23 08:15:36.304
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:15:37.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:15:37.322
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-5565 05/06/23 08:15:37.324
    STEP: creating a selector 05/06/23 08:15:37.324
    STEP: Creating the service pods in kubernetes 05/06/23 08:15:37.324
    May  6 08:15:37.324: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  6 08:15:37.373: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5565" to be "running and ready"
    May  6 08:15:37.384: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.669527ms
    May  6 08:15:37.384: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:15:39.387: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013820763s
    May  6 08:15:39.387: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 08:15:41.388: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015088422s
    May  6 08:15:41.388: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 08:15:43.387: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01368006s
    May  6 08:15:43.387: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 08:15:45.387: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013865213s
    May  6 08:15:45.387: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 08:15:47.387: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013923451s
    May  6 08:15:47.387: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 08:15:49.387: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.013557472s
    May  6 08:15:49.387: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 08:15:51.388: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.015206979s
    May  6 08:15:51.388: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 08:15:53.388: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014828652s
    May  6 08:15:53.388: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 08:15:55.387: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.013577694s
    May  6 08:15:55.387: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 08:15:57.387: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013671939s
    May  6 08:15:57.387: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  6 08:15:59.387: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.013428684s
    May  6 08:15:59.387: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  6 08:15:59.387: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  6 08:15:59.389: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5565" to be "running and ready"
    May  6 08:15:59.391: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.443314ms
    May  6 08:15:59.391: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  6 08:15:59.391: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  6 08:15:59.393: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5565" to be "running and ready"
    May  6 08:15:59.396: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.841641ms
    May  6 08:15:59.396: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  6 08:15:59.396: INFO: Pod "netserver-2" satisfied condition "running and ready"
    May  6 08:15:59.398: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-5565" to be "running and ready"
    May  6 08:15:59.400: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.04692ms
    May  6 08:15:59.400: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    May  6 08:15:59.400: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 05/06/23 08:15:59.402
    May  6 08:15:59.412: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5565" to be "running"
    May  6 08:15:59.416: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.510383ms
    May  6 08:16:01.419: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007089891s
    May  6 08:16:01.420: INFO: Pod "test-container-pod" satisfied condition "running"
    May  6 08:16:01.422: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5565" to be "running"
    May  6 08:16:01.424: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.162009ms
    May  6 08:16:01.424: INFO: Pod "host-test-container-pod" satisfied condition "running"
    May  6 08:16:01.426: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    May  6 08:16:01.426: INFO: Going to poll 10.244.174.141 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    May  6 08:16:01.428: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.174.141 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5565 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 08:16:01.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 08:16:01.428: INFO: ExecWithOptions: Clientset creation
    May  6 08:16:01.428: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5565/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.174.141+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  6 08:16:02.487: INFO: Found all 1 expected endpoints: [netserver-0]
    May  6 08:16:02.487: INFO: Going to poll 10.244.21.83 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    May  6 08:16:02.489: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.21.83 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5565 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 08:16:02.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 08:16:02.490: INFO: ExecWithOptions: Clientset creation
    May  6 08:16:02.490: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5565/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.21.83+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  6 08:16:03.541: INFO: Found all 1 expected endpoints: [netserver-1]
    May  6 08:16:03.541: INFO: Going to poll 10.244.20.164 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    May  6 08:16:03.543: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.20.164 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5565 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 08:16:03.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 08:16:03.544: INFO: ExecWithOptions: Clientset creation
    May  6 08:16:03.544: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5565/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.20.164+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  6 08:16:04.597: INFO: Found all 1 expected endpoints: [netserver-2]
    May  6 08:16:04.597: INFO: Going to poll 10.244.245.100 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    May  6 08:16:04.599: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.245.100 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5565 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 08:16:04.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 08:16:04.600: INFO: ExecWithOptions: Clientset creation
    May  6 08:16:04.600: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5565/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.245.100+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  6 08:16:05.664: INFO: Found all 1 expected endpoints: [netserver-3]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    May  6 08:16:05.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-5565" for this suite. 05/06/23 08:16:05.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:16:05.675
May  6 08:16:05.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename emptydir 05/06/23 08:16:05.676
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:16:06.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:16:06.691
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 05/06/23 08:16:06.692
May  6 08:16:06.701: INFO: Waiting up to 5m0s for pod "pod-41b32a61-551d-4082-8bd2-cbc470b644e4" in namespace "emptydir-614" to be "Succeeded or Failed"
May  6 08:16:06.703: INFO: Pod "pod-41b32a61-551d-4082-8bd2-cbc470b644e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.168131ms
May  6 08:16:08.706: INFO: Pod "pod-41b32a61-551d-4082-8bd2-cbc470b644e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005840636s
May  6 08:16:10.706: INFO: Pod "pod-41b32a61-551d-4082-8bd2-cbc470b644e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005643212s
STEP: Saw pod success 05/06/23 08:16:10.706
May  6 08:16:10.706: INFO: Pod "pod-41b32a61-551d-4082-8bd2-cbc470b644e4" satisfied condition "Succeeded or Failed"
May  6 08:16:10.709: INFO: Trying to get logs from node cncf-3 pod pod-41b32a61-551d-4082-8bd2-cbc470b644e4 container test-container: <nil>
STEP: delete the pod 05/06/23 08:16:10.719
May  6 08:16:10.729: INFO: Waiting for pod pod-41b32a61-551d-4082-8bd2-cbc470b644e4 to disappear
May  6 08:16:10.732: INFO: Pod pod-41b32a61-551d-4082-8bd2-cbc470b644e4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  6 08:16:10.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-614" for this suite. 05/06/23 08:16:10.736
------------------------------
â€¢ [SLOW TEST] [5.067 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:16:05.675
    May  6 08:16:05.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename emptydir 05/06/23 08:16:05.676
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:16:06.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:16:06.691
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 05/06/23 08:16:06.692
    May  6 08:16:06.701: INFO: Waiting up to 5m0s for pod "pod-41b32a61-551d-4082-8bd2-cbc470b644e4" in namespace "emptydir-614" to be "Succeeded or Failed"
    May  6 08:16:06.703: INFO: Pod "pod-41b32a61-551d-4082-8bd2-cbc470b644e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.168131ms
    May  6 08:16:08.706: INFO: Pod "pod-41b32a61-551d-4082-8bd2-cbc470b644e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005840636s
    May  6 08:16:10.706: INFO: Pod "pod-41b32a61-551d-4082-8bd2-cbc470b644e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005643212s
    STEP: Saw pod success 05/06/23 08:16:10.706
    May  6 08:16:10.706: INFO: Pod "pod-41b32a61-551d-4082-8bd2-cbc470b644e4" satisfied condition "Succeeded or Failed"
    May  6 08:16:10.709: INFO: Trying to get logs from node cncf-3 pod pod-41b32a61-551d-4082-8bd2-cbc470b644e4 container test-container: <nil>
    STEP: delete the pod 05/06/23 08:16:10.719
    May  6 08:16:10.729: INFO: Waiting for pod pod-41b32a61-551d-4082-8bd2-cbc470b644e4 to disappear
    May  6 08:16:10.732: INFO: Pod pod-41b32a61-551d-4082-8bd2-cbc470b644e4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  6 08:16:10.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-614" for this suite. 05/06/23 08:16:10.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:16:10.743
May  6 08:16:10.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 05/06/23 08:16:10.744
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:16:11.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:16:11.76
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 05/06/23 08:16:11.761
STEP: Creating hostNetwork=false pod 05/06/23 08:16:11.761
May  6 08:16:11.770: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-5640" to be "running and ready"
May  6 08:16:11.772: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.390923ms
May  6 08:16:11.772: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May  6 08:16:13.775: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005765537s
May  6 08:16:13.775: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May  6 08:16:15.775: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005888659s
May  6 08:16:15.775: INFO: The phase of Pod test-pod is Running (Ready = true)
May  6 08:16:15.775: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 05/06/23 08:16:15.778
May  6 08:16:15.784: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-5640" to be "running and ready"
May  6 08:16:15.787: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.567038ms
May  6 08:16:15.787: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
May  6 08:16:17.790: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005776234s
May  6 08:16:17.790: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
May  6 08:16:17.790: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 05/06/23 08:16:17.793
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 05/06/23 08:16:17.793
May  6 08:16:17.793: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 08:16:17.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 08:16:17.794: INFO: ExecWithOptions: Clientset creation
May  6 08:16:17.794: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  6 08:16:17.849: INFO: Exec stderr: ""
May  6 08:16:17.849: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 08:16:17.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 08:16:17.849: INFO: ExecWithOptions: Clientset creation
May  6 08:16:17.849: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  6 08:16:17.880: INFO: Exec stderr: ""
May  6 08:16:17.880: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 08:16:17.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 08:16:17.881: INFO: ExecWithOptions: Clientset creation
May  6 08:16:17.881: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  6 08:16:17.932: INFO: Exec stderr: ""
May  6 08:16:17.932: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 08:16:17.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 08:16:17.932: INFO: ExecWithOptions: Clientset creation
May  6 08:16:17.932: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  6 08:16:17.967: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 05/06/23 08:16:17.967
May  6 08:16:17.967: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 08:16:17.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 08:16:17.967: INFO: ExecWithOptions: Clientset creation
May  6 08:16:17.967: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
May  6 08:16:18.022: INFO: Exec stderr: ""
May  6 08:16:18.022: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 08:16:18.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 08:16:18.023: INFO: ExecWithOptions: Clientset creation
May  6 08:16:18.023: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
May  6 08:16:18.087: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 05/06/23 08:16:18.087
May  6 08:16:18.087: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 08:16:18.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 08:16:18.087: INFO: ExecWithOptions: Clientset creation
May  6 08:16:18.087: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  6 08:16:18.136: INFO: Exec stderr: ""
May  6 08:16:18.136: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 08:16:18.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 08:16:18.137: INFO: ExecWithOptions: Clientset creation
May  6 08:16:18.137: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  6 08:16:18.168: INFO: Exec stderr: ""
May  6 08:16:18.168: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 08:16:18.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 08:16:18.168: INFO: ExecWithOptions: Clientset creation
May  6 08:16:18.169: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  6 08:16:18.203: INFO: Exec stderr: ""
May  6 08:16:18.203: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 08:16:18.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 08:16:18.204: INFO: ExecWithOptions: Clientset creation
May  6 08:16:18.204: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  6 08:16:18.264: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
May  6 08:16:18.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5640" for this suite. 05/06/23 08:16:18.268
------------------------------
â€¢ [SLOW TEST] [7.529 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:16:10.743
    May  6 08:16:10.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 05/06/23 08:16:10.744
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:16:11.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:16:11.76
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 05/06/23 08:16:11.761
    STEP: Creating hostNetwork=false pod 05/06/23 08:16:11.761
    May  6 08:16:11.770: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-5640" to be "running and ready"
    May  6 08:16:11.772: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.390923ms
    May  6 08:16:11.772: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:16:13.775: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005765537s
    May  6 08:16:13.775: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:16:15.775: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005888659s
    May  6 08:16:15.775: INFO: The phase of Pod test-pod is Running (Ready = true)
    May  6 08:16:15.775: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 05/06/23 08:16:15.778
    May  6 08:16:15.784: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-5640" to be "running and ready"
    May  6 08:16:15.787: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.567038ms
    May  6 08:16:15.787: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:16:17.790: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005776234s
    May  6 08:16:17.790: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    May  6 08:16:17.790: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 05/06/23 08:16:17.793
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 05/06/23 08:16:17.793
    May  6 08:16:17.793: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 08:16:17.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 08:16:17.794: INFO: ExecWithOptions: Clientset creation
    May  6 08:16:17.794: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  6 08:16:17.849: INFO: Exec stderr: ""
    May  6 08:16:17.849: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 08:16:17.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 08:16:17.849: INFO: ExecWithOptions: Clientset creation
    May  6 08:16:17.849: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  6 08:16:17.880: INFO: Exec stderr: ""
    May  6 08:16:17.880: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 08:16:17.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 08:16:17.881: INFO: ExecWithOptions: Clientset creation
    May  6 08:16:17.881: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  6 08:16:17.932: INFO: Exec stderr: ""
    May  6 08:16:17.932: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 08:16:17.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 08:16:17.932: INFO: ExecWithOptions: Clientset creation
    May  6 08:16:17.932: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  6 08:16:17.967: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 05/06/23 08:16:17.967
    May  6 08:16:17.967: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 08:16:17.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 08:16:17.967: INFO: ExecWithOptions: Clientset creation
    May  6 08:16:17.967: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    May  6 08:16:18.022: INFO: Exec stderr: ""
    May  6 08:16:18.022: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 08:16:18.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 08:16:18.023: INFO: ExecWithOptions: Clientset creation
    May  6 08:16:18.023: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    May  6 08:16:18.087: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 05/06/23 08:16:18.087
    May  6 08:16:18.087: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 08:16:18.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 08:16:18.087: INFO: ExecWithOptions: Clientset creation
    May  6 08:16:18.087: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  6 08:16:18.136: INFO: Exec stderr: ""
    May  6 08:16:18.136: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 08:16:18.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 08:16:18.137: INFO: ExecWithOptions: Clientset creation
    May  6 08:16:18.137: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  6 08:16:18.168: INFO: Exec stderr: ""
    May  6 08:16:18.168: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 08:16:18.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 08:16:18.168: INFO: ExecWithOptions: Clientset creation
    May  6 08:16:18.169: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  6 08:16:18.203: INFO: Exec stderr: ""
    May  6 08:16:18.203: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5640 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 08:16:18.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 08:16:18.204: INFO: ExecWithOptions: Clientset creation
    May  6 08:16:18.204: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5640/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  6 08:16:18.264: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    May  6 08:16:18.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-5640" for this suite. 05/06/23 08:16:18.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:16:18.273
May  6 08:16:18.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename secrets 05/06/23 08:16:18.274
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:16:19.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:16:19.291
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-67bfed8e-56ca-4a26-96d2-04baaa49a40d 05/06/23 08:16:19.292
STEP: Creating a pod to test consume secrets 05/06/23 08:16:19.296
May  6 08:16:19.304: INFO: Waiting up to 5m0s for pod "pod-secrets-8aee1d9f-bce5-4f26-bbb6-c38759c8bb3a" in namespace "secrets-2863" to be "Succeeded or Failed"
May  6 08:16:19.306: INFO: Pod "pod-secrets-8aee1d9f-bce5-4f26-bbb6-c38759c8bb3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013236ms
May  6 08:16:21.309: INFO: Pod "pod-secrets-8aee1d9f-bce5-4f26-bbb6-c38759c8bb3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00495703s
May  6 08:16:23.310: INFO: Pod "pod-secrets-8aee1d9f-bce5-4f26-bbb6-c38759c8bb3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005871484s
STEP: Saw pod success 05/06/23 08:16:23.31
May  6 08:16:23.310: INFO: Pod "pod-secrets-8aee1d9f-bce5-4f26-bbb6-c38759c8bb3a" satisfied condition "Succeeded or Failed"
May  6 08:16:23.312: INFO: Trying to get logs from node cncf-2 pod pod-secrets-8aee1d9f-bce5-4f26-bbb6-c38759c8bb3a container secret-volume-test: <nil>
STEP: delete the pod 05/06/23 08:16:23.322
May  6 08:16:23.333: INFO: Waiting for pod pod-secrets-8aee1d9f-bce5-4f26-bbb6-c38759c8bb3a to disappear
May  6 08:16:23.337: INFO: Pod pod-secrets-8aee1d9f-bce5-4f26-bbb6-c38759c8bb3a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May  6 08:16:23.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2863" for this suite. 05/06/23 08:16:23.341
------------------------------
â€¢ [SLOW TEST] [5.073 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:16:18.273
    May  6 08:16:18.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename secrets 05/06/23 08:16:18.274
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:16:19.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:16:19.291
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-67bfed8e-56ca-4a26-96d2-04baaa49a40d 05/06/23 08:16:19.292
    STEP: Creating a pod to test consume secrets 05/06/23 08:16:19.296
    May  6 08:16:19.304: INFO: Waiting up to 5m0s for pod "pod-secrets-8aee1d9f-bce5-4f26-bbb6-c38759c8bb3a" in namespace "secrets-2863" to be "Succeeded or Failed"
    May  6 08:16:19.306: INFO: Pod "pod-secrets-8aee1d9f-bce5-4f26-bbb6-c38759c8bb3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013236ms
    May  6 08:16:21.309: INFO: Pod "pod-secrets-8aee1d9f-bce5-4f26-bbb6-c38759c8bb3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00495703s
    May  6 08:16:23.310: INFO: Pod "pod-secrets-8aee1d9f-bce5-4f26-bbb6-c38759c8bb3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005871484s
    STEP: Saw pod success 05/06/23 08:16:23.31
    May  6 08:16:23.310: INFO: Pod "pod-secrets-8aee1d9f-bce5-4f26-bbb6-c38759c8bb3a" satisfied condition "Succeeded or Failed"
    May  6 08:16:23.312: INFO: Trying to get logs from node cncf-2 pod pod-secrets-8aee1d9f-bce5-4f26-bbb6-c38759c8bb3a container secret-volume-test: <nil>
    STEP: delete the pod 05/06/23 08:16:23.322
    May  6 08:16:23.333: INFO: Waiting for pod pod-secrets-8aee1d9f-bce5-4f26-bbb6-c38759c8bb3a to disappear
    May  6 08:16:23.337: INFO: Pod pod-secrets-8aee1d9f-bce5-4f26-bbb6-c38759c8bb3a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May  6 08:16:23.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2863" for this suite. 05/06/23 08:16:23.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:16:23.347
May  6 08:16:23.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename containers 05/06/23 08:16:23.348
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:16:24.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:16:24.362
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
May  6 08:16:24.372: INFO: Waiting up to 5m0s for pod "client-containers-d53db385-f446-4a69-9ac2-93c85de7d466" in namespace "containers-8251" to be "running"
May  6 08:16:24.374: INFO: Pod "client-containers-d53db385-f446-4a69-9ac2-93c85de7d466": Phase="Pending", Reason="", readiness=false. Elapsed: 2.169353ms
May  6 08:16:26.377: INFO: Pod "client-containers-d53db385-f446-4a69-9ac2-93c85de7d466": Phase="Running", Reason="", readiness=true. Elapsed: 2.005220052s
May  6 08:16:26.377: INFO: Pod "client-containers-d53db385-f446-4a69-9ac2-93c85de7d466" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
May  6 08:16:26.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-8251" for this suite. 05/06/23 08:16:26.387
------------------------------
â€¢ [3.045 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:16:23.347
    May  6 08:16:23.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename containers 05/06/23 08:16:23.348
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:16:24.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:16:24.362
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    May  6 08:16:24.372: INFO: Waiting up to 5m0s for pod "client-containers-d53db385-f446-4a69-9ac2-93c85de7d466" in namespace "containers-8251" to be "running"
    May  6 08:16:24.374: INFO: Pod "client-containers-d53db385-f446-4a69-9ac2-93c85de7d466": Phase="Pending", Reason="", readiness=false. Elapsed: 2.169353ms
    May  6 08:16:26.377: INFO: Pod "client-containers-d53db385-f446-4a69-9ac2-93c85de7d466": Phase="Running", Reason="", readiness=true. Elapsed: 2.005220052s
    May  6 08:16:26.377: INFO: Pod "client-containers-d53db385-f446-4a69-9ac2-93c85de7d466" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    May  6 08:16:26.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-8251" for this suite. 05/06/23 08:16:26.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:16:26.393
May  6 08:16:26.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename daemonsets 05/06/23 08:16:26.394
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:16:27.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:16:27.412
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
May  6 08:16:28.432: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 05/06/23 08:16:28.438
May  6 08:16:28.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 08:16:28.441: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 05/06/23 08:16:28.441
May  6 08:16:28.457: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 08:16:28.457: INFO: Node cncf-2 is running 0 daemon pod, expected 1
May  6 08:16:29.460: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 08:16:29.460: INFO: Node cncf-2 is running 0 daemon pod, expected 1
May  6 08:16:30.460: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  6 08:16:30.460: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 05/06/23 08:16:30.462
May  6 08:16:30.477: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  6 08:16:30.477: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
May  6 08:16:31.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 08:16:31.480: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 05/06/23 08:16:31.48
May  6 08:16:31.492: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 08:16:31.492: INFO: Node cncf-2 is running 0 daemon pod, expected 1
May  6 08:16:32.496: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 08:16:32.496: INFO: Node cncf-2 is running 0 daemon pod, expected 1
May  6 08:16:33.497: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 08:16:33.498: INFO: Node cncf-2 is running 0 daemon pod, expected 1
May  6 08:16:34.495: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  6 08:16:34.495: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/06/23 08:16:34.499
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8880, will wait for the garbage collector to delete the pods 05/06/23 08:16:34.499
May  6 08:16:34.557: INFO: Deleting DaemonSet.extensions daemon-set took: 5.386486ms
May  6 08:16:34.658: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.774361ms
May  6 08:16:36.961: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  6 08:16:36.961: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  6 08:16:36.963: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"173330"},"items":null}

May  6 08:16:36.965: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"173330"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 08:16:36.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8880" for this suite. 05/06/23 08:16:36.99
------------------------------
â€¢ [SLOW TEST] [10.602 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:16:26.393
    May  6 08:16:26.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename daemonsets 05/06/23 08:16:26.394
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:16:27.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:16:27.412
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    May  6 08:16:28.432: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 05/06/23 08:16:28.438
    May  6 08:16:28.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 08:16:28.441: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 05/06/23 08:16:28.441
    May  6 08:16:28.457: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 08:16:28.457: INFO: Node cncf-2 is running 0 daemon pod, expected 1
    May  6 08:16:29.460: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 08:16:29.460: INFO: Node cncf-2 is running 0 daemon pod, expected 1
    May  6 08:16:30.460: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  6 08:16:30.460: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 05/06/23 08:16:30.462
    May  6 08:16:30.477: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  6 08:16:30.477: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    May  6 08:16:31.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 08:16:31.480: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 05/06/23 08:16:31.48
    May  6 08:16:31.492: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 08:16:31.492: INFO: Node cncf-2 is running 0 daemon pod, expected 1
    May  6 08:16:32.496: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 08:16:32.496: INFO: Node cncf-2 is running 0 daemon pod, expected 1
    May  6 08:16:33.497: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 08:16:33.498: INFO: Node cncf-2 is running 0 daemon pod, expected 1
    May  6 08:16:34.495: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  6 08:16:34.495: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/06/23 08:16:34.499
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8880, will wait for the garbage collector to delete the pods 05/06/23 08:16:34.499
    May  6 08:16:34.557: INFO: Deleting DaemonSet.extensions daemon-set took: 5.386486ms
    May  6 08:16:34.658: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.774361ms
    May  6 08:16:36.961: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  6 08:16:36.961: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  6 08:16:36.963: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"173330"},"items":null}

    May  6 08:16:36.965: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"173330"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 08:16:36.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8880" for this suite. 05/06/23 08:16:36.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:16:36.996
May  6 08:16:36.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename svcaccounts 05/06/23 08:16:36.997
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:16:38.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:16:38.013
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
May  6 08:16:38.028: INFO: created pod
May  6 08:16:38.028: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6635" to be "Succeeded or Failed"
May  6 08:16:38.030: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.695773ms
May  6 08:16:40.034: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006276489s
May  6 08:16:42.037: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009624393s
STEP: Saw pod success 05/06/23 08:16:42.037
May  6 08:16:42.037: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
May  6 08:17:12.038: INFO: polling logs
May  6 08:17:12.043: INFO: Pod logs: 
I0506 08:16:38.686969       1 log.go:198] OK: Got token
I0506 08:16:38.687149       1 log.go:198] validating with in-cluster discovery
I0506 08:16:38.687455       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0506 08:16:38.687539       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6635:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683361598, NotBefore:1683360998, IssuedAt:1683360998, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6635", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ee48da85-4ab6-42aa-91fa-a1bfd68451c2"}}}
I0506 08:16:38.699112       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0506 08:16:38.704839       1 log.go:198] OK: Validated signature on JWT
I0506 08:16:38.704900       1 log.go:198] OK: Got valid claims from token!
I0506 08:16:38.704918       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6635:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683361598, NotBefore:1683360998, IssuedAt:1683360998, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6635", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ee48da85-4ab6-42aa-91fa-a1bfd68451c2"}}}

May  6 08:17:12.043: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May  6 08:17:12.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6635" for this suite. 05/06/23 08:17:12.052
------------------------------
â€¢ [SLOW TEST] [35.062 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:16:36.996
    May  6 08:16:36.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename svcaccounts 05/06/23 08:16:36.997
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:16:38.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:16:38.013
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    May  6 08:16:38.028: INFO: created pod
    May  6 08:16:38.028: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6635" to be "Succeeded or Failed"
    May  6 08:16:38.030: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.695773ms
    May  6 08:16:40.034: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006276489s
    May  6 08:16:42.037: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009624393s
    STEP: Saw pod success 05/06/23 08:16:42.037
    May  6 08:16:42.037: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    May  6 08:17:12.038: INFO: polling logs
    May  6 08:17:12.043: INFO: Pod logs: 
    I0506 08:16:38.686969       1 log.go:198] OK: Got token
    I0506 08:16:38.687149       1 log.go:198] validating with in-cluster discovery
    I0506 08:16:38.687455       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0506 08:16:38.687539       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6635:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683361598, NotBefore:1683360998, IssuedAt:1683360998, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6635", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ee48da85-4ab6-42aa-91fa-a1bfd68451c2"}}}
    I0506 08:16:38.699112       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0506 08:16:38.704839       1 log.go:198] OK: Validated signature on JWT
    I0506 08:16:38.704900       1 log.go:198] OK: Got valid claims from token!
    I0506 08:16:38.704918       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6635:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683361598, NotBefore:1683360998, IssuedAt:1683360998, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6635", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ee48da85-4ab6-42aa-91fa-a1bfd68451c2"}}}

    May  6 08:17:12.043: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May  6 08:17:12.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6635" for this suite. 05/06/23 08:17:12.052
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:17:12.059
May  6 08:17:12.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename statefulset 05/06/23 08:17:12.06
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:17:13.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:17:13.075
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7400 05/06/23 08:17:13.078
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 05/06/23 08:17:13.083
May  6 08:17:13.091: INFO: Found 0 stateful pods, waiting for 3
May  6 08:17:23.095: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  6 08:17:23.095: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  6 08:17:23.095: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May  6 08:17:23.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-7400 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 08:17:23.218: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 08:17:23.218: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 08:17:23.218: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 05/06/23 08:17:33.229
May  6 08:17:33.247: INFO: Updating stateful set ss2
STEP: Creating a new revision 05/06/23 08:17:33.247
STEP: Updating Pods in reverse ordinal order 05/06/23 08:17:43.26
May  6 08:17:43.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-7400 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 08:17:43.362: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 08:17:43.362: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 08:17:43.362: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 05/06/23 08:17:53.379
May  6 08:17:53.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-7400 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 08:17:53.478: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 08:17:53.478: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 08:17:53.478: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 08:18:03.510: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 05/06/23 08:18:13.521
May  6 08:18:13.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-7400 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 08:18:13.630: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 08:18:13.630: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 08:18:13.630: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May  6 08:18:23.646: INFO: Deleting all statefulset in ns statefulset-7400
May  6 08:18:23.648: INFO: Scaling statefulset ss2 to 0
May  6 08:18:33.662: INFO: Waiting for statefulset status.replicas updated to 0
May  6 08:18:33.664: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May  6 08:18:33.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7400" for this suite. 05/06/23 08:18:33.68
------------------------------
â€¢ [SLOW TEST] [81.627 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:17:12.059
    May  6 08:17:12.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename statefulset 05/06/23 08:17:12.06
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:17:13.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:17:13.075
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7400 05/06/23 08:17:13.078
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 05/06/23 08:17:13.083
    May  6 08:17:13.091: INFO: Found 0 stateful pods, waiting for 3
    May  6 08:17:23.095: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May  6 08:17:23.095: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May  6 08:17:23.095: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    May  6 08:17:23.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-7400 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  6 08:17:23.218: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  6 08:17:23.218: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  6 08:17:23.218: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 05/06/23 08:17:33.229
    May  6 08:17:33.247: INFO: Updating stateful set ss2
    STEP: Creating a new revision 05/06/23 08:17:33.247
    STEP: Updating Pods in reverse ordinal order 05/06/23 08:17:43.26
    May  6 08:17:43.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-7400 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  6 08:17:43.362: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  6 08:17:43.362: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  6 08:17:43.362: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 05/06/23 08:17:53.379
    May  6 08:17:53.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-7400 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  6 08:17:53.478: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  6 08:17:53.478: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  6 08:17:53.478: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  6 08:18:03.510: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 05/06/23 08:18:13.521
    May  6 08:18:13.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-7400 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  6 08:18:13.630: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  6 08:18:13.630: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  6 08:18:13.630: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May  6 08:18:23.646: INFO: Deleting all statefulset in ns statefulset-7400
    May  6 08:18:23.648: INFO: Scaling statefulset ss2 to 0
    May  6 08:18:33.662: INFO: Waiting for statefulset status.replicas updated to 0
    May  6 08:18:33.664: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May  6 08:18:33.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7400" for this suite. 05/06/23 08:18:33.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:18:33.686
May  6 08:18:33.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename events 05/06/23 08:18:33.687
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:18:34.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:18:34.702
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 05/06/23 08:18:34.704
STEP: listing events in all namespaces 05/06/23 08:18:34.711
STEP: listing events in test namespace 05/06/23 08:18:34.714
STEP: listing events with field selection filtering on source 05/06/23 08:18:34.716
STEP: listing events with field selection filtering on reportingController 05/06/23 08:18:34.718
STEP: getting the test event 05/06/23 08:18:34.72
STEP: patching the test event 05/06/23 08:18:34.721
STEP: getting the test event 05/06/23 08:18:34.729
STEP: updating the test event 05/06/23 08:18:34.731
STEP: getting the test event 05/06/23 08:18:34.736
STEP: deleting the test event 05/06/23 08:18:34.738
STEP: listing events in all namespaces 05/06/23 08:18:34.744
STEP: listing events in test namespace 05/06/23 08:18:34.747
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
May  6 08:18:34.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7560" for this suite. 05/06/23 08:18:34.753
------------------------------
â€¢ [1.071 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:18:33.686
    May  6 08:18:33.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename events 05/06/23 08:18:33.687
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:18:34.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:18:34.702
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 05/06/23 08:18:34.704
    STEP: listing events in all namespaces 05/06/23 08:18:34.711
    STEP: listing events in test namespace 05/06/23 08:18:34.714
    STEP: listing events with field selection filtering on source 05/06/23 08:18:34.716
    STEP: listing events with field selection filtering on reportingController 05/06/23 08:18:34.718
    STEP: getting the test event 05/06/23 08:18:34.72
    STEP: patching the test event 05/06/23 08:18:34.721
    STEP: getting the test event 05/06/23 08:18:34.729
    STEP: updating the test event 05/06/23 08:18:34.731
    STEP: getting the test event 05/06/23 08:18:34.736
    STEP: deleting the test event 05/06/23 08:18:34.738
    STEP: listing events in all namespaces 05/06/23 08:18:34.744
    STEP: listing events in test namespace 05/06/23 08:18:34.747
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    May  6 08:18:34.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7560" for this suite. 05/06/23 08:18:34.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:18:34.763
May  6 08:18:34.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename prestop 05/06/23 08:18:34.763
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:18:35.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:18:35.78
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-2163 05/06/23 08:18:35.781
STEP: Waiting for pods to come up. 05/06/23 08:18:35.788
May  6 08:18:35.788: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2163" to be "running"
May  6 08:18:35.791: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.535939ms
May  6 08:18:37.794: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.005782565s
May  6 08:18:37.794: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-2163 05/06/23 08:18:37.796
May  6 08:18:37.803: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2163" to be "running"
May  6 08:18:37.804: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.896013ms
May  6 08:18:39.807: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.004793392s
May  6 08:18:39.807: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 05/06/23 08:18:39.807
May  6 08:18:44.820: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 05/06/23 08:18:44.82
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
May  6 08:18:44.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-2163" for this suite. 05/06/23 08:18:44.835
------------------------------
â€¢ [SLOW TEST] [10.080 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:18:34.763
    May  6 08:18:34.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename prestop 05/06/23 08:18:34.763
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:18:35.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:18:35.78
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-2163 05/06/23 08:18:35.781
    STEP: Waiting for pods to come up. 05/06/23 08:18:35.788
    May  6 08:18:35.788: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2163" to be "running"
    May  6 08:18:35.791: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.535939ms
    May  6 08:18:37.794: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.005782565s
    May  6 08:18:37.794: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-2163 05/06/23 08:18:37.796
    May  6 08:18:37.803: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2163" to be "running"
    May  6 08:18:37.804: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.896013ms
    May  6 08:18:39.807: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.004793392s
    May  6 08:18:39.807: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 05/06/23 08:18:39.807
    May  6 08:18:44.820: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 05/06/23 08:18:44.82
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    May  6 08:18:44.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-2163" for this suite. 05/06/23 08:18:44.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:18:44.844
May  6 08:18:44.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 08:18:44.845
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:18:45.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:18:45.861
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-33741579-b42f-4314-a231-2bfcd64067b4 05/06/23 08:18:45.862
STEP: Creating a pod to test consume secrets 05/06/23 08:18:45.868
May  6 08:18:45.876: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8f9ff638-c722-4ff1-bc25-26adb3821b4c" in namespace "projected-8691" to be "Succeeded or Failed"
May  6 08:18:45.878: INFO: Pod "pod-projected-secrets-8f9ff638-c722-4ff1-bc25-26adb3821b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.369082ms
May  6 08:18:47.881: INFO: Pod "pod-projected-secrets-8f9ff638-c722-4ff1-bc25-26adb3821b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00517816s
May  6 08:18:49.881: INFO: Pod "pod-projected-secrets-8f9ff638-c722-4ff1-bc25-26adb3821b4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004818455s
STEP: Saw pod success 05/06/23 08:18:49.881
May  6 08:18:49.881: INFO: Pod "pod-projected-secrets-8f9ff638-c722-4ff1-bc25-26adb3821b4c" satisfied condition "Succeeded or Failed"
May  6 08:18:49.884: INFO: Trying to get logs from node cncf-0 pod pod-projected-secrets-8f9ff638-c722-4ff1-bc25-26adb3821b4c container projected-secret-volume-test: <nil>
STEP: delete the pod 05/06/23 08:18:49.893
May  6 08:18:49.905: INFO: Waiting for pod pod-projected-secrets-8f9ff638-c722-4ff1-bc25-26adb3821b4c to disappear
May  6 08:18:49.907: INFO: Pod pod-projected-secrets-8f9ff638-c722-4ff1-bc25-26adb3821b4c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May  6 08:18:49.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8691" for this suite. 05/06/23 08:18:49.91
------------------------------
â€¢ [SLOW TEST] [5.072 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:18:44.844
    May  6 08:18:44.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 08:18:44.845
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:18:45.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:18:45.861
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-33741579-b42f-4314-a231-2bfcd64067b4 05/06/23 08:18:45.862
    STEP: Creating a pod to test consume secrets 05/06/23 08:18:45.868
    May  6 08:18:45.876: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8f9ff638-c722-4ff1-bc25-26adb3821b4c" in namespace "projected-8691" to be "Succeeded or Failed"
    May  6 08:18:45.878: INFO: Pod "pod-projected-secrets-8f9ff638-c722-4ff1-bc25-26adb3821b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.369082ms
    May  6 08:18:47.881: INFO: Pod "pod-projected-secrets-8f9ff638-c722-4ff1-bc25-26adb3821b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00517816s
    May  6 08:18:49.881: INFO: Pod "pod-projected-secrets-8f9ff638-c722-4ff1-bc25-26adb3821b4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004818455s
    STEP: Saw pod success 05/06/23 08:18:49.881
    May  6 08:18:49.881: INFO: Pod "pod-projected-secrets-8f9ff638-c722-4ff1-bc25-26adb3821b4c" satisfied condition "Succeeded or Failed"
    May  6 08:18:49.884: INFO: Trying to get logs from node cncf-0 pod pod-projected-secrets-8f9ff638-c722-4ff1-bc25-26adb3821b4c container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/06/23 08:18:49.893
    May  6 08:18:49.905: INFO: Waiting for pod pod-projected-secrets-8f9ff638-c722-4ff1-bc25-26adb3821b4c to disappear
    May  6 08:18:49.907: INFO: Pod pod-projected-secrets-8f9ff638-c722-4ff1-bc25-26adb3821b4c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May  6 08:18:49.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8691" for this suite. 05/06/23 08:18:49.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:18:49.917
May  6 08:18:49.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename sysctl 05/06/23 08:18:49.917
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:18:50.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:18:50.931
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 05/06/23 08:18:50.934
STEP: Watching for error events or started pod 05/06/23 08:18:50.942
STEP: Waiting for pod completion 05/06/23 08:18:52.945
May  6 08:18:52.945: INFO: Waiting up to 3m0s for pod "sysctl-5d6324c8-a383-4972-9fcb-6e6df78b1f02" in namespace "sysctl-8399" to be "completed"
May  6 08:18:52.948: INFO: Pod "sysctl-5d6324c8-a383-4972-9fcb-6e6df78b1f02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.683551ms
May  6 08:18:54.952: INFO: Pod "sysctl-5d6324c8-a383-4972-9fcb-6e6df78b1f02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006384522s
May  6 08:18:54.952: INFO: Pod "sysctl-5d6324c8-a383-4972-9fcb-6e6df78b1f02" satisfied condition "completed"
STEP: Checking that the pod succeeded 05/06/23 08:18:54.954
STEP: Getting logs from the pod 05/06/23 08:18:54.954
STEP: Checking that the sysctl is actually updated 05/06/23 08:18:54.96
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May  6 08:18:54.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-8399" for this suite. 05/06/23 08:18:54.963
------------------------------
â€¢ [SLOW TEST] [5.051 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:18:49.917
    May  6 08:18:49.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename sysctl 05/06/23 08:18:49.917
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:18:50.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:18:50.931
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 05/06/23 08:18:50.934
    STEP: Watching for error events or started pod 05/06/23 08:18:50.942
    STEP: Waiting for pod completion 05/06/23 08:18:52.945
    May  6 08:18:52.945: INFO: Waiting up to 3m0s for pod "sysctl-5d6324c8-a383-4972-9fcb-6e6df78b1f02" in namespace "sysctl-8399" to be "completed"
    May  6 08:18:52.948: INFO: Pod "sysctl-5d6324c8-a383-4972-9fcb-6e6df78b1f02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.683551ms
    May  6 08:18:54.952: INFO: Pod "sysctl-5d6324c8-a383-4972-9fcb-6e6df78b1f02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006384522s
    May  6 08:18:54.952: INFO: Pod "sysctl-5d6324c8-a383-4972-9fcb-6e6df78b1f02" satisfied condition "completed"
    STEP: Checking that the pod succeeded 05/06/23 08:18:54.954
    STEP: Getting logs from the pod 05/06/23 08:18:54.954
    STEP: Checking that the sysctl is actually updated 05/06/23 08:18:54.96
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May  6 08:18:54.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-8399" for this suite. 05/06/23 08:18:54.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:18:54.97
May  6 08:18:54.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename secrets 05/06/23 08:18:54.97
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:18:55.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:18:55.987
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-a459fdf9-b6f7-492f-a496-7c8f6b8378fd 05/06/23 08:18:57.007
STEP: Creating a pod to test consume secrets 05/06/23 08:18:57.011
May  6 08:18:57.019: INFO: Waiting up to 5m0s for pod "pod-secrets-46b8bac6-1e8c-4d49-962c-a3874c266228" in namespace "secrets-5186" to be "Succeeded or Failed"
May  6 08:18:57.021: INFO: Pod "pod-secrets-46b8bac6-1e8c-4d49-962c-a3874c266228": Phase="Pending", Reason="", readiness=false. Elapsed: 2.371446ms
May  6 08:18:59.024: INFO: Pod "pod-secrets-46b8bac6-1e8c-4d49-962c-a3874c266228": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005435706s
May  6 08:19:01.024: INFO: Pod "pod-secrets-46b8bac6-1e8c-4d49-962c-a3874c266228": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005416368s
STEP: Saw pod success 05/06/23 08:19:01.025
May  6 08:19:01.025: INFO: Pod "pod-secrets-46b8bac6-1e8c-4d49-962c-a3874c266228" satisfied condition "Succeeded or Failed"
May  6 08:19:01.027: INFO: Trying to get logs from node cncf-0 pod pod-secrets-46b8bac6-1e8c-4d49-962c-a3874c266228 container secret-volume-test: <nil>
STEP: delete the pod 05/06/23 08:19:01.031
May  6 08:19:01.042: INFO: Waiting for pod pod-secrets-46b8bac6-1e8c-4d49-962c-a3874c266228 to disappear
May  6 08:19:01.044: INFO: Pod pod-secrets-46b8bac6-1e8c-4d49-962c-a3874c266228 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May  6 08:19:01.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5186" for this suite. 05/06/23 08:19:01.047
STEP: Destroying namespace "secret-namespace-8183" for this suite. 05/06/23 08:19:01.054
------------------------------
â€¢ [SLOW TEST] [6.090 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:18:54.97
    May  6 08:18:54.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename secrets 05/06/23 08:18:54.97
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:18:55.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:18:55.987
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-a459fdf9-b6f7-492f-a496-7c8f6b8378fd 05/06/23 08:18:57.007
    STEP: Creating a pod to test consume secrets 05/06/23 08:18:57.011
    May  6 08:18:57.019: INFO: Waiting up to 5m0s for pod "pod-secrets-46b8bac6-1e8c-4d49-962c-a3874c266228" in namespace "secrets-5186" to be "Succeeded or Failed"
    May  6 08:18:57.021: INFO: Pod "pod-secrets-46b8bac6-1e8c-4d49-962c-a3874c266228": Phase="Pending", Reason="", readiness=false. Elapsed: 2.371446ms
    May  6 08:18:59.024: INFO: Pod "pod-secrets-46b8bac6-1e8c-4d49-962c-a3874c266228": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005435706s
    May  6 08:19:01.024: INFO: Pod "pod-secrets-46b8bac6-1e8c-4d49-962c-a3874c266228": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005416368s
    STEP: Saw pod success 05/06/23 08:19:01.025
    May  6 08:19:01.025: INFO: Pod "pod-secrets-46b8bac6-1e8c-4d49-962c-a3874c266228" satisfied condition "Succeeded or Failed"
    May  6 08:19:01.027: INFO: Trying to get logs from node cncf-0 pod pod-secrets-46b8bac6-1e8c-4d49-962c-a3874c266228 container secret-volume-test: <nil>
    STEP: delete the pod 05/06/23 08:19:01.031
    May  6 08:19:01.042: INFO: Waiting for pod pod-secrets-46b8bac6-1e8c-4d49-962c-a3874c266228 to disappear
    May  6 08:19:01.044: INFO: Pod pod-secrets-46b8bac6-1e8c-4d49-962c-a3874c266228 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May  6 08:19:01.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5186" for this suite. 05/06/23 08:19:01.047
    STEP: Destroying namespace "secret-namespace-8183" for this suite. 05/06/23 08:19:01.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:19:01.06
May  6 08:19:01.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename sched-preemption 05/06/23 08:19:01.061
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:19:02.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:19:02.076
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
May  6 08:19:02.092: INFO: Waiting up to 1m0s for all nodes to be ready
May  6 08:20:02.126: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 05/06/23 08:20:02.129
May  6 08:20:02.157: INFO: Created pod: pod0-0-sched-preemption-low-priority
May  6 08:20:02.165: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May  6 08:20:02.188: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May  6 08:20:02.198: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May  6 08:20:02.226: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May  6 08:20:02.234: INFO: Created pod: pod2-1-sched-preemption-medium-priority
May  6 08:20:02.256: INFO: Created pod: pod3-0-sched-preemption-medium-priority
May  6 08:20:02.263: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 05/06/23 08:20:02.263
May  6 08:20:02.263: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6904" to be "running"
May  6 08:20:02.270: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.974465ms
May  6 08:20:04.273: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.01054887s
May  6 08:20:04.273: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
May  6 08:20:04.273: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6904" to be "running"
May  6 08:20:04.276: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.506323ms
May  6 08:20:04.276: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
May  6 08:20:04.276: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6904" to be "running"
May  6 08:20:04.278: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.830611ms
May  6 08:20:04.278: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
May  6 08:20:04.278: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6904" to be "running"
May  6 08:20:04.280: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.23633ms
May  6 08:20:04.280: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
May  6 08:20:04.280: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-6904" to be "running"
May  6 08:20:04.282: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.239096ms
May  6 08:20:04.282: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
May  6 08:20:04.282: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-6904" to be "running"
May  6 08:20:04.284: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.894511ms
May  6 08:20:04.284: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
May  6 08:20:04.284: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-6904" to be "running"
May  6 08:20:04.286: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.910271ms
May  6 08:20:04.286: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
May  6 08:20:04.286: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-6904" to be "running"
May  6 08:20:04.288: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.967098ms
May  6 08:20:04.288: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 05/06/23 08:20:04.288
May  6 08:20:04.295: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-6904" to be "running"
May  6 08:20:04.297: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.325259ms
May  6 08:20:06.302: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006813296s
May  6 08:20:08.301: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006032702s
May  6 08:20:10.301: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006038092s
May  6 08:20:10.301: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 08:20:10.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6904" for this suite. 05/06/23 08:20:10.368
------------------------------
â€¢ [SLOW TEST] [69.313 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:19:01.06
    May  6 08:19:01.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename sched-preemption 05/06/23 08:19:01.061
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:19:02.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:19:02.076
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    May  6 08:19:02.092: INFO: Waiting up to 1m0s for all nodes to be ready
    May  6 08:20:02.126: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 05/06/23 08:20:02.129
    May  6 08:20:02.157: INFO: Created pod: pod0-0-sched-preemption-low-priority
    May  6 08:20:02.165: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    May  6 08:20:02.188: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    May  6 08:20:02.198: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    May  6 08:20:02.226: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    May  6 08:20:02.234: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    May  6 08:20:02.256: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    May  6 08:20:02.263: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 05/06/23 08:20:02.263
    May  6 08:20:02.263: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6904" to be "running"
    May  6 08:20:02.270: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.974465ms
    May  6 08:20:04.273: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.01054887s
    May  6 08:20:04.273: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    May  6 08:20:04.273: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6904" to be "running"
    May  6 08:20:04.276: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.506323ms
    May  6 08:20:04.276: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    May  6 08:20:04.276: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6904" to be "running"
    May  6 08:20:04.278: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.830611ms
    May  6 08:20:04.278: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    May  6 08:20:04.278: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6904" to be "running"
    May  6 08:20:04.280: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.23633ms
    May  6 08:20:04.280: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    May  6 08:20:04.280: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-6904" to be "running"
    May  6 08:20:04.282: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.239096ms
    May  6 08:20:04.282: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    May  6 08:20:04.282: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-6904" to be "running"
    May  6 08:20:04.284: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.894511ms
    May  6 08:20:04.284: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    May  6 08:20:04.284: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-6904" to be "running"
    May  6 08:20:04.286: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.910271ms
    May  6 08:20:04.286: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    May  6 08:20:04.286: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-6904" to be "running"
    May  6 08:20:04.288: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.967098ms
    May  6 08:20:04.288: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 05/06/23 08:20:04.288
    May  6 08:20:04.295: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-6904" to be "running"
    May  6 08:20:04.297: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.325259ms
    May  6 08:20:06.302: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006813296s
    May  6 08:20:08.301: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006032702s
    May  6 08:20:10.301: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006038092s
    May  6 08:20:10.301: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 08:20:10.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6904" for this suite. 05/06/23 08:20:10.368
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:20:10.374
May  6 08:20:10.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename deployment 05/06/23 08:20:10.374
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:20:11.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:20:11.398
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 05/06/23 08:20:11.403
STEP: waiting for Deployment to be created 05/06/23 08:20:11.407
STEP: waiting for all Replicas to be Ready 05/06/23 08:20:11.408
May  6 08:20:11.410: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  6 08:20:11.410: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  6 08:20:11.418: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  6 08:20:11.418: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  6 08:20:11.430: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  6 08:20:11.430: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  6 08:20:11.455: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  6 08:20:11.455: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  6 08:20:12.364: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May  6 08:20:12.364: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May  6 08:20:12.652: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 05/06/23 08:20:12.652
W0506 08:20:12.659320      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May  6 08:20:12.660: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 05/06/23 08:20:12.66
May  6 08:20:12.661: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0
May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0
May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0
May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0
May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0
May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0
May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0
May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0
May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
May  6 08:20:12.669: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
May  6 08:20:12.669: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
May  6 08:20:12.685: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
May  6 08:20:12.685: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
May  6 08:20:12.710: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
May  6 08:20:12.711: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
May  6 08:20:12.716: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
May  6 08:20:12.716: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
May  6 08:20:14.387: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
May  6 08:20:14.387: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
May  6 08:20:14.407: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
STEP: listing Deployments 05/06/23 08:20:14.408
May  6 08:20:14.412: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 05/06/23 08:20:14.412
May  6 08:20:14.423: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 05/06/23 08:20:14.423
May  6 08:20:14.428: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  6 08:20:14.435: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  6 08:20:14.453: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  6 08:20:14.468: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  6 08:20:14.486: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  6 08:20:15.296: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  6 08:20:15.407: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
May  6 08:20:15.434: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  6 08:20:15.451: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  6 08:20:16.685: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 05/06/23 08:20:16.711
STEP: fetching the DeploymentStatus 05/06/23 08:20:16.716
May  6 08:20:16.721: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
May  6 08:20:16.721: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
May  6 08:20:16.722: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
May  6 08:20:16.722: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
May  6 08:20:16.722: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
May  6 08:20:16.722: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
May  6 08:20:16.722: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 3
May  6 08:20:16.722: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
May  6 08:20:16.722: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
May  6 08:20:16.722: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 3
STEP: deleting the Deployment 05/06/23 08:20:16.722
May  6 08:20:16.731: INFO: observed event type MODIFIED
May  6 08:20:16.731: INFO: observed event type MODIFIED
May  6 08:20:16.731: INFO: observed event type MODIFIED
May  6 08:20:16.731: INFO: observed event type MODIFIED
May  6 08:20:16.731: INFO: observed event type MODIFIED
May  6 08:20:16.731: INFO: observed event type MODIFIED
May  6 08:20:16.731: INFO: observed event type MODIFIED
May  6 08:20:16.731: INFO: observed event type MODIFIED
May  6 08:20:16.731: INFO: observed event type MODIFIED
May  6 08:20:16.731: INFO: observed event type MODIFIED
May  6 08:20:16.731: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  6 08:20:16.734: INFO: Log out all the ReplicaSets if there is no deployment created
May  6 08:20:16.741: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-8519  c911d07b-638d-402f-a827-3bcf4159318f 175173 2 2023-05-06 08:20:14 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 7f022bf9-094f-4da5-94fe-3a20905b9448 0xc002e7e3c7 0xc002e7e3c8}] [] [{kube-controller-manager Update apps/v1 2023-05-06 08:20:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f022bf9-094f-4da5-94fe-3a20905b9448\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 08:20:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e7e7e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

May  6 08:20:16.745: INFO: pod: "test-deployment-7b7876f9d6-gzl84":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-gzl84 test-deployment-7b7876f9d6- deployment-8519  a066d783-c084-4532-827d-b2aa5febe4e5 175035 0 2023-05-06 08:20:14 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:a47fb6cd8f7366cd6906bf7fa06fdcc8025f9e27d415541aeb5313ff13d68cd0 cni.projectcalico.org/podIP:10.244.174.168/32 cni.projectcalico.org/podIPs:10.244.174.168/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 c911d07b-638d-402f-a827-3bcf4159318f 0xc002e7f4f7 0xc002e7f4f8}] [] [{calico Update v1 2023-05-06 08:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-06 08:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c911d07b-638d-402f-a827-3bcf4159318f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 08:20:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.174.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5qkbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5qkbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:10.244.174.168,StartTime:2023-05-06 08:20:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 08:20:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5f81c2a8ee550ef2fb9b943bef463e86b539c93654a520810c6c8dfa4fd00e7e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.174.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May  6 08:20:16.745: INFO: pod: "test-deployment-7b7876f9d6-rltnq":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-rltnq test-deployment-7b7876f9d6- deployment-8519  67282845-6799-473d-aa42-f449bb583ad1 175171 0 2023-05-06 08:20:15 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:69ee67afcb638578b665f9aa2e49e9bfe0e3b62823f8b668be591dcee812714e cni.projectcalico.org/podIP:10.244.245.93/32 cni.projectcalico.org/podIPs:10.244.245.93/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 c911d07b-638d-402f-a827-3bcf4159318f 0xc002e7f727 0xc002e7f728}] [] [{calico Update v1 2023-05-06 08:20:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-06 08:20:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c911d07b-638d-402f-a827-3bcf4159318f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 08:20:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.245.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hrgx5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hrgx5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:10.244.245.93,StartTime:2023-05-06 08:20:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 08:20:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e39e9ea910d7e134dcd8199e735f5336673991e16030495a4669b61416c127be,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.245.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May  6 08:20:16.745: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-8519  7795b3fa-2fa3-4dda-a7dc-f0861c482110 175185 4 2023-05-06 08:20:12 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 7f022bf9-094f-4da5-94fe-3a20905b9448 0xc002e7e887 0xc002e7e888}] [] [{kube-controller-manager Update apps/v1 2023-05-06 08:20:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f022bf9-094f-4da5-94fe-3a20905b9448\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 08:20:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e7ec30 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May  6 08:20:16.751: INFO: pod: "test-deployment-7df74c55ff-52g7n":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-52g7n test-deployment-7df74c55ff- deployment-8519  389e4d84-9a6b-42bd-b849-b6f97628ac75 175181 0 2023-05-06 08:20:14 +0000 UTC 2023-05-06 08:20:17 +0000 UTC 0xc004bae708 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:6f646e8fd38efc6b3504c6fc6858a473d928a79450460021ec68a15da3bfd062 cni.projectcalico.org/podIP:10.244.20.172/32 cni.projectcalico.org/podIPs:10.244.20.172/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 7795b3fa-2fa3-4dda-a7dc-f0861c482110 0xc004bae757 0xc004bae758}] [] [{calico Update v1 2023-05-06 08:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-06 08:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7795b3fa-2fa3-4dda-a7dc-f0861c482110\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 08:20:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.20.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j64nt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j64nt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:10.244.20.172,StartTime:2023-05-06 08:20:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 08:20:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://7d7c8fc7e7b8d3420c4d745773944ef6997062344b1086a76de5fe7dce895c1f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.20.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May  6 08:20:16.751: INFO: pod: "test-deployment-7df74c55ff-9gqhq":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-9gqhq test-deployment-7df74c55ff- deployment-8519  8352e55e-72e1-4e21-8595-ac171a8888fe 175159 0 2023-05-06 08:20:12 +0000 UTC 2023-05-06 08:20:16 +0000 UTC 0xc004bae950 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:18b524fd03dca42abeaf9824c0b83c3dbd7e987d99469f5b5f2d1e8b01c9396b cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs:] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 7795b3fa-2fa3-4dda-a7dc-f0861c482110 0xc004bae987 0xc004bae988}] [] [{kube-controller-manager Update v1 2023-05-06 08:20:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7795b3fa-2fa3-4dda-a7dc-f0861c482110\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 08:20:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.174.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2023-05-06 08:20:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9cblm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9cblm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:10.244.174.171,StartTime:2023-05-06 08:20:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 08:20:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://6d5b540fec2ee4bafd3b0c3144579f696ca96165ee26b68937efde5e96635090,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.174.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May  6 08:20:16.751: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-8519  6bccce2d-ff16-497e-a2b8-02ddc456e964 174980 3 2023-05-06 08:20:11 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 7f022bf9-094f-4da5-94fe-3a20905b9448 0xc002e7ed87 0xc002e7ed88}] [] [{kube-controller-manager Update apps/v1 2023-05-06 08:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f022bf9-094f-4da5-94fe-3a20905b9448\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 08:20:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e7f0a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May  6 08:20:16.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8519" for this suite. 05/06/23 08:20:16.761
------------------------------
â€¢ [SLOW TEST] [6.399 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:20:10.374
    May  6 08:20:10.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename deployment 05/06/23 08:20:10.374
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:20:11.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:20:11.398
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 05/06/23 08:20:11.403
    STEP: waiting for Deployment to be created 05/06/23 08:20:11.407
    STEP: waiting for all Replicas to be Ready 05/06/23 08:20:11.408
    May  6 08:20:11.410: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  6 08:20:11.410: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  6 08:20:11.418: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  6 08:20:11.418: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  6 08:20:11.430: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  6 08:20:11.430: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  6 08:20:11.455: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  6 08:20:11.455: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  6 08:20:12.364: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    May  6 08:20:12.364: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    May  6 08:20:12.652: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 05/06/23 08:20:12.652
    W0506 08:20:12.659320      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May  6 08:20:12.660: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 05/06/23 08:20:12.66
    May  6 08:20:12.661: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0
    May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0
    May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0
    May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0
    May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0
    May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0
    May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0
    May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 0
    May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
    May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
    May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
    May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
    May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
    May  6 08:20:12.662: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
    May  6 08:20:12.669: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
    May  6 08:20:12.669: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
    May  6 08:20:12.685: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
    May  6 08:20:12.685: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
    May  6 08:20:12.710: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
    May  6 08:20:12.711: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
    May  6 08:20:12.716: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
    May  6 08:20:12.716: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
    May  6 08:20:14.387: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
    May  6 08:20:14.387: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
    May  6 08:20:14.407: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
    STEP: listing Deployments 05/06/23 08:20:14.408
    May  6 08:20:14.412: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 05/06/23 08:20:14.412
    May  6 08:20:14.423: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 05/06/23 08:20:14.423
    May  6 08:20:14.428: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  6 08:20:14.435: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  6 08:20:14.453: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  6 08:20:14.468: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  6 08:20:14.486: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  6 08:20:15.296: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May  6 08:20:15.407: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    May  6 08:20:15.434: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May  6 08:20:15.451: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May  6 08:20:16.685: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 05/06/23 08:20:16.711
    STEP: fetching the DeploymentStatus 05/06/23 08:20:16.716
    May  6 08:20:16.721: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
    May  6 08:20:16.721: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
    May  6 08:20:16.722: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
    May  6 08:20:16.722: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
    May  6 08:20:16.722: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 1
    May  6 08:20:16.722: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
    May  6 08:20:16.722: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 3
    May  6 08:20:16.722: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
    May  6 08:20:16.722: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 2
    May  6 08:20:16.722: INFO: observed Deployment test-deployment in namespace deployment-8519 with ReadyReplicas 3
    STEP: deleting the Deployment 05/06/23 08:20:16.722
    May  6 08:20:16.731: INFO: observed event type MODIFIED
    May  6 08:20:16.731: INFO: observed event type MODIFIED
    May  6 08:20:16.731: INFO: observed event type MODIFIED
    May  6 08:20:16.731: INFO: observed event type MODIFIED
    May  6 08:20:16.731: INFO: observed event type MODIFIED
    May  6 08:20:16.731: INFO: observed event type MODIFIED
    May  6 08:20:16.731: INFO: observed event type MODIFIED
    May  6 08:20:16.731: INFO: observed event type MODIFIED
    May  6 08:20:16.731: INFO: observed event type MODIFIED
    May  6 08:20:16.731: INFO: observed event type MODIFIED
    May  6 08:20:16.731: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  6 08:20:16.734: INFO: Log out all the ReplicaSets if there is no deployment created
    May  6 08:20:16.741: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-8519  c911d07b-638d-402f-a827-3bcf4159318f 175173 2 2023-05-06 08:20:14 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 7f022bf9-094f-4da5-94fe-3a20905b9448 0xc002e7e3c7 0xc002e7e3c8}] [] [{kube-controller-manager Update apps/v1 2023-05-06 08:20:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f022bf9-094f-4da5-94fe-3a20905b9448\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 08:20:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e7e7e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    May  6 08:20:16.745: INFO: pod: "test-deployment-7b7876f9d6-gzl84":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-gzl84 test-deployment-7b7876f9d6- deployment-8519  a066d783-c084-4532-827d-b2aa5febe4e5 175035 0 2023-05-06 08:20:14 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:a47fb6cd8f7366cd6906bf7fa06fdcc8025f9e27d415541aeb5313ff13d68cd0 cni.projectcalico.org/podIP:10.244.174.168/32 cni.projectcalico.org/podIPs:10.244.174.168/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 c911d07b-638d-402f-a827-3bcf4159318f 0xc002e7f4f7 0xc002e7f4f8}] [] [{calico Update v1 2023-05-06 08:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-06 08:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c911d07b-638d-402f-a827-3bcf4159318f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 08:20:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.174.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5qkbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5qkbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:10.244.174.168,StartTime:2023-05-06 08:20:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 08:20:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5f81c2a8ee550ef2fb9b943bef463e86b539c93654a520810c6c8dfa4fd00e7e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.174.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May  6 08:20:16.745: INFO: pod: "test-deployment-7b7876f9d6-rltnq":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-rltnq test-deployment-7b7876f9d6- deployment-8519  67282845-6799-473d-aa42-f449bb583ad1 175171 0 2023-05-06 08:20:15 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:69ee67afcb638578b665f9aa2e49e9bfe0e3b62823f8b668be591dcee812714e cni.projectcalico.org/podIP:10.244.245.93/32 cni.projectcalico.org/podIPs:10.244.245.93/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 c911d07b-638d-402f-a827-3bcf4159318f 0xc002e7f727 0xc002e7f728}] [] [{calico Update v1 2023-05-06 08:20:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-06 08:20:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c911d07b-638d-402f-a827-3bcf4159318f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 08:20:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.245.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hrgx5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hrgx5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.240,PodIP:10.244.245.93,StartTime:2023-05-06 08:20:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 08:20:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e39e9ea910d7e134dcd8199e735f5336673991e16030495a4669b61416c127be,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.245.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May  6 08:20:16.745: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-8519  7795b3fa-2fa3-4dda-a7dc-f0861c482110 175185 4 2023-05-06 08:20:12 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 7f022bf9-094f-4da5-94fe-3a20905b9448 0xc002e7e887 0xc002e7e888}] [] [{kube-controller-manager Update apps/v1 2023-05-06 08:20:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f022bf9-094f-4da5-94fe-3a20905b9448\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 08:20:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e7ec30 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    May  6 08:20:16.751: INFO: pod: "test-deployment-7df74c55ff-52g7n":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-52g7n test-deployment-7df74c55ff- deployment-8519  389e4d84-9a6b-42bd-b849-b6f97628ac75 175181 0 2023-05-06 08:20:14 +0000 UTC 2023-05-06 08:20:17 +0000 UTC 0xc004bae708 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:6f646e8fd38efc6b3504c6fc6858a473d928a79450460021ec68a15da3bfd062 cni.projectcalico.org/podIP:10.244.20.172/32 cni.projectcalico.org/podIPs:10.244.20.172/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 7795b3fa-2fa3-4dda-a7dc-f0861c482110 0xc004bae757 0xc004bae758}] [] [{calico Update v1 2023-05-06 08:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-06 08:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7795b3fa-2fa3-4dda-a7dc-f0861c482110\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 08:20:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.20.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j64nt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j64nt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.180,PodIP:10.244.20.172,StartTime:2023-05-06 08:20:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 08:20:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://7d7c8fc7e7b8d3420c4d745773944ef6997062344b1086a76de5fe7dce895c1f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.20.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May  6 08:20:16.751: INFO: pod: "test-deployment-7df74c55ff-9gqhq":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-9gqhq test-deployment-7df74c55ff- deployment-8519  8352e55e-72e1-4e21-8595-ac171a8888fe 175159 0 2023-05-06 08:20:12 +0000 UTC 2023-05-06 08:20:16 +0000 UTC 0xc004bae950 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:18b524fd03dca42abeaf9824c0b83c3dbd7e987d99469f5b5f2d1e8b01c9396b cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs:] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 7795b3fa-2fa3-4dda-a7dc-f0861c482110 0xc004bae987 0xc004bae988}] [] [{kube-controller-manager Update v1 2023-05-06 08:20:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7795b3fa-2fa3-4dda-a7dc-f0861c482110\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 08:20:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.174.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2023-05-06 08:20:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9cblm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9cblm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:20:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:10.244.174.171,StartTime:2023-05-06 08:20:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 08:20:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://6d5b540fec2ee4bafd3b0c3144579f696ca96165ee26b68937efde5e96635090,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.174.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May  6 08:20:16.751: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-8519  6bccce2d-ff16-497e-a2b8-02ddc456e964 174980 3 2023-05-06 08:20:11 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 7f022bf9-094f-4da5-94fe-3a20905b9448 0xc002e7ed87 0xc002e7ed88}] [] [{kube-controller-manager Update apps/v1 2023-05-06 08:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f022bf9-094f-4da5-94fe-3a20905b9448\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 08:20:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e7f0a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May  6 08:20:16.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8519" for this suite. 05/06/23 08:20:16.761
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:20:16.773
May  6 08:20:16.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename crd-publish-openapi 05/06/23 08:20:16.773
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:20:17.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:20:17.789
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
May  6 08:20:17.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/06/23 08:20:19.717
May  6 08:20:19.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6250 --namespace=crd-publish-openapi-6250 create -f -'
May  6 08:20:20.319: INFO: stderr: ""
May  6 08:20:20.319: INFO: stdout: "e2e-test-crd-publish-openapi-8500-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May  6 08:20:20.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6250 --namespace=crd-publish-openapi-6250 delete e2e-test-crd-publish-openapi-8500-crds test-cr'
May  6 08:20:20.400: INFO: stderr: ""
May  6 08:20:20.400: INFO: stdout: "e2e-test-crd-publish-openapi-8500-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May  6 08:20:20.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6250 --namespace=crd-publish-openapi-6250 apply -f -'
May  6 08:20:20.922: INFO: stderr: ""
May  6 08:20:20.922: INFO: stdout: "e2e-test-crd-publish-openapi-8500-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May  6 08:20:20.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6250 --namespace=crd-publish-openapi-6250 delete e2e-test-crd-publish-openapi-8500-crds test-cr'
May  6 08:20:20.982: INFO: stderr: ""
May  6 08:20:20.982: INFO: stdout: "e2e-test-crd-publish-openapi-8500-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 05/06/23 08:20:20.982
May  6 08:20:20.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6250 explain e2e-test-crd-publish-openapi-8500-crds'
May  6 08:20:21.122: INFO: stderr: ""
May  6 08:20:21.122: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8500-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 08:20:23.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6250" for this suite. 05/06/23 08:20:23.543
------------------------------
â€¢ [SLOW TEST] [6.777 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:20:16.773
    May  6 08:20:16.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename crd-publish-openapi 05/06/23 08:20:16.773
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:20:17.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:20:17.789
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    May  6 08:20:17.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/06/23 08:20:19.717
    May  6 08:20:19.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6250 --namespace=crd-publish-openapi-6250 create -f -'
    May  6 08:20:20.319: INFO: stderr: ""
    May  6 08:20:20.319: INFO: stdout: "e2e-test-crd-publish-openapi-8500-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    May  6 08:20:20.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6250 --namespace=crd-publish-openapi-6250 delete e2e-test-crd-publish-openapi-8500-crds test-cr'
    May  6 08:20:20.400: INFO: stderr: ""
    May  6 08:20:20.400: INFO: stdout: "e2e-test-crd-publish-openapi-8500-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    May  6 08:20:20.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6250 --namespace=crd-publish-openapi-6250 apply -f -'
    May  6 08:20:20.922: INFO: stderr: ""
    May  6 08:20:20.922: INFO: stdout: "e2e-test-crd-publish-openapi-8500-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    May  6 08:20:20.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6250 --namespace=crd-publish-openapi-6250 delete e2e-test-crd-publish-openapi-8500-crds test-cr'
    May  6 08:20:20.982: INFO: stderr: ""
    May  6 08:20:20.982: INFO: stdout: "e2e-test-crd-publish-openapi-8500-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 05/06/23 08:20:20.982
    May  6 08:20:20.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=crd-publish-openapi-6250 explain e2e-test-crd-publish-openapi-8500-crds'
    May  6 08:20:21.122: INFO: stderr: ""
    May  6 08:20:21.122: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8500-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 08:20:23.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6250" for this suite. 05/06/23 08:20:23.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:20:23.55
May  6 08:20:23.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename csiinlinevolumes 05/06/23 08:20:23.551
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:20:24.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:20:24.567
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 05/06/23 08:20:24.569
STEP: getting 05/06/23 08:20:24.581
STEP: listing 05/06/23 08:20:24.585
STEP: deleting 05/06/23 08:20:24.587
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
May  6 08:20:24.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-3269" for this suite. 05/06/23 08:20:24.604
------------------------------
â€¢ [1.060 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:20:23.55
    May  6 08:20:23.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename csiinlinevolumes 05/06/23 08:20:23.551
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:20:24.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:20:24.567
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 05/06/23 08:20:24.569
    STEP: getting 05/06/23 08:20:24.581
    STEP: listing 05/06/23 08:20:24.585
    STEP: deleting 05/06/23 08:20:24.587
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    May  6 08:20:24.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-3269" for this suite. 05/06/23 08:20:24.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:20:24.61
May  6 08:20:24.610: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubectl 05/06/23 08:20:24.611
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:20:25.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:20:25.626
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 05/06/23 08:20:25.628
May  6 08:20:25.628: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8659 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 05/06/23 08:20:25.666
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  6 08:20:25.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8659" for this suite. 05/06/23 08:20:25.674
------------------------------
â€¢ [1.070 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:20:24.61
    May  6 08:20:24.610: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubectl 05/06/23 08:20:24.611
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:20:25.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:20:25.626
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 05/06/23 08:20:25.628
    May  6 08:20:25.628: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8659 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 05/06/23 08:20:25.666
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  6 08:20:25.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8659" for this suite. 05/06/23 08:20:25.674
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:20:25.68
May  6 08:20:25.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename services 05/06/23 08:20:25.68
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:20:26.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:20:26.699
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-7561 05/06/23 08:20:26.701
STEP: creating service affinity-nodeport-transition in namespace services-7561 05/06/23 08:20:26.701
STEP: creating replication controller affinity-nodeport-transition in namespace services-7561 05/06/23 08:20:26.715
I0506 08:20:26.721207      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-7561, replica count: 3
I0506 08:20:29.773603      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 08:20:29.783: INFO: Creating new exec pod
May  6 08:20:29.790: INFO: Waiting up to 5m0s for pod "execpod-affinityblr6j" in namespace "services-7561" to be "running"
May  6 08:20:29.792: INFO: Pod "execpod-affinityblr6j": Phase="Pending", Reason="", readiness=false. Elapsed: 1.92562ms
May  6 08:20:31.796: INFO: Pod "execpod-affinityblr6j": Phase="Running", Reason="", readiness=true. Elapsed: 2.005708079s
May  6 08:20:31.796: INFO: Pod "execpod-affinityblr6j" satisfied condition "running"
May  6 08:20:32.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7561 exec execpod-affinityblr6j -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
May  6 08:20:32.911: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May  6 08:20:32.911: INFO: stdout: ""
May  6 08:20:32.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7561 exec execpod-affinityblr6j -- /bin/sh -x -c nc -v -z -w 2 10.96.112.43 80'
May  6 08:20:33.022: INFO: stderr: "+ nc -v -z -w 2 10.96.112.43 80\nConnection to 10.96.112.43 80 port [tcp/http] succeeded!\n"
May  6 08:20:33.022: INFO: stdout: ""
May  6 08:20:33.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7561 exec execpod-affinityblr6j -- /bin/sh -x -c nc -v -z -w 2 10.0.0.134 31872'
May  6 08:20:33.111: INFO: stderr: "+ nc -v -z -w 2 10.0.0.134 31872\nConnection to 10.0.0.134 31872 port [tcp/*] succeeded!\n"
May  6 08:20:33.111: INFO: stdout: ""
May  6 08:20:33.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7561 exec execpod-affinityblr6j -- /bin/sh -x -c nc -v -z -w 2 10.0.0.240 31872'
May  6 08:20:33.209: INFO: stderr: "+ nc -v -z -w 2 10.0.0.240 31872\nConnection to 10.0.0.240 31872 port [tcp/*] succeeded!\n"
May  6 08:20:33.209: INFO: stdout: ""
May  6 08:20:33.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7561 exec execpod-affinityblr6j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.134:31872/ ; done'
May  6 08:20:33.374: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n"
May  6 08:20:33.374: INFO: stdout: "\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-2xgvv\naffinity-nodeport-transition-2xgvv\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-2xgvv\naffinity-nodeport-transition-t8bq2\naffinity-nodeport-transition-t8bq2\naffinity-nodeport-transition-t8bq2\naffinity-nodeport-transition-2xgvv\naffinity-nodeport-transition-2xgvv\naffinity-nodeport-transition-2xgvv\naffinity-nodeport-transition-t8bq2\naffinity-nodeport-transition-t8bq2\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-2xgvv"
May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-2xgvv
May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-2xgvv
May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-2xgvv
May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-t8bq2
May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-t8bq2
May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-t8bq2
May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-2xgvv
May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-2xgvv
May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-2xgvv
May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-t8bq2
May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-t8bq2
May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-2xgvv
May  6 08:20:33.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7561 exec execpod-affinityblr6j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.134:31872/ ; done'
May  6 08:20:33.541: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n"
May  6 08:20:33.541: INFO: stdout: "\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz"
May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
May  6 08:20:33.541: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7561, will wait for the garbage collector to delete the pods 05/06/23 08:20:33.553
May  6 08:20:33.622: INFO: Deleting ReplicationController affinity-nodeport-transition took: 14.103913ms
May  6 08:20:33.723: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.924847ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  6 08:20:35.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7561" for this suite. 05/06/23 08:20:35.752
------------------------------
â€¢ [SLOW TEST] [10.078 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:20:25.68
    May  6 08:20:25.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename services 05/06/23 08:20:25.68
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:20:26.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:20:26.699
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-7561 05/06/23 08:20:26.701
    STEP: creating service affinity-nodeport-transition in namespace services-7561 05/06/23 08:20:26.701
    STEP: creating replication controller affinity-nodeport-transition in namespace services-7561 05/06/23 08:20:26.715
    I0506 08:20:26.721207      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-7561, replica count: 3
    I0506 08:20:29.773603      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  6 08:20:29.783: INFO: Creating new exec pod
    May  6 08:20:29.790: INFO: Waiting up to 5m0s for pod "execpod-affinityblr6j" in namespace "services-7561" to be "running"
    May  6 08:20:29.792: INFO: Pod "execpod-affinityblr6j": Phase="Pending", Reason="", readiness=false. Elapsed: 1.92562ms
    May  6 08:20:31.796: INFO: Pod "execpod-affinityblr6j": Phase="Running", Reason="", readiness=true. Elapsed: 2.005708079s
    May  6 08:20:31.796: INFO: Pod "execpod-affinityblr6j" satisfied condition "running"
    May  6 08:20:32.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7561 exec execpod-affinityblr6j -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    May  6 08:20:32.911: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    May  6 08:20:32.911: INFO: stdout: ""
    May  6 08:20:32.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7561 exec execpod-affinityblr6j -- /bin/sh -x -c nc -v -z -w 2 10.96.112.43 80'
    May  6 08:20:33.022: INFO: stderr: "+ nc -v -z -w 2 10.96.112.43 80\nConnection to 10.96.112.43 80 port [tcp/http] succeeded!\n"
    May  6 08:20:33.022: INFO: stdout: ""
    May  6 08:20:33.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7561 exec execpod-affinityblr6j -- /bin/sh -x -c nc -v -z -w 2 10.0.0.134 31872'
    May  6 08:20:33.111: INFO: stderr: "+ nc -v -z -w 2 10.0.0.134 31872\nConnection to 10.0.0.134 31872 port [tcp/*] succeeded!\n"
    May  6 08:20:33.111: INFO: stdout: ""
    May  6 08:20:33.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7561 exec execpod-affinityblr6j -- /bin/sh -x -c nc -v -z -w 2 10.0.0.240 31872'
    May  6 08:20:33.209: INFO: stderr: "+ nc -v -z -w 2 10.0.0.240 31872\nConnection to 10.0.0.240 31872 port [tcp/*] succeeded!\n"
    May  6 08:20:33.209: INFO: stdout: ""
    May  6 08:20:33.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7561 exec execpod-affinityblr6j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.134:31872/ ; done'
    May  6 08:20:33.374: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n"
    May  6 08:20:33.374: INFO: stdout: "\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-2xgvv\naffinity-nodeport-transition-2xgvv\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-2xgvv\naffinity-nodeport-transition-t8bq2\naffinity-nodeport-transition-t8bq2\naffinity-nodeport-transition-t8bq2\naffinity-nodeport-transition-2xgvv\naffinity-nodeport-transition-2xgvv\naffinity-nodeport-transition-2xgvv\naffinity-nodeport-transition-t8bq2\naffinity-nodeport-transition-t8bq2\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-2xgvv"
    May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-2xgvv
    May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-2xgvv
    May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-2xgvv
    May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-t8bq2
    May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-t8bq2
    May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-t8bq2
    May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-2xgvv
    May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-2xgvv
    May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-2xgvv
    May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-t8bq2
    May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-t8bq2
    May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.374: INFO: Received response from host: affinity-nodeport-transition-2xgvv
    May  6 08:20:33.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-7561 exec execpod-affinityblr6j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.134:31872/ ; done'
    May  6 08:20:33.541: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.134:31872/\n"
    May  6 08:20:33.541: INFO: stdout: "\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz\naffinity-nodeport-transition-mjgdz"
    May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.541: INFO: Received response from host: affinity-nodeport-transition-mjgdz
    May  6 08:20:33.541: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7561, will wait for the garbage collector to delete the pods 05/06/23 08:20:33.553
    May  6 08:20:33.622: INFO: Deleting ReplicationController affinity-nodeport-transition took: 14.103913ms
    May  6 08:20:33.723: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.924847ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  6 08:20:35.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7561" for this suite. 05/06/23 08:20:35.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:20:35.758
May  6 08:20:35.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename services 05/06/23 08:20:35.759
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:20:36.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:20:36.776
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-4737 05/06/23 08:20:36.778
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4737 to expose endpoints map[] 05/06/23 08:20:36.79
May  6 08:20:36.793: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
May  6 08:20:37.800: INFO: successfully validated that service multi-endpoint-test in namespace services-4737 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4737 05/06/23 08:20:37.8
May  6 08:20:37.806: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4737" to be "running and ready"
May  6 08:20:37.808: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.314107ms
May  6 08:20:37.808: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  6 08:20:39.811: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005166785s
May  6 08:20:39.811: INFO: The phase of Pod pod1 is Running (Ready = true)
May  6 08:20:39.811: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4737 to expose endpoints map[pod1:[100]] 05/06/23 08:20:39.813
May  6 08:20:39.820: INFO: successfully validated that service multi-endpoint-test in namespace services-4737 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-4737 05/06/23 08:20:39.82
May  6 08:20:39.826: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4737" to be "running and ready"
May  6 08:20:39.828: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.994781ms
May  6 08:20:39.828: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  6 08:20:41.831: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005042209s
May  6 08:20:41.831: INFO: The phase of Pod pod2 is Running (Ready = true)
May  6 08:20:41.831: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4737 to expose endpoints map[pod1:[100] pod2:[101]] 05/06/23 08:20:41.834
May  6 08:20:41.843: INFO: successfully validated that service multi-endpoint-test in namespace services-4737 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 05/06/23 08:20:41.843
May  6 08:20:41.843: INFO: Creating new exec pod
May  6 08:20:41.847: INFO: Waiting up to 5m0s for pod "execpodlnktl" in namespace "services-4737" to be "running"
May  6 08:20:41.850: INFO: Pod "execpodlnktl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.656979ms
May  6 08:20:43.854: INFO: Pod "execpodlnktl": Phase="Running", Reason="", readiness=true. Elapsed: 2.006581898s
May  6 08:20:43.854: INFO: Pod "execpodlnktl" satisfied condition "running"
May  6 08:20:44.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-4737 exec execpodlnktl -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
May  6 08:20:44.959: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
May  6 08:20:44.959: INFO: stdout: ""
May  6 08:20:44.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-4737 exec execpodlnktl -- /bin/sh -x -c nc -v -z -w 2 10.98.118.102 80'
May  6 08:20:45.069: INFO: stderr: "+ nc -v -z -w 2 10.98.118.102 80\nConnection to 10.98.118.102 80 port [tcp/http] succeeded!\n"
May  6 08:20:45.069: INFO: stdout: ""
May  6 08:20:45.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-4737 exec execpodlnktl -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
May  6 08:20:45.179: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
May  6 08:20:45.179: INFO: stdout: ""
May  6 08:20:45.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-4737 exec execpodlnktl -- /bin/sh -x -c nc -v -z -w 2 10.98.118.102 81'
May  6 08:20:45.285: INFO: stderr: "+ nc -v -z -w 2 10.98.118.102 81\nConnection to 10.98.118.102 81 port [tcp/*] succeeded!\n"
May  6 08:20:45.285: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-4737 05/06/23 08:20:45.285
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4737 to expose endpoints map[pod2:[101]] 05/06/23 08:20:45.296
May  6 08:20:46.315: INFO: successfully validated that service multi-endpoint-test in namespace services-4737 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-4737 05/06/23 08:20:46.315
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4737 to expose endpoints map[] 05/06/23 08:20:46.333
May  6 08:20:46.341: INFO: successfully validated that service multi-endpoint-test in namespace services-4737 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  6 08:20:46.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4737" for this suite. 05/06/23 08:20:46.366
------------------------------
â€¢ [SLOW TEST] [10.614 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:20:35.758
    May  6 08:20:35.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename services 05/06/23 08:20:35.759
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:20:36.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:20:36.776
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-4737 05/06/23 08:20:36.778
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4737 to expose endpoints map[] 05/06/23 08:20:36.79
    May  6 08:20:36.793: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    May  6 08:20:37.800: INFO: successfully validated that service multi-endpoint-test in namespace services-4737 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-4737 05/06/23 08:20:37.8
    May  6 08:20:37.806: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4737" to be "running and ready"
    May  6 08:20:37.808: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.314107ms
    May  6 08:20:37.808: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:20:39.811: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005166785s
    May  6 08:20:39.811: INFO: The phase of Pod pod1 is Running (Ready = true)
    May  6 08:20:39.811: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4737 to expose endpoints map[pod1:[100]] 05/06/23 08:20:39.813
    May  6 08:20:39.820: INFO: successfully validated that service multi-endpoint-test in namespace services-4737 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-4737 05/06/23 08:20:39.82
    May  6 08:20:39.826: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4737" to be "running and ready"
    May  6 08:20:39.828: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.994781ms
    May  6 08:20:39.828: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:20:41.831: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005042209s
    May  6 08:20:41.831: INFO: The phase of Pod pod2 is Running (Ready = true)
    May  6 08:20:41.831: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4737 to expose endpoints map[pod1:[100] pod2:[101]] 05/06/23 08:20:41.834
    May  6 08:20:41.843: INFO: successfully validated that service multi-endpoint-test in namespace services-4737 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 05/06/23 08:20:41.843
    May  6 08:20:41.843: INFO: Creating new exec pod
    May  6 08:20:41.847: INFO: Waiting up to 5m0s for pod "execpodlnktl" in namespace "services-4737" to be "running"
    May  6 08:20:41.850: INFO: Pod "execpodlnktl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.656979ms
    May  6 08:20:43.854: INFO: Pod "execpodlnktl": Phase="Running", Reason="", readiness=true. Elapsed: 2.006581898s
    May  6 08:20:43.854: INFO: Pod "execpodlnktl" satisfied condition "running"
    May  6 08:20:44.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-4737 exec execpodlnktl -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    May  6 08:20:44.959: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    May  6 08:20:44.959: INFO: stdout: ""
    May  6 08:20:44.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-4737 exec execpodlnktl -- /bin/sh -x -c nc -v -z -w 2 10.98.118.102 80'
    May  6 08:20:45.069: INFO: stderr: "+ nc -v -z -w 2 10.98.118.102 80\nConnection to 10.98.118.102 80 port [tcp/http] succeeded!\n"
    May  6 08:20:45.069: INFO: stdout: ""
    May  6 08:20:45.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-4737 exec execpodlnktl -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    May  6 08:20:45.179: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    May  6 08:20:45.179: INFO: stdout: ""
    May  6 08:20:45.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-4737 exec execpodlnktl -- /bin/sh -x -c nc -v -z -w 2 10.98.118.102 81'
    May  6 08:20:45.285: INFO: stderr: "+ nc -v -z -w 2 10.98.118.102 81\nConnection to 10.98.118.102 81 port [tcp/*] succeeded!\n"
    May  6 08:20:45.285: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-4737 05/06/23 08:20:45.285
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4737 to expose endpoints map[pod2:[101]] 05/06/23 08:20:45.296
    May  6 08:20:46.315: INFO: successfully validated that service multi-endpoint-test in namespace services-4737 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-4737 05/06/23 08:20:46.315
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4737 to expose endpoints map[] 05/06/23 08:20:46.333
    May  6 08:20:46.341: INFO: successfully validated that service multi-endpoint-test in namespace services-4737 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  6 08:20:46.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4737" for this suite. 05/06/23 08:20:46.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:20:46.373
May  6 08:20:46.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 08:20:46.373
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:20:47.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:20:47.392
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-63e30223-bef6-4f24-9b97-786688cdb8a9 05/06/23 08:20:47.394
STEP: Creating a pod to test consume secrets 05/06/23 08:20:47.402
May  6 08:20:47.408: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-52b39c36-f15b-4732-b4c2-5ddf04a534cb" in namespace "projected-6322" to be "Succeeded or Failed"
May  6 08:20:47.413: INFO: Pod "pod-projected-secrets-52b39c36-f15b-4732-b4c2-5ddf04a534cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.961089ms
May  6 08:20:49.417: INFO: Pod "pod-projected-secrets-52b39c36-f15b-4732-b4c2-5ddf04a534cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008791351s
May  6 08:20:51.416: INFO: Pod "pod-projected-secrets-52b39c36-f15b-4732-b4c2-5ddf04a534cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007856996s
STEP: Saw pod success 05/06/23 08:20:51.416
May  6 08:20:51.416: INFO: Pod "pod-projected-secrets-52b39c36-f15b-4732-b4c2-5ddf04a534cb" satisfied condition "Succeeded or Failed"
May  6 08:20:51.419: INFO: Trying to get logs from node cncf-0 pod pod-projected-secrets-52b39c36-f15b-4732-b4c2-5ddf04a534cb container projected-secret-volume-test: <nil>
STEP: delete the pod 05/06/23 08:20:51.427
May  6 08:20:51.438: INFO: Waiting for pod pod-projected-secrets-52b39c36-f15b-4732-b4c2-5ddf04a534cb to disappear
May  6 08:20:51.440: INFO: Pod pod-projected-secrets-52b39c36-f15b-4732-b4c2-5ddf04a534cb no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May  6 08:20:51.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6322" for this suite. 05/06/23 08:20:51.443
------------------------------
â€¢ [SLOW TEST] [5.078 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:20:46.373
    May  6 08:20:46.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 08:20:46.373
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:20:47.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:20:47.392
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-63e30223-bef6-4f24-9b97-786688cdb8a9 05/06/23 08:20:47.394
    STEP: Creating a pod to test consume secrets 05/06/23 08:20:47.402
    May  6 08:20:47.408: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-52b39c36-f15b-4732-b4c2-5ddf04a534cb" in namespace "projected-6322" to be "Succeeded or Failed"
    May  6 08:20:47.413: INFO: Pod "pod-projected-secrets-52b39c36-f15b-4732-b4c2-5ddf04a534cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.961089ms
    May  6 08:20:49.417: INFO: Pod "pod-projected-secrets-52b39c36-f15b-4732-b4c2-5ddf04a534cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008791351s
    May  6 08:20:51.416: INFO: Pod "pod-projected-secrets-52b39c36-f15b-4732-b4c2-5ddf04a534cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007856996s
    STEP: Saw pod success 05/06/23 08:20:51.416
    May  6 08:20:51.416: INFO: Pod "pod-projected-secrets-52b39c36-f15b-4732-b4c2-5ddf04a534cb" satisfied condition "Succeeded or Failed"
    May  6 08:20:51.419: INFO: Trying to get logs from node cncf-0 pod pod-projected-secrets-52b39c36-f15b-4732-b4c2-5ddf04a534cb container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/06/23 08:20:51.427
    May  6 08:20:51.438: INFO: Waiting for pod pod-projected-secrets-52b39c36-f15b-4732-b4c2-5ddf04a534cb to disappear
    May  6 08:20:51.440: INFO: Pod pod-projected-secrets-52b39c36-f15b-4732-b4c2-5ddf04a534cb no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May  6 08:20:51.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6322" for this suite. 05/06/23 08:20:51.443
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:20:51.452
May  6 08:20:51.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename gc 05/06/23 08:20:51.452
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:20:52.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:20:52.474
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 05/06/23 08:20:52.478
STEP: delete the rc 05/06/23 08:20:57.493
STEP: wait for the rc to be deleted 05/06/23 08:20:57.516
May  6 08:20:58.570: INFO: 80 pods remaining
May  6 08:20:58.570: INFO: 80 pods has nil DeletionTimestamp
May  6 08:20:58.570: INFO: 
May  6 08:20:59.591: INFO: 72 pods remaining
May  6 08:20:59.591: INFO: 71 pods has nil DeletionTimestamp
May  6 08:20:59.591: INFO: 
May  6 08:21:00.545: INFO: 60 pods remaining
May  6 08:21:00.545: INFO: 59 pods has nil DeletionTimestamp
May  6 08:21:00.545: INFO: 
May  6 08:21:01.528: INFO: 40 pods remaining
May  6 08:21:01.528: INFO: 40 pods has nil DeletionTimestamp
May  6 08:21:01.528: INFO: 
May  6 08:21:02.552: INFO: 31 pods remaining
May  6 08:21:02.552: INFO: 31 pods has nil DeletionTimestamp
May  6 08:21:02.552: INFO: 
May  6 08:21:03.523: INFO: 20 pods remaining
May  6 08:21:03.523: INFO: 20 pods has nil DeletionTimestamp
May  6 08:21:03.523: INFO: 
STEP: Gathering metrics 05/06/23 08:21:04.524
May  6 08:21:04.914: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-2" in namespace "kube-system" to be "running and ready"
May  6 08:21:04.919: INFO: Pod "kube-controller-manager-cncf-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.507286ms
May  6 08:21:04.919: INFO: The phase of Pod kube-controller-manager-cncf-2 is Running (Ready = true)
May  6 08:21:04.919: INFO: Pod "kube-controller-manager-cncf-2" satisfied condition "running and ready"
May  6 08:21:05.342: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May  6 08:21:05.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2059" for this suite. 05/06/23 08:21:05.346
------------------------------
â€¢ [SLOW TEST] [13.900 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:20:51.452
    May  6 08:20:51.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename gc 05/06/23 08:20:51.452
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:20:52.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:20:52.474
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 05/06/23 08:20:52.478
    STEP: delete the rc 05/06/23 08:20:57.493
    STEP: wait for the rc to be deleted 05/06/23 08:20:57.516
    May  6 08:20:58.570: INFO: 80 pods remaining
    May  6 08:20:58.570: INFO: 80 pods has nil DeletionTimestamp
    May  6 08:20:58.570: INFO: 
    May  6 08:20:59.591: INFO: 72 pods remaining
    May  6 08:20:59.591: INFO: 71 pods has nil DeletionTimestamp
    May  6 08:20:59.591: INFO: 
    May  6 08:21:00.545: INFO: 60 pods remaining
    May  6 08:21:00.545: INFO: 59 pods has nil DeletionTimestamp
    May  6 08:21:00.545: INFO: 
    May  6 08:21:01.528: INFO: 40 pods remaining
    May  6 08:21:01.528: INFO: 40 pods has nil DeletionTimestamp
    May  6 08:21:01.528: INFO: 
    May  6 08:21:02.552: INFO: 31 pods remaining
    May  6 08:21:02.552: INFO: 31 pods has nil DeletionTimestamp
    May  6 08:21:02.552: INFO: 
    May  6 08:21:03.523: INFO: 20 pods remaining
    May  6 08:21:03.523: INFO: 20 pods has nil DeletionTimestamp
    May  6 08:21:03.523: INFO: 
    STEP: Gathering metrics 05/06/23 08:21:04.524
    May  6 08:21:04.914: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-2" in namespace "kube-system" to be "running and ready"
    May  6 08:21:04.919: INFO: Pod "kube-controller-manager-cncf-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.507286ms
    May  6 08:21:04.919: INFO: The phase of Pod kube-controller-manager-cncf-2 is Running (Ready = true)
    May  6 08:21:04.919: INFO: Pod "kube-controller-manager-cncf-2" satisfied condition "running and ready"
    May  6 08:21:05.342: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May  6 08:21:05.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2059" for this suite. 05/06/23 08:21:05.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:21:05.353
May  6 08:21:05.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename container-runtime 05/06/23 08:21:05.354
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:21:06.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:21:06.375
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 05/06/23 08:21:06.377
STEP: wait for the container to reach Succeeded 05/06/23 08:21:06.385
STEP: get the container status 05/06/23 08:21:12.409
STEP: the container should be terminated 05/06/23 08:21:12.411
STEP: the termination message should be set 05/06/23 08:21:12.411
May  6 08:21:12.412: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 05/06/23 08:21:12.412
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May  6 08:21:12.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8057" for this suite. 05/06/23 08:21:12.428
------------------------------
â€¢ [SLOW TEST] [7.081 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:21:05.353
    May  6 08:21:05.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename container-runtime 05/06/23 08:21:05.354
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:21:06.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:21:06.375
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 05/06/23 08:21:06.377
    STEP: wait for the container to reach Succeeded 05/06/23 08:21:06.385
    STEP: get the container status 05/06/23 08:21:12.409
    STEP: the container should be terminated 05/06/23 08:21:12.411
    STEP: the termination message should be set 05/06/23 08:21:12.411
    May  6 08:21:12.412: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 05/06/23 08:21:12.412
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May  6 08:21:12.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8057" for this suite. 05/06/23 08:21:12.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:21:12.435
May  6 08:21:12.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename deployment 05/06/23 08:21:12.436
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:21:13.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:21:13.454
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 05/06/23 08:21:13.459
May  6 08:21:13.459: INFO: Creating simple deployment test-deployment-8kp7m
May  6 08:21:13.469: INFO: new replicaset for deployment "test-deployment-8kp7m" is yet to be created
STEP: Getting /status 05/06/23 08:21:15.479
May  6 08:21:15.482: INFO: Deployment test-deployment-8kp7m has Conditions: [{Available True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8kp7m-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 05/06/23 08:21:15.482
May  6 08:21:15.490: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 8, 21, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 8, 21, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 8, 21, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 8, 21, 13, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-8kp7m-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 05/06/23 08:21:15.49
May  6 08:21:15.491: INFO: Observed &Deployment event: ADDED
May  6 08:21:15.491: INFO: Observed Deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8kp7m-54bc444df"}
May  6 08:21:15.492: INFO: Observed &Deployment event: MODIFIED
May  6 08:21:15.492: INFO: Observed Deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8kp7m-54bc444df"}
May  6 08:21:15.492: INFO: Observed Deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  6 08:21:15.492: INFO: Observed &Deployment event: MODIFIED
May  6 08:21:15.492: INFO: Observed Deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  6 08:21:15.492: INFO: Observed Deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-8kp7m-54bc444df" is progressing.}
May  6 08:21:15.492: INFO: Observed &Deployment event: MODIFIED
May  6 08:21:15.492: INFO: Observed Deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  6 08:21:15.492: INFO: Observed Deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8kp7m-54bc444df" has successfully progressed.}
May  6 08:21:15.492: INFO: Observed &Deployment event: MODIFIED
May  6 08:21:15.492: INFO: Observed Deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  6 08:21:15.492: INFO: Observed Deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8kp7m-54bc444df" has successfully progressed.}
May  6 08:21:15.492: INFO: Found Deployment test-deployment-8kp7m in namespace deployment-8759 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  6 08:21:15.492: INFO: Deployment test-deployment-8kp7m has an updated status
STEP: patching the Statefulset Status 05/06/23 08:21:15.492
May  6 08:21:15.492: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May  6 08:21:15.497: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 05/06/23 08:21:15.497
May  6 08:21:15.498: INFO: Observed &Deployment event: ADDED
May  6 08:21:15.498: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8kp7m-54bc444df"}
May  6 08:21:15.499: INFO: Observed &Deployment event: MODIFIED
May  6 08:21:15.499: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8kp7m-54bc444df"}
May  6 08:21:15.499: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  6 08:21:15.499: INFO: Observed &Deployment event: MODIFIED
May  6 08:21:15.499: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  6 08:21:15.499: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-8kp7m-54bc444df" is progressing.}
May  6 08:21:15.499: INFO: Observed &Deployment event: MODIFIED
May  6 08:21:15.499: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  6 08:21:15.499: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8kp7m-54bc444df" has successfully progressed.}
May  6 08:21:15.499: INFO: Observed &Deployment event: MODIFIED
May  6 08:21:15.499: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  6 08:21:15.499: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8kp7m-54bc444df" has successfully progressed.}
May  6 08:21:15.499: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  6 08:21:15.499: INFO: Observed &Deployment event: MODIFIED
May  6 08:21:15.499: INFO: Found deployment test-deployment-8kp7m in namespace deployment-8759 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
May  6 08:21:15.499: INFO: Deployment test-deployment-8kp7m has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  6 08:21:15.502: INFO: Deployment "test-deployment-8kp7m":
&Deployment{ObjectMeta:{test-deployment-8kp7m  deployment-8759  ad7a51c6-a78a-4e80-a2d5-eb5fcde9f326 177798 1 2023-05-06 08:21:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-06 08:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 08:21:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-05-06 08:21:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052ed808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  6 08:21:15.504: INFO: New ReplicaSet "test-deployment-8kp7m-54bc444df" of Deployment "test-deployment-8kp7m":
&ReplicaSet{ObjectMeta:{test-deployment-8kp7m-54bc444df  deployment-8759  99b45645-c0ad-4e88-bab1-af2bf236d761 177790 1 2023-05-06 08:21:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-8kp7m ad7a51c6-a78a-4e80-a2d5-eb5fcde9f326 0xc0052edba0 0xc0052edba1}] [] [{kube-controller-manager Update apps/v1 2023-05-06 08:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ad7a51c6-a78a-4e80-a2d5-eb5fcde9f326\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 08:21:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052edc48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  6 08:21:15.507: INFO: Pod "test-deployment-8kp7m-54bc444df-zwbs7" is available:
&Pod{ObjectMeta:{test-deployment-8kp7m-54bc444df-zwbs7 test-deployment-8kp7m-54bc444df- deployment-8759  a9aeaa33-7430-4358-9b85-031d0ee204e6 177789 0 2023-05-06 08:21:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:a73f663238d19b30e7e0d08e4f32fd68837ec0290a55f42d8104a47e80bfb8ae cni.projectcalico.org/podIP:10.244.174.138/32 cni.projectcalico.org/podIPs:10.244.174.138/32] [{apps/v1 ReplicaSet test-deployment-8kp7m-54bc444df 99b45645-c0ad-4e88-bab1-af2bf236d761 0xc00514dd87 0xc00514dd88}] [] [{calico Update v1 2023-05-06 08:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-06 08:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99b45645-c0ad-4e88-bab1-af2bf236d761\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 08:21:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.174.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-424rb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-424rb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:21:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:21:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:10.244.174.138,StartTime:2023-05-06 08:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 08:21:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://253b2a18c767940715f8cab0dbba037de65be410520feda9f6df5ba5f8cd1a92,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.174.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May  6 08:21:15.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8759" for this suite. 05/06/23 08:21:15.51
------------------------------
â€¢ [3.082 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:21:12.435
    May  6 08:21:12.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename deployment 05/06/23 08:21:12.436
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:21:13.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:21:13.454
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 05/06/23 08:21:13.459
    May  6 08:21:13.459: INFO: Creating simple deployment test-deployment-8kp7m
    May  6 08:21:13.469: INFO: new replicaset for deployment "test-deployment-8kp7m" is yet to be created
    STEP: Getting /status 05/06/23 08:21:15.479
    May  6 08:21:15.482: INFO: Deployment test-deployment-8kp7m has Conditions: [{Available True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8kp7m-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 05/06/23 08:21:15.482
    May  6 08:21:15.490: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 8, 21, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 8, 21, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 6, 8, 21, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 6, 8, 21, 13, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-8kp7m-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 05/06/23 08:21:15.49
    May  6 08:21:15.491: INFO: Observed &Deployment event: ADDED
    May  6 08:21:15.491: INFO: Observed Deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8kp7m-54bc444df"}
    May  6 08:21:15.492: INFO: Observed &Deployment event: MODIFIED
    May  6 08:21:15.492: INFO: Observed Deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8kp7m-54bc444df"}
    May  6 08:21:15.492: INFO: Observed Deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  6 08:21:15.492: INFO: Observed &Deployment event: MODIFIED
    May  6 08:21:15.492: INFO: Observed Deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  6 08:21:15.492: INFO: Observed Deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-8kp7m-54bc444df" is progressing.}
    May  6 08:21:15.492: INFO: Observed &Deployment event: MODIFIED
    May  6 08:21:15.492: INFO: Observed Deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  6 08:21:15.492: INFO: Observed Deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8kp7m-54bc444df" has successfully progressed.}
    May  6 08:21:15.492: INFO: Observed &Deployment event: MODIFIED
    May  6 08:21:15.492: INFO: Observed Deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  6 08:21:15.492: INFO: Observed Deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8kp7m-54bc444df" has successfully progressed.}
    May  6 08:21:15.492: INFO: Found Deployment test-deployment-8kp7m in namespace deployment-8759 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  6 08:21:15.492: INFO: Deployment test-deployment-8kp7m has an updated status
    STEP: patching the Statefulset Status 05/06/23 08:21:15.492
    May  6 08:21:15.492: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May  6 08:21:15.497: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 05/06/23 08:21:15.497
    May  6 08:21:15.498: INFO: Observed &Deployment event: ADDED
    May  6 08:21:15.498: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8kp7m-54bc444df"}
    May  6 08:21:15.499: INFO: Observed &Deployment event: MODIFIED
    May  6 08:21:15.499: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8kp7m-54bc444df"}
    May  6 08:21:15.499: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  6 08:21:15.499: INFO: Observed &Deployment event: MODIFIED
    May  6 08:21:15.499: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  6 08:21:15.499: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:13 +0000 UTC 2023-05-06 08:21:13 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-8kp7m-54bc444df" is progressing.}
    May  6 08:21:15.499: INFO: Observed &Deployment event: MODIFIED
    May  6 08:21:15.499: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  6 08:21:15.499: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8kp7m-54bc444df" has successfully progressed.}
    May  6 08:21:15.499: INFO: Observed &Deployment event: MODIFIED
    May  6 08:21:15.499: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  6 08:21:15.499: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-06 08:21:14 +0000 UTC 2023-05-06 08:21:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8kp7m-54bc444df" has successfully progressed.}
    May  6 08:21:15.499: INFO: Observed deployment test-deployment-8kp7m in namespace deployment-8759 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  6 08:21:15.499: INFO: Observed &Deployment event: MODIFIED
    May  6 08:21:15.499: INFO: Found deployment test-deployment-8kp7m in namespace deployment-8759 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    May  6 08:21:15.499: INFO: Deployment test-deployment-8kp7m has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  6 08:21:15.502: INFO: Deployment "test-deployment-8kp7m":
    &Deployment{ObjectMeta:{test-deployment-8kp7m  deployment-8759  ad7a51c6-a78a-4e80-a2d5-eb5fcde9f326 177798 1 2023-05-06 08:21:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-06 08:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 08:21:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-05-06 08:21:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052ed808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  6 08:21:15.504: INFO: New ReplicaSet "test-deployment-8kp7m-54bc444df" of Deployment "test-deployment-8kp7m":
    &ReplicaSet{ObjectMeta:{test-deployment-8kp7m-54bc444df  deployment-8759  99b45645-c0ad-4e88-bab1-af2bf236d761 177790 1 2023-05-06 08:21:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-8kp7m ad7a51c6-a78a-4e80-a2d5-eb5fcde9f326 0xc0052edba0 0xc0052edba1}] [] [{kube-controller-manager Update apps/v1 2023-05-06 08:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ad7a51c6-a78a-4e80-a2d5-eb5fcde9f326\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 08:21:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052edc48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  6 08:21:15.507: INFO: Pod "test-deployment-8kp7m-54bc444df-zwbs7" is available:
    &Pod{ObjectMeta:{test-deployment-8kp7m-54bc444df-zwbs7 test-deployment-8kp7m-54bc444df- deployment-8759  a9aeaa33-7430-4358-9b85-031d0ee204e6 177789 0 2023-05-06 08:21:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:a73f663238d19b30e7e0d08e4f32fd68837ec0290a55f42d8104a47e80bfb8ae cni.projectcalico.org/podIP:10.244.174.138/32 cni.projectcalico.org/podIPs:10.244.174.138/32] [{apps/v1 ReplicaSet test-deployment-8kp7m-54bc444df 99b45645-c0ad-4e88-bab1-af2bf236d761 0xc00514dd87 0xc00514dd88}] [] [{calico Update v1 2023-05-06 08:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-06 08:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99b45645-c0ad-4e88-bab1-af2bf236d761\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 08:21:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.174.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-424rb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-424rb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:21:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:21:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:10.244.174.138,StartTime:2023-05-06 08:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-06 08:21:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://253b2a18c767940715f8cab0dbba037de65be410520feda9f6df5ba5f8cd1a92,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.174.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May  6 08:21:15.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8759" for this suite. 05/06/23 08:21:15.51
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:21:15.518
May  6 08:21:15.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename services 05/06/23 08:21:15.519
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:21:16.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:21:16.536
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-1291 05/06/23 08:21:16.538
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1291 to expose endpoints map[] 05/06/23 08:21:16.55
May  6 08:21:16.552: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
May  6 08:21:17.558: INFO: successfully validated that service endpoint-test2 in namespace services-1291 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1291 05/06/23 08:21:17.558
May  6 08:21:17.565: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1291" to be "running and ready"
May  6 08:21:17.567: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.594171ms
May  6 08:21:17.567: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  6 08:21:19.571: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006280513s
May  6 08:21:19.571: INFO: The phase of Pod pod1 is Running (Ready = true)
May  6 08:21:19.571: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1291 to expose endpoints map[pod1:[80]] 05/06/23 08:21:19.573
May  6 08:21:19.581: INFO: successfully validated that service endpoint-test2 in namespace services-1291 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 05/06/23 08:21:19.581
May  6 08:21:19.581: INFO: Creating new exec pod
May  6 08:21:19.585: INFO: Waiting up to 5m0s for pod "execpod64djn" in namespace "services-1291" to be "running"
May  6 08:21:19.588: INFO: Pod "execpod64djn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.754905ms
May  6 08:21:21.592: INFO: Pod "execpod64djn": Phase="Running", Reason="", readiness=true. Elapsed: 2.006407852s
May  6 08:21:21.592: INFO: Pod "execpod64djn" satisfied condition "running"
May  6 08:21:22.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-1291 exec execpod64djn -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
May  6 08:21:22.709: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May  6 08:21:22.709: INFO: stdout: ""
May  6 08:21:22.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-1291 exec execpod64djn -- /bin/sh -x -c nc -v -z -w 2 10.109.137.5 80'
May  6 08:21:22.816: INFO: stderr: "+ nc -v -z -w 2 10.109.137.5 80\nConnection to 10.109.137.5 80 port [tcp/http] succeeded!\n"
May  6 08:21:22.816: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-1291 05/06/23 08:21:22.816
May  6 08:21:22.823: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1291" to be "running and ready"
May  6 08:21:22.825: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.209548ms
May  6 08:21:22.825: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  6 08:21:24.829: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005910636s
May  6 08:21:24.829: INFO: The phase of Pod pod2 is Running (Ready = true)
May  6 08:21:24.829: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1291 to expose endpoints map[pod1:[80] pod2:[80]] 05/06/23 08:21:24.831
May  6 08:21:24.840: INFO: successfully validated that service endpoint-test2 in namespace services-1291 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 05/06/23 08:21:24.84
May  6 08:21:25.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-1291 exec execpod64djn -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
May  6 08:21:25.937: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May  6 08:21:25.937: INFO: stdout: ""
May  6 08:21:25.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-1291 exec execpod64djn -- /bin/sh -x -c nc -v -z -w 2 10.109.137.5 80'
May  6 08:21:26.042: INFO: stderr: "+ nc -v -z -w 2 10.109.137.5 80\nConnection to 10.109.137.5 80 port [tcp/http] succeeded!\n"
May  6 08:21:26.042: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-1291 05/06/23 08:21:26.042
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1291 to expose endpoints map[pod2:[80]] 05/06/23 08:21:26.052
May  6 08:21:27.073: INFO: successfully validated that service endpoint-test2 in namespace services-1291 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 05/06/23 08:21:27.073
May  6 08:21:28.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-1291 exec execpod64djn -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
May  6 08:21:28.178: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May  6 08:21:28.178: INFO: stdout: ""
May  6 08:21:28.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-1291 exec execpod64djn -- /bin/sh -x -c nc -v -z -w 2 10.109.137.5 80'
May  6 08:21:28.287: INFO: stderr: "+ nc -v -z -w 2 10.109.137.5 80\nConnection to 10.109.137.5 80 port [tcp/http] succeeded!\n"
May  6 08:21:28.287: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-1291 05/06/23 08:21:28.287
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1291 to expose endpoints map[] 05/06/23 08:21:28.299
May  6 08:21:28.317: INFO: successfully validated that service endpoint-test2 in namespace services-1291 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  6 08:21:28.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1291" for this suite. 05/06/23 08:21:28.351
------------------------------
â€¢ [SLOW TEST] [12.839 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:21:15.518
    May  6 08:21:15.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename services 05/06/23 08:21:15.519
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:21:16.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:21:16.536
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-1291 05/06/23 08:21:16.538
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1291 to expose endpoints map[] 05/06/23 08:21:16.55
    May  6 08:21:16.552: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    May  6 08:21:17.558: INFO: successfully validated that service endpoint-test2 in namespace services-1291 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1291 05/06/23 08:21:17.558
    May  6 08:21:17.565: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1291" to be "running and ready"
    May  6 08:21:17.567: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.594171ms
    May  6 08:21:17.567: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:21:19.571: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006280513s
    May  6 08:21:19.571: INFO: The phase of Pod pod1 is Running (Ready = true)
    May  6 08:21:19.571: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1291 to expose endpoints map[pod1:[80]] 05/06/23 08:21:19.573
    May  6 08:21:19.581: INFO: successfully validated that service endpoint-test2 in namespace services-1291 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 05/06/23 08:21:19.581
    May  6 08:21:19.581: INFO: Creating new exec pod
    May  6 08:21:19.585: INFO: Waiting up to 5m0s for pod "execpod64djn" in namespace "services-1291" to be "running"
    May  6 08:21:19.588: INFO: Pod "execpod64djn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.754905ms
    May  6 08:21:21.592: INFO: Pod "execpod64djn": Phase="Running", Reason="", readiness=true. Elapsed: 2.006407852s
    May  6 08:21:21.592: INFO: Pod "execpod64djn" satisfied condition "running"
    May  6 08:21:22.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-1291 exec execpod64djn -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    May  6 08:21:22.709: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May  6 08:21:22.709: INFO: stdout: ""
    May  6 08:21:22.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-1291 exec execpod64djn -- /bin/sh -x -c nc -v -z -w 2 10.109.137.5 80'
    May  6 08:21:22.816: INFO: stderr: "+ nc -v -z -w 2 10.109.137.5 80\nConnection to 10.109.137.5 80 port [tcp/http] succeeded!\n"
    May  6 08:21:22.816: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-1291 05/06/23 08:21:22.816
    May  6 08:21:22.823: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1291" to be "running and ready"
    May  6 08:21:22.825: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.209548ms
    May  6 08:21:22.825: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:21:24.829: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005910636s
    May  6 08:21:24.829: INFO: The phase of Pod pod2 is Running (Ready = true)
    May  6 08:21:24.829: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1291 to expose endpoints map[pod1:[80] pod2:[80]] 05/06/23 08:21:24.831
    May  6 08:21:24.840: INFO: successfully validated that service endpoint-test2 in namespace services-1291 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 05/06/23 08:21:24.84
    May  6 08:21:25.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-1291 exec execpod64djn -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    May  6 08:21:25.937: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May  6 08:21:25.937: INFO: stdout: ""
    May  6 08:21:25.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-1291 exec execpod64djn -- /bin/sh -x -c nc -v -z -w 2 10.109.137.5 80'
    May  6 08:21:26.042: INFO: stderr: "+ nc -v -z -w 2 10.109.137.5 80\nConnection to 10.109.137.5 80 port [tcp/http] succeeded!\n"
    May  6 08:21:26.042: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-1291 05/06/23 08:21:26.042
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1291 to expose endpoints map[pod2:[80]] 05/06/23 08:21:26.052
    May  6 08:21:27.073: INFO: successfully validated that service endpoint-test2 in namespace services-1291 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 05/06/23 08:21:27.073
    May  6 08:21:28.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-1291 exec execpod64djn -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    May  6 08:21:28.178: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May  6 08:21:28.178: INFO: stdout: ""
    May  6 08:21:28.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-1291 exec execpod64djn -- /bin/sh -x -c nc -v -z -w 2 10.109.137.5 80'
    May  6 08:21:28.287: INFO: stderr: "+ nc -v -z -w 2 10.109.137.5 80\nConnection to 10.109.137.5 80 port [tcp/http] succeeded!\n"
    May  6 08:21:28.287: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-1291 05/06/23 08:21:28.287
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1291 to expose endpoints map[] 05/06/23 08:21:28.299
    May  6 08:21:28.317: INFO: successfully validated that service endpoint-test2 in namespace services-1291 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  6 08:21:28.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1291" for this suite. 05/06/23 08:21:28.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:21:28.358
May  6 08:21:28.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename replicaset 05/06/23 08:21:28.359
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:21:29.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:21:29.377
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
May  6 08:21:29.391: INFO: Pod name sample-pod: Found 0 pods out of 1
May  6 08:21:34.395: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/06/23 08:21:34.395
STEP: Scaling up "test-rs" replicaset  05/06/23 08:21:34.395
May  6 08:21:34.403: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 05/06/23 08:21:34.403
W0506 08:21:34.411027      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May  6 08:21:34.412: INFO: observed ReplicaSet test-rs in namespace replicaset-9002 with ReadyReplicas 1, AvailableReplicas 1
May  6 08:21:34.424: INFO: observed ReplicaSet test-rs in namespace replicaset-9002 with ReadyReplicas 1, AvailableReplicas 1
May  6 08:21:34.443: INFO: observed ReplicaSet test-rs in namespace replicaset-9002 with ReadyReplicas 1, AvailableReplicas 1
May  6 08:21:34.450: INFO: observed ReplicaSet test-rs in namespace replicaset-9002 with ReadyReplicas 1, AvailableReplicas 1
May  6 08:21:35.739: INFO: observed ReplicaSet test-rs in namespace replicaset-9002 with ReadyReplicas 2, AvailableReplicas 2
May  6 08:21:35.953: INFO: observed Replicaset test-rs in namespace replicaset-9002 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May  6 08:21:35.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9002" for this suite. 05/06/23 08:21:35.957
------------------------------
â€¢ [SLOW TEST] [7.605 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:21:28.358
    May  6 08:21:28.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename replicaset 05/06/23 08:21:28.359
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:21:29.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:21:29.377
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    May  6 08:21:29.391: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  6 08:21:34.395: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/06/23 08:21:34.395
    STEP: Scaling up "test-rs" replicaset  05/06/23 08:21:34.395
    May  6 08:21:34.403: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 05/06/23 08:21:34.403
    W0506 08:21:34.411027      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May  6 08:21:34.412: INFO: observed ReplicaSet test-rs in namespace replicaset-9002 with ReadyReplicas 1, AvailableReplicas 1
    May  6 08:21:34.424: INFO: observed ReplicaSet test-rs in namespace replicaset-9002 with ReadyReplicas 1, AvailableReplicas 1
    May  6 08:21:34.443: INFO: observed ReplicaSet test-rs in namespace replicaset-9002 with ReadyReplicas 1, AvailableReplicas 1
    May  6 08:21:34.450: INFO: observed ReplicaSet test-rs in namespace replicaset-9002 with ReadyReplicas 1, AvailableReplicas 1
    May  6 08:21:35.739: INFO: observed ReplicaSet test-rs in namespace replicaset-9002 with ReadyReplicas 2, AvailableReplicas 2
    May  6 08:21:35.953: INFO: observed Replicaset test-rs in namespace replicaset-9002 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May  6 08:21:35.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9002" for this suite. 05/06/23 08:21:35.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:21:35.964
May  6 08:21:35.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 08:21:35.965
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:21:36.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:21:36.983
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-8a170960-06f7-494a-866e-a115b2f8af05 05/06/23 08:21:36.984
STEP: Creating a pod to test consume configMaps 05/06/23 08:21:36.99
May  6 08:21:36.996: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7a48d244-6e4a-486e-b748-1b3cedb48981" in namespace "projected-8538" to be "Succeeded or Failed"
May  6 08:21:36.998: INFO: Pod "pod-projected-configmaps-7a48d244-6e4a-486e-b748-1b3cedb48981": Phase="Pending", Reason="", readiness=false. Elapsed: 2.459153ms
May  6 08:21:39.001: INFO: Pod "pod-projected-configmaps-7a48d244-6e4a-486e-b748-1b3cedb48981": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005140484s
May  6 08:21:41.002: INFO: Pod "pod-projected-configmaps-7a48d244-6e4a-486e-b748-1b3cedb48981": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006165489s
STEP: Saw pod success 05/06/23 08:21:41.002
May  6 08:21:41.002: INFO: Pod "pod-projected-configmaps-7a48d244-6e4a-486e-b748-1b3cedb48981" satisfied condition "Succeeded or Failed"
May  6 08:21:41.004: INFO: Trying to get logs from node cncf-0 pod pod-projected-configmaps-7a48d244-6e4a-486e-b748-1b3cedb48981 container agnhost-container: <nil>
STEP: delete the pod 05/06/23 08:21:41.012
May  6 08:21:41.026: INFO: Waiting for pod pod-projected-configmaps-7a48d244-6e4a-486e-b748-1b3cedb48981 to disappear
May  6 08:21:41.028: INFO: Pod pod-projected-configmaps-7a48d244-6e4a-486e-b748-1b3cedb48981 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May  6 08:21:41.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8538" for this suite. 05/06/23 08:21:41.032
------------------------------
â€¢ [SLOW TEST] [5.074 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:21:35.964
    May  6 08:21:35.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 08:21:35.965
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:21:36.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:21:36.983
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-8a170960-06f7-494a-866e-a115b2f8af05 05/06/23 08:21:36.984
    STEP: Creating a pod to test consume configMaps 05/06/23 08:21:36.99
    May  6 08:21:36.996: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7a48d244-6e4a-486e-b748-1b3cedb48981" in namespace "projected-8538" to be "Succeeded or Failed"
    May  6 08:21:36.998: INFO: Pod "pod-projected-configmaps-7a48d244-6e4a-486e-b748-1b3cedb48981": Phase="Pending", Reason="", readiness=false. Elapsed: 2.459153ms
    May  6 08:21:39.001: INFO: Pod "pod-projected-configmaps-7a48d244-6e4a-486e-b748-1b3cedb48981": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005140484s
    May  6 08:21:41.002: INFO: Pod "pod-projected-configmaps-7a48d244-6e4a-486e-b748-1b3cedb48981": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006165489s
    STEP: Saw pod success 05/06/23 08:21:41.002
    May  6 08:21:41.002: INFO: Pod "pod-projected-configmaps-7a48d244-6e4a-486e-b748-1b3cedb48981" satisfied condition "Succeeded or Failed"
    May  6 08:21:41.004: INFO: Trying to get logs from node cncf-0 pod pod-projected-configmaps-7a48d244-6e4a-486e-b748-1b3cedb48981 container agnhost-container: <nil>
    STEP: delete the pod 05/06/23 08:21:41.012
    May  6 08:21:41.026: INFO: Waiting for pod pod-projected-configmaps-7a48d244-6e4a-486e-b748-1b3cedb48981 to disappear
    May  6 08:21:41.028: INFO: Pod pod-projected-configmaps-7a48d244-6e4a-486e-b748-1b3cedb48981 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May  6 08:21:41.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8538" for this suite. 05/06/23 08:21:41.032
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:21:41.038
May  6 08:21:41.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename hostport 05/06/23 08:21:41.039
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:21:42.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:21:42.059
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 05/06/23 08:21:42.065
May  6 08:21:42.072: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-3519" to be "running and ready"
May  6 08:21:42.075: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.73076ms
May  6 08:21:42.075: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  6 08:21:44.079: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006447025s
May  6 08:21:44.079: INFO: The phase of Pod pod1 is Running (Ready = true)
May  6 08:21:44.079: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.0.107 on the node which pod1 resides and expect scheduled 05/06/23 08:21:44.079
May  6 08:21:44.085: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-3519" to be "running and ready"
May  6 08:21:44.088: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.025509ms
May  6 08:21:44.088: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  6 08:21:46.093: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.007881984s
May  6 08:21:46.093: INFO: The phase of Pod pod2 is Running (Ready = false)
May  6 08:21:48.091: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.006426423s
May  6 08:21:48.091: INFO: The phase of Pod pod2 is Running (Ready = true)
May  6 08:21:48.091: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.0.107 but use UDP protocol on the node which pod2 resides 05/06/23 08:21:48.091
May  6 08:21:48.098: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-3519" to be "running and ready"
May  6 08:21:48.100: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.709269ms
May  6 08:21:48.100: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
May  6 08:21:50.103: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004865538s
May  6 08:21:50.103: INFO: The phase of Pod pod3 is Running (Ready = true)
May  6 08:21:50.103: INFO: Pod "pod3" satisfied condition "running and ready"
May  6 08:21:50.111: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-3519" to be "running and ready"
May  6 08:21:50.113: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.918255ms
May  6 08:21:50.113: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
May  6 08:21:52.116: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.004725432s
May  6 08:21:52.116: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
May  6 08:21:52.116: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 05/06/23 08:21:52.118
May  6 08:21:52.119: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.107 http://127.0.0.1:54323/hostname] Namespace:hostport-3519 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 08:21:52.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 08:21:52.119: INFO: ExecWithOptions: Clientset creation
May  6 08:21:52.119: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-3519/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.0.107+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.107, port: 54323 05/06/23 08:21:52.218
May  6 08:21:52.218: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.107:54323/hostname] Namespace:hostport-3519 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 08:21:52.218: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 08:21:52.219: INFO: ExecWithOptions: Clientset creation
May  6 08:21:52.219: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-3519/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.0.107%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.107, port: 54323 UDP 05/06/23 08:21:52.275
May  6 08:21:52.275: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.0.107 54323] Namespace:hostport-3519 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  6 08:21:52.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
May  6 08:21:52.275: INFO: ExecWithOptions: Clientset creation
May  6 08:21:52.275: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-3519/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.0.107+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
May  6 08:21:57.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-3519" for this suite. 05/06/23 08:21:57.325
------------------------------
â€¢ [SLOW TEST] [16.293 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:21:41.038
    May  6 08:21:41.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename hostport 05/06/23 08:21:41.039
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:21:42.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:21:42.059
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 05/06/23 08:21:42.065
    May  6 08:21:42.072: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-3519" to be "running and ready"
    May  6 08:21:42.075: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.73076ms
    May  6 08:21:42.075: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:21:44.079: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006447025s
    May  6 08:21:44.079: INFO: The phase of Pod pod1 is Running (Ready = true)
    May  6 08:21:44.079: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.0.107 on the node which pod1 resides and expect scheduled 05/06/23 08:21:44.079
    May  6 08:21:44.085: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-3519" to be "running and ready"
    May  6 08:21:44.088: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.025509ms
    May  6 08:21:44.088: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:21:46.093: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.007881984s
    May  6 08:21:46.093: INFO: The phase of Pod pod2 is Running (Ready = false)
    May  6 08:21:48.091: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.006426423s
    May  6 08:21:48.091: INFO: The phase of Pod pod2 is Running (Ready = true)
    May  6 08:21:48.091: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.0.107 but use UDP protocol on the node which pod2 resides 05/06/23 08:21:48.091
    May  6 08:21:48.098: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-3519" to be "running and ready"
    May  6 08:21:48.100: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.709269ms
    May  6 08:21:48.100: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:21:50.103: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004865538s
    May  6 08:21:50.103: INFO: The phase of Pod pod3 is Running (Ready = true)
    May  6 08:21:50.103: INFO: Pod "pod3" satisfied condition "running and ready"
    May  6 08:21:50.111: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-3519" to be "running and ready"
    May  6 08:21:50.113: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.918255ms
    May  6 08:21:50.113: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:21:52.116: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.004725432s
    May  6 08:21:52.116: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    May  6 08:21:52.116: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 05/06/23 08:21:52.118
    May  6 08:21:52.119: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.107 http://127.0.0.1:54323/hostname] Namespace:hostport-3519 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 08:21:52.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 08:21:52.119: INFO: ExecWithOptions: Clientset creation
    May  6 08:21:52.119: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-3519/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.0.107+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.107, port: 54323 05/06/23 08:21:52.218
    May  6 08:21:52.218: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.107:54323/hostname] Namespace:hostport-3519 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 08:21:52.218: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 08:21:52.219: INFO: ExecWithOptions: Clientset creation
    May  6 08:21:52.219: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-3519/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.0.107%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.107, port: 54323 UDP 05/06/23 08:21:52.275
    May  6 08:21:52.275: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.0.107 54323] Namespace:hostport-3519 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  6 08:21:52.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    May  6 08:21:52.275: INFO: ExecWithOptions: Clientset creation
    May  6 08:21:52.275: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-3519/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.0.107+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    May  6 08:21:57.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-3519" for this suite. 05/06/23 08:21:57.325
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:21:57.331
May  6 08:21:57.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename resourcequota 05/06/23 08:21:57.332
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:21:58.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:21:58.35
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 05/06/23 08:21:58.352
STEP: Getting a ResourceQuota 05/06/23 08:21:58.356
STEP: Updating a ResourceQuota 05/06/23 08:21:58.359
STEP: Verifying a ResourceQuota was modified 05/06/23 08:21:58.375
STEP: Deleting a ResourceQuota 05/06/23 08:21:58.376
STEP: Verifying the deleted ResourceQuota 05/06/23 08:21:58.382
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  6 08:21:58.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7739" for this suite. 05/06/23 08:21:58.387
------------------------------
â€¢ [1.062 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:21:57.331
    May  6 08:21:57.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename resourcequota 05/06/23 08:21:57.332
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:21:58.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:21:58.35
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 05/06/23 08:21:58.352
    STEP: Getting a ResourceQuota 05/06/23 08:21:58.356
    STEP: Updating a ResourceQuota 05/06/23 08:21:58.359
    STEP: Verifying a ResourceQuota was modified 05/06/23 08:21:58.375
    STEP: Deleting a ResourceQuota 05/06/23 08:21:58.376
    STEP: Verifying the deleted ResourceQuota 05/06/23 08:21:58.382
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  6 08:21:58.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7739" for this suite. 05/06/23 08:21:58.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:21:58.394
May  6 08:21:58.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename container-probe 05/06/23 08:21:58.395
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:21:59.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:21:59.412
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
May  6 08:21:59.422: INFO: Waiting up to 5m0s for pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475" in namespace "container-probe-900" to be "running and ready"
May  6 08:21:59.425: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Pending", Reason="", readiness=false. Elapsed: 2.8463ms
May  6 08:21:59.425: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Pending, waiting for it to be Running (with Ready = true)
May  6 08:22:01.428: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 2.005818124s
May  6 08:22:01.428: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
May  6 08:22:03.428: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 4.006618254s
May  6 08:22:03.428: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
May  6 08:22:05.428: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 6.006400818s
May  6 08:22:05.428: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
May  6 08:22:07.429: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 8.007016609s
May  6 08:22:07.429: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
May  6 08:22:09.428: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 10.006298871s
May  6 08:22:09.428: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
May  6 08:22:11.427: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 12.005471588s
May  6 08:22:11.427: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
May  6 08:22:13.428: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 14.006561829s
May  6 08:22:13.428: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
May  6 08:22:15.428: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 16.006614577s
May  6 08:22:15.428: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
May  6 08:22:17.429: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 18.006910926s
May  6 08:22:17.429: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
May  6 08:22:19.429: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 20.006719688s
May  6 08:22:19.429: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
May  6 08:22:21.428: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=true. Elapsed: 22.005908093s
May  6 08:22:21.428: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = true)
May  6 08:22:21.428: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475" satisfied condition "running and ready"
May  6 08:22:21.430: INFO: Container started at 2023-05-06 08:22:00 +0000 UTC, pod became ready at 2023-05-06 08:22:19 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May  6 08:22:21.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-900" for this suite. 05/06/23 08:22:21.432
------------------------------
â€¢ [SLOW TEST] [23.044 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:21:58.394
    May  6 08:21:58.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename container-probe 05/06/23 08:21:58.395
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:21:59.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:21:59.412
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    May  6 08:21:59.422: INFO: Waiting up to 5m0s for pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475" in namespace "container-probe-900" to be "running and ready"
    May  6 08:21:59.425: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Pending", Reason="", readiness=false. Elapsed: 2.8463ms
    May  6 08:21:59.425: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:22:01.428: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 2.005818124s
    May  6 08:22:01.428: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
    May  6 08:22:03.428: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 4.006618254s
    May  6 08:22:03.428: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
    May  6 08:22:05.428: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 6.006400818s
    May  6 08:22:05.428: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
    May  6 08:22:07.429: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 8.007016609s
    May  6 08:22:07.429: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
    May  6 08:22:09.428: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 10.006298871s
    May  6 08:22:09.428: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
    May  6 08:22:11.427: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 12.005471588s
    May  6 08:22:11.427: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
    May  6 08:22:13.428: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 14.006561829s
    May  6 08:22:13.428: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
    May  6 08:22:15.428: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 16.006614577s
    May  6 08:22:15.428: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
    May  6 08:22:17.429: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 18.006910926s
    May  6 08:22:17.429: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
    May  6 08:22:19.429: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=false. Elapsed: 20.006719688s
    May  6 08:22:19.429: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = false)
    May  6 08:22:21.428: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475": Phase="Running", Reason="", readiness=true. Elapsed: 22.005908093s
    May  6 08:22:21.428: INFO: The phase of Pod test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475 is Running (Ready = true)
    May  6 08:22:21.428: INFO: Pod "test-webserver-aff0f8f0-90b7-43a6-8d5a-3a7144fc4475" satisfied condition "running and ready"
    May  6 08:22:21.430: INFO: Container started at 2023-05-06 08:22:00 +0000 UTC, pod became ready at 2023-05-06 08:22:19 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May  6 08:22:21.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-900" for this suite. 05/06/23 08:22:21.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:22:21.439
May  6 08:22:21.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubelet-test 05/06/23 08:22:21.44
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:22:22.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:22:22.456
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
May  6 08:22:22.466: INFO: Waiting up to 5m0s for pod "busybox-scheduling-058580c8-19cf-4919-8b2e-ad85d38f380e" in namespace "kubelet-test-7738" to be "running and ready"
May  6 08:22:22.469: INFO: Pod "busybox-scheduling-058580c8-19cf-4919-8b2e-ad85d38f380e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.685234ms
May  6 08:22:22.469: INFO: The phase of Pod busybox-scheduling-058580c8-19cf-4919-8b2e-ad85d38f380e is Pending, waiting for it to be Running (with Ready = true)
May  6 08:22:24.472: INFO: Pod "busybox-scheduling-058580c8-19cf-4919-8b2e-ad85d38f380e": Phase="Running", Reason="", readiness=true. Elapsed: 2.00564745s
May  6 08:22:24.472: INFO: The phase of Pod busybox-scheduling-058580c8-19cf-4919-8b2e-ad85d38f380e is Running (Ready = true)
May  6 08:22:24.472: INFO: Pod "busybox-scheduling-058580c8-19cf-4919-8b2e-ad85d38f380e" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May  6 08:22:24.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7738" for this suite. 05/06/23 08:22:24.483
------------------------------
â€¢ [3.052 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:22:21.439
    May  6 08:22:21.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubelet-test 05/06/23 08:22:21.44
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:22:22.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:22:22.456
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    May  6 08:22:22.466: INFO: Waiting up to 5m0s for pod "busybox-scheduling-058580c8-19cf-4919-8b2e-ad85d38f380e" in namespace "kubelet-test-7738" to be "running and ready"
    May  6 08:22:22.469: INFO: Pod "busybox-scheduling-058580c8-19cf-4919-8b2e-ad85d38f380e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.685234ms
    May  6 08:22:22.469: INFO: The phase of Pod busybox-scheduling-058580c8-19cf-4919-8b2e-ad85d38f380e is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:22:24.472: INFO: Pod "busybox-scheduling-058580c8-19cf-4919-8b2e-ad85d38f380e": Phase="Running", Reason="", readiness=true. Elapsed: 2.00564745s
    May  6 08:22:24.472: INFO: The phase of Pod busybox-scheduling-058580c8-19cf-4919-8b2e-ad85d38f380e is Running (Ready = true)
    May  6 08:22:24.472: INFO: Pod "busybox-scheduling-058580c8-19cf-4919-8b2e-ad85d38f380e" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May  6 08:22:24.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7738" for this suite. 05/06/23 08:22:24.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:22:24.491
May  6 08:22:24.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename containers 05/06/23 08:22:24.492
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:22:25.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:22:25.508
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 05/06/23 08:22:25.51
May  6 08:22:25.518: INFO: Waiting up to 5m0s for pod "client-containers-a7a81a05-1260-4348-968d-29241fc80c79" in namespace "containers-9416" to be "Succeeded or Failed"
May  6 08:22:25.521: INFO: Pod "client-containers-a7a81a05-1260-4348-968d-29241fc80c79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.223226ms
May  6 08:22:27.523: INFO: Pod "client-containers-a7a81a05-1260-4348-968d-29241fc80c79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004996344s
May  6 08:22:29.525: INFO: Pod "client-containers-a7a81a05-1260-4348-968d-29241fc80c79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006332722s
STEP: Saw pod success 05/06/23 08:22:29.525
May  6 08:22:29.525: INFO: Pod "client-containers-a7a81a05-1260-4348-968d-29241fc80c79" satisfied condition "Succeeded or Failed"
May  6 08:22:29.527: INFO: Trying to get logs from node cncf-3 pod client-containers-a7a81a05-1260-4348-968d-29241fc80c79 container agnhost-container: <nil>
STEP: delete the pod 05/06/23 08:22:29.537
May  6 08:22:29.559: INFO: Waiting for pod client-containers-a7a81a05-1260-4348-968d-29241fc80c79 to disappear
May  6 08:22:29.560: INFO: Pod client-containers-a7a81a05-1260-4348-968d-29241fc80c79 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
May  6 08:22:29.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9416" for this suite. 05/06/23 08:22:29.565
------------------------------
â€¢ [SLOW TEST] [5.080 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:22:24.491
    May  6 08:22:24.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename containers 05/06/23 08:22:24.492
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:22:25.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:22:25.508
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 05/06/23 08:22:25.51
    May  6 08:22:25.518: INFO: Waiting up to 5m0s for pod "client-containers-a7a81a05-1260-4348-968d-29241fc80c79" in namespace "containers-9416" to be "Succeeded or Failed"
    May  6 08:22:25.521: INFO: Pod "client-containers-a7a81a05-1260-4348-968d-29241fc80c79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.223226ms
    May  6 08:22:27.523: INFO: Pod "client-containers-a7a81a05-1260-4348-968d-29241fc80c79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004996344s
    May  6 08:22:29.525: INFO: Pod "client-containers-a7a81a05-1260-4348-968d-29241fc80c79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006332722s
    STEP: Saw pod success 05/06/23 08:22:29.525
    May  6 08:22:29.525: INFO: Pod "client-containers-a7a81a05-1260-4348-968d-29241fc80c79" satisfied condition "Succeeded or Failed"
    May  6 08:22:29.527: INFO: Trying to get logs from node cncf-3 pod client-containers-a7a81a05-1260-4348-968d-29241fc80c79 container agnhost-container: <nil>
    STEP: delete the pod 05/06/23 08:22:29.537
    May  6 08:22:29.559: INFO: Waiting for pod client-containers-a7a81a05-1260-4348-968d-29241fc80c79 to disappear
    May  6 08:22:29.560: INFO: Pod client-containers-a7a81a05-1260-4348-968d-29241fc80c79 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    May  6 08:22:29.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9416" for this suite. 05/06/23 08:22:29.565
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:22:29.571
May  6 08:22:29.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename runtimeclass 05/06/23 08:22:29.572
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:22:30.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:22:30.59
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 05/06/23 08:22:30.592
STEP: getting /apis/node.k8s.io 05/06/23 08:22:30.594
STEP: getting /apis/node.k8s.io/v1 05/06/23 08:22:30.595
STEP: creating 05/06/23 08:22:30.595
STEP: watching 05/06/23 08:22:30.609
May  6 08:22:30.609: INFO: starting watch
STEP: getting 05/06/23 08:22:30.614
STEP: listing 05/06/23 08:22:30.616
STEP: patching 05/06/23 08:22:30.618
STEP: updating 05/06/23 08:22:30.624
May  6 08:22:30.631: INFO: waiting for watch events with expected annotations
STEP: deleting 05/06/23 08:22:30.631
STEP: deleting a collection 05/06/23 08:22:30.639
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May  6 08:22:30.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2107" for this suite. 05/06/23 08:22:30.654
------------------------------
â€¢ [1.087 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:22:29.571
    May  6 08:22:29.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename runtimeclass 05/06/23 08:22:29.572
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:22:30.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:22:30.59
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 05/06/23 08:22:30.592
    STEP: getting /apis/node.k8s.io 05/06/23 08:22:30.594
    STEP: getting /apis/node.k8s.io/v1 05/06/23 08:22:30.595
    STEP: creating 05/06/23 08:22:30.595
    STEP: watching 05/06/23 08:22:30.609
    May  6 08:22:30.609: INFO: starting watch
    STEP: getting 05/06/23 08:22:30.614
    STEP: listing 05/06/23 08:22:30.616
    STEP: patching 05/06/23 08:22:30.618
    STEP: updating 05/06/23 08:22:30.624
    May  6 08:22:30.631: INFO: waiting for watch events with expected annotations
    STEP: deleting 05/06/23 08:22:30.631
    STEP: deleting a collection 05/06/23 08:22:30.639
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May  6 08:22:30.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2107" for this suite. 05/06/23 08:22:30.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:22:30.659
May  6 08:22:30.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename security-context-test 05/06/23 08:22:30.66
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:22:31.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:22:31.679
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
May  6 08:22:31.689: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-03f74c7d-ab8a-460d-b89e-96e8112ec171" in namespace "security-context-test-3688" to be "Succeeded or Failed"
May  6 08:22:31.691: INFO: Pod "alpine-nnp-false-03f74c7d-ab8a-460d-b89e-96e8112ec171": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063392ms
May  6 08:22:33.694: INFO: Pod "alpine-nnp-false-03f74c7d-ab8a-460d-b89e-96e8112ec171": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004629718s
May  6 08:22:35.695: INFO: Pod "alpine-nnp-false-03f74c7d-ab8a-460d-b89e-96e8112ec171": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006464158s
May  6 08:22:37.694: INFO: Pod "alpine-nnp-false-03f74c7d-ab8a-460d-b89e-96e8112ec171": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004684442s
May  6 08:22:37.694: INFO: Pod "alpine-nnp-false-03f74c7d-ab8a-460d-b89e-96e8112ec171" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May  6 08:22:37.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3688" for this suite. 05/06/23 08:22:37.702
------------------------------
â€¢ [SLOW TEST] [7.050 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:22:30.659
    May  6 08:22:30.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename security-context-test 05/06/23 08:22:30.66
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:22:31.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:22:31.679
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    May  6 08:22:31.689: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-03f74c7d-ab8a-460d-b89e-96e8112ec171" in namespace "security-context-test-3688" to be "Succeeded or Failed"
    May  6 08:22:31.691: INFO: Pod "alpine-nnp-false-03f74c7d-ab8a-460d-b89e-96e8112ec171": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063392ms
    May  6 08:22:33.694: INFO: Pod "alpine-nnp-false-03f74c7d-ab8a-460d-b89e-96e8112ec171": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004629718s
    May  6 08:22:35.695: INFO: Pod "alpine-nnp-false-03f74c7d-ab8a-460d-b89e-96e8112ec171": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006464158s
    May  6 08:22:37.694: INFO: Pod "alpine-nnp-false-03f74c7d-ab8a-460d-b89e-96e8112ec171": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004684442s
    May  6 08:22:37.694: INFO: Pod "alpine-nnp-false-03f74c7d-ab8a-460d-b89e-96e8112ec171" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May  6 08:22:37.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3688" for this suite. 05/06/23 08:22:37.702
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:22:37.71
May  6 08:22:37.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename emptydir-wrapper 05/06/23 08:22:37.711
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:22:38.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:22:38.735
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 05/06/23 08:22:38.737
STEP: Creating RC which spawns configmap-volume pods 05/06/23 08:22:38.935
May  6 08:22:39.031: INFO: Pod name wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/06/23 08:22:39.031
May  6 08:22:39.033: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9" in namespace "emptydir-wrapper-9218" to be "running"
May  6 08:22:39.072: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9": Phase="Pending", Reason="", readiness=false. Elapsed: 39.379298ms
May  6 08:22:41.076: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043503315s
May  6 08:22:43.077: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043820846s
May  6 08:22:45.076: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042736246s
May  6 08:22:47.076: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.042701089s
May  6 08:22:49.076: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.043045434s
May  6 08:22:51.077: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.043590196s
May  6 08:22:53.077: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9": Phase="Running", Reason="", readiness=true. Elapsed: 14.044211376s
May  6 08:22:53.077: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9" satisfied condition "running"
May  6 08:22:53.077: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-6l4lj" in namespace "emptydir-wrapper-9218" to be "running"
May  6 08:22:53.081: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-6l4lj": Phase="Running", Reason="", readiness=true. Elapsed: 3.359855ms
May  6 08:22:53.081: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-6l4lj" satisfied condition "running"
May  6 08:22:53.081: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-fs8w5" in namespace "emptydir-wrapper-9218" to be "running"
May  6 08:22:53.083: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-fs8w5": Phase="Running", Reason="", readiness=true. Elapsed: 2.121351ms
May  6 08:22:53.083: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-fs8w5" satisfied condition "running"
May  6 08:22:53.083: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-gr78r" in namespace "emptydir-wrapper-9218" to be "running"
May  6 08:22:53.086: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-gr78r": Phase="Running", Reason="", readiness=true. Elapsed: 2.875224ms
May  6 08:22:53.086: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-gr78r" satisfied condition "running"
May  6 08:22:53.086: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-l766m" in namespace "emptydir-wrapper-9218" to be "running"
May  6 08:22:53.088: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-l766m": Phase="Running", Reason="", readiness=true. Elapsed: 2.115311ms
May  6 08:22:53.088: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-l766m" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf in namespace emptydir-wrapper-9218, will wait for the garbage collector to delete the pods 05/06/23 08:22:53.088
May  6 08:22:53.148: INFO: Deleting ReplicationController wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf took: 6.861039ms
May  6 08:22:53.249: INFO: Terminating ReplicationController wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf pods took: 100.510429ms
STEP: Creating RC which spawns configmap-volume pods 05/06/23 08:22:56.653
May  6 08:22:56.665: INFO: Pod name wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31: Found 0 pods out of 5
May  6 08:23:01.670: INFO: Pod name wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/06/23 08:23:01.67
May  6 08:23:01.670: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-8vnxt" in namespace "emptydir-wrapper-9218" to be "running"
May  6 08:23:01.673: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-8vnxt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188891ms
May  6 08:23:03.676: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-8vnxt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005854896s
May  6 08:23:05.676: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-8vnxt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005894625s
May  6 08:23:07.676: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-8vnxt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005624204s
May  6 08:23:09.677: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-8vnxt": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006269503s
May  6 08:23:11.677: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-8vnxt": Phase="Running", Reason="", readiness=true. Elapsed: 10.007109572s
May  6 08:23:11.678: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-8vnxt" satisfied condition "running"
May  6 08:23:11.678: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-k42sq" in namespace "emptydir-wrapper-9218" to be "running"
May  6 08:23:11.681: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-k42sq": Phase="Running", Reason="", readiness=true. Elapsed: 3.116272ms
May  6 08:23:11.681: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-k42sq" satisfied condition "running"
May  6 08:23:11.681: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-ptzxg" in namespace "emptydir-wrapper-9218" to be "running"
May  6 08:23:11.683: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-ptzxg": Phase="Running", Reason="", readiness=true. Elapsed: 2.180233ms
May  6 08:23:11.683: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-ptzxg" satisfied condition "running"
May  6 08:23:11.683: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-qrxs5" in namespace "emptydir-wrapper-9218" to be "running"
May  6 08:23:11.686: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-qrxs5": Phase="Running", Reason="", readiness=true. Elapsed: 2.827563ms
May  6 08:23:11.686: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-qrxs5" satisfied condition "running"
May  6 08:23:11.686: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-xgzx2" in namespace "emptydir-wrapper-9218" to be "running"
May  6 08:23:11.688: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-xgzx2": Phase="Running", Reason="", readiness=true. Elapsed: 1.99907ms
May  6 08:23:11.688: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-xgzx2" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31 in namespace emptydir-wrapper-9218, will wait for the garbage collector to delete the pods 05/06/23 08:23:11.688
May  6 08:23:11.750: INFO: Deleting ReplicationController wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31 took: 8.144528ms
May  6 08:23:11.850: INFO: Terminating ReplicationController wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31 pods took: 100.459773ms
STEP: Creating RC which spawns configmap-volume pods 05/06/23 08:23:14.754
May  6 08:23:14.766: INFO: Pod name wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022: Found 0 pods out of 5
May  6 08:23:19.771: INFO: Pod name wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/06/23 08:23:19.771
May  6 08:23:19.772: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-4458d" in namespace "emptydir-wrapper-9218" to be "running"
May  6 08:23:19.774: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-4458d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.597716ms
May  6 08:23:21.778: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-4458d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006023602s
May  6 08:23:23.778: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-4458d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006390728s
May  6 08:23:25.778: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-4458d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006430649s
May  6 08:23:27.777: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-4458d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005915041s
May  6 08:23:29.778: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-4458d": Phase="Running", Reason="", readiness=true. Elapsed: 10.00691866s
May  6 08:23:29.778: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-4458d" satisfied condition "running"
May  6 08:23:29.778: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-blssc" in namespace "emptydir-wrapper-9218" to be "running"
May  6 08:23:29.781: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-blssc": Phase="Running", Reason="", readiness=true. Elapsed: 2.795051ms
May  6 08:23:29.781: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-blssc" satisfied condition "running"
May  6 08:23:29.781: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-p5sjs" in namespace "emptydir-wrapper-9218" to be "running"
May  6 08:23:29.783: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-p5sjs": Phase="Running", Reason="", readiness=true. Elapsed: 2.091685ms
May  6 08:23:29.783: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-p5sjs" satisfied condition "running"
May  6 08:23:29.783: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-qcppn" in namespace "emptydir-wrapper-9218" to be "running"
May  6 08:23:29.786: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-qcppn": Phase="Running", Reason="", readiness=true. Elapsed: 2.700221ms
May  6 08:23:29.786: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-qcppn" satisfied condition "running"
May  6 08:23:29.786: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-tw757" in namespace "emptydir-wrapper-9218" to be "running"
May  6 08:23:29.788: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-tw757": Phase="Running", Reason="", readiness=true. Elapsed: 2.060004ms
May  6 08:23:29.788: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-tw757" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022 in namespace emptydir-wrapper-9218, will wait for the garbage collector to delete the pods 05/06/23 08:23:29.788
May  6 08:23:29.848: INFO: Deleting ReplicationController wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022 took: 6.352711ms
May  6 08:23:29.949: INFO: Terminating ReplicationController wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022 pods took: 101.05178ms
STEP: Cleaning up the configMaps 05/06/23 08:23:32.45
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
May  6 08:23:32.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-9218" for this suite. 05/06/23 08:23:32.698
------------------------------
â€¢ [SLOW TEST] [54.992 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:22:37.71
    May  6 08:22:37.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename emptydir-wrapper 05/06/23 08:22:37.711
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:22:38.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:22:38.735
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 05/06/23 08:22:38.737
    STEP: Creating RC which spawns configmap-volume pods 05/06/23 08:22:38.935
    May  6 08:22:39.031: INFO: Pod name wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/06/23 08:22:39.031
    May  6 08:22:39.033: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9" in namespace "emptydir-wrapper-9218" to be "running"
    May  6 08:22:39.072: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9": Phase="Pending", Reason="", readiness=false. Elapsed: 39.379298ms
    May  6 08:22:41.076: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043503315s
    May  6 08:22:43.077: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043820846s
    May  6 08:22:45.076: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042736246s
    May  6 08:22:47.076: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.042701089s
    May  6 08:22:49.076: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.043045434s
    May  6 08:22:51.077: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.043590196s
    May  6 08:22:53.077: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9": Phase="Running", Reason="", readiness=true. Elapsed: 14.044211376s
    May  6 08:22:53.077: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-5r5w9" satisfied condition "running"
    May  6 08:22:53.077: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-6l4lj" in namespace "emptydir-wrapper-9218" to be "running"
    May  6 08:22:53.081: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-6l4lj": Phase="Running", Reason="", readiness=true. Elapsed: 3.359855ms
    May  6 08:22:53.081: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-6l4lj" satisfied condition "running"
    May  6 08:22:53.081: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-fs8w5" in namespace "emptydir-wrapper-9218" to be "running"
    May  6 08:22:53.083: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-fs8w5": Phase="Running", Reason="", readiness=true. Elapsed: 2.121351ms
    May  6 08:22:53.083: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-fs8w5" satisfied condition "running"
    May  6 08:22:53.083: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-gr78r" in namespace "emptydir-wrapper-9218" to be "running"
    May  6 08:22:53.086: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-gr78r": Phase="Running", Reason="", readiness=true. Elapsed: 2.875224ms
    May  6 08:22:53.086: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-gr78r" satisfied condition "running"
    May  6 08:22:53.086: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-l766m" in namespace "emptydir-wrapper-9218" to be "running"
    May  6 08:22:53.088: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-l766m": Phase="Running", Reason="", readiness=true. Elapsed: 2.115311ms
    May  6 08:22:53.088: INFO: Pod "wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf-l766m" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf in namespace emptydir-wrapper-9218, will wait for the garbage collector to delete the pods 05/06/23 08:22:53.088
    May  6 08:22:53.148: INFO: Deleting ReplicationController wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf took: 6.861039ms
    May  6 08:22:53.249: INFO: Terminating ReplicationController wrapped-volume-race-8a47556d-beaf-4016-be21-b898199383cf pods took: 100.510429ms
    STEP: Creating RC which spawns configmap-volume pods 05/06/23 08:22:56.653
    May  6 08:22:56.665: INFO: Pod name wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31: Found 0 pods out of 5
    May  6 08:23:01.670: INFO: Pod name wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/06/23 08:23:01.67
    May  6 08:23:01.670: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-8vnxt" in namespace "emptydir-wrapper-9218" to be "running"
    May  6 08:23:01.673: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-8vnxt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188891ms
    May  6 08:23:03.676: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-8vnxt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005854896s
    May  6 08:23:05.676: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-8vnxt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005894625s
    May  6 08:23:07.676: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-8vnxt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005624204s
    May  6 08:23:09.677: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-8vnxt": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006269503s
    May  6 08:23:11.677: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-8vnxt": Phase="Running", Reason="", readiness=true. Elapsed: 10.007109572s
    May  6 08:23:11.678: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-8vnxt" satisfied condition "running"
    May  6 08:23:11.678: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-k42sq" in namespace "emptydir-wrapper-9218" to be "running"
    May  6 08:23:11.681: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-k42sq": Phase="Running", Reason="", readiness=true. Elapsed: 3.116272ms
    May  6 08:23:11.681: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-k42sq" satisfied condition "running"
    May  6 08:23:11.681: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-ptzxg" in namespace "emptydir-wrapper-9218" to be "running"
    May  6 08:23:11.683: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-ptzxg": Phase="Running", Reason="", readiness=true. Elapsed: 2.180233ms
    May  6 08:23:11.683: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-ptzxg" satisfied condition "running"
    May  6 08:23:11.683: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-qrxs5" in namespace "emptydir-wrapper-9218" to be "running"
    May  6 08:23:11.686: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-qrxs5": Phase="Running", Reason="", readiness=true. Elapsed: 2.827563ms
    May  6 08:23:11.686: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-qrxs5" satisfied condition "running"
    May  6 08:23:11.686: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-xgzx2" in namespace "emptydir-wrapper-9218" to be "running"
    May  6 08:23:11.688: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-xgzx2": Phase="Running", Reason="", readiness=true. Elapsed: 1.99907ms
    May  6 08:23:11.688: INFO: Pod "wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31-xgzx2" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31 in namespace emptydir-wrapper-9218, will wait for the garbage collector to delete the pods 05/06/23 08:23:11.688
    May  6 08:23:11.750: INFO: Deleting ReplicationController wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31 took: 8.144528ms
    May  6 08:23:11.850: INFO: Terminating ReplicationController wrapped-volume-race-3af957c2-0dc3-4493-b706-6caa2cf00c31 pods took: 100.459773ms
    STEP: Creating RC which spawns configmap-volume pods 05/06/23 08:23:14.754
    May  6 08:23:14.766: INFO: Pod name wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022: Found 0 pods out of 5
    May  6 08:23:19.771: INFO: Pod name wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/06/23 08:23:19.771
    May  6 08:23:19.772: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-4458d" in namespace "emptydir-wrapper-9218" to be "running"
    May  6 08:23:19.774: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-4458d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.597716ms
    May  6 08:23:21.778: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-4458d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006023602s
    May  6 08:23:23.778: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-4458d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006390728s
    May  6 08:23:25.778: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-4458d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006430649s
    May  6 08:23:27.777: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-4458d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005915041s
    May  6 08:23:29.778: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-4458d": Phase="Running", Reason="", readiness=true. Elapsed: 10.00691866s
    May  6 08:23:29.778: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-4458d" satisfied condition "running"
    May  6 08:23:29.778: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-blssc" in namespace "emptydir-wrapper-9218" to be "running"
    May  6 08:23:29.781: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-blssc": Phase="Running", Reason="", readiness=true. Elapsed: 2.795051ms
    May  6 08:23:29.781: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-blssc" satisfied condition "running"
    May  6 08:23:29.781: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-p5sjs" in namespace "emptydir-wrapper-9218" to be "running"
    May  6 08:23:29.783: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-p5sjs": Phase="Running", Reason="", readiness=true. Elapsed: 2.091685ms
    May  6 08:23:29.783: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-p5sjs" satisfied condition "running"
    May  6 08:23:29.783: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-qcppn" in namespace "emptydir-wrapper-9218" to be "running"
    May  6 08:23:29.786: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-qcppn": Phase="Running", Reason="", readiness=true. Elapsed: 2.700221ms
    May  6 08:23:29.786: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-qcppn" satisfied condition "running"
    May  6 08:23:29.786: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-tw757" in namespace "emptydir-wrapper-9218" to be "running"
    May  6 08:23:29.788: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-tw757": Phase="Running", Reason="", readiness=true. Elapsed: 2.060004ms
    May  6 08:23:29.788: INFO: Pod "wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022-tw757" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022 in namespace emptydir-wrapper-9218, will wait for the garbage collector to delete the pods 05/06/23 08:23:29.788
    May  6 08:23:29.848: INFO: Deleting ReplicationController wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022 took: 6.352711ms
    May  6 08:23:29.949: INFO: Terminating ReplicationController wrapped-volume-race-3f018f81-66d8-4ea1-b7b1-81371b24b022 pods took: 101.05178ms
    STEP: Cleaning up the configMaps 05/06/23 08:23:32.45
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    May  6 08:23:32.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-9218" for this suite. 05/06/23 08:23:32.698
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:23:32.703
May  6 08:23:32.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename proxy 05/06/23 08:23:32.704
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:23:33.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:23:33.722
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
May  6 08:23:33.724: INFO: Creating pod...
May  6 08:23:33.732: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3692" to be "running"
May  6 08:23:33.734: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.909107ms
May  6 08:23:35.736: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.004375524s
May  6 08:23:35.736: INFO: Pod "agnhost" satisfied condition "running"
May  6 08:23:35.736: INFO: Creating service...
May  6 08:23:35.749: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/pods/agnhost/proxy/some/path/with/DELETE
May  6 08:23:35.756: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  6 08:23:35.756: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/pods/agnhost/proxy/some/path/with/GET
May  6 08:23:35.759: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May  6 08:23:35.759: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/pods/agnhost/proxy/some/path/with/HEAD
May  6 08:23:35.764: INFO: http.Client request:HEAD | StatusCode:200
May  6 08:23:35.764: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/pods/agnhost/proxy/some/path/with/OPTIONS
May  6 08:23:35.767: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  6 08:23:35.767: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/pods/agnhost/proxy/some/path/with/PATCH
May  6 08:23:35.771: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  6 08:23:35.771: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/pods/agnhost/proxy/some/path/with/POST
May  6 08:23:35.774: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  6 08:23:35.774: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/pods/agnhost/proxy/some/path/with/PUT
May  6 08:23:35.779: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May  6 08:23:35.779: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/services/test-service/proxy/some/path/with/DELETE
May  6 08:23:35.783: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  6 08:23:35.783: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/services/test-service/proxy/some/path/with/GET
May  6 08:23:35.788: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May  6 08:23:35.788: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/services/test-service/proxy/some/path/with/HEAD
May  6 08:23:35.792: INFO: http.Client request:HEAD | StatusCode:200
May  6 08:23:35.792: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/services/test-service/proxy/some/path/with/OPTIONS
May  6 08:23:35.797: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  6 08:23:35.797: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/services/test-service/proxy/some/path/with/PATCH
May  6 08:23:35.801: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  6 08:23:35.801: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/services/test-service/proxy/some/path/with/POST
May  6 08:23:35.807: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  6 08:23:35.808: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/services/test-service/proxy/some/path/with/PUT
May  6 08:23:35.812: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
May  6 08:23:35.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-3692" for this suite. 05/06/23 08:23:35.816
------------------------------
â€¢ [3.118 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:23:32.703
    May  6 08:23:32.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename proxy 05/06/23 08:23:32.704
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:23:33.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:23:33.722
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    May  6 08:23:33.724: INFO: Creating pod...
    May  6 08:23:33.732: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3692" to be "running"
    May  6 08:23:33.734: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.909107ms
    May  6 08:23:35.736: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.004375524s
    May  6 08:23:35.736: INFO: Pod "agnhost" satisfied condition "running"
    May  6 08:23:35.736: INFO: Creating service...
    May  6 08:23:35.749: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/pods/agnhost/proxy/some/path/with/DELETE
    May  6 08:23:35.756: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  6 08:23:35.756: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/pods/agnhost/proxy/some/path/with/GET
    May  6 08:23:35.759: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    May  6 08:23:35.759: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/pods/agnhost/proxy/some/path/with/HEAD
    May  6 08:23:35.764: INFO: http.Client request:HEAD | StatusCode:200
    May  6 08:23:35.764: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/pods/agnhost/proxy/some/path/with/OPTIONS
    May  6 08:23:35.767: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  6 08:23:35.767: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/pods/agnhost/proxy/some/path/with/PATCH
    May  6 08:23:35.771: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  6 08:23:35.771: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/pods/agnhost/proxy/some/path/with/POST
    May  6 08:23:35.774: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  6 08:23:35.774: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/pods/agnhost/proxy/some/path/with/PUT
    May  6 08:23:35.779: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May  6 08:23:35.779: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/services/test-service/proxy/some/path/with/DELETE
    May  6 08:23:35.783: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  6 08:23:35.783: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/services/test-service/proxy/some/path/with/GET
    May  6 08:23:35.788: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    May  6 08:23:35.788: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/services/test-service/proxy/some/path/with/HEAD
    May  6 08:23:35.792: INFO: http.Client request:HEAD | StatusCode:200
    May  6 08:23:35.792: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/services/test-service/proxy/some/path/with/OPTIONS
    May  6 08:23:35.797: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  6 08:23:35.797: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/services/test-service/proxy/some/path/with/PATCH
    May  6 08:23:35.801: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  6 08:23:35.801: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/services/test-service/proxy/some/path/with/POST
    May  6 08:23:35.807: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  6 08:23:35.808: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3692/services/test-service/proxy/some/path/with/PUT
    May  6 08:23:35.812: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    May  6 08:23:35.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-3692" for this suite. 05/06/23 08:23:35.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:23:35.823
May  6 08:23:35.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename services 05/06/23 08:23:35.823
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:23:36.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:23:36.841
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 05/06/23 08:23:36.843
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  6 08:23:36.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7829" for this suite. 05/06/23 08:23:36.849
------------------------------
â€¢ [1.032 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:23:35.823
    May  6 08:23:35.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename services 05/06/23 08:23:35.823
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:23:36.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:23:36.841
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 05/06/23 08:23:36.843
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  6 08:23:36.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7829" for this suite. 05/06/23 08:23:36.849
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:23:36.855
May  6 08:23:36.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename services 05/06/23 08:23:36.856
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:23:37.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:23:37.873
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4525 05/06/23 08:23:37.875
STEP: changing the ExternalName service to type=NodePort 05/06/23 08:23:37.879
STEP: creating replication controller externalname-service in namespace services-4525 05/06/23 08:23:37.898
I0506 08:23:37.903765      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4525, replica count: 2
I0506 08:23:40.954345      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 08:23:40.954: INFO: Creating new exec pod
May  6 08:23:40.963: INFO: Waiting up to 5m0s for pod "execpod9v79p" in namespace "services-4525" to be "running"
May  6 08:23:40.965: INFO: Pod "execpod9v79p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183201ms
May  6 08:23:42.969: INFO: Pod "execpod9v79p": Phase="Running", Reason="", readiness=true. Elapsed: 2.005998888s
May  6 08:23:42.969: INFO: Pod "execpod9v79p" satisfied condition "running"
May  6 08:23:43.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-4525 exec execpod9v79p -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
May  6 08:23:44.090: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  6 08:23:44.090: INFO: stdout: ""
May  6 08:23:44.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-4525 exec execpod9v79p -- /bin/sh -x -c nc -v -z -w 2 10.107.122.147 80'
May  6 08:23:44.191: INFO: stderr: "+ nc -v -z -w 2 10.107.122.147 80\nConnection to 10.107.122.147 80 port [tcp/http] succeeded!\n"
May  6 08:23:44.191: INFO: stdout: ""
May  6 08:23:44.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-4525 exec execpod9v79p -- /bin/sh -x -c nc -v -z -w 2 10.0.0.107 32171'
May  6 08:23:44.289: INFO: stderr: "+ nc -v -z -w 2 10.0.0.107 32171\nConnection to 10.0.0.107 32171 port [tcp/*] succeeded!\n"
May  6 08:23:44.289: INFO: stdout: ""
May  6 08:23:44.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-4525 exec execpod9v79p -- /bin/sh -x -c nc -v -z -w 2 10.0.0.180 32171'
May  6 08:23:44.389: INFO: stderr: "+ nc -v -z -w 2 10.0.0.180 32171\nConnection to 10.0.0.180 32171 port [tcp/*] succeeded!\n"
May  6 08:23:44.389: INFO: stdout: ""
May  6 08:23:44.389: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  6 08:23:44.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4525" for this suite. 05/06/23 08:23:44.421
------------------------------
â€¢ [SLOW TEST] [7.572 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:23:36.855
    May  6 08:23:36.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename services 05/06/23 08:23:36.856
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:23:37.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:23:37.873
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4525 05/06/23 08:23:37.875
    STEP: changing the ExternalName service to type=NodePort 05/06/23 08:23:37.879
    STEP: creating replication controller externalname-service in namespace services-4525 05/06/23 08:23:37.898
    I0506 08:23:37.903765      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4525, replica count: 2
    I0506 08:23:40.954345      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  6 08:23:40.954: INFO: Creating new exec pod
    May  6 08:23:40.963: INFO: Waiting up to 5m0s for pod "execpod9v79p" in namespace "services-4525" to be "running"
    May  6 08:23:40.965: INFO: Pod "execpod9v79p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183201ms
    May  6 08:23:42.969: INFO: Pod "execpod9v79p": Phase="Running", Reason="", readiness=true. Elapsed: 2.005998888s
    May  6 08:23:42.969: INFO: Pod "execpod9v79p" satisfied condition "running"
    May  6 08:23:43.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-4525 exec execpod9v79p -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    May  6 08:23:44.090: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    May  6 08:23:44.090: INFO: stdout: ""
    May  6 08:23:44.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-4525 exec execpod9v79p -- /bin/sh -x -c nc -v -z -w 2 10.107.122.147 80'
    May  6 08:23:44.191: INFO: stderr: "+ nc -v -z -w 2 10.107.122.147 80\nConnection to 10.107.122.147 80 port [tcp/http] succeeded!\n"
    May  6 08:23:44.191: INFO: stdout: ""
    May  6 08:23:44.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-4525 exec execpod9v79p -- /bin/sh -x -c nc -v -z -w 2 10.0.0.107 32171'
    May  6 08:23:44.289: INFO: stderr: "+ nc -v -z -w 2 10.0.0.107 32171\nConnection to 10.0.0.107 32171 port [tcp/*] succeeded!\n"
    May  6 08:23:44.289: INFO: stdout: ""
    May  6 08:23:44.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-4525 exec execpod9v79p -- /bin/sh -x -c nc -v -z -w 2 10.0.0.180 32171'
    May  6 08:23:44.389: INFO: stderr: "+ nc -v -z -w 2 10.0.0.180 32171\nConnection to 10.0.0.180 32171 port [tcp/*] succeeded!\n"
    May  6 08:23:44.389: INFO: stdout: ""
    May  6 08:23:44.389: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  6 08:23:44.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4525" for this suite. 05/06/23 08:23:44.421
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:23:44.428
May  6 08:23:44.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename events 05/06/23 08:23:44.429
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:23:45.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:23:45.448
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 05/06/23 08:23:45.45
STEP: listing all events in all namespaces 05/06/23 08:23:45.454
STEP: patching the test event 05/06/23 08:23:45.458
STEP: fetching the test event 05/06/23 08:23:45.464
STEP: updating the test event 05/06/23 08:23:45.466
STEP: getting the test event 05/06/23 08:23:45.483
STEP: deleting the test event 05/06/23 08:23:45.485
STEP: listing all events in all namespaces 05/06/23 08:23:45.491
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
May  6 08:23:45.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4545" for this suite. 05/06/23 08:23:45.497
------------------------------
â€¢ [1.074 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:23:44.428
    May  6 08:23:44.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename events 05/06/23 08:23:44.429
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:23:45.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:23:45.448
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 05/06/23 08:23:45.45
    STEP: listing all events in all namespaces 05/06/23 08:23:45.454
    STEP: patching the test event 05/06/23 08:23:45.458
    STEP: fetching the test event 05/06/23 08:23:45.464
    STEP: updating the test event 05/06/23 08:23:45.466
    STEP: getting the test event 05/06/23 08:23:45.483
    STEP: deleting the test event 05/06/23 08:23:45.485
    STEP: listing all events in all namespaces 05/06/23 08:23:45.491
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    May  6 08:23:45.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4545" for this suite. 05/06/23 08:23:45.497
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:23:45.502
May  6 08:23:45.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename emptydir 05/06/23 08:23:45.503
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:23:46.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:23:46.52
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 05/06/23 08:23:46.522
May  6 08:23:46.529: INFO: Waiting up to 5m0s for pod "pod-e38ba4e0-1ab0-4bd3-87fb-8febf5c4cab1" in namespace "emptydir-6646" to be "Succeeded or Failed"
May  6 08:23:46.534: INFO: Pod "pod-e38ba4e0-1ab0-4bd3-87fb-8febf5c4cab1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.075597ms
May  6 08:23:48.537: INFO: Pod "pod-e38ba4e0-1ab0-4bd3-87fb-8febf5c4cab1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007917224s
May  6 08:23:50.537: INFO: Pod "pod-e38ba4e0-1ab0-4bd3-87fb-8febf5c4cab1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007768444s
STEP: Saw pod success 05/06/23 08:23:50.537
May  6 08:23:50.537: INFO: Pod "pod-e38ba4e0-1ab0-4bd3-87fb-8febf5c4cab1" satisfied condition "Succeeded or Failed"
May  6 08:23:50.540: INFO: Trying to get logs from node cncf-0 pod pod-e38ba4e0-1ab0-4bd3-87fb-8febf5c4cab1 container test-container: <nil>
STEP: delete the pod 05/06/23 08:23:50.546
May  6 08:23:50.561: INFO: Waiting for pod pod-e38ba4e0-1ab0-4bd3-87fb-8febf5c4cab1 to disappear
May  6 08:23:50.564: INFO: Pod pod-e38ba4e0-1ab0-4bd3-87fb-8febf5c4cab1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  6 08:23:50.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6646" for this suite. 05/06/23 08:23:50.566
------------------------------
â€¢ [SLOW TEST] [5.070 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:23:45.502
    May  6 08:23:45.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename emptydir 05/06/23 08:23:45.503
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:23:46.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:23:46.52
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 05/06/23 08:23:46.522
    May  6 08:23:46.529: INFO: Waiting up to 5m0s for pod "pod-e38ba4e0-1ab0-4bd3-87fb-8febf5c4cab1" in namespace "emptydir-6646" to be "Succeeded or Failed"
    May  6 08:23:46.534: INFO: Pod "pod-e38ba4e0-1ab0-4bd3-87fb-8febf5c4cab1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.075597ms
    May  6 08:23:48.537: INFO: Pod "pod-e38ba4e0-1ab0-4bd3-87fb-8febf5c4cab1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007917224s
    May  6 08:23:50.537: INFO: Pod "pod-e38ba4e0-1ab0-4bd3-87fb-8febf5c4cab1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007768444s
    STEP: Saw pod success 05/06/23 08:23:50.537
    May  6 08:23:50.537: INFO: Pod "pod-e38ba4e0-1ab0-4bd3-87fb-8febf5c4cab1" satisfied condition "Succeeded or Failed"
    May  6 08:23:50.540: INFO: Trying to get logs from node cncf-0 pod pod-e38ba4e0-1ab0-4bd3-87fb-8febf5c4cab1 container test-container: <nil>
    STEP: delete the pod 05/06/23 08:23:50.546
    May  6 08:23:50.561: INFO: Waiting for pod pod-e38ba4e0-1ab0-4bd3-87fb-8febf5c4cab1 to disappear
    May  6 08:23:50.564: INFO: Pod pod-e38ba4e0-1ab0-4bd3-87fb-8febf5c4cab1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  6 08:23:50.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6646" for this suite. 05/06/23 08:23:50.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:23:50.572
May  6 08:23:50.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubectl 05/06/23 08:23:50.573
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:23:51.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:23:51.589
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 05/06/23 08:23:51.591
May  6 08:23:51.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 create -f -'
May  6 08:23:52.096: INFO: stderr: ""
May  6 08:23:52.096: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/06/23 08:23:52.096
May  6 08:23:52.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  6 08:23:52.155: INFO: stderr: ""
May  6 08:23:52.155: INFO: stdout: "update-demo-nautilus-jgff4 update-demo-nautilus-qrkpb "
May  6 08:23:52.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-jgff4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  6 08:23:52.207: INFO: stderr: ""
May  6 08:23:52.207: INFO: stdout: ""
May  6 08:23:52.207: INFO: update-demo-nautilus-jgff4 is created but not running
May  6 08:23:57.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  6 08:23:57.259: INFO: stderr: ""
May  6 08:23:57.259: INFO: stdout: "update-demo-nautilus-jgff4 update-demo-nautilus-qrkpb "
May  6 08:23:57.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-jgff4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  6 08:23:57.309: INFO: stderr: ""
May  6 08:23:57.309: INFO: stdout: "true"
May  6 08:23:57.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-jgff4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  6 08:23:57.358: INFO: stderr: ""
May  6 08:23:57.358: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May  6 08:23:57.358: INFO: validating pod update-demo-nautilus-jgff4
May  6 08:23:57.362: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 08:23:57.362: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 08:23:57.362: INFO: update-demo-nautilus-jgff4 is verified up and running
May  6 08:23:57.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-qrkpb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  6 08:23:57.410: INFO: stderr: ""
May  6 08:23:57.410: INFO: stdout: "true"
May  6 08:23:57.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-qrkpb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  6 08:23:57.467: INFO: stderr: ""
May  6 08:23:57.467: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May  6 08:23:57.467: INFO: validating pod update-demo-nautilus-qrkpb
May  6 08:23:57.470: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 08:23:57.470: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 08:23:57.470: INFO: update-demo-nautilus-qrkpb is verified up and running
STEP: scaling down the replication controller 05/06/23 08:23:57.47
May  6 08:23:57.472: INFO: scanned /root for discovery docs: <nil>
May  6 08:23:57.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May  6 08:23:58.538: INFO: stderr: ""
May  6 08:23:58.538: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/06/23 08:23:58.538
May  6 08:23:58.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  6 08:23:58.610: INFO: stderr: ""
May  6 08:23:58.610: INFO: stdout: "update-demo-nautilus-jgff4 update-demo-nautilus-qrkpb "
STEP: Replicas for name=update-demo: expected=1 actual=2 05/06/23 08:23:58.61
May  6 08:24:03.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  6 08:24:03.666: INFO: stderr: ""
May  6 08:24:03.666: INFO: stdout: "update-demo-nautilus-jgff4 "
May  6 08:24:03.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-jgff4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  6 08:24:03.713: INFO: stderr: ""
May  6 08:24:03.713: INFO: stdout: "true"
May  6 08:24:03.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-jgff4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  6 08:24:03.761: INFO: stderr: ""
May  6 08:24:03.761: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May  6 08:24:03.761: INFO: validating pod update-demo-nautilus-jgff4
May  6 08:24:03.766: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 08:24:03.766: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 08:24:03.766: INFO: update-demo-nautilus-jgff4 is verified up and running
STEP: scaling up the replication controller 05/06/23 08:24:03.766
May  6 08:24:03.767: INFO: scanned /root for discovery docs: <nil>
May  6 08:24:03.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May  6 08:24:04.839: INFO: stderr: ""
May  6 08:24:04.839: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/06/23 08:24:04.839
May  6 08:24:04.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  6 08:24:04.892: INFO: stderr: ""
May  6 08:24:04.892: INFO: stdout: "update-demo-nautilus-69zht update-demo-nautilus-jgff4 "
May  6 08:24:04.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-69zht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  6 08:24:04.942: INFO: stderr: ""
May  6 08:24:04.942: INFO: stdout: ""
May  6 08:24:04.942: INFO: update-demo-nautilus-69zht is created but not running
May  6 08:24:09.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  6 08:24:09.994: INFO: stderr: ""
May  6 08:24:09.994: INFO: stdout: "update-demo-nautilus-69zht update-demo-nautilus-jgff4 "
May  6 08:24:09.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-69zht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  6 08:24:10.043: INFO: stderr: ""
May  6 08:24:10.043: INFO: stdout: "true"
May  6 08:24:10.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-69zht -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  6 08:24:10.098: INFO: stderr: ""
May  6 08:24:10.099: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May  6 08:24:10.099: INFO: validating pod update-demo-nautilus-69zht
May  6 08:24:10.103: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 08:24:10.103: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 08:24:10.103: INFO: update-demo-nautilus-69zht is verified up and running
May  6 08:24:10.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-jgff4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  6 08:24:10.151: INFO: stderr: ""
May  6 08:24:10.151: INFO: stdout: "true"
May  6 08:24:10.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-jgff4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  6 08:24:10.202: INFO: stderr: ""
May  6 08:24:10.202: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May  6 08:24:10.202: INFO: validating pod update-demo-nautilus-jgff4
May  6 08:24:10.205: INFO: got data: {
  "image": "nautilus.jpg"
}

May  6 08:24:10.205: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  6 08:24:10.205: INFO: update-demo-nautilus-jgff4 is verified up and running
STEP: using delete to clean up resources 05/06/23 08:24:10.205
May  6 08:24:10.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 delete --grace-period=0 --force -f -'
May  6 08:24:10.259: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  6 08:24:10.259: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  6 08:24:10.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get rc,svc -l name=update-demo --no-headers'
May  6 08:24:10.327: INFO: stderr: "No resources found in kubectl-7 namespace.\n"
May  6 08:24:10.327: INFO: stdout: ""
May  6 08:24:10.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  6 08:24:10.386: INFO: stderr: ""
May  6 08:24:10.386: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  6 08:24:10.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7" for this suite. 05/06/23 08:24:10.389
------------------------------
â€¢ [SLOW TEST] [19.825 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:23:50.572
    May  6 08:23:50.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubectl 05/06/23 08:23:50.573
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:23:51.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:23:51.589
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 05/06/23 08:23:51.591
    May  6 08:23:51.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 create -f -'
    May  6 08:23:52.096: INFO: stderr: ""
    May  6 08:23:52.096: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/06/23 08:23:52.096
    May  6 08:23:52.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  6 08:23:52.155: INFO: stderr: ""
    May  6 08:23:52.155: INFO: stdout: "update-demo-nautilus-jgff4 update-demo-nautilus-qrkpb "
    May  6 08:23:52.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-jgff4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  6 08:23:52.207: INFO: stderr: ""
    May  6 08:23:52.207: INFO: stdout: ""
    May  6 08:23:52.207: INFO: update-demo-nautilus-jgff4 is created but not running
    May  6 08:23:57.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  6 08:23:57.259: INFO: stderr: ""
    May  6 08:23:57.259: INFO: stdout: "update-demo-nautilus-jgff4 update-demo-nautilus-qrkpb "
    May  6 08:23:57.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-jgff4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  6 08:23:57.309: INFO: stderr: ""
    May  6 08:23:57.309: INFO: stdout: "true"
    May  6 08:23:57.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-jgff4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  6 08:23:57.358: INFO: stderr: ""
    May  6 08:23:57.358: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May  6 08:23:57.358: INFO: validating pod update-demo-nautilus-jgff4
    May  6 08:23:57.362: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  6 08:23:57.362: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  6 08:23:57.362: INFO: update-demo-nautilus-jgff4 is verified up and running
    May  6 08:23:57.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-qrkpb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  6 08:23:57.410: INFO: stderr: ""
    May  6 08:23:57.410: INFO: stdout: "true"
    May  6 08:23:57.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-qrkpb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  6 08:23:57.467: INFO: stderr: ""
    May  6 08:23:57.467: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May  6 08:23:57.467: INFO: validating pod update-demo-nautilus-qrkpb
    May  6 08:23:57.470: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  6 08:23:57.470: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  6 08:23:57.470: INFO: update-demo-nautilus-qrkpb is verified up and running
    STEP: scaling down the replication controller 05/06/23 08:23:57.47
    May  6 08:23:57.472: INFO: scanned /root for discovery docs: <nil>
    May  6 08:23:57.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    May  6 08:23:58.538: INFO: stderr: ""
    May  6 08:23:58.538: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/06/23 08:23:58.538
    May  6 08:23:58.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  6 08:23:58.610: INFO: stderr: ""
    May  6 08:23:58.610: INFO: stdout: "update-demo-nautilus-jgff4 update-demo-nautilus-qrkpb "
    STEP: Replicas for name=update-demo: expected=1 actual=2 05/06/23 08:23:58.61
    May  6 08:24:03.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  6 08:24:03.666: INFO: stderr: ""
    May  6 08:24:03.666: INFO: stdout: "update-demo-nautilus-jgff4 "
    May  6 08:24:03.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-jgff4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  6 08:24:03.713: INFO: stderr: ""
    May  6 08:24:03.713: INFO: stdout: "true"
    May  6 08:24:03.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-jgff4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  6 08:24:03.761: INFO: stderr: ""
    May  6 08:24:03.761: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May  6 08:24:03.761: INFO: validating pod update-demo-nautilus-jgff4
    May  6 08:24:03.766: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  6 08:24:03.766: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  6 08:24:03.766: INFO: update-demo-nautilus-jgff4 is verified up and running
    STEP: scaling up the replication controller 05/06/23 08:24:03.766
    May  6 08:24:03.767: INFO: scanned /root for discovery docs: <nil>
    May  6 08:24:03.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    May  6 08:24:04.839: INFO: stderr: ""
    May  6 08:24:04.839: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/06/23 08:24:04.839
    May  6 08:24:04.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  6 08:24:04.892: INFO: stderr: ""
    May  6 08:24:04.892: INFO: stdout: "update-demo-nautilus-69zht update-demo-nautilus-jgff4 "
    May  6 08:24:04.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-69zht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  6 08:24:04.942: INFO: stderr: ""
    May  6 08:24:04.942: INFO: stdout: ""
    May  6 08:24:04.942: INFO: update-demo-nautilus-69zht is created but not running
    May  6 08:24:09.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  6 08:24:09.994: INFO: stderr: ""
    May  6 08:24:09.994: INFO: stdout: "update-demo-nautilus-69zht update-demo-nautilus-jgff4 "
    May  6 08:24:09.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-69zht -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  6 08:24:10.043: INFO: stderr: ""
    May  6 08:24:10.043: INFO: stdout: "true"
    May  6 08:24:10.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-69zht -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  6 08:24:10.098: INFO: stderr: ""
    May  6 08:24:10.099: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May  6 08:24:10.099: INFO: validating pod update-demo-nautilus-69zht
    May  6 08:24:10.103: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  6 08:24:10.103: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  6 08:24:10.103: INFO: update-demo-nautilus-69zht is verified up and running
    May  6 08:24:10.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-jgff4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  6 08:24:10.151: INFO: stderr: ""
    May  6 08:24:10.151: INFO: stdout: "true"
    May  6 08:24:10.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods update-demo-nautilus-jgff4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  6 08:24:10.202: INFO: stderr: ""
    May  6 08:24:10.202: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May  6 08:24:10.202: INFO: validating pod update-demo-nautilus-jgff4
    May  6 08:24:10.205: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  6 08:24:10.205: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  6 08:24:10.205: INFO: update-demo-nautilus-jgff4 is verified up and running
    STEP: using delete to clean up resources 05/06/23 08:24:10.205
    May  6 08:24:10.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 delete --grace-period=0 --force -f -'
    May  6 08:24:10.259: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  6 08:24:10.259: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    May  6 08:24:10.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get rc,svc -l name=update-demo --no-headers'
    May  6 08:24:10.327: INFO: stderr: "No resources found in kubectl-7 namespace.\n"
    May  6 08:24:10.327: INFO: stdout: ""
    May  6 08:24:10.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-7 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May  6 08:24:10.386: INFO: stderr: ""
    May  6 08:24:10.386: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  6 08:24:10.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7" for this suite. 05/06/23 08:24:10.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:24:10.397
May  6 08:24:10.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename svc-latency 05/06/23 08:24:10.398
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:24:11.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:24:11.414
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
May  6 08:24:11.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2819 05/06/23 08:24:11.417
I0506 08:24:11.423071      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2819, replica count: 1
I0506 08:24:12.473813      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0506 08:24:13.474336      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 08:24:13.588: INFO: Created: latency-svc-n8sh5
May  6 08:24:13.595: INFO: Got endpoints: latency-svc-n8sh5 [20.233833ms]
May  6 08:24:13.609: INFO: Created: latency-svc-5s6t5
May  6 08:24:13.615: INFO: Got endpoints: latency-svc-5s6t5 [20.387724ms]
May  6 08:24:13.618: INFO: Created: latency-svc-9vrq8
May  6 08:24:13.624: INFO: Got endpoints: latency-svc-9vrq8 [28.69359ms]
May  6 08:24:13.628: INFO: Created: latency-svc-4xxmh
May  6 08:24:13.634: INFO: Got endpoints: latency-svc-4xxmh [38.322459ms]
May  6 08:24:13.637: INFO: Created: latency-svc-94w85
May  6 08:24:13.645: INFO: Got endpoints: latency-svc-94w85 [49.054012ms]
May  6 08:24:13.650: INFO: Created: latency-svc-nxcpj
May  6 08:24:13.657: INFO: Got endpoints: latency-svc-nxcpj [60.846723ms]
May  6 08:24:13.661: INFO: Created: latency-svc-fj6k2
May  6 08:24:13.669: INFO: Created: latency-svc-wft56
May  6 08:24:13.672: INFO: Got endpoints: latency-svc-fj6k2 [76.064883ms]
May  6 08:24:13.674: INFO: Got endpoints: latency-svc-wft56 [78.022073ms]
May  6 08:24:13.685: INFO: Created: latency-svc-gr9lk
May  6 08:24:13.691: INFO: Got endpoints: latency-svc-gr9lk [94.651727ms]
May  6 08:24:13.692: INFO: Created: latency-svc-8nvtx
May  6 08:24:13.700: INFO: Got endpoints: latency-svc-8nvtx [102.98278ms]
May  6 08:24:13.704: INFO: Created: latency-svc-7sfsb
May  6 08:24:13.714: INFO: Got endpoints: latency-svc-7sfsb [116.725466ms]
May  6 08:24:13.734: INFO: Created: latency-svc-qh2vt
May  6 08:24:13.737: INFO: Created: latency-svc-zlzv8
May  6 08:24:13.739: INFO: Got endpoints: latency-svc-qh2vt [141.826588ms]
May  6 08:24:13.746: INFO: Got endpoints: latency-svc-zlzv8 [149.615419ms]
May  6 08:24:13.747: INFO: Created: latency-svc-mhtlt
May  6 08:24:13.754: INFO: Got endpoints: latency-svc-mhtlt [157.19282ms]
May  6 08:24:13.759: INFO: Created: latency-svc-q85bf
May  6 08:24:13.765: INFO: Got endpoints: latency-svc-q85bf [167.59087ms]
May  6 08:24:13.765: INFO: Created: latency-svc-qtds2
May  6 08:24:13.777: INFO: Got endpoints: latency-svc-qtds2 [179.990213ms]
May  6 08:24:13.778: INFO: Created: latency-svc-s8r9b
May  6 08:24:13.787: INFO: Got endpoints: latency-svc-s8r9b [171.593548ms]
May  6 08:24:13.793: INFO: Created: latency-svc-6w68v
May  6 08:24:13.799: INFO: Got endpoints: latency-svc-6w68v [174.825269ms]
May  6 08:24:13.809: INFO: Created: latency-svc-4r4ts
May  6 08:24:13.815: INFO: Got endpoints: latency-svc-4r4ts [180.097189ms]
May  6 08:24:13.817: INFO: Created: latency-svc-vz8d7
May  6 08:24:13.823: INFO: Got endpoints: latency-svc-vz8d7 [178.857501ms]
May  6 08:24:13.832: INFO: Created: latency-svc-wj9s9
May  6 08:24:13.835: INFO: Created: latency-svc-44grl
May  6 08:24:13.836: INFO: Got endpoints: latency-svc-wj9s9 [179.871289ms]
May  6 08:24:13.841: INFO: Got endpoints: latency-svc-44grl [169.019757ms]
May  6 08:24:13.851: INFO: Created: latency-svc-bwl8g
May  6 08:24:13.859: INFO: Created: latency-svc-f8tdq
May  6 08:24:13.859: INFO: Got endpoints: latency-svc-bwl8g [184.937377ms]
May  6 08:24:13.867: INFO: Got endpoints: latency-svc-f8tdq [176.374483ms]
May  6 08:24:13.870: INFO: Created: latency-svc-8qvmx
May  6 08:24:13.876: INFO: Got endpoints: latency-svc-8qvmx [176.171638ms]
May  6 08:24:13.878: INFO: Created: latency-svc-gqmw6
May  6 08:24:13.884: INFO: Got endpoints: latency-svc-gqmw6 [170.024867ms]
May  6 08:24:13.894: INFO: Created: latency-svc-f2zph
May  6 08:24:13.901: INFO: Got endpoints: latency-svc-f2zph [162.473286ms]
May  6 08:24:13.904: INFO: Created: latency-svc-mxx5x
May  6 08:24:13.910: INFO: Got endpoints: latency-svc-mxx5x [163.355492ms]
May  6 08:24:13.913: INFO: Created: latency-svc-56f5h
May  6 08:24:13.920: INFO: Got endpoints: latency-svc-56f5h [165.549903ms]
May  6 08:24:13.921: INFO: Created: latency-svc-cwbnt
May  6 08:24:13.929: INFO: Got endpoints: latency-svc-cwbnt [163.915487ms]
May  6 08:24:13.936: INFO: Created: latency-svc-gn4kw
May  6 08:24:13.941: INFO: Created: latency-svc-qdhmj
May  6 08:24:13.941: INFO: Got endpoints: latency-svc-gn4kw [163.596401ms]
May  6 08:24:13.945: INFO: Got endpoints: latency-svc-qdhmj [158.193113ms]
May  6 08:24:13.949: INFO: Created: latency-svc-frmjc
May  6 08:24:13.954: INFO: Got endpoints: latency-svc-frmjc [154.710274ms]
May  6 08:24:13.969: INFO: Created: latency-svc-2f9m8
May  6 08:24:13.974: INFO: Got endpoints: latency-svc-2f9m8 [159.751444ms]
May  6 08:24:13.977: INFO: Created: latency-svc-z4cfl
May  6 08:24:13.983: INFO: Got endpoints: latency-svc-z4cfl [159.865861ms]
May  6 08:24:13.988: INFO: Created: latency-svc-bcvhh
May  6 08:24:13.995: INFO: Got endpoints: latency-svc-bcvhh [158.321896ms]
May  6 08:24:14.004: INFO: Created: latency-svc-f7r9f
May  6 08:24:14.009: INFO: Created: latency-svc-5c9hg
May  6 08:24:14.011: INFO: Got endpoints: latency-svc-f7r9f [169.888837ms]
May  6 08:24:14.014: INFO: Got endpoints: latency-svc-5c9hg [155.107979ms]
May  6 08:24:14.017: INFO: Created: latency-svc-bbf55
May  6 08:24:14.022: INFO: Created: latency-svc-8zr7r
May  6 08:24:14.032: INFO: Created: latency-svc-kzf6r
May  6 08:24:14.038: INFO: Created: latency-svc-qw4bl
May  6 08:24:14.044: INFO: Got endpoints: latency-svc-bbf55 [176.429658ms]
May  6 08:24:14.046: INFO: Created: latency-svc-9h5cz
May  6 08:24:14.053: INFO: Created: latency-svc-x27bm
May  6 08:24:14.063: INFO: Created: latency-svc-vl8rk
May  6 08:24:14.073: INFO: Created: latency-svc-q25xt
May  6 08:24:14.083: INFO: Created: latency-svc-mgsqj
May  6 08:24:14.089: INFO: Created: latency-svc-58l4n
May  6 08:24:14.097: INFO: Got endpoints: latency-svc-8zr7r [221.6947ms]
May  6 08:24:14.100: INFO: Created: latency-svc-nq47g
May  6 08:24:14.120: INFO: Created: latency-svc-h7sg5
May  6 08:24:14.126: INFO: Created: latency-svc-7gcgp
May  6 08:24:14.131: INFO: Created: latency-svc-9qf7h
May  6 08:24:14.140: INFO: Created: latency-svc-7jbns
May  6 08:24:14.146: INFO: Got endpoints: latency-svc-kzf6r [261.555052ms]
May  6 08:24:14.148: INFO: Created: latency-svc-l92qz
May  6 08:24:14.157: INFO: Created: latency-svc-rx2pw
May  6 08:24:14.167: INFO: Created: latency-svc-n7h2l
May  6 08:24:14.193: INFO: Got endpoints: latency-svc-qw4bl [291.600229ms]
May  6 08:24:14.207: INFO: Created: latency-svc-7rwcd
May  6 08:24:14.243: INFO: Got endpoints: latency-svc-9h5cz [332.721315ms]
May  6 08:24:14.254: INFO: Created: latency-svc-dfhln
May  6 08:24:14.293: INFO: Got endpoints: latency-svc-x27bm [373.898648ms]
May  6 08:24:14.315: INFO: Created: latency-svc-gcgj6
May  6 08:24:14.343: INFO: Got endpoints: latency-svc-vl8rk [414.242447ms]
May  6 08:24:14.354: INFO: Created: latency-svc-czqv5
May  6 08:24:14.392: INFO: Got endpoints: latency-svc-q25xt [450.868772ms]
May  6 08:24:14.404: INFO: Created: latency-svc-6jdxr
May  6 08:24:14.443: INFO: Got endpoints: latency-svc-mgsqj [497.525298ms]
May  6 08:24:14.455: INFO: Created: latency-svc-k8hrs
May  6 08:24:14.494: INFO: Got endpoints: latency-svc-58l4n [540.142619ms]
May  6 08:24:14.505: INFO: Created: latency-svc-2hj7k
May  6 08:24:14.542: INFO: Got endpoints: latency-svc-nq47g [567.519466ms]
May  6 08:24:14.558: INFO: Created: latency-svc-ktzwn
May  6 08:24:14.593: INFO: Got endpoints: latency-svc-h7sg5 [609.914162ms]
May  6 08:24:14.609: INFO: Created: latency-svc-qfc49
May  6 08:24:14.644: INFO: Got endpoints: latency-svc-7gcgp [649.541159ms]
May  6 08:24:14.664: INFO: Created: latency-svc-jxhg9
May  6 08:24:14.693: INFO: Got endpoints: latency-svc-9qf7h [681.671509ms]
May  6 08:24:14.704: INFO: Created: latency-svc-xm8zl
May  6 08:24:14.744: INFO: Got endpoints: latency-svc-7jbns [729.995615ms]
May  6 08:24:14.755: INFO: Created: latency-svc-6fzcj
May  6 08:24:14.793: INFO: Got endpoints: latency-svc-l92qz [749.28934ms]
May  6 08:24:14.806: INFO: Created: latency-svc-w2xwp
May  6 08:24:14.843: INFO: Got endpoints: latency-svc-rx2pw [745.096843ms]
May  6 08:24:14.854: INFO: Created: latency-svc-r4pq5
May  6 08:24:14.893: INFO: Got endpoints: latency-svc-n7h2l [747.775123ms]
May  6 08:24:14.904: INFO: Created: latency-svc-sc56d
May  6 08:24:14.945: INFO: Got endpoints: latency-svc-7rwcd [752.363823ms]
May  6 08:24:14.956: INFO: Created: latency-svc-tvrm6
May  6 08:24:14.992: INFO: Got endpoints: latency-svc-dfhln [749.661678ms]
May  6 08:24:15.004: INFO: Created: latency-svc-k4gb9
May  6 08:24:15.044: INFO: Got endpoints: latency-svc-gcgj6 [750.571197ms]
May  6 08:24:15.055: INFO: Created: latency-svc-j2nwv
May  6 08:24:15.096: INFO: Got endpoints: latency-svc-czqv5 [753.334369ms]
May  6 08:24:15.107: INFO: Created: latency-svc-2zmsm
May  6 08:24:15.142: INFO: Got endpoints: latency-svc-6jdxr [750.404371ms]
May  6 08:24:15.154: INFO: Created: latency-svc-x6ckn
May  6 08:24:15.193: INFO: Got endpoints: latency-svc-k8hrs [749.829218ms]
May  6 08:24:15.204: INFO: Created: latency-svc-zwm42
May  6 08:24:15.244: INFO: Got endpoints: latency-svc-2hj7k [749.936491ms]
May  6 08:24:15.255: INFO: Created: latency-svc-6lgkj
May  6 08:24:15.292: INFO: Got endpoints: latency-svc-ktzwn [749.97815ms]
May  6 08:24:15.303: INFO: Created: latency-svc-tfhpl
May  6 08:24:15.344: INFO: Got endpoints: latency-svc-qfc49 [751.141931ms]
May  6 08:24:15.354: INFO: Created: latency-svc-fnsn2
May  6 08:24:15.392: INFO: Got endpoints: latency-svc-jxhg9 [747.4847ms]
May  6 08:24:15.406: INFO: Created: latency-svc-jz6jn
May  6 08:24:15.442: INFO: Got endpoints: latency-svc-xm8zl [749.023686ms]
May  6 08:24:15.455: INFO: Created: latency-svc-sxzcv
May  6 08:24:15.494: INFO: Got endpoints: latency-svc-6fzcj [749.9835ms]
May  6 08:24:15.507: INFO: Created: latency-svc-q8zd9
May  6 08:24:15.542: INFO: Got endpoints: latency-svc-w2xwp [749.023706ms]
May  6 08:24:15.560: INFO: Created: latency-svc-hnxpc
May  6 08:24:15.593: INFO: Got endpoints: latency-svc-r4pq5 [750.37844ms]
May  6 08:24:15.606: INFO: Created: latency-svc-dvwsn
May  6 08:24:15.645: INFO: Got endpoints: latency-svc-sc56d [751.753523ms]
May  6 08:24:15.658: INFO: Created: latency-svc-xjdbx
May  6 08:24:15.693: INFO: Got endpoints: latency-svc-tvrm6 [747.253782ms]
May  6 08:24:15.704: INFO: Created: latency-svc-pfl9v
May  6 08:24:15.744: INFO: Got endpoints: latency-svc-k4gb9 [751.872099ms]
May  6 08:24:15.756: INFO: Created: latency-svc-2lc9h
May  6 08:24:15.792: INFO: Got endpoints: latency-svc-j2nwv [748.323255ms]
May  6 08:24:15.805: INFO: Created: latency-svc-zkrqf
May  6 08:24:15.843: INFO: Got endpoints: latency-svc-2zmsm [747.022402ms]
May  6 08:24:15.866: INFO: Created: latency-svc-ljmqf
May  6 08:24:15.892: INFO: Got endpoints: latency-svc-x6ckn [750.110291ms]
May  6 08:24:15.904: INFO: Created: latency-svc-5phft
May  6 08:24:15.942: INFO: Got endpoints: latency-svc-zwm42 [749.648774ms]
May  6 08:24:15.954: INFO: Created: latency-svc-gvggb
May  6 08:24:15.993: INFO: Got endpoints: latency-svc-6lgkj [749.35667ms]
May  6 08:24:16.025: INFO: Created: latency-svc-gqtdn
May  6 08:24:16.044: INFO: Got endpoints: latency-svc-tfhpl [752.237304ms]
May  6 08:24:16.055: INFO: Created: latency-svc-n42zf
May  6 08:24:16.092: INFO: Got endpoints: latency-svc-fnsn2 [747.884633ms]
May  6 08:24:16.107: INFO: Created: latency-svc-nf7qc
May  6 08:24:16.147: INFO: Got endpoints: latency-svc-jz6jn [754.761611ms]
May  6 08:24:16.157: INFO: Created: latency-svc-9mfzs
May  6 08:24:16.194: INFO: Got endpoints: latency-svc-sxzcv [752.753356ms]
May  6 08:24:16.205: INFO: Created: latency-svc-kzjnv
May  6 08:24:16.243: INFO: Got endpoints: latency-svc-q8zd9 [748.973973ms]
May  6 08:24:16.254: INFO: Created: latency-svc-8n6zp
May  6 08:24:16.293: INFO: Got endpoints: latency-svc-hnxpc [750.729268ms]
May  6 08:24:16.304: INFO: Created: latency-svc-xfxmw
May  6 08:24:16.346: INFO: Got endpoints: latency-svc-dvwsn [752.699704ms]
May  6 08:24:16.358: INFO: Created: latency-svc-zm8kp
May  6 08:24:16.392: INFO: Got endpoints: latency-svc-xjdbx [746.849054ms]
May  6 08:24:16.404: INFO: Created: latency-svc-m56fz
May  6 08:24:16.444: INFO: Got endpoints: latency-svc-pfl9v [751.383342ms]
May  6 08:24:16.455: INFO: Created: latency-svc-tbm5k
May  6 08:24:16.493: INFO: Got endpoints: latency-svc-2lc9h [749.193992ms]
May  6 08:24:16.505: INFO: Created: latency-svc-np9j7
May  6 08:24:16.543: INFO: Got endpoints: latency-svc-zkrqf [750.436223ms]
May  6 08:24:16.554: INFO: Created: latency-svc-8gw97
May  6 08:24:16.600: INFO: Got endpoints: latency-svc-ljmqf [756.77081ms]
May  6 08:24:16.612: INFO: Created: latency-svc-h5fld
May  6 08:24:16.646: INFO: Got endpoints: latency-svc-5phft [754.058396ms]
May  6 08:24:16.659: INFO: Created: latency-svc-2dtqg
May  6 08:24:16.693: INFO: Got endpoints: latency-svc-gvggb [750.262562ms]
May  6 08:24:16.705: INFO: Created: latency-svc-fl92b
May  6 08:24:16.744: INFO: Got endpoints: latency-svc-gqtdn [750.833385ms]
May  6 08:24:16.755: INFO: Created: latency-svc-r8skt
May  6 08:24:16.794: INFO: Got endpoints: latency-svc-n42zf [749.470827ms]
May  6 08:24:16.805: INFO: Created: latency-svc-hvjjk
May  6 08:24:16.842: INFO: Got endpoints: latency-svc-nf7qc [749.82013ms]
May  6 08:24:16.853: INFO: Created: latency-svc-ztdw2
May  6 08:24:16.893: INFO: Got endpoints: latency-svc-9mfzs [746.722293ms]
May  6 08:24:16.905: INFO: Created: latency-svc-xtb2c
May  6 08:24:16.944: INFO: Got endpoints: latency-svc-kzjnv [749.778932ms]
May  6 08:24:16.955: INFO: Created: latency-svc-7cpgx
May  6 08:24:16.996: INFO: Got endpoints: latency-svc-8n6zp [752.601767ms]
May  6 08:24:17.009: INFO: Created: latency-svc-zjt58
May  6 08:24:17.042: INFO: Got endpoints: latency-svc-xfxmw [749.397116ms]
May  6 08:24:17.054: INFO: Created: latency-svc-cvrcr
May  6 08:24:17.094: INFO: Got endpoints: latency-svc-zm8kp [748.201625ms]
May  6 08:24:17.105: INFO: Created: latency-svc-8d7j6
May  6 08:24:17.145: INFO: Got endpoints: latency-svc-m56fz [752.765389ms]
May  6 08:24:17.158: INFO: Created: latency-svc-4mkgc
May  6 08:24:17.193: INFO: Got endpoints: latency-svc-tbm5k [748.368693ms]
May  6 08:24:17.204: INFO: Created: latency-svc-nvn59
May  6 08:24:17.243: INFO: Got endpoints: latency-svc-np9j7 [749.924999ms]
May  6 08:24:17.254: INFO: Created: latency-svc-2nc9v
May  6 08:24:17.294: INFO: Got endpoints: latency-svc-8gw97 [750.987088ms]
May  6 08:24:17.307: INFO: Created: latency-svc-qsj8v
May  6 08:24:17.343: INFO: Got endpoints: latency-svc-h5fld [743.351036ms]
May  6 08:24:17.361: INFO: Created: latency-svc-hqmjb
May  6 08:24:17.394: INFO: Got endpoints: latency-svc-2dtqg [747.355096ms]
May  6 08:24:17.408: INFO: Created: latency-svc-cw4bj
May  6 08:24:17.443: INFO: Got endpoints: latency-svc-fl92b [749.792037ms]
May  6 08:24:17.454: INFO: Created: latency-svc-8snw7
May  6 08:24:17.492: INFO: Got endpoints: latency-svc-r8skt [748.386975ms]
May  6 08:24:17.503: INFO: Created: latency-svc-dltkw
May  6 08:24:17.544: INFO: Got endpoints: latency-svc-hvjjk [750.143314ms]
May  6 08:24:17.554: INFO: Created: latency-svc-bw68j
May  6 08:24:17.592: INFO: Got endpoints: latency-svc-ztdw2 [749.640809ms]
May  6 08:24:17.607: INFO: Created: latency-svc-d6bf2
May  6 08:24:17.645: INFO: Got endpoints: latency-svc-xtb2c [751.104471ms]
May  6 08:24:17.656: INFO: Created: latency-svc-67rbr
May  6 08:24:17.693: INFO: Got endpoints: latency-svc-7cpgx [748.827402ms]
May  6 08:24:17.706: INFO: Created: latency-svc-tlp2w
May  6 08:24:17.743: INFO: Got endpoints: latency-svc-zjt58 [746.98901ms]
May  6 08:24:17.755: INFO: Created: latency-svc-sr8g2
May  6 08:24:17.792: INFO: Got endpoints: latency-svc-cvrcr [749.314548ms]
May  6 08:24:17.803: INFO: Created: latency-svc-wjtbh
May  6 08:24:17.844: INFO: Got endpoints: latency-svc-8d7j6 [750.117354ms]
May  6 08:24:17.855: INFO: Created: latency-svc-n2tz2
May  6 08:24:17.893: INFO: Got endpoints: latency-svc-4mkgc [747.813295ms]
May  6 08:24:17.912: INFO: Created: latency-svc-mdnzr
May  6 08:24:17.942: INFO: Got endpoints: latency-svc-nvn59 [749.357238ms]
May  6 08:24:17.954: INFO: Created: latency-svc-w7bbd
May  6 08:24:17.993: INFO: Got endpoints: latency-svc-2nc9v [749.817144ms]
May  6 08:24:18.005: INFO: Created: latency-svc-qbwms
May  6 08:24:18.041: INFO: Got endpoints: latency-svc-qsj8v [747.478207ms]
May  6 08:24:18.053: INFO: Created: latency-svc-jg9f2
May  6 08:24:18.093: INFO: Got endpoints: latency-svc-hqmjb [749.582267ms]
May  6 08:24:18.104: INFO: Created: latency-svc-vkhwr
May  6 08:24:18.144: INFO: Got endpoints: latency-svc-cw4bj [749.649865ms]
May  6 08:24:18.155: INFO: Created: latency-svc-x6rpf
May  6 08:24:18.193: INFO: Got endpoints: latency-svc-8snw7 [750.812425ms]
May  6 08:24:18.207: INFO: Created: latency-svc-kfwnx
May  6 08:24:18.243: INFO: Got endpoints: latency-svc-dltkw [750.40902ms]
May  6 08:24:18.255: INFO: Created: latency-svc-4wbxk
May  6 08:24:18.294: INFO: Got endpoints: latency-svc-bw68j [749.733895ms]
May  6 08:24:18.307: INFO: Created: latency-svc-5vxhw
May  6 08:24:18.342: INFO: Got endpoints: latency-svc-d6bf2 [750.432224ms]
May  6 08:24:18.354: INFO: Created: latency-svc-rhzgl
May  6 08:24:18.392: INFO: Got endpoints: latency-svc-67rbr [747.122954ms]
May  6 08:24:18.404: INFO: Created: latency-svc-sswzn
May  6 08:24:18.444: INFO: Got endpoints: latency-svc-tlp2w [750.792918ms]
May  6 08:24:18.454: INFO: Created: latency-svc-g2fgb
May  6 08:24:18.492: INFO: Got endpoints: latency-svc-sr8g2 [749.224968ms]
May  6 08:24:18.514: INFO: Created: latency-svc-xz469
May  6 08:24:18.543: INFO: Got endpoints: latency-svc-wjtbh [751.128106ms]
May  6 08:24:18.554: INFO: Created: latency-svc-6rwnw
May  6 08:24:18.595: INFO: Got endpoints: latency-svc-n2tz2 [750.557361ms]
May  6 08:24:18.608: INFO: Created: latency-svc-vkzs9
May  6 08:24:18.646: INFO: Got endpoints: latency-svc-mdnzr [753.367082ms]
May  6 08:24:18.657: INFO: Created: latency-svc-rn98g
May  6 08:24:18.692: INFO: Got endpoints: latency-svc-w7bbd [750.45113ms]
May  6 08:24:18.705: INFO: Created: latency-svc-sxn4f
May  6 08:24:18.745: INFO: Got endpoints: latency-svc-qbwms [751.574455ms]
May  6 08:24:18.755: INFO: Created: latency-svc-qd7w8
May  6 08:24:18.793: INFO: Got endpoints: latency-svc-jg9f2 [752.060308ms]
May  6 08:24:18.805: INFO: Created: latency-svc-gtr77
May  6 08:24:18.844: INFO: Got endpoints: latency-svc-vkhwr [750.818959ms]
May  6 08:24:18.856: INFO: Created: latency-svc-2btl7
May  6 08:24:18.896: INFO: Got endpoints: latency-svc-x6rpf [752.454777ms]
May  6 08:24:18.908: INFO: Created: latency-svc-fr67n
May  6 08:24:18.944: INFO: Got endpoints: latency-svc-kfwnx [750.094652ms]
May  6 08:24:18.955: INFO: Created: latency-svc-5qcmj
May  6 08:24:18.993: INFO: Got endpoints: latency-svc-4wbxk [750.348104ms]
May  6 08:24:19.006: INFO: Created: latency-svc-jphbs
May  6 08:24:19.044: INFO: Got endpoints: latency-svc-5vxhw [750.336572ms]
May  6 08:24:19.058: INFO: Created: latency-svc-zjn2l
May  6 08:24:19.094: INFO: Got endpoints: latency-svc-rhzgl [751.403899ms]
May  6 08:24:19.106: INFO: Created: latency-svc-x8lls
May  6 08:24:19.142: INFO: Got endpoints: latency-svc-sswzn [750.064625ms]
May  6 08:24:19.154: INFO: Created: latency-svc-ptqts
May  6 08:24:19.195: INFO: Got endpoints: latency-svc-g2fgb [750.637524ms]
May  6 08:24:19.206: INFO: Created: latency-svc-8ltn2
May  6 08:24:19.244: INFO: Got endpoints: latency-svc-xz469 [751.75624ms]
May  6 08:24:19.257: INFO: Created: latency-svc-4lccp
May  6 08:24:19.293: INFO: Got endpoints: latency-svc-6rwnw [750.117566ms]
May  6 08:24:19.307: INFO: Created: latency-svc-4hgzn
May  6 08:24:19.343: INFO: Got endpoints: latency-svc-vkzs9 [748.18317ms]
May  6 08:24:19.354: INFO: Created: latency-svc-nb4cp
May  6 08:24:19.396: INFO: Got endpoints: latency-svc-rn98g [749.764223ms]
May  6 08:24:19.419: INFO: Created: latency-svc-jdbfm
May  6 08:24:19.443: INFO: Got endpoints: latency-svc-sxn4f [751.060337ms]
May  6 08:24:19.455: INFO: Created: latency-svc-8km5p
May  6 08:24:19.493: INFO: Got endpoints: latency-svc-qd7w8 [747.980785ms]
May  6 08:24:19.505: INFO: Created: latency-svc-dgdnz
May  6 08:24:19.545: INFO: Got endpoints: latency-svc-gtr77 [751.075826ms]
May  6 08:24:19.556: INFO: Created: latency-svc-2cd2j
May  6 08:24:19.595: INFO: Got endpoints: latency-svc-2btl7 [750.962952ms]
May  6 08:24:19.615: INFO: Created: latency-svc-6lp9z
May  6 08:24:19.646: INFO: Got endpoints: latency-svc-fr67n [749.786747ms]
May  6 08:24:19.672: INFO: Created: latency-svc-7dbrv
May  6 08:24:19.694: INFO: Got endpoints: latency-svc-5qcmj [749.99365ms]
May  6 08:24:19.705: INFO: Created: latency-svc-h9gvh
May  6 08:24:19.743: INFO: Got endpoints: latency-svc-jphbs [750.131761ms]
May  6 08:24:19.755: INFO: Created: latency-svc-dsrgl
May  6 08:24:19.792: INFO: Got endpoints: latency-svc-zjn2l [748.43145ms]
May  6 08:24:19.804: INFO: Created: latency-svc-tr62s
May  6 08:24:19.844: INFO: Got endpoints: latency-svc-x8lls [750.191183ms]
May  6 08:24:19.856: INFO: Created: latency-svc-hjv69
May  6 08:24:19.894: INFO: Got endpoints: latency-svc-ptqts [752.295242ms]
May  6 08:24:19.905: INFO: Created: latency-svc-lb27l
May  6 08:24:19.942: INFO: Got endpoints: latency-svc-8ltn2 [747.325939ms]
May  6 08:24:19.960: INFO: Created: latency-svc-db7ns
May  6 08:24:19.994: INFO: Got endpoints: latency-svc-4lccp [749.961528ms]
May  6 08:24:20.006: INFO: Created: latency-svc-h2pjr
May  6 08:24:20.044: INFO: Got endpoints: latency-svc-4hgzn [750.850487ms]
May  6 08:24:20.054: INFO: Created: latency-svc-nlgsg
May  6 08:24:20.095: INFO: Got endpoints: latency-svc-nb4cp [751.936292ms]
May  6 08:24:20.111: INFO: Created: latency-svc-pg8lt
May  6 08:24:20.142: INFO: Got endpoints: latency-svc-jdbfm [746.104589ms]
May  6 08:24:20.154: INFO: Created: latency-svc-4s62q
May  6 08:24:20.194: INFO: Got endpoints: latency-svc-8km5p [750.831543ms]
May  6 08:24:20.208: INFO: Created: latency-svc-snjqt
May  6 08:24:20.242: INFO: Got endpoints: latency-svc-dgdnz [749.295703ms]
May  6 08:24:20.254: INFO: Created: latency-svc-lpw8r
May  6 08:24:20.301: INFO: Got endpoints: latency-svc-2cd2j [756.667022ms]
May  6 08:24:20.312: INFO: Created: latency-svc-cn52c
May  6 08:24:20.344: INFO: Got endpoints: latency-svc-6lp9z [748.748885ms]
May  6 08:24:20.355: INFO: Created: latency-svc-p827r
May  6 08:24:20.396: INFO: Got endpoints: latency-svc-7dbrv [750.025752ms]
May  6 08:24:20.409: INFO: Created: latency-svc-2mf9c
May  6 08:24:20.442: INFO: Got endpoints: latency-svc-h9gvh [748.35612ms]
May  6 08:24:20.452: INFO: Created: latency-svc-x9n5n
May  6 08:24:20.494: INFO: Got endpoints: latency-svc-dsrgl [750.842956ms]
May  6 08:24:20.508: INFO: Created: latency-svc-wgq8k
May  6 08:24:20.543: INFO: Got endpoints: latency-svc-tr62s [750.126023ms]
May  6 08:24:20.554: INFO: Created: latency-svc-r5z8c
May  6 08:24:20.596: INFO: Got endpoints: latency-svc-hjv69 [752.17087ms]
May  6 08:24:20.624: INFO: Created: latency-svc-qb5rf
May  6 08:24:20.648: INFO: Got endpoints: latency-svc-lb27l [753.653618ms]
May  6 08:24:20.659: INFO: Created: latency-svc-7vpqp
May  6 08:24:20.695: INFO: Got endpoints: latency-svc-db7ns [752.636154ms]
May  6 08:24:20.706: INFO: Created: latency-svc-z9nwl
May  6 08:24:20.743: INFO: Got endpoints: latency-svc-h2pjr [748.358745ms]
May  6 08:24:20.753: INFO: Created: latency-svc-khn9s
May  6 08:24:20.793: INFO: Got endpoints: latency-svc-nlgsg [748.885686ms]
May  6 08:24:20.804: INFO: Created: latency-svc-6k9qs
May  6 08:24:20.842: INFO: Got endpoints: latency-svc-pg8lt [747.606084ms]
May  6 08:24:20.853: INFO: Created: latency-svc-2fmpx
May  6 08:24:20.893: INFO: Got endpoints: latency-svc-4s62q [750.716867ms]
May  6 08:24:20.905: INFO: Created: latency-svc-c54t2
May  6 08:24:20.944: INFO: Got endpoints: latency-svc-snjqt [750.024861ms]
May  6 08:24:20.956: INFO: Created: latency-svc-fht8c
May  6 08:24:20.992: INFO: Got endpoints: latency-svc-lpw8r [750.246953ms]
May  6 08:24:21.004: INFO: Created: latency-svc-k8kzg
May  6 08:24:21.045: INFO: Got endpoints: latency-svc-cn52c [743.60516ms]
May  6 08:24:21.057: INFO: Created: latency-svc-c7mb7
May  6 08:24:21.095: INFO: Got endpoints: latency-svc-p827r [751.151199ms]
May  6 08:24:21.105: INFO: Created: latency-svc-wk88j
May  6 08:24:21.142: INFO: Got endpoints: latency-svc-2mf9c [745.938424ms]
May  6 08:24:21.153: INFO: Created: latency-svc-rd278
May  6 08:24:21.191: INFO: Got endpoints: latency-svc-x9n5n [749.17698ms]
May  6 08:24:21.202: INFO: Created: latency-svc-tgwkg
May  6 08:24:21.243: INFO: Got endpoints: latency-svc-wgq8k [748.871468ms]
May  6 08:24:21.255: INFO: Created: latency-svc-b9pcb
May  6 08:24:21.294: INFO: Got endpoints: latency-svc-r5z8c [751.309401ms]
May  6 08:24:21.320: INFO: Created: latency-svc-lq27s
May  6 08:24:21.342: INFO: Got endpoints: latency-svc-qb5rf [745.580645ms]
May  6 08:24:21.354: INFO: Created: latency-svc-jfwk6
May  6 08:24:21.393: INFO: Got endpoints: latency-svc-7vpqp [745.087356ms]
May  6 08:24:21.405: INFO: Created: latency-svc-nc6fq
May  6 08:24:21.444: INFO: Got endpoints: latency-svc-z9nwl [748.837672ms]
May  6 08:24:21.494: INFO: Got endpoints: latency-svc-khn9s [751.659727ms]
May  6 08:24:21.544: INFO: Got endpoints: latency-svc-6k9qs [750.699051ms]
May  6 08:24:21.593: INFO: Got endpoints: latency-svc-2fmpx [750.143987ms]
May  6 08:24:21.644: INFO: Got endpoints: latency-svc-c54t2 [751.247362ms]
May  6 08:24:21.692: INFO: Got endpoints: latency-svc-fht8c [747.958562ms]
May  6 08:24:21.744: INFO: Got endpoints: latency-svc-k8kzg [751.785514ms]
May  6 08:24:21.792: INFO: Got endpoints: latency-svc-c7mb7 [747.47332ms]
May  6 08:24:21.844: INFO: Got endpoints: latency-svc-wk88j [749.754796ms]
May  6 08:24:21.892: INFO: Got endpoints: latency-svc-rd278 [749.797087ms]
May  6 08:24:21.944: INFO: Got endpoints: latency-svc-tgwkg [752.687319ms]
May  6 08:24:21.993: INFO: Got endpoints: latency-svc-b9pcb [749.56716ms]
May  6 08:24:22.044: INFO: Got endpoints: latency-svc-lq27s [750.17713ms]
May  6 08:24:22.092: INFO: Got endpoints: latency-svc-jfwk6 [750.126313ms]
May  6 08:24:22.143: INFO: Got endpoints: latency-svc-nc6fq [750.119379ms]
May  6 08:24:22.143: INFO: Latencies: [20.387724ms 28.69359ms 38.322459ms 49.054012ms 60.846723ms 76.064883ms 78.022073ms 94.651727ms 102.98278ms 116.725466ms 141.826588ms 149.615419ms 154.710274ms 155.107979ms 157.19282ms 158.193113ms 158.321896ms 159.751444ms 159.865861ms 162.473286ms 163.355492ms 163.596401ms 163.915487ms 165.549903ms 167.59087ms 169.019757ms 169.888837ms 170.024867ms 171.593548ms 174.825269ms 176.171638ms 176.374483ms 176.429658ms 178.857501ms 179.871289ms 179.990213ms 180.097189ms 184.937377ms 221.6947ms 261.555052ms 291.600229ms 332.721315ms 373.898648ms 414.242447ms 450.868772ms 497.525298ms 540.142619ms 567.519466ms 609.914162ms 649.541159ms 681.671509ms 729.995615ms 743.351036ms 743.60516ms 745.087356ms 745.096843ms 745.580645ms 745.938424ms 746.104589ms 746.722293ms 746.849054ms 746.98901ms 747.022402ms 747.122954ms 747.253782ms 747.325939ms 747.355096ms 747.47332ms 747.478207ms 747.4847ms 747.606084ms 747.775123ms 747.813295ms 747.884633ms 747.958562ms 747.980785ms 748.18317ms 748.201625ms 748.323255ms 748.35612ms 748.358745ms 748.368693ms 748.386975ms 748.43145ms 748.748885ms 748.827402ms 748.837672ms 748.871468ms 748.885686ms 748.973973ms 749.023686ms 749.023706ms 749.17698ms 749.193992ms 749.224968ms 749.28934ms 749.295703ms 749.314548ms 749.35667ms 749.357238ms 749.397116ms 749.470827ms 749.56716ms 749.582267ms 749.640809ms 749.648774ms 749.649865ms 749.661678ms 749.733895ms 749.754796ms 749.764223ms 749.778932ms 749.786747ms 749.792037ms 749.797087ms 749.817144ms 749.82013ms 749.829218ms 749.924999ms 749.936491ms 749.961528ms 749.97815ms 749.9835ms 749.99365ms 750.024861ms 750.025752ms 750.064625ms 750.094652ms 750.110291ms 750.117354ms 750.117566ms 750.119379ms 750.126023ms 750.126313ms 750.131761ms 750.143314ms 750.143987ms 750.17713ms 750.191183ms 750.246953ms 750.262562ms 750.336572ms 750.348104ms 750.37844ms 750.404371ms 750.40902ms 750.432224ms 750.436223ms 750.45113ms 750.557361ms 750.571197ms 750.637524ms 750.699051ms 750.716867ms 750.729268ms 750.792918ms 750.812425ms 750.818959ms 750.831543ms 750.833385ms 750.842956ms 750.850487ms 750.962952ms 750.987088ms 751.060337ms 751.075826ms 751.104471ms 751.128106ms 751.141931ms 751.151199ms 751.247362ms 751.309401ms 751.383342ms 751.403899ms 751.574455ms 751.659727ms 751.753523ms 751.75624ms 751.785514ms 751.872099ms 751.936292ms 752.060308ms 752.17087ms 752.237304ms 752.295242ms 752.363823ms 752.454777ms 752.601767ms 752.636154ms 752.687319ms 752.699704ms 752.753356ms 752.765389ms 753.334369ms 753.367082ms 753.653618ms 754.058396ms 754.761611ms 756.667022ms 756.77081ms]
May  6 08:24:22.143: INFO: 50 %ile: 749.397116ms
May  6 08:24:22.143: INFO: 90 %ile: 751.936292ms
May  6 08:24:22.143: INFO: 99 %ile: 756.667022ms
May  6 08:24:22.143: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
May  6 08:24:22.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-2819" for this suite. 05/06/23 08:24:22.148
------------------------------
â€¢ [SLOW TEST] [11.756 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:24:10.397
    May  6 08:24:10.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename svc-latency 05/06/23 08:24:10.398
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:24:11.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:24:11.414
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    May  6 08:24:11.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-2819 05/06/23 08:24:11.417
    I0506 08:24:11.423071      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2819, replica count: 1
    I0506 08:24:12.473813      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0506 08:24:13.474336      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  6 08:24:13.588: INFO: Created: latency-svc-n8sh5
    May  6 08:24:13.595: INFO: Got endpoints: latency-svc-n8sh5 [20.233833ms]
    May  6 08:24:13.609: INFO: Created: latency-svc-5s6t5
    May  6 08:24:13.615: INFO: Got endpoints: latency-svc-5s6t5 [20.387724ms]
    May  6 08:24:13.618: INFO: Created: latency-svc-9vrq8
    May  6 08:24:13.624: INFO: Got endpoints: latency-svc-9vrq8 [28.69359ms]
    May  6 08:24:13.628: INFO: Created: latency-svc-4xxmh
    May  6 08:24:13.634: INFO: Got endpoints: latency-svc-4xxmh [38.322459ms]
    May  6 08:24:13.637: INFO: Created: latency-svc-94w85
    May  6 08:24:13.645: INFO: Got endpoints: latency-svc-94w85 [49.054012ms]
    May  6 08:24:13.650: INFO: Created: latency-svc-nxcpj
    May  6 08:24:13.657: INFO: Got endpoints: latency-svc-nxcpj [60.846723ms]
    May  6 08:24:13.661: INFO: Created: latency-svc-fj6k2
    May  6 08:24:13.669: INFO: Created: latency-svc-wft56
    May  6 08:24:13.672: INFO: Got endpoints: latency-svc-fj6k2 [76.064883ms]
    May  6 08:24:13.674: INFO: Got endpoints: latency-svc-wft56 [78.022073ms]
    May  6 08:24:13.685: INFO: Created: latency-svc-gr9lk
    May  6 08:24:13.691: INFO: Got endpoints: latency-svc-gr9lk [94.651727ms]
    May  6 08:24:13.692: INFO: Created: latency-svc-8nvtx
    May  6 08:24:13.700: INFO: Got endpoints: latency-svc-8nvtx [102.98278ms]
    May  6 08:24:13.704: INFO: Created: latency-svc-7sfsb
    May  6 08:24:13.714: INFO: Got endpoints: latency-svc-7sfsb [116.725466ms]
    May  6 08:24:13.734: INFO: Created: latency-svc-qh2vt
    May  6 08:24:13.737: INFO: Created: latency-svc-zlzv8
    May  6 08:24:13.739: INFO: Got endpoints: latency-svc-qh2vt [141.826588ms]
    May  6 08:24:13.746: INFO: Got endpoints: latency-svc-zlzv8 [149.615419ms]
    May  6 08:24:13.747: INFO: Created: latency-svc-mhtlt
    May  6 08:24:13.754: INFO: Got endpoints: latency-svc-mhtlt [157.19282ms]
    May  6 08:24:13.759: INFO: Created: latency-svc-q85bf
    May  6 08:24:13.765: INFO: Got endpoints: latency-svc-q85bf [167.59087ms]
    May  6 08:24:13.765: INFO: Created: latency-svc-qtds2
    May  6 08:24:13.777: INFO: Got endpoints: latency-svc-qtds2 [179.990213ms]
    May  6 08:24:13.778: INFO: Created: latency-svc-s8r9b
    May  6 08:24:13.787: INFO: Got endpoints: latency-svc-s8r9b [171.593548ms]
    May  6 08:24:13.793: INFO: Created: latency-svc-6w68v
    May  6 08:24:13.799: INFO: Got endpoints: latency-svc-6w68v [174.825269ms]
    May  6 08:24:13.809: INFO: Created: latency-svc-4r4ts
    May  6 08:24:13.815: INFO: Got endpoints: latency-svc-4r4ts [180.097189ms]
    May  6 08:24:13.817: INFO: Created: latency-svc-vz8d7
    May  6 08:24:13.823: INFO: Got endpoints: latency-svc-vz8d7 [178.857501ms]
    May  6 08:24:13.832: INFO: Created: latency-svc-wj9s9
    May  6 08:24:13.835: INFO: Created: latency-svc-44grl
    May  6 08:24:13.836: INFO: Got endpoints: latency-svc-wj9s9 [179.871289ms]
    May  6 08:24:13.841: INFO: Got endpoints: latency-svc-44grl [169.019757ms]
    May  6 08:24:13.851: INFO: Created: latency-svc-bwl8g
    May  6 08:24:13.859: INFO: Created: latency-svc-f8tdq
    May  6 08:24:13.859: INFO: Got endpoints: latency-svc-bwl8g [184.937377ms]
    May  6 08:24:13.867: INFO: Got endpoints: latency-svc-f8tdq [176.374483ms]
    May  6 08:24:13.870: INFO: Created: latency-svc-8qvmx
    May  6 08:24:13.876: INFO: Got endpoints: latency-svc-8qvmx [176.171638ms]
    May  6 08:24:13.878: INFO: Created: latency-svc-gqmw6
    May  6 08:24:13.884: INFO: Got endpoints: latency-svc-gqmw6 [170.024867ms]
    May  6 08:24:13.894: INFO: Created: latency-svc-f2zph
    May  6 08:24:13.901: INFO: Got endpoints: latency-svc-f2zph [162.473286ms]
    May  6 08:24:13.904: INFO: Created: latency-svc-mxx5x
    May  6 08:24:13.910: INFO: Got endpoints: latency-svc-mxx5x [163.355492ms]
    May  6 08:24:13.913: INFO: Created: latency-svc-56f5h
    May  6 08:24:13.920: INFO: Got endpoints: latency-svc-56f5h [165.549903ms]
    May  6 08:24:13.921: INFO: Created: latency-svc-cwbnt
    May  6 08:24:13.929: INFO: Got endpoints: latency-svc-cwbnt [163.915487ms]
    May  6 08:24:13.936: INFO: Created: latency-svc-gn4kw
    May  6 08:24:13.941: INFO: Created: latency-svc-qdhmj
    May  6 08:24:13.941: INFO: Got endpoints: latency-svc-gn4kw [163.596401ms]
    May  6 08:24:13.945: INFO: Got endpoints: latency-svc-qdhmj [158.193113ms]
    May  6 08:24:13.949: INFO: Created: latency-svc-frmjc
    May  6 08:24:13.954: INFO: Got endpoints: latency-svc-frmjc [154.710274ms]
    May  6 08:24:13.969: INFO: Created: latency-svc-2f9m8
    May  6 08:24:13.974: INFO: Got endpoints: latency-svc-2f9m8 [159.751444ms]
    May  6 08:24:13.977: INFO: Created: latency-svc-z4cfl
    May  6 08:24:13.983: INFO: Got endpoints: latency-svc-z4cfl [159.865861ms]
    May  6 08:24:13.988: INFO: Created: latency-svc-bcvhh
    May  6 08:24:13.995: INFO: Got endpoints: latency-svc-bcvhh [158.321896ms]
    May  6 08:24:14.004: INFO: Created: latency-svc-f7r9f
    May  6 08:24:14.009: INFO: Created: latency-svc-5c9hg
    May  6 08:24:14.011: INFO: Got endpoints: latency-svc-f7r9f [169.888837ms]
    May  6 08:24:14.014: INFO: Got endpoints: latency-svc-5c9hg [155.107979ms]
    May  6 08:24:14.017: INFO: Created: latency-svc-bbf55
    May  6 08:24:14.022: INFO: Created: latency-svc-8zr7r
    May  6 08:24:14.032: INFO: Created: latency-svc-kzf6r
    May  6 08:24:14.038: INFO: Created: latency-svc-qw4bl
    May  6 08:24:14.044: INFO: Got endpoints: latency-svc-bbf55 [176.429658ms]
    May  6 08:24:14.046: INFO: Created: latency-svc-9h5cz
    May  6 08:24:14.053: INFO: Created: latency-svc-x27bm
    May  6 08:24:14.063: INFO: Created: latency-svc-vl8rk
    May  6 08:24:14.073: INFO: Created: latency-svc-q25xt
    May  6 08:24:14.083: INFO: Created: latency-svc-mgsqj
    May  6 08:24:14.089: INFO: Created: latency-svc-58l4n
    May  6 08:24:14.097: INFO: Got endpoints: latency-svc-8zr7r [221.6947ms]
    May  6 08:24:14.100: INFO: Created: latency-svc-nq47g
    May  6 08:24:14.120: INFO: Created: latency-svc-h7sg5
    May  6 08:24:14.126: INFO: Created: latency-svc-7gcgp
    May  6 08:24:14.131: INFO: Created: latency-svc-9qf7h
    May  6 08:24:14.140: INFO: Created: latency-svc-7jbns
    May  6 08:24:14.146: INFO: Got endpoints: latency-svc-kzf6r [261.555052ms]
    May  6 08:24:14.148: INFO: Created: latency-svc-l92qz
    May  6 08:24:14.157: INFO: Created: latency-svc-rx2pw
    May  6 08:24:14.167: INFO: Created: latency-svc-n7h2l
    May  6 08:24:14.193: INFO: Got endpoints: latency-svc-qw4bl [291.600229ms]
    May  6 08:24:14.207: INFO: Created: latency-svc-7rwcd
    May  6 08:24:14.243: INFO: Got endpoints: latency-svc-9h5cz [332.721315ms]
    May  6 08:24:14.254: INFO: Created: latency-svc-dfhln
    May  6 08:24:14.293: INFO: Got endpoints: latency-svc-x27bm [373.898648ms]
    May  6 08:24:14.315: INFO: Created: latency-svc-gcgj6
    May  6 08:24:14.343: INFO: Got endpoints: latency-svc-vl8rk [414.242447ms]
    May  6 08:24:14.354: INFO: Created: latency-svc-czqv5
    May  6 08:24:14.392: INFO: Got endpoints: latency-svc-q25xt [450.868772ms]
    May  6 08:24:14.404: INFO: Created: latency-svc-6jdxr
    May  6 08:24:14.443: INFO: Got endpoints: latency-svc-mgsqj [497.525298ms]
    May  6 08:24:14.455: INFO: Created: latency-svc-k8hrs
    May  6 08:24:14.494: INFO: Got endpoints: latency-svc-58l4n [540.142619ms]
    May  6 08:24:14.505: INFO: Created: latency-svc-2hj7k
    May  6 08:24:14.542: INFO: Got endpoints: latency-svc-nq47g [567.519466ms]
    May  6 08:24:14.558: INFO: Created: latency-svc-ktzwn
    May  6 08:24:14.593: INFO: Got endpoints: latency-svc-h7sg5 [609.914162ms]
    May  6 08:24:14.609: INFO: Created: latency-svc-qfc49
    May  6 08:24:14.644: INFO: Got endpoints: latency-svc-7gcgp [649.541159ms]
    May  6 08:24:14.664: INFO: Created: latency-svc-jxhg9
    May  6 08:24:14.693: INFO: Got endpoints: latency-svc-9qf7h [681.671509ms]
    May  6 08:24:14.704: INFO: Created: latency-svc-xm8zl
    May  6 08:24:14.744: INFO: Got endpoints: latency-svc-7jbns [729.995615ms]
    May  6 08:24:14.755: INFO: Created: latency-svc-6fzcj
    May  6 08:24:14.793: INFO: Got endpoints: latency-svc-l92qz [749.28934ms]
    May  6 08:24:14.806: INFO: Created: latency-svc-w2xwp
    May  6 08:24:14.843: INFO: Got endpoints: latency-svc-rx2pw [745.096843ms]
    May  6 08:24:14.854: INFO: Created: latency-svc-r4pq5
    May  6 08:24:14.893: INFO: Got endpoints: latency-svc-n7h2l [747.775123ms]
    May  6 08:24:14.904: INFO: Created: latency-svc-sc56d
    May  6 08:24:14.945: INFO: Got endpoints: latency-svc-7rwcd [752.363823ms]
    May  6 08:24:14.956: INFO: Created: latency-svc-tvrm6
    May  6 08:24:14.992: INFO: Got endpoints: latency-svc-dfhln [749.661678ms]
    May  6 08:24:15.004: INFO: Created: latency-svc-k4gb9
    May  6 08:24:15.044: INFO: Got endpoints: latency-svc-gcgj6 [750.571197ms]
    May  6 08:24:15.055: INFO: Created: latency-svc-j2nwv
    May  6 08:24:15.096: INFO: Got endpoints: latency-svc-czqv5 [753.334369ms]
    May  6 08:24:15.107: INFO: Created: latency-svc-2zmsm
    May  6 08:24:15.142: INFO: Got endpoints: latency-svc-6jdxr [750.404371ms]
    May  6 08:24:15.154: INFO: Created: latency-svc-x6ckn
    May  6 08:24:15.193: INFO: Got endpoints: latency-svc-k8hrs [749.829218ms]
    May  6 08:24:15.204: INFO: Created: latency-svc-zwm42
    May  6 08:24:15.244: INFO: Got endpoints: latency-svc-2hj7k [749.936491ms]
    May  6 08:24:15.255: INFO: Created: latency-svc-6lgkj
    May  6 08:24:15.292: INFO: Got endpoints: latency-svc-ktzwn [749.97815ms]
    May  6 08:24:15.303: INFO: Created: latency-svc-tfhpl
    May  6 08:24:15.344: INFO: Got endpoints: latency-svc-qfc49 [751.141931ms]
    May  6 08:24:15.354: INFO: Created: latency-svc-fnsn2
    May  6 08:24:15.392: INFO: Got endpoints: latency-svc-jxhg9 [747.4847ms]
    May  6 08:24:15.406: INFO: Created: latency-svc-jz6jn
    May  6 08:24:15.442: INFO: Got endpoints: latency-svc-xm8zl [749.023686ms]
    May  6 08:24:15.455: INFO: Created: latency-svc-sxzcv
    May  6 08:24:15.494: INFO: Got endpoints: latency-svc-6fzcj [749.9835ms]
    May  6 08:24:15.507: INFO: Created: latency-svc-q8zd9
    May  6 08:24:15.542: INFO: Got endpoints: latency-svc-w2xwp [749.023706ms]
    May  6 08:24:15.560: INFO: Created: latency-svc-hnxpc
    May  6 08:24:15.593: INFO: Got endpoints: latency-svc-r4pq5 [750.37844ms]
    May  6 08:24:15.606: INFO: Created: latency-svc-dvwsn
    May  6 08:24:15.645: INFO: Got endpoints: latency-svc-sc56d [751.753523ms]
    May  6 08:24:15.658: INFO: Created: latency-svc-xjdbx
    May  6 08:24:15.693: INFO: Got endpoints: latency-svc-tvrm6 [747.253782ms]
    May  6 08:24:15.704: INFO: Created: latency-svc-pfl9v
    May  6 08:24:15.744: INFO: Got endpoints: latency-svc-k4gb9 [751.872099ms]
    May  6 08:24:15.756: INFO: Created: latency-svc-2lc9h
    May  6 08:24:15.792: INFO: Got endpoints: latency-svc-j2nwv [748.323255ms]
    May  6 08:24:15.805: INFO: Created: latency-svc-zkrqf
    May  6 08:24:15.843: INFO: Got endpoints: latency-svc-2zmsm [747.022402ms]
    May  6 08:24:15.866: INFO: Created: latency-svc-ljmqf
    May  6 08:24:15.892: INFO: Got endpoints: latency-svc-x6ckn [750.110291ms]
    May  6 08:24:15.904: INFO: Created: latency-svc-5phft
    May  6 08:24:15.942: INFO: Got endpoints: latency-svc-zwm42 [749.648774ms]
    May  6 08:24:15.954: INFO: Created: latency-svc-gvggb
    May  6 08:24:15.993: INFO: Got endpoints: latency-svc-6lgkj [749.35667ms]
    May  6 08:24:16.025: INFO: Created: latency-svc-gqtdn
    May  6 08:24:16.044: INFO: Got endpoints: latency-svc-tfhpl [752.237304ms]
    May  6 08:24:16.055: INFO: Created: latency-svc-n42zf
    May  6 08:24:16.092: INFO: Got endpoints: latency-svc-fnsn2 [747.884633ms]
    May  6 08:24:16.107: INFO: Created: latency-svc-nf7qc
    May  6 08:24:16.147: INFO: Got endpoints: latency-svc-jz6jn [754.761611ms]
    May  6 08:24:16.157: INFO: Created: latency-svc-9mfzs
    May  6 08:24:16.194: INFO: Got endpoints: latency-svc-sxzcv [752.753356ms]
    May  6 08:24:16.205: INFO: Created: latency-svc-kzjnv
    May  6 08:24:16.243: INFO: Got endpoints: latency-svc-q8zd9 [748.973973ms]
    May  6 08:24:16.254: INFO: Created: latency-svc-8n6zp
    May  6 08:24:16.293: INFO: Got endpoints: latency-svc-hnxpc [750.729268ms]
    May  6 08:24:16.304: INFO: Created: latency-svc-xfxmw
    May  6 08:24:16.346: INFO: Got endpoints: latency-svc-dvwsn [752.699704ms]
    May  6 08:24:16.358: INFO: Created: latency-svc-zm8kp
    May  6 08:24:16.392: INFO: Got endpoints: latency-svc-xjdbx [746.849054ms]
    May  6 08:24:16.404: INFO: Created: latency-svc-m56fz
    May  6 08:24:16.444: INFO: Got endpoints: latency-svc-pfl9v [751.383342ms]
    May  6 08:24:16.455: INFO: Created: latency-svc-tbm5k
    May  6 08:24:16.493: INFO: Got endpoints: latency-svc-2lc9h [749.193992ms]
    May  6 08:24:16.505: INFO: Created: latency-svc-np9j7
    May  6 08:24:16.543: INFO: Got endpoints: latency-svc-zkrqf [750.436223ms]
    May  6 08:24:16.554: INFO: Created: latency-svc-8gw97
    May  6 08:24:16.600: INFO: Got endpoints: latency-svc-ljmqf [756.77081ms]
    May  6 08:24:16.612: INFO: Created: latency-svc-h5fld
    May  6 08:24:16.646: INFO: Got endpoints: latency-svc-5phft [754.058396ms]
    May  6 08:24:16.659: INFO: Created: latency-svc-2dtqg
    May  6 08:24:16.693: INFO: Got endpoints: latency-svc-gvggb [750.262562ms]
    May  6 08:24:16.705: INFO: Created: latency-svc-fl92b
    May  6 08:24:16.744: INFO: Got endpoints: latency-svc-gqtdn [750.833385ms]
    May  6 08:24:16.755: INFO: Created: latency-svc-r8skt
    May  6 08:24:16.794: INFO: Got endpoints: latency-svc-n42zf [749.470827ms]
    May  6 08:24:16.805: INFO: Created: latency-svc-hvjjk
    May  6 08:24:16.842: INFO: Got endpoints: latency-svc-nf7qc [749.82013ms]
    May  6 08:24:16.853: INFO: Created: latency-svc-ztdw2
    May  6 08:24:16.893: INFO: Got endpoints: latency-svc-9mfzs [746.722293ms]
    May  6 08:24:16.905: INFO: Created: latency-svc-xtb2c
    May  6 08:24:16.944: INFO: Got endpoints: latency-svc-kzjnv [749.778932ms]
    May  6 08:24:16.955: INFO: Created: latency-svc-7cpgx
    May  6 08:24:16.996: INFO: Got endpoints: latency-svc-8n6zp [752.601767ms]
    May  6 08:24:17.009: INFO: Created: latency-svc-zjt58
    May  6 08:24:17.042: INFO: Got endpoints: latency-svc-xfxmw [749.397116ms]
    May  6 08:24:17.054: INFO: Created: latency-svc-cvrcr
    May  6 08:24:17.094: INFO: Got endpoints: latency-svc-zm8kp [748.201625ms]
    May  6 08:24:17.105: INFO: Created: latency-svc-8d7j6
    May  6 08:24:17.145: INFO: Got endpoints: latency-svc-m56fz [752.765389ms]
    May  6 08:24:17.158: INFO: Created: latency-svc-4mkgc
    May  6 08:24:17.193: INFO: Got endpoints: latency-svc-tbm5k [748.368693ms]
    May  6 08:24:17.204: INFO: Created: latency-svc-nvn59
    May  6 08:24:17.243: INFO: Got endpoints: latency-svc-np9j7 [749.924999ms]
    May  6 08:24:17.254: INFO: Created: latency-svc-2nc9v
    May  6 08:24:17.294: INFO: Got endpoints: latency-svc-8gw97 [750.987088ms]
    May  6 08:24:17.307: INFO: Created: latency-svc-qsj8v
    May  6 08:24:17.343: INFO: Got endpoints: latency-svc-h5fld [743.351036ms]
    May  6 08:24:17.361: INFO: Created: latency-svc-hqmjb
    May  6 08:24:17.394: INFO: Got endpoints: latency-svc-2dtqg [747.355096ms]
    May  6 08:24:17.408: INFO: Created: latency-svc-cw4bj
    May  6 08:24:17.443: INFO: Got endpoints: latency-svc-fl92b [749.792037ms]
    May  6 08:24:17.454: INFO: Created: latency-svc-8snw7
    May  6 08:24:17.492: INFO: Got endpoints: latency-svc-r8skt [748.386975ms]
    May  6 08:24:17.503: INFO: Created: latency-svc-dltkw
    May  6 08:24:17.544: INFO: Got endpoints: latency-svc-hvjjk [750.143314ms]
    May  6 08:24:17.554: INFO: Created: latency-svc-bw68j
    May  6 08:24:17.592: INFO: Got endpoints: latency-svc-ztdw2 [749.640809ms]
    May  6 08:24:17.607: INFO: Created: latency-svc-d6bf2
    May  6 08:24:17.645: INFO: Got endpoints: latency-svc-xtb2c [751.104471ms]
    May  6 08:24:17.656: INFO: Created: latency-svc-67rbr
    May  6 08:24:17.693: INFO: Got endpoints: latency-svc-7cpgx [748.827402ms]
    May  6 08:24:17.706: INFO: Created: latency-svc-tlp2w
    May  6 08:24:17.743: INFO: Got endpoints: latency-svc-zjt58 [746.98901ms]
    May  6 08:24:17.755: INFO: Created: latency-svc-sr8g2
    May  6 08:24:17.792: INFO: Got endpoints: latency-svc-cvrcr [749.314548ms]
    May  6 08:24:17.803: INFO: Created: latency-svc-wjtbh
    May  6 08:24:17.844: INFO: Got endpoints: latency-svc-8d7j6 [750.117354ms]
    May  6 08:24:17.855: INFO: Created: latency-svc-n2tz2
    May  6 08:24:17.893: INFO: Got endpoints: latency-svc-4mkgc [747.813295ms]
    May  6 08:24:17.912: INFO: Created: latency-svc-mdnzr
    May  6 08:24:17.942: INFO: Got endpoints: latency-svc-nvn59 [749.357238ms]
    May  6 08:24:17.954: INFO: Created: latency-svc-w7bbd
    May  6 08:24:17.993: INFO: Got endpoints: latency-svc-2nc9v [749.817144ms]
    May  6 08:24:18.005: INFO: Created: latency-svc-qbwms
    May  6 08:24:18.041: INFO: Got endpoints: latency-svc-qsj8v [747.478207ms]
    May  6 08:24:18.053: INFO: Created: latency-svc-jg9f2
    May  6 08:24:18.093: INFO: Got endpoints: latency-svc-hqmjb [749.582267ms]
    May  6 08:24:18.104: INFO: Created: latency-svc-vkhwr
    May  6 08:24:18.144: INFO: Got endpoints: latency-svc-cw4bj [749.649865ms]
    May  6 08:24:18.155: INFO: Created: latency-svc-x6rpf
    May  6 08:24:18.193: INFO: Got endpoints: latency-svc-8snw7 [750.812425ms]
    May  6 08:24:18.207: INFO: Created: latency-svc-kfwnx
    May  6 08:24:18.243: INFO: Got endpoints: latency-svc-dltkw [750.40902ms]
    May  6 08:24:18.255: INFO: Created: latency-svc-4wbxk
    May  6 08:24:18.294: INFO: Got endpoints: latency-svc-bw68j [749.733895ms]
    May  6 08:24:18.307: INFO: Created: latency-svc-5vxhw
    May  6 08:24:18.342: INFO: Got endpoints: latency-svc-d6bf2 [750.432224ms]
    May  6 08:24:18.354: INFO: Created: latency-svc-rhzgl
    May  6 08:24:18.392: INFO: Got endpoints: latency-svc-67rbr [747.122954ms]
    May  6 08:24:18.404: INFO: Created: latency-svc-sswzn
    May  6 08:24:18.444: INFO: Got endpoints: latency-svc-tlp2w [750.792918ms]
    May  6 08:24:18.454: INFO: Created: latency-svc-g2fgb
    May  6 08:24:18.492: INFO: Got endpoints: latency-svc-sr8g2 [749.224968ms]
    May  6 08:24:18.514: INFO: Created: latency-svc-xz469
    May  6 08:24:18.543: INFO: Got endpoints: latency-svc-wjtbh [751.128106ms]
    May  6 08:24:18.554: INFO: Created: latency-svc-6rwnw
    May  6 08:24:18.595: INFO: Got endpoints: latency-svc-n2tz2 [750.557361ms]
    May  6 08:24:18.608: INFO: Created: latency-svc-vkzs9
    May  6 08:24:18.646: INFO: Got endpoints: latency-svc-mdnzr [753.367082ms]
    May  6 08:24:18.657: INFO: Created: latency-svc-rn98g
    May  6 08:24:18.692: INFO: Got endpoints: latency-svc-w7bbd [750.45113ms]
    May  6 08:24:18.705: INFO: Created: latency-svc-sxn4f
    May  6 08:24:18.745: INFO: Got endpoints: latency-svc-qbwms [751.574455ms]
    May  6 08:24:18.755: INFO: Created: latency-svc-qd7w8
    May  6 08:24:18.793: INFO: Got endpoints: latency-svc-jg9f2 [752.060308ms]
    May  6 08:24:18.805: INFO: Created: latency-svc-gtr77
    May  6 08:24:18.844: INFO: Got endpoints: latency-svc-vkhwr [750.818959ms]
    May  6 08:24:18.856: INFO: Created: latency-svc-2btl7
    May  6 08:24:18.896: INFO: Got endpoints: latency-svc-x6rpf [752.454777ms]
    May  6 08:24:18.908: INFO: Created: latency-svc-fr67n
    May  6 08:24:18.944: INFO: Got endpoints: latency-svc-kfwnx [750.094652ms]
    May  6 08:24:18.955: INFO: Created: latency-svc-5qcmj
    May  6 08:24:18.993: INFO: Got endpoints: latency-svc-4wbxk [750.348104ms]
    May  6 08:24:19.006: INFO: Created: latency-svc-jphbs
    May  6 08:24:19.044: INFO: Got endpoints: latency-svc-5vxhw [750.336572ms]
    May  6 08:24:19.058: INFO: Created: latency-svc-zjn2l
    May  6 08:24:19.094: INFO: Got endpoints: latency-svc-rhzgl [751.403899ms]
    May  6 08:24:19.106: INFO: Created: latency-svc-x8lls
    May  6 08:24:19.142: INFO: Got endpoints: latency-svc-sswzn [750.064625ms]
    May  6 08:24:19.154: INFO: Created: latency-svc-ptqts
    May  6 08:24:19.195: INFO: Got endpoints: latency-svc-g2fgb [750.637524ms]
    May  6 08:24:19.206: INFO: Created: latency-svc-8ltn2
    May  6 08:24:19.244: INFO: Got endpoints: latency-svc-xz469 [751.75624ms]
    May  6 08:24:19.257: INFO: Created: latency-svc-4lccp
    May  6 08:24:19.293: INFO: Got endpoints: latency-svc-6rwnw [750.117566ms]
    May  6 08:24:19.307: INFO: Created: latency-svc-4hgzn
    May  6 08:24:19.343: INFO: Got endpoints: latency-svc-vkzs9 [748.18317ms]
    May  6 08:24:19.354: INFO: Created: latency-svc-nb4cp
    May  6 08:24:19.396: INFO: Got endpoints: latency-svc-rn98g [749.764223ms]
    May  6 08:24:19.419: INFO: Created: latency-svc-jdbfm
    May  6 08:24:19.443: INFO: Got endpoints: latency-svc-sxn4f [751.060337ms]
    May  6 08:24:19.455: INFO: Created: latency-svc-8km5p
    May  6 08:24:19.493: INFO: Got endpoints: latency-svc-qd7w8 [747.980785ms]
    May  6 08:24:19.505: INFO: Created: latency-svc-dgdnz
    May  6 08:24:19.545: INFO: Got endpoints: latency-svc-gtr77 [751.075826ms]
    May  6 08:24:19.556: INFO: Created: latency-svc-2cd2j
    May  6 08:24:19.595: INFO: Got endpoints: latency-svc-2btl7 [750.962952ms]
    May  6 08:24:19.615: INFO: Created: latency-svc-6lp9z
    May  6 08:24:19.646: INFO: Got endpoints: latency-svc-fr67n [749.786747ms]
    May  6 08:24:19.672: INFO: Created: latency-svc-7dbrv
    May  6 08:24:19.694: INFO: Got endpoints: latency-svc-5qcmj [749.99365ms]
    May  6 08:24:19.705: INFO: Created: latency-svc-h9gvh
    May  6 08:24:19.743: INFO: Got endpoints: latency-svc-jphbs [750.131761ms]
    May  6 08:24:19.755: INFO: Created: latency-svc-dsrgl
    May  6 08:24:19.792: INFO: Got endpoints: latency-svc-zjn2l [748.43145ms]
    May  6 08:24:19.804: INFO: Created: latency-svc-tr62s
    May  6 08:24:19.844: INFO: Got endpoints: latency-svc-x8lls [750.191183ms]
    May  6 08:24:19.856: INFO: Created: latency-svc-hjv69
    May  6 08:24:19.894: INFO: Got endpoints: latency-svc-ptqts [752.295242ms]
    May  6 08:24:19.905: INFO: Created: latency-svc-lb27l
    May  6 08:24:19.942: INFO: Got endpoints: latency-svc-8ltn2 [747.325939ms]
    May  6 08:24:19.960: INFO: Created: latency-svc-db7ns
    May  6 08:24:19.994: INFO: Got endpoints: latency-svc-4lccp [749.961528ms]
    May  6 08:24:20.006: INFO: Created: latency-svc-h2pjr
    May  6 08:24:20.044: INFO: Got endpoints: latency-svc-4hgzn [750.850487ms]
    May  6 08:24:20.054: INFO: Created: latency-svc-nlgsg
    May  6 08:24:20.095: INFO: Got endpoints: latency-svc-nb4cp [751.936292ms]
    May  6 08:24:20.111: INFO: Created: latency-svc-pg8lt
    May  6 08:24:20.142: INFO: Got endpoints: latency-svc-jdbfm [746.104589ms]
    May  6 08:24:20.154: INFO: Created: latency-svc-4s62q
    May  6 08:24:20.194: INFO: Got endpoints: latency-svc-8km5p [750.831543ms]
    May  6 08:24:20.208: INFO: Created: latency-svc-snjqt
    May  6 08:24:20.242: INFO: Got endpoints: latency-svc-dgdnz [749.295703ms]
    May  6 08:24:20.254: INFO: Created: latency-svc-lpw8r
    May  6 08:24:20.301: INFO: Got endpoints: latency-svc-2cd2j [756.667022ms]
    May  6 08:24:20.312: INFO: Created: latency-svc-cn52c
    May  6 08:24:20.344: INFO: Got endpoints: latency-svc-6lp9z [748.748885ms]
    May  6 08:24:20.355: INFO: Created: latency-svc-p827r
    May  6 08:24:20.396: INFO: Got endpoints: latency-svc-7dbrv [750.025752ms]
    May  6 08:24:20.409: INFO: Created: latency-svc-2mf9c
    May  6 08:24:20.442: INFO: Got endpoints: latency-svc-h9gvh [748.35612ms]
    May  6 08:24:20.452: INFO: Created: latency-svc-x9n5n
    May  6 08:24:20.494: INFO: Got endpoints: latency-svc-dsrgl [750.842956ms]
    May  6 08:24:20.508: INFO: Created: latency-svc-wgq8k
    May  6 08:24:20.543: INFO: Got endpoints: latency-svc-tr62s [750.126023ms]
    May  6 08:24:20.554: INFO: Created: latency-svc-r5z8c
    May  6 08:24:20.596: INFO: Got endpoints: latency-svc-hjv69 [752.17087ms]
    May  6 08:24:20.624: INFO: Created: latency-svc-qb5rf
    May  6 08:24:20.648: INFO: Got endpoints: latency-svc-lb27l [753.653618ms]
    May  6 08:24:20.659: INFO: Created: latency-svc-7vpqp
    May  6 08:24:20.695: INFO: Got endpoints: latency-svc-db7ns [752.636154ms]
    May  6 08:24:20.706: INFO: Created: latency-svc-z9nwl
    May  6 08:24:20.743: INFO: Got endpoints: latency-svc-h2pjr [748.358745ms]
    May  6 08:24:20.753: INFO: Created: latency-svc-khn9s
    May  6 08:24:20.793: INFO: Got endpoints: latency-svc-nlgsg [748.885686ms]
    May  6 08:24:20.804: INFO: Created: latency-svc-6k9qs
    May  6 08:24:20.842: INFO: Got endpoints: latency-svc-pg8lt [747.606084ms]
    May  6 08:24:20.853: INFO: Created: latency-svc-2fmpx
    May  6 08:24:20.893: INFO: Got endpoints: latency-svc-4s62q [750.716867ms]
    May  6 08:24:20.905: INFO: Created: latency-svc-c54t2
    May  6 08:24:20.944: INFO: Got endpoints: latency-svc-snjqt [750.024861ms]
    May  6 08:24:20.956: INFO: Created: latency-svc-fht8c
    May  6 08:24:20.992: INFO: Got endpoints: latency-svc-lpw8r [750.246953ms]
    May  6 08:24:21.004: INFO: Created: latency-svc-k8kzg
    May  6 08:24:21.045: INFO: Got endpoints: latency-svc-cn52c [743.60516ms]
    May  6 08:24:21.057: INFO: Created: latency-svc-c7mb7
    May  6 08:24:21.095: INFO: Got endpoints: latency-svc-p827r [751.151199ms]
    May  6 08:24:21.105: INFO: Created: latency-svc-wk88j
    May  6 08:24:21.142: INFO: Got endpoints: latency-svc-2mf9c [745.938424ms]
    May  6 08:24:21.153: INFO: Created: latency-svc-rd278
    May  6 08:24:21.191: INFO: Got endpoints: latency-svc-x9n5n [749.17698ms]
    May  6 08:24:21.202: INFO: Created: latency-svc-tgwkg
    May  6 08:24:21.243: INFO: Got endpoints: latency-svc-wgq8k [748.871468ms]
    May  6 08:24:21.255: INFO: Created: latency-svc-b9pcb
    May  6 08:24:21.294: INFO: Got endpoints: latency-svc-r5z8c [751.309401ms]
    May  6 08:24:21.320: INFO: Created: latency-svc-lq27s
    May  6 08:24:21.342: INFO: Got endpoints: latency-svc-qb5rf [745.580645ms]
    May  6 08:24:21.354: INFO: Created: latency-svc-jfwk6
    May  6 08:24:21.393: INFO: Got endpoints: latency-svc-7vpqp [745.087356ms]
    May  6 08:24:21.405: INFO: Created: latency-svc-nc6fq
    May  6 08:24:21.444: INFO: Got endpoints: latency-svc-z9nwl [748.837672ms]
    May  6 08:24:21.494: INFO: Got endpoints: latency-svc-khn9s [751.659727ms]
    May  6 08:24:21.544: INFO: Got endpoints: latency-svc-6k9qs [750.699051ms]
    May  6 08:24:21.593: INFO: Got endpoints: latency-svc-2fmpx [750.143987ms]
    May  6 08:24:21.644: INFO: Got endpoints: latency-svc-c54t2 [751.247362ms]
    May  6 08:24:21.692: INFO: Got endpoints: latency-svc-fht8c [747.958562ms]
    May  6 08:24:21.744: INFO: Got endpoints: latency-svc-k8kzg [751.785514ms]
    May  6 08:24:21.792: INFO: Got endpoints: latency-svc-c7mb7 [747.47332ms]
    May  6 08:24:21.844: INFO: Got endpoints: latency-svc-wk88j [749.754796ms]
    May  6 08:24:21.892: INFO: Got endpoints: latency-svc-rd278 [749.797087ms]
    May  6 08:24:21.944: INFO: Got endpoints: latency-svc-tgwkg [752.687319ms]
    May  6 08:24:21.993: INFO: Got endpoints: latency-svc-b9pcb [749.56716ms]
    May  6 08:24:22.044: INFO: Got endpoints: latency-svc-lq27s [750.17713ms]
    May  6 08:24:22.092: INFO: Got endpoints: latency-svc-jfwk6 [750.126313ms]
    May  6 08:24:22.143: INFO: Got endpoints: latency-svc-nc6fq [750.119379ms]
    May  6 08:24:22.143: INFO: Latencies: [20.387724ms 28.69359ms 38.322459ms 49.054012ms 60.846723ms 76.064883ms 78.022073ms 94.651727ms 102.98278ms 116.725466ms 141.826588ms 149.615419ms 154.710274ms 155.107979ms 157.19282ms 158.193113ms 158.321896ms 159.751444ms 159.865861ms 162.473286ms 163.355492ms 163.596401ms 163.915487ms 165.549903ms 167.59087ms 169.019757ms 169.888837ms 170.024867ms 171.593548ms 174.825269ms 176.171638ms 176.374483ms 176.429658ms 178.857501ms 179.871289ms 179.990213ms 180.097189ms 184.937377ms 221.6947ms 261.555052ms 291.600229ms 332.721315ms 373.898648ms 414.242447ms 450.868772ms 497.525298ms 540.142619ms 567.519466ms 609.914162ms 649.541159ms 681.671509ms 729.995615ms 743.351036ms 743.60516ms 745.087356ms 745.096843ms 745.580645ms 745.938424ms 746.104589ms 746.722293ms 746.849054ms 746.98901ms 747.022402ms 747.122954ms 747.253782ms 747.325939ms 747.355096ms 747.47332ms 747.478207ms 747.4847ms 747.606084ms 747.775123ms 747.813295ms 747.884633ms 747.958562ms 747.980785ms 748.18317ms 748.201625ms 748.323255ms 748.35612ms 748.358745ms 748.368693ms 748.386975ms 748.43145ms 748.748885ms 748.827402ms 748.837672ms 748.871468ms 748.885686ms 748.973973ms 749.023686ms 749.023706ms 749.17698ms 749.193992ms 749.224968ms 749.28934ms 749.295703ms 749.314548ms 749.35667ms 749.357238ms 749.397116ms 749.470827ms 749.56716ms 749.582267ms 749.640809ms 749.648774ms 749.649865ms 749.661678ms 749.733895ms 749.754796ms 749.764223ms 749.778932ms 749.786747ms 749.792037ms 749.797087ms 749.817144ms 749.82013ms 749.829218ms 749.924999ms 749.936491ms 749.961528ms 749.97815ms 749.9835ms 749.99365ms 750.024861ms 750.025752ms 750.064625ms 750.094652ms 750.110291ms 750.117354ms 750.117566ms 750.119379ms 750.126023ms 750.126313ms 750.131761ms 750.143314ms 750.143987ms 750.17713ms 750.191183ms 750.246953ms 750.262562ms 750.336572ms 750.348104ms 750.37844ms 750.404371ms 750.40902ms 750.432224ms 750.436223ms 750.45113ms 750.557361ms 750.571197ms 750.637524ms 750.699051ms 750.716867ms 750.729268ms 750.792918ms 750.812425ms 750.818959ms 750.831543ms 750.833385ms 750.842956ms 750.850487ms 750.962952ms 750.987088ms 751.060337ms 751.075826ms 751.104471ms 751.128106ms 751.141931ms 751.151199ms 751.247362ms 751.309401ms 751.383342ms 751.403899ms 751.574455ms 751.659727ms 751.753523ms 751.75624ms 751.785514ms 751.872099ms 751.936292ms 752.060308ms 752.17087ms 752.237304ms 752.295242ms 752.363823ms 752.454777ms 752.601767ms 752.636154ms 752.687319ms 752.699704ms 752.753356ms 752.765389ms 753.334369ms 753.367082ms 753.653618ms 754.058396ms 754.761611ms 756.667022ms 756.77081ms]
    May  6 08:24:22.143: INFO: 50 %ile: 749.397116ms
    May  6 08:24:22.143: INFO: 90 %ile: 751.936292ms
    May  6 08:24:22.143: INFO: 99 %ile: 756.667022ms
    May  6 08:24:22.143: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    May  6 08:24:22.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-2819" for this suite. 05/06/23 08:24:22.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:24:22.165
May  6 08:24:22.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename namespaces 05/06/23 08:24:22.166
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:24:23.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:24:23.186
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-7567" 05/06/23 08:24:23.188
May  6 08:24:24.196: INFO: Namespace "namespaces-7567" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"a676f9dd-4d35-458a-89a0-5f79d2a64d54", "kubernetes.io/metadata.name":"namespaces-7567", "namespaces-7567":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 08:24:24.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7567" for this suite. 05/06/23 08:24:24.2
------------------------------
â€¢ [2.041 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:24:22.165
    May  6 08:24:22.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename namespaces 05/06/23 08:24:22.166
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:24:23.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:24:23.186
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-7567" 05/06/23 08:24:23.188
    May  6 08:24:24.196: INFO: Namespace "namespaces-7567" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"a676f9dd-4d35-458a-89a0-5f79d2a64d54", "kubernetes.io/metadata.name":"namespaces-7567", "namespaces-7567":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 08:24:24.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7567" for this suite. 05/06/23 08:24:24.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:24:24.207
May  6 08:24:24.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename sched-preemption 05/06/23 08:24:24.208
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:24:25.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:24:25.231
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
May  6 08:24:25.243: INFO: Waiting up to 1m0s for all nodes to be ready
May  6 08:25:25.266: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:25:25.269
May  6 08:25:25.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename sched-preemption-path 05/06/23 08:25:25.271
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:25:26.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:25:26.288
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
May  6 08:25:26.307: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
May  6 08:25:26.309: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
May  6 08:25:26.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 08:25:26.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-1389" for this suite. 05/06/23 08:25:26.396
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5875" for this suite. 05/06/23 08:25:26.402
------------------------------
â€¢ [SLOW TEST] [62.200 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:24:24.207
    May  6 08:24:24.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename sched-preemption 05/06/23 08:24:24.208
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:24:25.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:24:25.231
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    May  6 08:24:25.243: INFO: Waiting up to 1m0s for all nodes to be ready
    May  6 08:25:25.266: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:25:25.269
    May  6 08:25:25.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename sched-preemption-path 05/06/23 08:25:25.271
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:25:26.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:25:26.288
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    May  6 08:25:26.307: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    May  6 08:25:26.309: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    May  6 08:25:26.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 08:25:26.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-1389" for this suite. 05/06/23 08:25:26.396
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5875" for this suite. 05/06/23 08:25:26.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:25:26.408
May  6 08:25:26.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubectl 05/06/23 08:25:26.409
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:25:27.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:25:27.429
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 05/06/23 08:25:27.431
May  6 08:25:27.431: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-420 proxy --unix-socket=/tmp/kubectl-proxy-unix4061202949/test'
STEP: retrieving proxy /api/ output 05/06/23 08:25:27.468
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  6 08:25:27.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-420" for this suite. 05/06/23 08:25:27.472
------------------------------
â€¢ [1.069 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:25:26.408
    May  6 08:25:26.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubectl 05/06/23 08:25:26.409
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:25:27.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:25:27.429
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 05/06/23 08:25:27.431
    May  6 08:25:27.431: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-420 proxy --unix-socket=/tmp/kubectl-proxy-unix4061202949/test'
    STEP: retrieving proxy /api/ output 05/06/23 08:25:27.468
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  6 08:25:27.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-420" for this suite. 05/06/23 08:25:27.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:25:27.478
May  6 08:25:27.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename events 05/06/23 08:25:27.479
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:25:28.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:25:28.498
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 05/06/23 08:25:28.5
STEP: get a list of Events with a label in the current namespace 05/06/23 08:25:28.513
STEP: delete a list of events 05/06/23 08:25:28.516
May  6 08:25:28.516: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 05/06/23 08:25:28.535
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
May  6 08:25:28.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6096" for this suite. 05/06/23 08:25:28.54
------------------------------
â€¢ [1.067 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:25:27.478
    May  6 08:25:27.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename events 05/06/23 08:25:27.479
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:25:28.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:25:28.498
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 05/06/23 08:25:28.5
    STEP: get a list of Events with a label in the current namespace 05/06/23 08:25:28.513
    STEP: delete a list of events 05/06/23 08:25:28.516
    May  6 08:25:28.516: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 05/06/23 08:25:28.535
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    May  6 08:25:28.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6096" for this suite. 05/06/23 08:25:28.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:25:28.545
May  6 08:25:28.545: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename configmap 05/06/23 08:25:28.546
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:25:29.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:25:29.562
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-8523cd77-4bb0-436c-8d16-afdf5715ce7c 05/06/23 08:25:29.564
STEP: Creating a pod to test consume configMaps 05/06/23 08:25:29.568
May  6 08:25:29.575: INFO: Waiting up to 5m0s for pod "pod-configmaps-97ec4fd2-85e9-41e6-9311-0c259ff3d74c" in namespace "configmap-3393" to be "Succeeded or Failed"
May  6 08:25:29.578: INFO: Pod "pod-configmaps-97ec4fd2-85e9-41e6-9311-0c259ff3d74c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.464634ms
May  6 08:25:31.581: INFO: Pod "pod-configmaps-97ec4fd2-85e9-41e6-9311-0c259ff3d74c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005964556s
STEP: Saw pod success 05/06/23 08:25:31.581
May  6 08:25:31.581: INFO: Pod "pod-configmaps-97ec4fd2-85e9-41e6-9311-0c259ff3d74c" satisfied condition "Succeeded or Failed"
May  6 08:25:31.584: INFO: Trying to get logs from node cncf-0 pod pod-configmaps-97ec4fd2-85e9-41e6-9311-0c259ff3d74c container configmap-volume-test: <nil>
STEP: delete the pod 05/06/23 08:25:31.595
May  6 08:25:31.605: INFO: Waiting for pod pod-configmaps-97ec4fd2-85e9-41e6-9311-0c259ff3d74c to disappear
May  6 08:25:31.608: INFO: Pod pod-configmaps-97ec4fd2-85e9-41e6-9311-0c259ff3d74c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  6 08:25:31.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3393" for this suite. 05/06/23 08:25:31.611
------------------------------
â€¢ [3.071 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:25:28.545
    May  6 08:25:28.545: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename configmap 05/06/23 08:25:28.546
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:25:29.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:25:29.562
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-8523cd77-4bb0-436c-8d16-afdf5715ce7c 05/06/23 08:25:29.564
    STEP: Creating a pod to test consume configMaps 05/06/23 08:25:29.568
    May  6 08:25:29.575: INFO: Waiting up to 5m0s for pod "pod-configmaps-97ec4fd2-85e9-41e6-9311-0c259ff3d74c" in namespace "configmap-3393" to be "Succeeded or Failed"
    May  6 08:25:29.578: INFO: Pod "pod-configmaps-97ec4fd2-85e9-41e6-9311-0c259ff3d74c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.464634ms
    May  6 08:25:31.581: INFO: Pod "pod-configmaps-97ec4fd2-85e9-41e6-9311-0c259ff3d74c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005964556s
    STEP: Saw pod success 05/06/23 08:25:31.581
    May  6 08:25:31.581: INFO: Pod "pod-configmaps-97ec4fd2-85e9-41e6-9311-0c259ff3d74c" satisfied condition "Succeeded or Failed"
    May  6 08:25:31.584: INFO: Trying to get logs from node cncf-0 pod pod-configmaps-97ec4fd2-85e9-41e6-9311-0c259ff3d74c container configmap-volume-test: <nil>
    STEP: delete the pod 05/06/23 08:25:31.595
    May  6 08:25:31.605: INFO: Waiting for pod pod-configmaps-97ec4fd2-85e9-41e6-9311-0c259ff3d74c to disappear
    May  6 08:25:31.608: INFO: Pod pod-configmaps-97ec4fd2-85e9-41e6-9311-0c259ff3d74c no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  6 08:25:31.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3393" for this suite. 05/06/23 08:25:31.611
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:25:31.617
May  6 08:25:31.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename secrets 05/06/23 08:25:31.618
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:25:32.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:25:32.636
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-9578c84a-001d-4b9c-97b5-bf93b4f3aa4d 05/06/23 08:25:32.638
STEP: Creating a pod to test consume secrets 05/06/23 08:25:32.642
May  6 08:25:32.650: INFO: Waiting up to 5m0s for pod "pod-secrets-92977cab-c14e-4930-9bef-b1a8f8a3cc42" in namespace "secrets-9528" to be "Succeeded or Failed"
May  6 08:25:32.652: INFO: Pod "pod-secrets-92977cab-c14e-4930-9bef-b1a8f8a3cc42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185974ms
May  6 08:25:34.655: INFO: Pod "pod-secrets-92977cab-c14e-4930-9bef-b1a8f8a3cc42": Phase="Running", Reason="", readiness=false. Elapsed: 2.005494664s
May  6 08:25:36.656: INFO: Pod "pod-secrets-92977cab-c14e-4930-9bef-b1a8f8a3cc42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005987653s
STEP: Saw pod success 05/06/23 08:25:36.656
May  6 08:25:36.656: INFO: Pod "pod-secrets-92977cab-c14e-4930-9bef-b1a8f8a3cc42" satisfied condition "Succeeded or Failed"
May  6 08:25:36.659: INFO: Trying to get logs from node cncf-0 pod pod-secrets-92977cab-c14e-4930-9bef-b1a8f8a3cc42 container secret-volume-test: <nil>
STEP: delete the pod 05/06/23 08:25:36.665
May  6 08:25:36.679: INFO: Waiting for pod pod-secrets-92977cab-c14e-4930-9bef-b1a8f8a3cc42 to disappear
May  6 08:25:36.683: INFO: Pod pod-secrets-92977cab-c14e-4930-9bef-b1a8f8a3cc42 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May  6 08:25:36.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9528" for this suite. 05/06/23 08:25:36.686
------------------------------
â€¢ [SLOW TEST] [5.074 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:25:31.617
    May  6 08:25:31.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename secrets 05/06/23 08:25:31.618
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:25:32.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:25:32.636
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-9578c84a-001d-4b9c-97b5-bf93b4f3aa4d 05/06/23 08:25:32.638
    STEP: Creating a pod to test consume secrets 05/06/23 08:25:32.642
    May  6 08:25:32.650: INFO: Waiting up to 5m0s for pod "pod-secrets-92977cab-c14e-4930-9bef-b1a8f8a3cc42" in namespace "secrets-9528" to be "Succeeded or Failed"
    May  6 08:25:32.652: INFO: Pod "pod-secrets-92977cab-c14e-4930-9bef-b1a8f8a3cc42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185974ms
    May  6 08:25:34.655: INFO: Pod "pod-secrets-92977cab-c14e-4930-9bef-b1a8f8a3cc42": Phase="Running", Reason="", readiness=false. Elapsed: 2.005494664s
    May  6 08:25:36.656: INFO: Pod "pod-secrets-92977cab-c14e-4930-9bef-b1a8f8a3cc42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005987653s
    STEP: Saw pod success 05/06/23 08:25:36.656
    May  6 08:25:36.656: INFO: Pod "pod-secrets-92977cab-c14e-4930-9bef-b1a8f8a3cc42" satisfied condition "Succeeded or Failed"
    May  6 08:25:36.659: INFO: Trying to get logs from node cncf-0 pod pod-secrets-92977cab-c14e-4930-9bef-b1a8f8a3cc42 container secret-volume-test: <nil>
    STEP: delete the pod 05/06/23 08:25:36.665
    May  6 08:25:36.679: INFO: Waiting for pod pod-secrets-92977cab-c14e-4930-9bef-b1a8f8a3cc42 to disappear
    May  6 08:25:36.683: INFO: Pod pod-secrets-92977cab-c14e-4930-9bef-b1a8f8a3cc42 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May  6 08:25:36.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9528" for this suite. 05/06/23 08:25:36.686
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:25:36.691
May  6 08:25:36.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename webhook 05/06/23 08:25:36.692
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:25:37.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:25:37.711
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/06/23 08:25:39.727
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 08:25:39.982
STEP: Deploying the webhook pod 05/06/23 08:25:39.99
STEP: Wait for the deployment to be ready 05/06/23 08:25:40
May  6 08:25:40.005: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/06/23 08:25:42.013
STEP: Verifying the service has paired with the endpoint 05/06/23 08:25:42.024
May  6 08:25:43.025: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 05/06/23 08:25:43.074
STEP: Creating a configMap that should be mutated 05/06/23 08:25:44.115
STEP: Deleting the collection of validation webhooks 05/06/23 08:25:45.155
STEP: Creating a configMap that should not be mutated 05/06/23 08:25:45.193
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 08:25:45.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6987" for this suite. 05/06/23 08:25:45.242
STEP: Destroying namespace "webhook-6987-markers" for this suite. 05/06/23 08:25:45.249
------------------------------
â€¢ [SLOW TEST] [8.566 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:25:36.691
    May  6 08:25:36.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename webhook 05/06/23 08:25:36.692
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:25:37.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:25:37.711
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/06/23 08:25:39.727
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 08:25:39.982
    STEP: Deploying the webhook pod 05/06/23 08:25:39.99
    STEP: Wait for the deployment to be ready 05/06/23 08:25:40
    May  6 08:25:40.005: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/06/23 08:25:42.013
    STEP: Verifying the service has paired with the endpoint 05/06/23 08:25:42.024
    May  6 08:25:43.025: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 05/06/23 08:25:43.074
    STEP: Creating a configMap that should be mutated 05/06/23 08:25:44.115
    STEP: Deleting the collection of validation webhooks 05/06/23 08:25:45.155
    STEP: Creating a configMap that should not be mutated 05/06/23 08:25:45.193
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 08:25:45.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6987" for this suite. 05/06/23 08:25:45.242
    STEP: Destroying namespace "webhook-6987-markers" for this suite. 05/06/23 08:25:45.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:25:45.257
May  6 08:25:45.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename job 05/06/23 08:25:45.258
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:25:46.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:25:46.276
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 05/06/23 08:25:46.277
STEP: Ensuring job reaches completions 05/06/23 08:25:46.283
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May  6 08:25:58.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2459" for this suite. 05/06/23 08:25:58.291
------------------------------
â€¢ [SLOW TEST] [13.040 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:25:45.257
    May  6 08:25:45.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename job 05/06/23 08:25:45.258
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:25:46.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:25:46.276
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 05/06/23 08:25:46.277
    STEP: Ensuring job reaches completions 05/06/23 08:25:46.283
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May  6 08:25:58.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2459" for this suite. 05/06/23 08:25:58.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:25:58.298
May  6 08:25:58.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename secrets 05/06/23 08:25:58.299
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:25:59.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:25:59.317
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-4b2de696-6cfb-45da-a2fc-3f3e79152b83 05/06/23 08:25:59.319
STEP: Creating a pod to test consume secrets 05/06/23 08:25:59.323
May  6 08:25:59.330: INFO: Waiting up to 5m0s for pod "pod-secrets-55663c23-18f6-4b39-b564-f082b08943b7" in namespace "secrets-1650" to be "Succeeded or Failed"
May  6 08:25:59.332: INFO: Pod "pod-secrets-55663c23-18f6-4b39-b564-f082b08943b7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.968691ms
May  6 08:26:01.335: INFO: Pod "pod-secrets-55663c23-18f6-4b39-b564-f082b08943b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005276777s
May  6 08:26:03.335: INFO: Pod "pod-secrets-55663c23-18f6-4b39-b564-f082b08943b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005063596s
STEP: Saw pod success 05/06/23 08:26:03.335
May  6 08:26:03.335: INFO: Pod "pod-secrets-55663c23-18f6-4b39-b564-f082b08943b7" satisfied condition "Succeeded or Failed"
May  6 08:26:03.339: INFO: Trying to get logs from node cncf-0 pod pod-secrets-55663c23-18f6-4b39-b564-f082b08943b7 container secret-volume-test: <nil>
STEP: delete the pod 05/06/23 08:26:03.343
May  6 08:26:03.356: INFO: Waiting for pod pod-secrets-55663c23-18f6-4b39-b564-f082b08943b7 to disappear
May  6 08:26:03.359: INFO: Pod pod-secrets-55663c23-18f6-4b39-b564-f082b08943b7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May  6 08:26:03.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1650" for this suite. 05/06/23 08:26:03.363
------------------------------
â€¢ [SLOW TEST] [5.070 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:25:58.298
    May  6 08:25:58.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename secrets 05/06/23 08:25:58.299
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:25:59.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:25:59.317
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-4b2de696-6cfb-45da-a2fc-3f3e79152b83 05/06/23 08:25:59.319
    STEP: Creating a pod to test consume secrets 05/06/23 08:25:59.323
    May  6 08:25:59.330: INFO: Waiting up to 5m0s for pod "pod-secrets-55663c23-18f6-4b39-b564-f082b08943b7" in namespace "secrets-1650" to be "Succeeded or Failed"
    May  6 08:25:59.332: INFO: Pod "pod-secrets-55663c23-18f6-4b39-b564-f082b08943b7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.968691ms
    May  6 08:26:01.335: INFO: Pod "pod-secrets-55663c23-18f6-4b39-b564-f082b08943b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005276777s
    May  6 08:26:03.335: INFO: Pod "pod-secrets-55663c23-18f6-4b39-b564-f082b08943b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005063596s
    STEP: Saw pod success 05/06/23 08:26:03.335
    May  6 08:26:03.335: INFO: Pod "pod-secrets-55663c23-18f6-4b39-b564-f082b08943b7" satisfied condition "Succeeded or Failed"
    May  6 08:26:03.339: INFO: Trying to get logs from node cncf-0 pod pod-secrets-55663c23-18f6-4b39-b564-f082b08943b7 container secret-volume-test: <nil>
    STEP: delete the pod 05/06/23 08:26:03.343
    May  6 08:26:03.356: INFO: Waiting for pod pod-secrets-55663c23-18f6-4b39-b564-f082b08943b7 to disappear
    May  6 08:26:03.359: INFO: Pod pod-secrets-55663c23-18f6-4b39-b564-f082b08943b7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May  6 08:26:03.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1650" for this suite. 05/06/23 08:26:03.363
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:26:03.368
May  6 08:26:03.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 08:26:03.369
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:26:04.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:26:04.387
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-1ff52857-e77d-4f50-ab64-296ad916af77 05/06/23 08:26:04.392
STEP: Creating configMap with name cm-test-opt-upd-f5c35913-e5ad-4524-ad63-c5148cc68c7e 05/06/23 08:26:04.396
STEP: Creating the pod 05/06/23 08:26:04.399
May  6 08:26:04.408: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-db577a15-8f5a-4518-8060-bb0a8ee059b3" in namespace "projected-8389" to be "running and ready"
May  6 08:26:04.411: INFO: Pod "pod-projected-configmaps-db577a15-8f5a-4518-8060-bb0a8ee059b3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.408118ms
May  6 08:26:04.411: INFO: The phase of Pod pod-projected-configmaps-db577a15-8f5a-4518-8060-bb0a8ee059b3 is Pending, waiting for it to be Running (with Ready = true)
May  6 08:26:06.415: INFO: Pod "pod-projected-configmaps-db577a15-8f5a-4518-8060-bb0a8ee059b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.00699553s
May  6 08:26:06.415: INFO: The phase of Pod pod-projected-configmaps-db577a15-8f5a-4518-8060-bb0a8ee059b3 is Running (Ready = true)
May  6 08:26:06.415: INFO: Pod "pod-projected-configmaps-db577a15-8f5a-4518-8060-bb0a8ee059b3" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-1ff52857-e77d-4f50-ab64-296ad916af77 05/06/23 08:26:06.429
STEP: Updating configmap cm-test-opt-upd-f5c35913-e5ad-4524-ad63-c5148cc68c7e 05/06/23 08:26:06.435
STEP: Creating configMap with name cm-test-opt-create-e13676c4-9625-4fcf-8c39-b22f5d1b9a7c 05/06/23 08:26:06.441
STEP: waiting to observe update in volume 05/06/23 08:26:06.446
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May  6 08:26:08.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8389" for this suite. 05/06/23 08:26:08.47
------------------------------
â€¢ [SLOW TEST] [5.107 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:26:03.368
    May  6 08:26:03.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 08:26:03.369
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:26:04.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:26:04.387
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-1ff52857-e77d-4f50-ab64-296ad916af77 05/06/23 08:26:04.392
    STEP: Creating configMap with name cm-test-opt-upd-f5c35913-e5ad-4524-ad63-c5148cc68c7e 05/06/23 08:26:04.396
    STEP: Creating the pod 05/06/23 08:26:04.399
    May  6 08:26:04.408: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-db577a15-8f5a-4518-8060-bb0a8ee059b3" in namespace "projected-8389" to be "running and ready"
    May  6 08:26:04.411: INFO: Pod "pod-projected-configmaps-db577a15-8f5a-4518-8060-bb0a8ee059b3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.408118ms
    May  6 08:26:04.411: INFO: The phase of Pod pod-projected-configmaps-db577a15-8f5a-4518-8060-bb0a8ee059b3 is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:26:06.415: INFO: Pod "pod-projected-configmaps-db577a15-8f5a-4518-8060-bb0a8ee059b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.00699553s
    May  6 08:26:06.415: INFO: The phase of Pod pod-projected-configmaps-db577a15-8f5a-4518-8060-bb0a8ee059b3 is Running (Ready = true)
    May  6 08:26:06.415: INFO: Pod "pod-projected-configmaps-db577a15-8f5a-4518-8060-bb0a8ee059b3" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-1ff52857-e77d-4f50-ab64-296ad916af77 05/06/23 08:26:06.429
    STEP: Updating configmap cm-test-opt-upd-f5c35913-e5ad-4524-ad63-c5148cc68c7e 05/06/23 08:26:06.435
    STEP: Creating configMap with name cm-test-opt-create-e13676c4-9625-4fcf-8c39-b22f5d1b9a7c 05/06/23 08:26:06.441
    STEP: waiting to observe update in volume 05/06/23 08:26:06.446
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May  6 08:26:08.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8389" for this suite. 05/06/23 08:26:08.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:26:08.476
May  6 08:26:08.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename containers 05/06/23 08:26:08.477
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:26:09.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:26:09.495
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 05/06/23 08:26:09.497
May  6 08:26:09.504: INFO: Waiting up to 5m0s for pod "client-containers-a4f39c81-318a-4714-944e-5a1a7b04b736" in namespace "containers-6383" to be "Succeeded or Failed"
May  6 08:26:09.506: INFO: Pod "client-containers-a4f39c81-318a-4714-944e-5a1a7b04b736": Phase="Pending", Reason="", readiness=false. Elapsed: 1.958111ms
May  6 08:26:11.511: INFO: Pod "client-containers-a4f39c81-318a-4714-944e-5a1a7b04b736": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006307641s
May  6 08:26:13.511: INFO: Pod "client-containers-a4f39c81-318a-4714-944e-5a1a7b04b736": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006105064s
STEP: Saw pod success 05/06/23 08:26:13.511
May  6 08:26:13.511: INFO: Pod "client-containers-a4f39c81-318a-4714-944e-5a1a7b04b736" satisfied condition "Succeeded or Failed"
May  6 08:26:13.513: INFO: Trying to get logs from node cncf-3 pod client-containers-a4f39c81-318a-4714-944e-5a1a7b04b736 container agnhost-container: <nil>
STEP: delete the pod 05/06/23 08:26:13.523
May  6 08:26:13.533: INFO: Waiting for pod client-containers-a4f39c81-318a-4714-944e-5a1a7b04b736 to disappear
May  6 08:26:13.539: INFO: Pod client-containers-a4f39c81-318a-4714-944e-5a1a7b04b736 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
May  6 08:26:13.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6383" for this suite. 05/06/23 08:26:13.544
------------------------------
â€¢ [SLOW TEST] [5.075 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:26:08.476
    May  6 08:26:08.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename containers 05/06/23 08:26:08.477
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:26:09.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:26:09.495
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 05/06/23 08:26:09.497
    May  6 08:26:09.504: INFO: Waiting up to 5m0s for pod "client-containers-a4f39c81-318a-4714-944e-5a1a7b04b736" in namespace "containers-6383" to be "Succeeded or Failed"
    May  6 08:26:09.506: INFO: Pod "client-containers-a4f39c81-318a-4714-944e-5a1a7b04b736": Phase="Pending", Reason="", readiness=false. Elapsed: 1.958111ms
    May  6 08:26:11.511: INFO: Pod "client-containers-a4f39c81-318a-4714-944e-5a1a7b04b736": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006307641s
    May  6 08:26:13.511: INFO: Pod "client-containers-a4f39c81-318a-4714-944e-5a1a7b04b736": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006105064s
    STEP: Saw pod success 05/06/23 08:26:13.511
    May  6 08:26:13.511: INFO: Pod "client-containers-a4f39c81-318a-4714-944e-5a1a7b04b736" satisfied condition "Succeeded or Failed"
    May  6 08:26:13.513: INFO: Trying to get logs from node cncf-3 pod client-containers-a4f39c81-318a-4714-944e-5a1a7b04b736 container agnhost-container: <nil>
    STEP: delete the pod 05/06/23 08:26:13.523
    May  6 08:26:13.533: INFO: Waiting for pod client-containers-a4f39c81-318a-4714-944e-5a1a7b04b736 to disappear
    May  6 08:26:13.539: INFO: Pod client-containers-a4f39c81-318a-4714-944e-5a1a7b04b736 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    May  6 08:26:13.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6383" for this suite. 05/06/23 08:26:13.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:26:13.554
May  6 08:26:13.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename resourcequota 05/06/23 08:26:13.555
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:26:14.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:26:14.573
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 05/06/23 08:26:14.575
STEP: Creating a ResourceQuota 05/06/23 08:26:19.579
STEP: Ensuring resource quota status is calculated 05/06/23 08:26:19.584
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  6 08:26:21.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8552" for this suite. 05/06/23 08:26:21.59
------------------------------
â€¢ [SLOW TEST] [8.042 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:26:13.554
    May  6 08:26:13.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename resourcequota 05/06/23 08:26:13.555
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:26:14.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:26:14.573
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 05/06/23 08:26:14.575
    STEP: Creating a ResourceQuota 05/06/23 08:26:19.579
    STEP: Ensuring resource quota status is calculated 05/06/23 08:26:19.584
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  6 08:26:21.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8552" for this suite. 05/06/23 08:26:21.59
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:26:21.597
May  6 08:26:21.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename secrets 05/06/23 08:26:21.597
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:26:22.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:26:22.616
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-d68cb778-166a-4f3d-a1f4-1dde249c6c4e 05/06/23 08:26:22.618
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
May  6 08:26:22.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1021" for this suite. 05/06/23 08:26:22.622
------------------------------
â€¢ [1.031 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:26:21.597
    May  6 08:26:21.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename secrets 05/06/23 08:26:21.597
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:26:22.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:26:22.616
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-d68cb778-166a-4f3d-a1f4-1dde249c6c4e 05/06/23 08:26:22.618
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    May  6 08:26:22.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1021" for this suite. 05/06/23 08:26:22.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:26:22.628
May  6 08:26:22.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename container-runtime 05/06/23 08:26:22.629
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:26:23.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:26:23.647
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 05/06/23 08:26:23.654
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 05/06/23 08:26:42.718
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 05/06/23 08:26:42.72
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 05/06/23 08:26:42.724
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 05/06/23 08:26:42.724
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 05/06/23 08:26:42.744
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 05/06/23 08:26:44.753
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 05/06/23 08:26:46.762
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 05/06/23 08:26:46.767
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 05/06/23 08:26:46.767
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 05/06/23 08:26:46.785
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 05/06/23 08:26:47.79
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 05/06/23 08:26:49.799
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 05/06/23 08:26:49.804
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 05/06/23 08:26:49.804
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May  6 08:26:49.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-369" for this suite. 05/06/23 08:26:49.825
------------------------------
â€¢ [SLOW TEST] [27.202 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:26:22.628
    May  6 08:26:22.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename container-runtime 05/06/23 08:26:22.629
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:26:23.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:26:23.647
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 05/06/23 08:26:23.654
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 05/06/23 08:26:42.718
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 05/06/23 08:26:42.72
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 05/06/23 08:26:42.724
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 05/06/23 08:26:42.724
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 05/06/23 08:26:42.744
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 05/06/23 08:26:44.753
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 05/06/23 08:26:46.762
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 05/06/23 08:26:46.767
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 05/06/23 08:26:46.767
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 05/06/23 08:26:46.785
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 05/06/23 08:26:47.79
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 05/06/23 08:26:49.799
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 05/06/23 08:26:49.804
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 05/06/23 08:26:49.804
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May  6 08:26:49.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-369" for this suite. 05/06/23 08:26:49.825
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:26:49.83
May  6 08:26:49.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename proxy 05/06/23 08:26:49.831
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:26:50.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:26:50.848
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 05/06/23 08:26:50.861
STEP: creating replication controller proxy-service-wpvnv in namespace proxy-2437 05/06/23 08:26:50.861
I0506 08:26:50.869705      21 runners.go:193] Created replication controller with name: proxy-service-wpvnv, namespace: proxy-2437, replica count: 1
I0506 08:26:51.920799      21 runners.go:193] proxy-service-wpvnv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0506 08:26:52.921240      21 runners.go:193] proxy-service-wpvnv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0506 08:26:53.921996      21 runners.go:193] proxy-service-wpvnv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0506 08:26:54.922246      21 runners.go:193] proxy-service-wpvnv Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 08:26:54.924: INFO: setup took 4.074463893s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 05/06/23 08:26:54.924
May  6 08:26:54.931: INFO: (0) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 7.143606ms)
May  6 08:26:54.934: INFO: (0) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 9.724721ms)
May  6 08:26:54.934: INFO: (0) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 10.017618ms)
May  6 08:26:54.935: INFO: (0) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 10.138718ms)
May  6 08:26:54.935: INFO: (0) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 10.474465ms)
May  6 08:26:54.937: INFO: (0) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 12.618661ms)
May  6 08:26:54.938: INFO: (0) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 13.38096ms)
May  6 08:26:54.938: INFO: (0) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 13.582944ms)
May  6 08:26:54.938: INFO: (0) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 14.007249ms)
May  6 08:26:54.941: INFO: (0) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 16.51699ms)
May  6 08:26:54.942: INFO: (0) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 17.420476ms)
May  6 08:26:54.942: INFO: (0) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 17.653469ms)
May  6 08:26:54.942: INFO: (0) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 17.739082ms)
May  6 08:26:54.944: INFO: (0) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 19.21691ms)
May  6 08:26:54.944: INFO: (0) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 19.002683ms)
May  6 08:26:54.945: INFO: (0) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 20.020556ms)
May  6 08:26:54.951: INFO: (1) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 5.923527ms)
May  6 08:26:54.952: INFO: (1) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 7.233547ms)
May  6 08:26:54.952: INFO: (1) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 6.98284ms)
May  6 08:26:54.952: INFO: (1) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 7.217066ms)
May  6 08:26:54.953: INFO: (1) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 8.465868ms)
May  6 08:26:54.953: INFO: (1) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 8.696337ms)
May  6 08:26:54.955: INFO: (1) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 10.487802ms)
May  6 08:26:54.955: INFO: (1) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 10.509532ms)
May  6 08:26:54.957: INFO: (1) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 12.378042ms)
May  6 08:26:54.957: INFO: (1) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 12.702981ms)
May  6 08:26:54.958: INFO: (1) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 13.462425ms)
May  6 08:26:54.958: INFO: (1) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 13.779919ms)
May  6 08:26:54.959: INFO: (1) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 14.04471ms)
May  6 08:26:54.960: INFO: (1) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 15.043179ms)
May  6 08:26:54.960: INFO: (1) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 15.343459ms)
May  6 08:26:54.961: INFO: (1) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 16.63299ms)
May  6 08:26:54.968: INFO: (2) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 6.272701ms)
May  6 08:26:54.968: INFO: (2) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 6.752522ms)
May  6 08:26:54.969: INFO: (2) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 7.430351ms)
May  6 08:26:54.970: INFO: (2) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 8.712066ms)
May  6 08:26:54.971: INFO: (2) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 9.662432ms)
May  6 08:26:54.971: INFO: (2) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 10.001286ms)
May  6 08:26:54.972: INFO: (2) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 10.29195ms)
May  6 08:26:54.973: INFO: (2) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 11.269878ms)
May  6 08:26:54.974: INFO: (2) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 11.958315ms)
May  6 08:26:54.974: INFO: (2) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 12.62367ms)
May  6 08:26:54.975: INFO: (2) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 13.036956ms)
May  6 08:26:54.975: INFO: (2) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 13.701519ms)
May  6 08:26:54.975: INFO: (2) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 13.680539ms)
May  6 08:26:54.975: INFO: (2) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 13.890237ms)
May  6 08:26:54.976: INFO: (2) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 14.076631ms)
May  6 08:26:54.976: INFO: (2) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 14.451654ms)
May  6 08:26:54.985: INFO: (3) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 8.77726ms)
May  6 08:26:54.985: INFO: (3) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 9.151081ms)
May  6 08:26:54.987: INFO: (3) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 11.275358ms)
May  6 08:26:54.988: INFO: (3) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 11.469978ms)
May  6 08:26:54.989: INFO: (3) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 12.825664ms)
May  6 08:26:54.991: INFO: (3) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 14.854591ms)
May  6 08:26:54.991: INFO: (3) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 15.131627ms)
May  6 08:26:54.991: INFO: (3) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 15.159739ms)
May  6 08:26:54.992: INFO: (3) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 15.395478ms)
May  6 08:26:54.992: INFO: (3) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 15.360031ms)
May  6 08:26:54.992: INFO: (3) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 16.256986ms)
May  6 08:26:54.993: INFO: (3) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 16.974949ms)
May  6 08:26:54.993: INFO: (3) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 17.261143ms)
May  6 08:26:54.994: INFO: (3) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 17.664289ms)
May  6 08:26:54.995: INFO: (3) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 18.684088ms)
May  6 08:26:54.995: INFO: (3) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 18.939713ms)
May  6 08:26:55.002: INFO: (4) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 6.333316ms)
May  6 08:26:55.003: INFO: (4) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 7.347282ms)
May  6 08:26:55.004: INFO: (4) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 7.741131ms)
May  6 08:26:55.004: INFO: (4) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 8.401887ms)
May  6 08:26:55.004: INFO: (4) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 8.678834ms)
May  6 08:26:55.005: INFO: (4) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 8.940971ms)
May  6 08:26:55.005: INFO: (4) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 9.41847ms)
May  6 08:26:55.007: INFO: (4) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 10.835201ms)
May  6 08:26:55.007: INFO: (4) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 11.142777ms)
May  6 08:26:55.007: INFO: (4) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 11.322958ms)
May  6 08:26:55.008: INFO: (4) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 12.096629ms)
May  6 08:26:55.009: INFO: (4) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 12.924091ms)
May  6 08:26:55.009: INFO: (4) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 13.091479ms)
May  6 08:26:55.009: INFO: (4) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 13.249039ms)
May  6 08:26:55.010: INFO: (4) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 14.441675ms)
May  6 08:26:55.011: INFO: (4) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 14.800947ms)
May  6 08:26:55.016: INFO: (5) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 5.371779ms)
May  6 08:26:55.016: INFO: (5) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 5.754727ms)
May  6 08:26:55.019: INFO: (5) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 8.137045ms)
May  6 08:26:55.020: INFO: (5) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 9.189283ms)
May  6 08:26:55.020: INFO: (5) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 9.279224ms)
May  6 08:26:55.021: INFO: (5) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 10.162414ms)
May  6 08:26:55.022: INFO: (5) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 11.133769ms)
May  6 08:26:55.022: INFO: (5) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 11.312258ms)
May  6 08:26:55.022: INFO: (5) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 11.408822ms)
May  6 08:26:55.023: INFO: (5) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 12.280739ms)
May  6 08:26:55.024: INFO: (5) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 13.300206ms)
May  6 08:26:55.025: INFO: (5) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 13.593954ms)
May  6 08:26:55.025: INFO: (5) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 14.113041ms)
May  6 08:26:55.026: INFO: (5) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 14.65448ms)
May  6 08:26:55.026: INFO: (5) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 14.751885ms)
May  6 08:26:55.027: INFO: (5) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 15.533219ms)
May  6 08:26:55.032: INFO: (6) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 4.933896ms)
May  6 08:26:55.032: INFO: (6) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 5.46748ms)
May  6 08:26:55.033: INFO: (6) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 6.168483ms)
May  6 08:26:55.034: INFO: (6) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 7.457382ms)
May  6 08:26:55.034: INFO: (6) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 7.770548ms)
May  6 08:26:55.036: INFO: (6) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 9.03502ms)
May  6 08:26:55.037: INFO: (6) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 10.109061ms)
May  6 08:26:55.037: INFO: (6) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 10.496898ms)
May  6 08:26:55.038: INFO: (6) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 10.990898ms)
May  6 08:26:55.039: INFO: (6) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 11.828509ms)
May  6 08:26:55.040: INFO: (6) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 13.115786ms)
May  6 08:26:55.040: INFO: (6) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 13.261603ms)
May  6 08:26:55.041: INFO: (6) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 13.947417ms)
May  6 08:26:55.041: INFO: (6) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 14.000267ms)
May  6 08:26:55.041: INFO: (6) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 14.385739ms)
May  6 08:26:55.042: INFO: (6) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 15.353808ms)
May  6 08:26:55.052: INFO: (7) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 9.394554ms)
May  6 08:26:55.052: INFO: (7) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 9.459987ms)
May  6 08:26:55.053: INFO: (7) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 11.115344ms)
May  6 08:26:55.054: INFO: (7) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 11.222998ms)
May  6 08:26:55.054: INFO: (7) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 12.085358ms)
May  6 08:26:55.055: INFO: (7) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 12.538178ms)
May  6 08:26:55.055: INFO: (7) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 12.782161ms)
May  6 08:26:55.056: INFO: (7) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 13.081078ms)
May  6 08:26:55.056: INFO: (7) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 13.452576ms)
May  6 08:26:55.056: INFO: (7) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 13.689786ms)
May  6 08:26:55.059: INFO: (7) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 16.281723ms)
May  6 08:26:55.059: INFO: (7) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 16.820927ms)
May  6 08:26:55.060: INFO: (7) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 17.592272ms)
May  6 08:26:55.061: INFO: (7) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 18.421146ms)
May  6 08:26:55.063: INFO: (7) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 20.780862ms)
May  6 08:26:55.064: INFO: (7) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 20.999026ms)
May  6 08:26:55.070: INFO: (8) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 6.163683ms)
May  6 08:26:55.070: INFO: (8) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 6.679815ms)
May  6 08:26:55.073: INFO: (8) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 9.028117ms)
May  6 08:26:55.073: INFO: (8) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 9.417849ms)
May  6 08:26:55.075: INFO: (8) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 10.89774ms)
May  6 08:26:55.076: INFO: (8) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 11.903ms)
May  6 08:26:55.076: INFO: (8) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 12.293322ms)
May  6 08:26:55.076: INFO: (8) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 12.309684ms)
May  6 08:26:55.076: INFO: (8) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 12.339721ms)
May  6 08:26:55.076: INFO: (8) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 12.387291ms)
May  6 08:26:55.077: INFO: (8) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 12.563817ms)
May  6 08:26:55.077: INFO: (8) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 13.050111ms)
May  6 08:26:55.078: INFO: (8) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 14.021767ms)
May  6 08:26:55.078: INFO: (8) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 13.958145ms)
May  6 08:26:55.079: INFO: (8) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 15.021266ms)
May  6 08:26:55.079: INFO: (8) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 14.718561ms)
May  6 08:26:55.085: INFO: (9) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 6.329588ms)
May  6 08:26:55.085: INFO: (9) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 6.576928ms)
May  6 08:26:55.086: INFO: (9) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 6.863111ms)
May  6 08:26:55.087: INFO: (9) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 7.860609ms)
May  6 08:26:55.087: INFO: (9) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 8.132915ms)
May  6 08:26:55.087: INFO: (9) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 8.051291ms)
May  6 08:26:55.089: INFO: (9) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 10.15051ms)
May  6 08:26:55.090: INFO: (9) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 11.100515ms)
May  6 08:26:55.090: INFO: (9) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 11.285146ms)
May  6 08:26:55.092: INFO: (9) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 12.758968ms)
May  6 08:26:55.092: INFO: (9) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 12.869887ms)
May  6 08:26:55.093: INFO: (9) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 13.486991ms)
May  6 08:26:55.093: INFO: (9) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 14.478537ms)
May  6 08:26:55.093: INFO: (9) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 14.406829ms)
May  6 08:26:55.094: INFO: (9) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 14.928891ms)
May  6 08:26:55.094: INFO: (9) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 15.361724ms)
May  6 08:26:55.100: INFO: (10) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 5.766638ms)
May  6 08:26:55.100: INFO: (10) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 6.040428ms)
May  6 08:26:55.103: INFO: (10) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 8.678332ms)
May  6 08:26:55.103: INFO: (10) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 8.692198ms)
May  6 08:26:55.104: INFO: (10) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 10.060229ms)
May  6 08:26:55.104: INFO: (10) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 9.994954ms)
May  6 08:26:55.106: INFO: (10) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 11.595015ms)
May  6 08:26:55.107: INFO: (10) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 12.263746ms)
May  6 08:26:55.107: INFO: (10) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 12.709994ms)
May  6 08:26:55.108: INFO: (10) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 13.880279ms)
May  6 08:26:55.108: INFO: (10) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 14.113302ms)
May  6 08:26:55.109: INFO: (10) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 14.241745ms)
May  6 08:26:55.109: INFO: (10) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 14.655011ms)
May  6 08:26:55.113: INFO: (10) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 18.425246ms)
May  6 08:26:55.113: INFO: (10) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 18.437228ms)
May  6 08:26:55.113: INFO: (10) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 18.467655ms)
May  6 08:26:55.123: INFO: (11) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 9.927978ms)
May  6 08:26:55.124: INFO: (11) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 10.757815ms)
May  6 08:26:55.124: INFO: (11) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 11.017689ms)
May  6 08:26:55.125: INFO: (11) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 12.131786ms)
May  6 08:26:55.126: INFO: (11) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 12.485929ms)
May  6 08:26:55.126: INFO: (11) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 12.869707ms)
May  6 08:26:55.126: INFO: (11) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 13.202279ms)
May  6 08:26:55.127: INFO: (11) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 13.826095ms)
May  6 08:26:55.128: INFO: (11) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 14.436155ms)
May  6 08:26:55.129: INFO: (11) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 15.483054ms)
May  6 08:26:55.130: INFO: (11) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 16.514064ms)
May  6 08:26:55.130: INFO: (11) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 17.122449ms)
May  6 08:26:55.131: INFO: (11) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 17.672996ms)
May  6 08:26:55.131: INFO: (11) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 18.044943ms)
May  6 08:26:55.132: INFO: (11) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 18.670621ms)
May  6 08:26:55.132: INFO: (11) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 18.98578ms)
May  6 08:26:55.137: INFO: (12) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 4.945498ms)
May  6 08:26:55.138: INFO: (12) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 5.838536ms)
May  6 08:26:55.139: INFO: (12) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 6.53575ms)
May  6 08:26:55.139: INFO: (12) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 6.755568ms)
May  6 08:26:55.141: INFO: (12) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 8.085135ms)
May  6 08:26:55.142: INFO: (12) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 9.473613ms)
May  6 08:26:55.143: INFO: (12) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 10.451993ms)
May  6 08:26:55.143: INFO: (12) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 10.702248ms)
May  6 08:26:55.144: INFO: (12) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 11.806057ms)
May  6 08:26:55.145: INFO: (12) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 12.708921ms)
May  6 08:26:55.146: INFO: (12) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 13.320293ms)
May  6 08:26:55.147: INFO: (12) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 14.444139ms)
May  6 08:26:55.148: INFO: (12) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 15.352786ms)
May  6 08:26:55.148: INFO: (12) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 15.415786ms)
May  6 08:26:55.149: INFO: (12) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 16.080608ms)
May  6 08:26:55.150: INFO: (12) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 17.851056ms)
May  6 08:26:55.158: INFO: (13) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 7.676628ms)
May  6 08:26:55.160: INFO: (13) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 9.278713ms)
May  6 08:26:55.162: INFO: (13) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 11.352294ms)
May  6 08:26:55.162: INFO: (13) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 11.385337ms)
May  6 08:26:55.162: INFO: (13) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 11.633188ms)
May  6 08:26:55.162: INFO: (13) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 11.599776ms)
May  6 08:26:55.163: INFO: (13) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 12.49719ms)
May  6 08:26:55.163: INFO: (13) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 12.496488ms)
May  6 08:26:55.164: INFO: (13) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 12.98155ms)
May  6 08:26:55.164: INFO: (13) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 12.942235ms)
May  6 08:26:55.167: INFO: (13) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 16.351896ms)
May  6 08:26:55.167: INFO: (13) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 16.242928ms)
May  6 08:26:55.167: INFO: (13) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 16.333161ms)
May  6 08:26:55.167: INFO: (13) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 16.661533ms)
May  6 08:26:55.168: INFO: (13) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 16.804756ms)
May  6 08:26:55.168: INFO: (13) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 17.231487ms)
May  6 08:26:55.174: INFO: (14) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 5.574624ms)
May  6 08:26:55.174: INFO: (14) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 5.636421ms)
May  6 08:26:55.176: INFO: (14) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 7.401726ms)
May  6 08:26:55.177: INFO: (14) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 8.929459ms)
May  6 08:26:55.178: INFO: (14) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 10.114462ms)
May  6 08:26:55.178: INFO: (14) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 9.993602ms)
May  6 08:26:55.180: INFO: (14) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 11.380338ms)
May  6 08:26:55.180: INFO: (14) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 11.908701ms)
May  6 08:26:55.181: INFO: (14) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 12.772604ms)
May  6 08:26:55.181: INFO: (14) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 12.799345ms)
May  6 08:26:55.182: INFO: (14) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 13.273795ms)
May  6 08:26:55.182: INFO: (14) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 13.626866ms)
May  6 08:26:55.182: INFO: (14) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 13.917849ms)
May  6 08:26:55.182: INFO: (14) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 13.926408ms)
May  6 08:26:55.183: INFO: (14) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 14.667024ms)
May  6 08:26:55.183: INFO: (14) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 14.812498ms)
May  6 08:26:55.189: INFO: (15) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 5.623558ms)
May  6 08:26:55.190: INFO: (15) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 6.404661ms)
May  6 08:26:55.190: INFO: (15) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 7.261429ms)
May  6 08:26:55.191: INFO: (15) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 7.424028ms)
May  6 08:26:55.192: INFO: (15) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 8.396357ms)
May  6 08:26:55.192: INFO: (15) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 8.58762ms)
May  6 08:26:55.194: INFO: (15) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 10.12367ms)
May  6 08:26:55.195: INFO: (15) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 11.246573ms)
May  6 08:26:55.195: INFO: (15) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 11.444549ms)
May  6 08:26:55.196: INFO: (15) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 12.089394ms)
May  6 08:26:55.196: INFO: (15) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 12.141834ms)
May  6 08:26:55.196: INFO: (15) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 12.268155ms)
May  6 08:26:55.197: INFO: (15) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 13.976441ms)
May  6 08:26:55.198: INFO: (15) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 14.837237ms)
May  6 08:26:55.199: INFO: (15) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 15.159569ms)
May  6 08:26:55.199: INFO: (15) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 15.995027ms)
May  6 08:26:55.206: INFO: (16) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 6.049949ms)
May  6 08:26:55.206: INFO: (16) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 6.06193ms)
May  6 08:26:55.207: INFO: (16) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 7.36664ms)
May  6 08:26:55.208: INFO: (16) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 7.77203ms)
May  6 08:26:55.208: INFO: (16) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 7.952323ms)
May  6 08:26:55.208: INFO: (16) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 8.64575ms)
May  6 08:26:55.208: INFO: (16) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 8.891117ms)
May  6 08:26:55.209: INFO: (16) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 9.07687ms)
May  6 08:26:55.210: INFO: (16) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 10.373363ms)
May  6 08:26:55.211: INFO: (16) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 10.920714ms)
May  6 08:26:55.212: INFO: (16) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 11.815374ms)
May  6 08:26:55.213: INFO: (16) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 12.922839ms)
May  6 08:26:55.213: INFO: (16) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 13.047696ms)
May  6 08:26:55.213: INFO: (16) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 13.67091ms)
May  6 08:26:55.213: INFO: (16) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 13.830293ms)
May  6 08:26:55.214: INFO: (16) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 14.552897ms)
May  6 08:26:55.220: INFO: (17) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 6.106265ms)
May  6 08:26:55.221: INFO: (17) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 6.315051ms)
May  6 08:26:55.221: INFO: (17) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 6.709641ms)
May  6 08:26:55.223: INFO: (17) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 8.578112ms)
May  6 08:26:55.223: INFO: (17) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 8.601216ms)
May  6 08:26:55.223: INFO: (17) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 9.031273ms)
May  6 08:26:55.225: INFO: (17) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 10.086467ms)
May  6 08:26:55.225: INFO: (17) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 10.326945ms)
May  6 08:26:55.226: INFO: (17) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 11.930474ms)
May  6 08:26:55.227: INFO: (17) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 12.221827ms)
May  6 08:26:55.227: INFO: (17) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 12.184305ms)
May  6 08:26:55.227: INFO: (17) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 12.584104ms)
May  6 08:26:55.228: INFO: (17) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 13.921647ms)
May  6 08:26:55.229: INFO: (17) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 14.140122ms)
May  6 08:26:55.229: INFO: (17) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 14.747906ms)
May  6 08:26:55.230: INFO: (17) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 15.511769ms)
May  6 08:26:55.237: INFO: (18) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 6.848155ms)
May  6 08:26:55.237: INFO: (18) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 7.207898ms)
May  6 08:26:55.239: INFO: (18) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 9.180937ms)
May  6 08:26:55.240: INFO: (18) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 10.246623ms)
May  6 08:26:55.241: INFO: (18) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 10.730473ms)
May  6 08:26:55.242: INFO: (18) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 11.777231ms)
May  6 08:26:55.243: INFO: (18) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 12.526346ms)
May  6 08:26:55.243: INFO: (18) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 12.867924ms)
May  6 08:26:55.243: INFO: (18) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 12.890556ms)
May  6 08:26:55.244: INFO: (18) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 13.452525ms)
May  6 08:26:55.245: INFO: (18) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 14.483634ms)
May  6 08:26:55.245: INFO: (18) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 14.432598ms)
May  6 08:26:55.246: INFO: (18) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 15.299255ms)
May  6 08:26:55.246: INFO: (18) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 15.629232ms)
May  6 08:26:55.247: INFO: (18) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 16.183374ms)
May  6 08:26:55.247: INFO: (18) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 16.477285ms)
May  6 08:26:55.252: INFO: (19) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 4.624168ms)
May  6 08:26:55.253: INFO: (19) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 5.941662ms)
May  6 08:26:55.256: INFO: (19) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 8.677028ms)
May  6 08:26:55.257: INFO: (19) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 9.264376ms)
May  6 08:26:55.258: INFO: (19) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 9.910975ms)
May  6 08:26:55.259: INFO: (19) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 11.668254ms)
May  6 08:26:55.259: INFO: (19) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 11.263825ms)
May  6 08:26:55.260: INFO: (19) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 12.820225ms)
May  6 08:26:55.261: INFO: (19) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 12.730023ms)
May  6 08:26:55.261: INFO: (19) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 13.025324ms)
May  6 08:26:55.262: INFO: (19) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 14.125383ms)
May  6 08:26:55.262: INFO: (19) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 14.29198ms)
May  6 08:26:55.263: INFO: (19) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 15.197812ms)
May  6 08:26:55.263: INFO: (19) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 15.300498ms)
May  6 08:26:55.264: INFO: (19) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 16.704654ms)
May  6 08:26:55.264: INFO: (19) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 17.033139ms)
STEP: deleting ReplicationController proxy-service-wpvnv in namespace proxy-2437, will wait for the garbage collector to delete the pods 05/06/23 08:26:55.265
May  6 08:26:55.323: INFO: Deleting ReplicationController proxy-service-wpvnv took: 5.518958ms
May  6 08:26:55.423: INFO: Terminating ReplicationController proxy-service-wpvnv pods took: 100.770796ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
May  6 08:26:57.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2437" for this suite. 05/06/23 08:26:57.628
------------------------------
â€¢ [SLOW TEST] [7.803 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:26:49.83
    May  6 08:26:49.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename proxy 05/06/23 08:26:49.831
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:26:50.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:26:50.848
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 05/06/23 08:26:50.861
    STEP: creating replication controller proxy-service-wpvnv in namespace proxy-2437 05/06/23 08:26:50.861
    I0506 08:26:50.869705      21 runners.go:193] Created replication controller with name: proxy-service-wpvnv, namespace: proxy-2437, replica count: 1
    I0506 08:26:51.920799      21 runners.go:193] proxy-service-wpvnv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0506 08:26:52.921240      21 runners.go:193] proxy-service-wpvnv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0506 08:26:53.921996      21 runners.go:193] proxy-service-wpvnv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0506 08:26:54.922246      21 runners.go:193] proxy-service-wpvnv Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  6 08:26:54.924: INFO: setup took 4.074463893s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 05/06/23 08:26:54.924
    May  6 08:26:54.931: INFO: (0) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 7.143606ms)
    May  6 08:26:54.934: INFO: (0) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 9.724721ms)
    May  6 08:26:54.934: INFO: (0) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 10.017618ms)
    May  6 08:26:54.935: INFO: (0) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 10.138718ms)
    May  6 08:26:54.935: INFO: (0) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 10.474465ms)
    May  6 08:26:54.937: INFO: (0) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 12.618661ms)
    May  6 08:26:54.938: INFO: (0) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 13.38096ms)
    May  6 08:26:54.938: INFO: (0) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 13.582944ms)
    May  6 08:26:54.938: INFO: (0) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 14.007249ms)
    May  6 08:26:54.941: INFO: (0) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 16.51699ms)
    May  6 08:26:54.942: INFO: (0) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 17.420476ms)
    May  6 08:26:54.942: INFO: (0) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 17.653469ms)
    May  6 08:26:54.942: INFO: (0) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 17.739082ms)
    May  6 08:26:54.944: INFO: (0) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 19.21691ms)
    May  6 08:26:54.944: INFO: (0) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 19.002683ms)
    May  6 08:26:54.945: INFO: (0) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 20.020556ms)
    May  6 08:26:54.951: INFO: (1) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 5.923527ms)
    May  6 08:26:54.952: INFO: (1) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 7.233547ms)
    May  6 08:26:54.952: INFO: (1) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 6.98284ms)
    May  6 08:26:54.952: INFO: (1) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 7.217066ms)
    May  6 08:26:54.953: INFO: (1) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 8.465868ms)
    May  6 08:26:54.953: INFO: (1) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 8.696337ms)
    May  6 08:26:54.955: INFO: (1) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 10.487802ms)
    May  6 08:26:54.955: INFO: (1) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 10.509532ms)
    May  6 08:26:54.957: INFO: (1) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 12.378042ms)
    May  6 08:26:54.957: INFO: (1) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 12.702981ms)
    May  6 08:26:54.958: INFO: (1) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 13.462425ms)
    May  6 08:26:54.958: INFO: (1) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 13.779919ms)
    May  6 08:26:54.959: INFO: (1) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 14.04471ms)
    May  6 08:26:54.960: INFO: (1) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 15.043179ms)
    May  6 08:26:54.960: INFO: (1) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 15.343459ms)
    May  6 08:26:54.961: INFO: (1) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 16.63299ms)
    May  6 08:26:54.968: INFO: (2) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 6.272701ms)
    May  6 08:26:54.968: INFO: (2) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 6.752522ms)
    May  6 08:26:54.969: INFO: (2) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 7.430351ms)
    May  6 08:26:54.970: INFO: (2) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 8.712066ms)
    May  6 08:26:54.971: INFO: (2) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 9.662432ms)
    May  6 08:26:54.971: INFO: (2) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 10.001286ms)
    May  6 08:26:54.972: INFO: (2) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 10.29195ms)
    May  6 08:26:54.973: INFO: (2) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 11.269878ms)
    May  6 08:26:54.974: INFO: (2) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 11.958315ms)
    May  6 08:26:54.974: INFO: (2) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 12.62367ms)
    May  6 08:26:54.975: INFO: (2) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 13.036956ms)
    May  6 08:26:54.975: INFO: (2) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 13.701519ms)
    May  6 08:26:54.975: INFO: (2) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 13.680539ms)
    May  6 08:26:54.975: INFO: (2) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 13.890237ms)
    May  6 08:26:54.976: INFO: (2) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 14.076631ms)
    May  6 08:26:54.976: INFO: (2) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 14.451654ms)
    May  6 08:26:54.985: INFO: (3) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 8.77726ms)
    May  6 08:26:54.985: INFO: (3) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 9.151081ms)
    May  6 08:26:54.987: INFO: (3) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 11.275358ms)
    May  6 08:26:54.988: INFO: (3) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 11.469978ms)
    May  6 08:26:54.989: INFO: (3) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 12.825664ms)
    May  6 08:26:54.991: INFO: (3) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 14.854591ms)
    May  6 08:26:54.991: INFO: (3) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 15.131627ms)
    May  6 08:26:54.991: INFO: (3) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 15.159739ms)
    May  6 08:26:54.992: INFO: (3) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 15.395478ms)
    May  6 08:26:54.992: INFO: (3) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 15.360031ms)
    May  6 08:26:54.992: INFO: (3) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 16.256986ms)
    May  6 08:26:54.993: INFO: (3) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 16.974949ms)
    May  6 08:26:54.993: INFO: (3) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 17.261143ms)
    May  6 08:26:54.994: INFO: (3) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 17.664289ms)
    May  6 08:26:54.995: INFO: (3) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 18.684088ms)
    May  6 08:26:54.995: INFO: (3) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 18.939713ms)
    May  6 08:26:55.002: INFO: (4) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 6.333316ms)
    May  6 08:26:55.003: INFO: (4) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 7.347282ms)
    May  6 08:26:55.004: INFO: (4) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 7.741131ms)
    May  6 08:26:55.004: INFO: (4) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 8.401887ms)
    May  6 08:26:55.004: INFO: (4) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 8.678834ms)
    May  6 08:26:55.005: INFO: (4) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 8.940971ms)
    May  6 08:26:55.005: INFO: (4) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 9.41847ms)
    May  6 08:26:55.007: INFO: (4) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 10.835201ms)
    May  6 08:26:55.007: INFO: (4) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 11.142777ms)
    May  6 08:26:55.007: INFO: (4) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 11.322958ms)
    May  6 08:26:55.008: INFO: (4) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 12.096629ms)
    May  6 08:26:55.009: INFO: (4) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 12.924091ms)
    May  6 08:26:55.009: INFO: (4) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 13.091479ms)
    May  6 08:26:55.009: INFO: (4) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 13.249039ms)
    May  6 08:26:55.010: INFO: (4) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 14.441675ms)
    May  6 08:26:55.011: INFO: (4) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 14.800947ms)
    May  6 08:26:55.016: INFO: (5) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 5.371779ms)
    May  6 08:26:55.016: INFO: (5) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 5.754727ms)
    May  6 08:26:55.019: INFO: (5) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 8.137045ms)
    May  6 08:26:55.020: INFO: (5) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 9.189283ms)
    May  6 08:26:55.020: INFO: (5) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 9.279224ms)
    May  6 08:26:55.021: INFO: (5) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 10.162414ms)
    May  6 08:26:55.022: INFO: (5) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 11.133769ms)
    May  6 08:26:55.022: INFO: (5) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 11.312258ms)
    May  6 08:26:55.022: INFO: (5) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 11.408822ms)
    May  6 08:26:55.023: INFO: (5) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 12.280739ms)
    May  6 08:26:55.024: INFO: (5) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 13.300206ms)
    May  6 08:26:55.025: INFO: (5) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 13.593954ms)
    May  6 08:26:55.025: INFO: (5) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 14.113041ms)
    May  6 08:26:55.026: INFO: (5) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 14.65448ms)
    May  6 08:26:55.026: INFO: (5) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 14.751885ms)
    May  6 08:26:55.027: INFO: (5) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 15.533219ms)
    May  6 08:26:55.032: INFO: (6) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 4.933896ms)
    May  6 08:26:55.032: INFO: (6) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 5.46748ms)
    May  6 08:26:55.033: INFO: (6) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 6.168483ms)
    May  6 08:26:55.034: INFO: (6) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 7.457382ms)
    May  6 08:26:55.034: INFO: (6) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 7.770548ms)
    May  6 08:26:55.036: INFO: (6) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 9.03502ms)
    May  6 08:26:55.037: INFO: (6) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 10.109061ms)
    May  6 08:26:55.037: INFO: (6) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 10.496898ms)
    May  6 08:26:55.038: INFO: (6) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 10.990898ms)
    May  6 08:26:55.039: INFO: (6) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 11.828509ms)
    May  6 08:26:55.040: INFO: (6) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 13.115786ms)
    May  6 08:26:55.040: INFO: (6) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 13.261603ms)
    May  6 08:26:55.041: INFO: (6) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 13.947417ms)
    May  6 08:26:55.041: INFO: (6) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 14.000267ms)
    May  6 08:26:55.041: INFO: (6) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 14.385739ms)
    May  6 08:26:55.042: INFO: (6) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 15.353808ms)
    May  6 08:26:55.052: INFO: (7) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 9.394554ms)
    May  6 08:26:55.052: INFO: (7) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 9.459987ms)
    May  6 08:26:55.053: INFO: (7) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 11.115344ms)
    May  6 08:26:55.054: INFO: (7) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 11.222998ms)
    May  6 08:26:55.054: INFO: (7) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 12.085358ms)
    May  6 08:26:55.055: INFO: (7) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 12.538178ms)
    May  6 08:26:55.055: INFO: (7) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 12.782161ms)
    May  6 08:26:55.056: INFO: (7) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 13.081078ms)
    May  6 08:26:55.056: INFO: (7) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 13.452576ms)
    May  6 08:26:55.056: INFO: (7) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 13.689786ms)
    May  6 08:26:55.059: INFO: (7) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 16.281723ms)
    May  6 08:26:55.059: INFO: (7) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 16.820927ms)
    May  6 08:26:55.060: INFO: (7) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 17.592272ms)
    May  6 08:26:55.061: INFO: (7) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 18.421146ms)
    May  6 08:26:55.063: INFO: (7) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 20.780862ms)
    May  6 08:26:55.064: INFO: (7) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 20.999026ms)
    May  6 08:26:55.070: INFO: (8) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 6.163683ms)
    May  6 08:26:55.070: INFO: (8) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 6.679815ms)
    May  6 08:26:55.073: INFO: (8) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 9.028117ms)
    May  6 08:26:55.073: INFO: (8) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 9.417849ms)
    May  6 08:26:55.075: INFO: (8) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 10.89774ms)
    May  6 08:26:55.076: INFO: (8) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 11.903ms)
    May  6 08:26:55.076: INFO: (8) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 12.293322ms)
    May  6 08:26:55.076: INFO: (8) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 12.309684ms)
    May  6 08:26:55.076: INFO: (8) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 12.339721ms)
    May  6 08:26:55.076: INFO: (8) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 12.387291ms)
    May  6 08:26:55.077: INFO: (8) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 12.563817ms)
    May  6 08:26:55.077: INFO: (8) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 13.050111ms)
    May  6 08:26:55.078: INFO: (8) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 14.021767ms)
    May  6 08:26:55.078: INFO: (8) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 13.958145ms)
    May  6 08:26:55.079: INFO: (8) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 15.021266ms)
    May  6 08:26:55.079: INFO: (8) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 14.718561ms)
    May  6 08:26:55.085: INFO: (9) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 6.329588ms)
    May  6 08:26:55.085: INFO: (9) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 6.576928ms)
    May  6 08:26:55.086: INFO: (9) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 6.863111ms)
    May  6 08:26:55.087: INFO: (9) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 7.860609ms)
    May  6 08:26:55.087: INFO: (9) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 8.132915ms)
    May  6 08:26:55.087: INFO: (9) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 8.051291ms)
    May  6 08:26:55.089: INFO: (9) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 10.15051ms)
    May  6 08:26:55.090: INFO: (9) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 11.100515ms)
    May  6 08:26:55.090: INFO: (9) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 11.285146ms)
    May  6 08:26:55.092: INFO: (9) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 12.758968ms)
    May  6 08:26:55.092: INFO: (9) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 12.869887ms)
    May  6 08:26:55.093: INFO: (9) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 13.486991ms)
    May  6 08:26:55.093: INFO: (9) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 14.478537ms)
    May  6 08:26:55.093: INFO: (9) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 14.406829ms)
    May  6 08:26:55.094: INFO: (9) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 14.928891ms)
    May  6 08:26:55.094: INFO: (9) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 15.361724ms)
    May  6 08:26:55.100: INFO: (10) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 5.766638ms)
    May  6 08:26:55.100: INFO: (10) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 6.040428ms)
    May  6 08:26:55.103: INFO: (10) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 8.678332ms)
    May  6 08:26:55.103: INFO: (10) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 8.692198ms)
    May  6 08:26:55.104: INFO: (10) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 10.060229ms)
    May  6 08:26:55.104: INFO: (10) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 9.994954ms)
    May  6 08:26:55.106: INFO: (10) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 11.595015ms)
    May  6 08:26:55.107: INFO: (10) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 12.263746ms)
    May  6 08:26:55.107: INFO: (10) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 12.709994ms)
    May  6 08:26:55.108: INFO: (10) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 13.880279ms)
    May  6 08:26:55.108: INFO: (10) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 14.113302ms)
    May  6 08:26:55.109: INFO: (10) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 14.241745ms)
    May  6 08:26:55.109: INFO: (10) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 14.655011ms)
    May  6 08:26:55.113: INFO: (10) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 18.425246ms)
    May  6 08:26:55.113: INFO: (10) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 18.437228ms)
    May  6 08:26:55.113: INFO: (10) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 18.467655ms)
    May  6 08:26:55.123: INFO: (11) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 9.927978ms)
    May  6 08:26:55.124: INFO: (11) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 10.757815ms)
    May  6 08:26:55.124: INFO: (11) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 11.017689ms)
    May  6 08:26:55.125: INFO: (11) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 12.131786ms)
    May  6 08:26:55.126: INFO: (11) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 12.485929ms)
    May  6 08:26:55.126: INFO: (11) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 12.869707ms)
    May  6 08:26:55.126: INFO: (11) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 13.202279ms)
    May  6 08:26:55.127: INFO: (11) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 13.826095ms)
    May  6 08:26:55.128: INFO: (11) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 14.436155ms)
    May  6 08:26:55.129: INFO: (11) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 15.483054ms)
    May  6 08:26:55.130: INFO: (11) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 16.514064ms)
    May  6 08:26:55.130: INFO: (11) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 17.122449ms)
    May  6 08:26:55.131: INFO: (11) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 17.672996ms)
    May  6 08:26:55.131: INFO: (11) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 18.044943ms)
    May  6 08:26:55.132: INFO: (11) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 18.670621ms)
    May  6 08:26:55.132: INFO: (11) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 18.98578ms)
    May  6 08:26:55.137: INFO: (12) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 4.945498ms)
    May  6 08:26:55.138: INFO: (12) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 5.838536ms)
    May  6 08:26:55.139: INFO: (12) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 6.53575ms)
    May  6 08:26:55.139: INFO: (12) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 6.755568ms)
    May  6 08:26:55.141: INFO: (12) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 8.085135ms)
    May  6 08:26:55.142: INFO: (12) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 9.473613ms)
    May  6 08:26:55.143: INFO: (12) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 10.451993ms)
    May  6 08:26:55.143: INFO: (12) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 10.702248ms)
    May  6 08:26:55.144: INFO: (12) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 11.806057ms)
    May  6 08:26:55.145: INFO: (12) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 12.708921ms)
    May  6 08:26:55.146: INFO: (12) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 13.320293ms)
    May  6 08:26:55.147: INFO: (12) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 14.444139ms)
    May  6 08:26:55.148: INFO: (12) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 15.352786ms)
    May  6 08:26:55.148: INFO: (12) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 15.415786ms)
    May  6 08:26:55.149: INFO: (12) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 16.080608ms)
    May  6 08:26:55.150: INFO: (12) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 17.851056ms)
    May  6 08:26:55.158: INFO: (13) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 7.676628ms)
    May  6 08:26:55.160: INFO: (13) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 9.278713ms)
    May  6 08:26:55.162: INFO: (13) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 11.352294ms)
    May  6 08:26:55.162: INFO: (13) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 11.385337ms)
    May  6 08:26:55.162: INFO: (13) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 11.633188ms)
    May  6 08:26:55.162: INFO: (13) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 11.599776ms)
    May  6 08:26:55.163: INFO: (13) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 12.49719ms)
    May  6 08:26:55.163: INFO: (13) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 12.496488ms)
    May  6 08:26:55.164: INFO: (13) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 12.98155ms)
    May  6 08:26:55.164: INFO: (13) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 12.942235ms)
    May  6 08:26:55.167: INFO: (13) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 16.351896ms)
    May  6 08:26:55.167: INFO: (13) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 16.242928ms)
    May  6 08:26:55.167: INFO: (13) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 16.333161ms)
    May  6 08:26:55.167: INFO: (13) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 16.661533ms)
    May  6 08:26:55.168: INFO: (13) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 16.804756ms)
    May  6 08:26:55.168: INFO: (13) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 17.231487ms)
    May  6 08:26:55.174: INFO: (14) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 5.574624ms)
    May  6 08:26:55.174: INFO: (14) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 5.636421ms)
    May  6 08:26:55.176: INFO: (14) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 7.401726ms)
    May  6 08:26:55.177: INFO: (14) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 8.929459ms)
    May  6 08:26:55.178: INFO: (14) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 10.114462ms)
    May  6 08:26:55.178: INFO: (14) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 9.993602ms)
    May  6 08:26:55.180: INFO: (14) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 11.380338ms)
    May  6 08:26:55.180: INFO: (14) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 11.908701ms)
    May  6 08:26:55.181: INFO: (14) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 12.772604ms)
    May  6 08:26:55.181: INFO: (14) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 12.799345ms)
    May  6 08:26:55.182: INFO: (14) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 13.273795ms)
    May  6 08:26:55.182: INFO: (14) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 13.626866ms)
    May  6 08:26:55.182: INFO: (14) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 13.917849ms)
    May  6 08:26:55.182: INFO: (14) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 13.926408ms)
    May  6 08:26:55.183: INFO: (14) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 14.667024ms)
    May  6 08:26:55.183: INFO: (14) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 14.812498ms)
    May  6 08:26:55.189: INFO: (15) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 5.623558ms)
    May  6 08:26:55.190: INFO: (15) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 6.404661ms)
    May  6 08:26:55.190: INFO: (15) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 7.261429ms)
    May  6 08:26:55.191: INFO: (15) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 7.424028ms)
    May  6 08:26:55.192: INFO: (15) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 8.396357ms)
    May  6 08:26:55.192: INFO: (15) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 8.58762ms)
    May  6 08:26:55.194: INFO: (15) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 10.12367ms)
    May  6 08:26:55.195: INFO: (15) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 11.246573ms)
    May  6 08:26:55.195: INFO: (15) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 11.444549ms)
    May  6 08:26:55.196: INFO: (15) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 12.089394ms)
    May  6 08:26:55.196: INFO: (15) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 12.141834ms)
    May  6 08:26:55.196: INFO: (15) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 12.268155ms)
    May  6 08:26:55.197: INFO: (15) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 13.976441ms)
    May  6 08:26:55.198: INFO: (15) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 14.837237ms)
    May  6 08:26:55.199: INFO: (15) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 15.159569ms)
    May  6 08:26:55.199: INFO: (15) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 15.995027ms)
    May  6 08:26:55.206: INFO: (16) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 6.049949ms)
    May  6 08:26:55.206: INFO: (16) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 6.06193ms)
    May  6 08:26:55.207: INFO: (16) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 7.36664ms)
    May  6 08:26:55.208: INFO: (16) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 7.77203ms)
    May  6 08:26:55.208: INFO: (16) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 7.952323ms)
    May  6 08:26:55.208: INFO: (16) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 8.64575ms)
    May  6 08:26:55.208: INFO: (16) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 8.891117ms)
    May  6 08:26:55.209: INFO: (16) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 9.07687ms)
    May  6 08:26:55.210: INFO: (16) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 10.373363ms)
    May  6 08:26:55.211: INFO: (16) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 10.920714ms)
    May  6 08:26:55.212: INFO: (16) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 11.815374ms)
    May  6 08:26:55.213: INFO: (16) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 12.922839ms)
    May  6 08:26:55.213: INFO: (16) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 13.047696ms)
    May  6 08:26:55.213: INFO: (16) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 13.67091ms)
    May  6 08:26:55.213: INFO: (16) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 13.830293ms)
    May  6 08:26:55.214: INFO: (16) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 14.552897ms)
    May  6 08:26:55.220: INFO: (17) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 6.106265ms)
    May  6 08:26:55.221: INFO: (17) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 6.315051ms)
    May  6 08:26:55.221: INFO: (17) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 6.709641ms)
    May  6 08:26:55.223: INFO: (17) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 8.578112ms)
    May  6 08:26:55.223: INFO: (17) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 8.601216ms)
    May  6 08:26:55.223: INFO: (17) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 9.031273ms)
    May  6 08:26:55.225: INFO: (17) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 10.086467ms)
    May  6 08:26:55.225: INFO: (17) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 10.326945ms)
    May  6 08:26:55.226: INFO: (17) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 11.930474ms)
    May  6 08:26:55.227: INFO: (17) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 12.221827ms)
    May  6 08:26:55.227: INFO: (17) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 12.184305ms)
    May  6 08:26:55.227: INFO: (17) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 12.584104ms)
    May  6 08:26:55.228: INFO: (17) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 13.921647ms)
    May  6 08:26:55.229: INFO: (17) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 14.140122ms)
    May  6 08:26:55.229: INFO: (17) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 14.747906ms)
    May  6 08:26:55.230: INFO: (17) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 15.511769ms)
    May  6 08:26:55.237: INFO: (18) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 6.848155ms)
    May  6 08:26:55.237: INFO: (18) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 7.207898ms)
    May  6 08:26:55.239: INFO: (18) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 9.180937ms)
    May  6 08:26:55.240: INFO: (18) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 10.246623ms)
    May  6 08:26:55.241: INFO: (18) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 10.730473ms)
    May  6 08:26:55.242: INFO: (18) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 11.777231ms)
    May  6 08:26:55.243: INFO: (18) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 12.526346ms)
    May  6 08:26:55.243: INFO: (18) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 12.867924ms)
    May  6 08:26:55.243: INFO: (18) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 12.890556ms)
    May  6 08:26:55.244: INFO: (18) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 13.452525ms)
    May  6 08:26:55.245: INFO: (18) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 14.483634ms)
    May  6 08:26:55.245: INFO: (18) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 14.432598ms)
    May  6 08:26:55.246: INFO: (18) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 15.299255ms)
    May  6 08:26:55.246: INFO: (18) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 15.629232ms)
    May  6 08:26:55.247: INFO: (18) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 16.183374ms)
    May  6 08:26:55.247: INFO: (18) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 16.477285ms)
    May  6 08:26:55.252: INFO: (19) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 4.624168ms)
    May  6 08:26:55.253: INFO: (19) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:162/proxy/: bar (200; 5.941662ms)
    May  6 08:26:55.256: INFO: (19) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 8.677028ms)
    May  6 08:26:55.257: INFO: (19) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:460/proxy/: tls baz (200; 9.264376ms)
    May  6 08:26:55.258: INFO: (19) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:160/proxy/: foo (200; 9.910975ms)
    May  6 08:26:55.259: INFO: (19) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:462/proxy/: tls qux (200; 11.668254ms)
    May  6 08:26:55.259: INFO: (19) /api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/http:proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">... (200; 11.263825ms)
    May  6 08:26:55.260: INFO: (19) /api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/https:proxy-service-wpvnv-kzpjq:443/proxy/tlsrewritem... (200; 12.820225ms)
    May  6 08:26:55.261: INFO: (19) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq:1080/proxy/rewriteme">test<... (200; 12.730023ms)
    May  6 08:26:55.261: INFO: (19) /api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/: <a href="/api/v1/namespaces/proxy-2437/pods/proxy-service-wpvnv-kzpjq/proxy/rewriteme">test</a> (200; 13.025324ms)
    May  6 08:26:55.262: INFO: (19) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname1/proxy/: foo (200; 14.125383ms)
    May  6 08:26:55.262: INFO: (19) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname2/proxy/: bar (200; 14.29198ms)
    May  6 08:26:55.263: INFO: (19) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname2/proxy/: tls qux (200; 15.197812ms)
    May  6 08:26:55.263: INFO: (19) /api/v1/namespaces/proxy-2437/services/http:proxy-service-wpvnv:portname1/proxy/: foo (200; 15.300498ms)
    May  6 08:26:55.264: INFO: (19) /api/v1/namespaces/proxy-2437/services/https:proxy-service-wpvnv:tlsportname1/proxy/: tls baz (200; 16.704654ms)
    May  6 08:26:55.264: INFO: (19) /api/v1/namespaces/proxy-2437/services/proxy-service-wpvnv:portname2/proxy/: bar (200; 17.033139ms)
    STEP: deleting ReplicationController proxy-service-wpvnv in namespace proxy-2437, will wait for the garbage collector to delete the pods 05/06/23 08:26:55.265
    May  6 08:26:55.323: INFO: Deleting ReplicationController proxy-service-wpvnv took: 5.518958ms
    May  6 08:26:55.423: INFO: Terminating ReplicationController proxy-service-wpvnv pods took: 100.770796ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    May  6 08:26:57.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2437" for this suite. 05/06/23 08:26:57.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:26:57.634
May  6 08:26:57.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename custom-resource-definition 05/06/23 08:26:57.635
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:26:58.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:26:58.652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
May  6 08:26:58.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 08:27:04.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9334" for this suite. 05/06/23 08:27:04.982
------------------------------
â€¢ [SLOW TEST] [7.353 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:26:57.634
    May  6 08:26:57.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename custom-resource-definition 05/06/23 08:26:57.635
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:26:58.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:26:58.652
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    May  6 08:26:58.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 08:27:04.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9334" for this suite. 05/06/23 08:27:04.982
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:27:04.987
May  6 08:27:04.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename emptydir 05/06/23 08:27:04.988
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:27:06.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:27:06.008
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 05/06/23 08:27:06.01
May  6 08:27:06.018: INFO: Waiting up to 5m0s for pod "pod-98e398f7-17f9-46b7-8a3f-9ffc44dc7008" in namespace "emptydir-1678" to be "Succeeded or Failed"
May  6 08:27:06.021: INFO: Pod "pod-98e398f7-17f9-46b7-8a3f-9ffc44dc7008": Phase="Pending", Reason="", readiness=false. Elapsed: 2.822414ms
May  6 08:27:08.025: INFO: Pod "pod-98e398f7-17f9-46b7-8a3f-9ffc44dc7008": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006977202s
May  6 08:27:10.024: INFO: Pod "pod-98e398f7-17f9-46b7-8a3f-9ffc44dc7008": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005868882s
STEP: Saw pod success 05/06/23 08:27:10.024
May  6 08:27:10.024: INFO: Pod "pod-98e398f7-17f9-46b7-8a3f-9ffc44dc7008" satisfied condition "Succeeded or Failed"
May  6 08:27:10.027: INFO: Trying to get logs from node cncf-0 pod pod-98e398f7-17f9-46b7-8a3f-9ffc44dc7008 container test-container: <nil>
STEP: delete the pod 05/06/23 08:27:10.033
May  6 08:27:10.045: INFO: Waiting for pod pod-98e398f7-17f9-46b7-8a3f-9ffc44dc7008 to disappear
May  6 08:27:10.046: INFO: Pod pod-98e398f7-17f9-46b7-8a3f-9ffc44dc7008 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  6 08:27:10.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1678" for this suite. 05/06/23 08:27:10.05
------------------------------
â€¢ [SLOW TEST] [5.068 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:27:04.987
    May  6 08:27:04.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename emptydir 05/06/23 08:27:04.988
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:27:06.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:27:06.008
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 05/06/23 08:27:06.01
    May  6 08:27:06.018: INFO: Waiting up to 5m0s for pod "pod-98e398f7-17f9-46b7-8a3f-9ffc44dc7008" in namespace "emptydir-1678" to be "Succeeded or Failed"
    May  6 08:27:06.021: INFO: Pod "pod-98e398f7-17f9-46b7-8a3f-9ffc44dc7008": Phase="Pending", Reason="", readiness=false. Elapsed: 2.822414ms
    May  6 08:27:08.025: INFO: Pod "pod-98e398f7-17f9-46b7-8a3f-9ffc44dc7008": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006977202s
    May  6 08:27:10.024: INFO: Pod "pod-98e398f7-17f9-46b7-8a3f-9ffc44dc7008": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005868882s
    STEP: Saw pod success 05/06/23 08:27:10.024
    May  6 08:27:10.024: INFO: Pod "pod-98e398f7-17f9-46b7-8a3f-9ffc44dc7008" satisfied condition "Succeeded or Failed"
    May  6 08:27:10.027: INFO: Trying to get logs from node cncf-0 pod pod-98e398f7-17f9-46b7-8a3f-9ffc44dc7008 container test-container: <nil>
    STEP: delete the pod 05/06/23 08:27:10.033
    May  6 08:27:10.045: INFO: Waiting for pod pod-98e398f7-17f9-46b7-8a3f-9ffc44dc7008 to disappear
    May  6 08:27:10.046: INFO: Pod pod-98e398f7-17f9-46b7-8a3f-9ffc44dc7008 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  6 08:27:10.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1678" for this suite. 05/06/23 08:27:10.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:27:10.056
May  6 08:27:10.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename downward-api 05/06/23 08:27:10.057
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:27:11.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:27:11.076
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 05/06/23 08:27:11.078
May  6 08:27:11.085: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54da5c2b-24b2-478f-813d-4b0b92de0ef7" in namespace "downward-api-4372" to be "Succeeded or Failed"
May  6 08:27:11.087: INFO: Pod "downwardapi-volume-54da5c2b-24b2-478f-813d-4b0b92de0ef7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.145328ms
May  6 08:27:13.090: INFO: Pod "downwardapi-volume-54da5c2b-24b2-478f-813d-4b0b92de0ef7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005345605s
May  6 08:27:15.090: INFO: Pod "downwardapi-volume-54da5c2b-24b2-478f-813d-4b0b92de0ef7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005042245s
STEP: Saw pod success 05/06/23 08:27:15.09
May  6 08:27:15.090: INFO: Pod "downwardapi-volume-54da5c2b-24b2-478f-813d-4b0b92de0ef7" satisfied condition "Succeeded or Failed"
May  6 08:27:15.093: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-54da5c2b-24b2-478f-813d-4b0b92de0ef7 container client-container: <nil>
STEP: delete the pod 05/06/23 08:27:15.097
May  6 08:27:15.110: INFO: Waiting for pod downwardapi-volume-54da5c2b-24b2-478f-813d-4b0b92de0ef7 to disappear
May  6 08:27:15.111: INFO: Pod downwardapi-volume-54da5c2b-24b2-478f-813d-4b0b92de0ef7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  6 08:27:15.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4372" for this suite. 05/06/23 08:27:15.114
------------------------------
â€¢ [SLOW TEST] [5.064 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:27:10.056
    May  6 08:27:10.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename downward-api 05/06/23 08:27:10.057
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:27:11.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:27:11.076
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 05/06/23 08:27:11.078
    May  6 08:27:11.085: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54da5c2b-24b2-478f-813d-4b0b92de0ef7" in namespace "downward-api-4372" to be "Succeeded or Failed"
    May  6 08:27:11.087: INFO: Pod "downwardapi-volume-54da5c2b-24b2-478f-813d-4b0b92de0ef7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.145328ms
    May  6 08:27:13.090: INFO: Pod "downwardapi-volume-54da5c2b-24b2-478f-813d-4b0b92de0ef7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005345605s
    May  6 08:27:15.090: INFO: Pod "downwardapi-volume-54da5c2b-24b2-478f-813d-4b0b92de0ef7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005042245s
    STEP: Saw pod success 05/06/23 08:27:15.09
    May  6 08:27:15.090: INFO: Pod "downwardapi-volume-54da5c2b-24b2-478f-813d-4b0b92de0ef7" satisfied condition "Succeeded or Failed"
    May  6 08:27:15.093: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-54da5c2b-24b2-478f-813d-4b0b92de0ef7 container client-container: <nil>
    STEP: delete the pod 05/06/23 08:27:15.097
    May  6 08:27:15.110: INFO: Waiting for pod downwardapi-volume-54da5c2b-24b2-478f-813d-4b0b92de0ef7 to disappear
    May  6 08:27:15.111: INFO: Pod downwardapi-volume-54da5c2b-24b2-478f-813d-4b0b92de0ef7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  6 08:27:15.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4372" for this suite. 05/06/23 08:27:15.114
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:27:15.121
May  6 08:27:15.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename pods 05/06/23 08:27:15.121
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:27:16.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:27:16.139
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 05/06/23 08:27:16.141
May  6 08:27:16.148: INFO: created test-pod-1
May  6 08:27:16.154: INFO: created test-pod-2
May  6 08:27:16.162: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 05/06/23 08:27:16.162
May  6 08:27:16.162: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6889' to be running and ready
May  6 08:27:16.178: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  6 08:27:16.178: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  6 08:27:16.178: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  6 08:27:16.178: INFO: 0 / 3 pods in namespace 'pods-6889' are running and ready (0 seconds elapsed)
May  6 08:27:16.178: INFO: expected 0 pod replicas in namespace 'pods-6889', 0 are Running and Ready.
May  6 08:27:16.178: INFO: POD         NODE    PHASE    GRACE  CONDITIONS
May  6 08:27:16.178: INFO: test-pod-1  cncf-0  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:27:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:27:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:27:16 +0000 UTC  }]
May  6 08:27:16.178: INFO: test-pod-2  cncf-0  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:27:16 +0000 UTC  }]
May  6 08:27:16.178: INFO: test-pod-3  cncf-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:27:16 +0000 UTC  }]
May  6 08:27:16.178: INFO: 
May  6 08:27:18.186: INFO: 3 / 3 pods in namespace 'pods-6889' are running and ready (2 seconds elapsed)
May  6 08:27:18.186: INFO: expected 0 pod replicas in namespace 'pods-6889', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 05/06/23 08:27:18.206
May  6 08:27:18.208: INFO: Pod quantity 3 is different from expected quantity 0
May  6 08:27:19.212: INFO: Pod quantity 3 is different from expected quantity 0
May  6 08:27:20.214: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  6 08:27:21.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6889" for this suite. 05/06/23 08:27:21.215
------------------------------
â€¢ [SLOW TEST] [6.100 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:27:15.121
    May  6 08:27:15.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename pods 05/06/23 08:27:15.121
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:27:16.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:27:16.139
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 05/06/23 08:27:16.141
    May  6 08:27:16.148: INFO: created test-pod-1
    May  6 08:27:16.154: INFO: created test-pod-2
    May  6 08:27:16.162: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 05/06/23 08:27:16.162
    May  6 08:27:16.162: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6889' to be running and ready
    May  6 08:27:16.178: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  6 08:27:16.178: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  6 08:27:16.178: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  6 08:27:16.178: INFO: 0 / 3 pods in namespace 'pods-6889' are running and ready (0 seconds elapsed)
    May  6 08:27:16.178: INFO: expected 0 pod replicas in namespace 'pods-6889', 0 are Running and Ready.
    May  6 08:27:16.178: INFO: POD         NODE    PHASE    GRACE  CONDITIONS
    May  6 08:27:16.178: INFO: test-pod-1  cncf-0  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:27:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:27:16 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:27:16 +0000 UTC  }]
    May  6 08:27:16.178: INFO: test-pod-2  cncf-0  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:27:16 +0000 UTC  }]
    May  6 08:27:16.178: INFO: test-pod-3  cncf-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:27:16 +0000 UTC  }]
    May  6 08:27:16.178: INFO: 
    May  6 08:27:18.186: INFO: 3 / 3 pods in namespace 'pods-6889' are running and ready (2 seconds elapsed)
    May  6 08:27:18.186: INFO: expected 0 pod replicas in namespace 'pods-6889', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 05/06/23 08:27:18.206
    May  6 08:27:18.208: INFO: Pod quantity 3 is different from expected quantity 0
    May  6 08:27:19.212: INFO: Pod quantity 3 is different from expected quantity 0
    May  6 08:27:20.214: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  6 08:27:21.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6889" for this suite. 05/06/23 08:27:21.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:27:21.221
May  6 08:27:21.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename webhook 05/06/23 08:27:21.221
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:27:22.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:27:22.239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/06/23 08:27:24.255
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 08:27:24.431
STEP: Deploying the webhook pod 05/06/23 08:27:24.439
STEP: Wait for the deployment to be ready 05/06/23 08:27:24.449
May  6 08:27:24.453: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/06/23 08:27:26.461
STEP: Verifying the service has paired with the endpoint 05/06/23 08:27:26.473
May  6 08:27:27.473: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 05/06/23 08:27:27.477
STEP: Registering slow webhook via the AdmissionRegistration API 05/06/23 08:27:27.477
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 05/06/23 08:27:28.499
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 05/06/23 08:27:29.507
STEP: Registering slow webhook via the AdmissionRegistration API 05/06/23 08:27:29.507
STEP: Having no error when timeout is longer than webhook latency 05/06/23 08:27:30.527
STEP: Registering slow webhook via the AdmissionRegistration API 05/06/23 08:27:30.527
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 05/06/23 08:27:36.584
STEP: Registering slow webhook via the AdmissionRegistration API 05/06/23 08:27:36.585
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 08:27:41.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2226" for this suite. 05/06/23 08:27:41.647
STEP: Destroying namespace "webhook-2226-markers" for this suite. 05/06/23 08:27:41.661
------------------------------
â€¢ [SLOW TEST] [20.459 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:27:21.221
    May  6 08:27:21.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename webhook 05/06/23 08:27:21.221
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:27:22.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:27:22.239
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/06/23 08:27:24.255
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 08:27:24.431
    STEP: Deploying the webhook pod 05/06/23 08:27:24.439
    STEP: Wait for the deployment to be ready 05/06/23 08:27:24.449
    May  6 08:27:24.453: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/06/23 08:27:26.461
    STEP: Verifying the service has paired with the endpoint 05/06/23 08:27:26.473
    May  6 08:27:27.473: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 05/06/23 08:27:27.477
    STEP: Registering slow webhook via the AdmissionRegistration API 05/06/23 08:27:27.477
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 05/06/23 08:27:28.499
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 05/06/23 08:27:29.507
    STEP: Registering slow webhook via the AdmissionRegistration API 05/06/23 08:27:29.507
    STEP: Having no error when timeout is longer than webhook latency 05/06/23 08:27:30.527
    STEP: Registering slow webhook via the AdmissionRegistration API 05/06/23 08:27:30.527
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 05/06/23 08:27:36.584
    STEP: Registering slow webhook via the AdmissionRegistration API 05/06/23 08:27:36.585
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 08:27:41.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2226" for this suite. 05/06/23 08:27:41.647
    STEP: Destroying namespace "webhook-2226-markers" for this suite. 05/06/23 08:27:41.661
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:27:41.681
May  6 08:27:41.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename secrets 05/06/23 08:27:41.682
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:27:42.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:27:42.707
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-7931c942-c18b-410e-92d4-55aafcc64ebf 05/06/23 08:27:42.71
STEP: Creating a pod to test consume secrets 05/06/23 08:27:42.714
May  6 08:27:42.721: INFO: Waiting up to 5m0s for pod "pod-secrets-8d67e526-c0c2-4031-a75d-f593c0c97539" in namespace "secrets-3512" to be "Succeeded or Failed"
May  6 08:27:42.724: INFO: Pod "pod-secrets-8d67e526-c0c2-4031-a75d-f593c0c97539": Phase="Pending", Reason="", readiness=false. Elapsed: 2.815962ms
May  6 08:27:44.727: INFO: Pod "pod-secrets-8d67e526-c0c2-4031-a75d-f593c0c97539": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005863069s
May  6 08:27:46.728: INFO: Pod "pod-secrets-8d67e526-c0c2-4031-a75d-f593c0c97539": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007567668s
STEP: Saw pod success 05/06/23 08:27:46.729
May  6 08:27:46.729: INFO: Pod "pod-secrets-8d67e526-c0c2-4031-a75d-f593c0c97539" satisfied condition "Succeeded or Failed"
May  6 08:27:46.731: INFO: Trying to get logs from node cncf-0 pod pod-secrets-8d67e526-c0c2-4031-a75d-f593c0c97539 container secret-volume-test: <nil>
STEP: delete the pod 05/06/23 08:27:46.737
May  6 08:27:46.752: INFO: Waiting for pod pod-secrets-8d67e526-c0c2-4031-a75d-f593c0c97539 to disappear
May  6 08:27:46.754: INFO: Pod pod-secrets-8d67e526-c0c2-4031-a75d-f593c0c97539 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May  6 08:27:46.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3512" for this suite. 05/06/23 08:27:46.758
------------------------------
â€¢ [SLOW TEST] [5.083 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:27:41.681
    May  6 08:27:41.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename secrets 05/06/23 08:27:41.682
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:27:42.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:27:42.707
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-7931c942-c18b-410e-92d4-55aafcc64ebf 05/06/23 08:27:42.71
    STEP: Creating a pod to test consume secrets 05/06/23 08:27:42.714
    May  6 08:27:42.721: INFO: Waiting up to 5m0s for pod "pod-secrets-8d67e526-c0c2-4031-a75d-f593c0c97539" in namespace "secrets-3512" to be "Succeeded or Failed"
    May  6 08:27:42.724: INFO: Pod "pod-secrets-8d67e526-c0c2-4031-a75d-f593c0c97539": Phase="Pending", Reason="", readiness=false. Elapsed: 2.815962ms
    May  6 08:27:44.727: INFO: Pod "pod-secrets-8d67e526-c0c2-4031-a75d-f593c0c97539": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005863069s
    May  6 08:27:46.728: INFO: Pod "pod-secrets-8d67e526-c0c2-4031-a75d-f593c0c97539": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007567668s
    STEP: Saw pod success 05/06/23 08:27:46.729
    May  6 08:27:46.729: INFO: Pod "pod-secrets-8d67e526-c0c2-4031-a75d-f593c0c97539" satisfied condition "Succeeded or Failed"
    May  6 08:27:46.731: INFO: Trying to get logs from node cncf-0 pod pod-secrets-8d67e526-c0c2-4031-a75d-f593c0c97539 container secret-volume-test: <nil>
    STEP: delete the pod 05/06/23 08:27:46.737
    May  6 08:27:46.752: INFO: Waiting for pod pod-secrets-8d67e526-c0c2-4031-a75d-f593c0c97539 to disappear
    May  6 08:27:46.754: INFO: Pod pod-secrets-8d67e526-c0c2-4031-a75d-f593c0c97539 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May  6 08:27:46.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3512" for this suite. 05/06/23 08:27:46.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:27:46.765
May  6 08:27:46.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename configmap 05/06/23 08:27:46.766
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:27:47.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:27:47.784
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-292b99ca-3667-467d-a3ec-19c9d447e521 05/06/23 08:27:47.786
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  6 08:27:47.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-685" for this suite. 05/06/23 08:27:47.79
------------------------------
â€¢ [1.030 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:27:46.765
    May  6 08:27:46.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename configmap 05/06/23 08:27:46.766
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:27:47.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:27:47.784
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-292b99ca-3667-467d-a3ec-19c9d447e521 05/06/23 08:27:47.786
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  6 08:27:47.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-685" for this suite. 05/06/23 08:27:47.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:27:47.796
May  6 08:27:47.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename taint-multiple-pods 05/06/23 08:27:47.796
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:27:47.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:27:47.813
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
May  6 08:27:47.814: INFO: Waiting up to 1m0s for all nodes to be ready
May  6 08:28:47.835: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
May  6 08:28:47.838: INFO: Starting informer...
STEP: Starting pods... 05/06/23 08:28:47.838
May  6 08:28:48.053: INFO: Pod1 is running on cncf-0. Tainting Node
May  6 08:28:48.262: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8336" to be "running"
May  6 08:28:48.265: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.391655ms
May  6 08:28:50.268: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006098165s
May  6 08:28:50.268: INFO: Pod "taint-eviction-b1" satisfied condition "running"
May  6 08:28:50.268: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8336" to be "running"
May  6 08:28:50.271: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.610531ms
May  6 08:28:50.271: INFO: Pod "taint-eviction-b2" satisfied condition "running"
May  6 08:28:50.271: INFO: Pod2 is running on cncf-0. Tainting Node
STEP: Trying to apply a taint on the Node 05/06/23 08:28:50.271
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/06/23 08:28:50.282
STEP: Waiting for Pod1 and Pod2 to be deleted 05/06/23 08:28:50.285
May  6 08:28:55.783: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May  6 08:29:15.810: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/06/23 08:29:15.821
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 08:29:15.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-8336" for this suite. 05/06/23 08:29:15.828
------------------------------
â€¢ [SLOW TEST] [88.040 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:27:47.796
    May  6 08:27:47.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename taint-multiple-pods 05/06/23 08:27:47.796
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:27:47.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:27:47.813
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    May  6 08:27:47.814: INFO: Waiting up to 1m0s for all nodes to be ready
    May  6 08:28:47.835: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    May  6 08:28:47.838: INFO: Starting informer...
    STEP: Starting pods... 05/06/23 08:28:47.838
    May  6 08:28:48.053: INFO: Pod1 is running on cncf-0. Tainting Node
    May  6 08:28:48.262: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8336" to be "running"
    May  6 08:28:48.265: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.391655ms
    May  6 08:28:50.268: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006098165s
    May  6 08:28:50.268: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    May  6 08:28:50.268: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8336" to be "running"
    May  6 08:28:50.271: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.610531ms
    May  6 08:28:50.271: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    May  6 08:28:50.271: INFO: Pod2 is running on cncf-0. Tainting Node
    STEP: Trying to apply a taint on the Node 05/06/23 08:28:50.271
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/06/23 08:28:50.282
    STEP: Waiting for Pod1 and Pod2 to be deleted 05/06/23 08:28:50.285
    May  6 08:28:55.783: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    May  6 08:29:15.810: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/06/23 08:29:15.821
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 08:29:15.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-8336" for this suite. 05/06/23 08:29:15.828
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:29:15.836
May  6 08:29:15.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 08:29:15.837
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:29:15.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:29:15.854
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-7760336b-8a2c-4fae-9766-a7ef0773ce91 05/06/23 08:29:15.856
STEP: Creating a pod to test consume configMaps 05/06/23 08:29:15.86
May  6 08:29:15.866: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-37d85c36-91d4-4f08-9806-3eb57ad35a73" in namespace "projected-3082" to be "Succeeded or Failed"
May  6 08:29:15.869: INFO: Pod "pod-projected-configmaps-37d85c36-91d4-4f08-9806-3eb57ad35a73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.496124ms
May  6 08:29:17.872: INFO: Pod "pod-projected-configmaps-37d85c36-91d4-4f08-9806-3eb57ad35a73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005758215s
May  6 08:29:19.872: INFO: Pod "pod-projected-configmaps-37d85c36-91d4-4f08-9806-3eb57ad35a73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005464849s
STEP: Saw pod success 05/06/23 08:29:19.872
May  6 08:29:19.872: INFO: Pod "pod-projected-configmaps-37d85c36-91d4-4f08-9806-3eb57ad35a73" satisfied condition "Succeeded or Failed"
May  6 08:29:19.875: INFO: Trying to get logs from node cncf-0 pod pod-projected-configmaps-37d85c36-91d4-4f08-9806-3eb57ad35a73 container agnhost-container: <nil>
STEP: delete the pod 05/06/23 08:29:19.883
May  6 08:29:19.893: INFO: Waiting for pod pod-projected-configmaps-37d85c36-91d4-4f08-9806-3eb57ad35a73 to disappear
May  6 08:29:19.895: INFO: Pod pod-projected-configmaps-37d85c36-91d4-4f08-9806-3eb57ad35a73 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May  6 08:29:19.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3082" for this suite. 05/06/23 08:29:19.897
------------------------------
â€¢ [4.066 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:29:15.836
    May  6 08:29:15.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 08:29:15.837
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:29:15.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:29:15.854
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-7760336b-8a2c-4fae-9766-a7ef0773ce91 05/06/23 08:29:15.856
    STEP: Creating a pod to test consume configMaps 05/06/23 08:29:15.86
    May  6 08:29:15.866: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-37d85c36-91d4-4f08-9806-3eb57ad35a73" in namespace "projected-3082" to be "Succeeded or Failed"
    May  6 08:29:15.869: INFO: Pod "pod-projected-configmaps-37d85c36-91d4-4f08-9806-3eb57ad35a73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.496124ms
    May  6 08:29:17.872: INFO: Pod "pod-projected-configmaps-37d85c36-91d4-4f08-9806-3eb57ad35a73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005758215s
    May  6 08:29:19.872: INFO: Pod "pod-projected-configmaps-37d85c36-91d4-4f08-9806-3eb57ad35a73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005464849s
    STEP: Saw pod success 05/06/23 08:29:19.872
    May  6 08:29:19.872: INFO: Pod "pod-projected-configmaps-37d85c36-91d4-4f08-9806-3eb57ad35a73" satisfied condition "Succeeded or Failed"
    May  6 08:29:19.875: INFO: Trying to get logs from node cncf-0 pod pod-projected-configmaps-37d85c36-91d4-4f08-9806-3eb57ad35a73 container agnhost-container: <nil>
    STEP: delete the pod 05/06/23 08:29:19.883
    May  6 08:29:19.893: INFO: Waiting for pod pod-projected-configmaps-37d85c36-91d4-4f08-9806-3eb57ad35a73 to disappear
    May  6 08:29:19.895: INFO: Pod pod-projected-configmaps-37d85c36-91d4-4f08-9806-3eb57ad35a73 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May  6 08:29:19.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3082" for this suite. 05/06/23 08:29:19.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:29:19.903
May  6 08:29:19.903: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename pods 05/06/23 08:29:19.903
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:29:19.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:29:19.918
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 05/06/23 08:29:19.924
STEP: watching for Pod to be ready 05/06/23 08:29:19.931
May  6 08:29:19.932: INFO: observed Pod pod-test in namespace pods-8119 in phase Pending with labels: map[test-pod-static:true] & conditions []
May  6 08:29:19.934: INFO: observed Pod pod-test in namespace pods-8119 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC  }]
May  6 08:29:19.945: INFO: observed Pod pod-test in namespace pods-8119 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC  }]
May  6 08:29:20.405: INFO: observed Pod pod-test in namespace pods-8119 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC  }]
May  6 08:29:20.808: INFO: Found Pod pod-test in namespace pods-8119 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 05/06/23 08:29:20.811
STEP: getting the Pod and ensuring that it's patched 05/06/23 08:29:20.823
STEP: replacing the Pod's status Ready condition to False 05/06/23 08:29:20.825
STEP: check the Pod again to ensure its Ready conditions are False 05/06/23 08:29:20.834
STEP: deleting the Pod via a Collection with a LabelSelector 05/06/23 08:29:20.834
STEP: watching for the Pod to be deleted 05/06/23 08:29:20.842
May  6 08:29:20.843: INFO: observed event type MODIFIED
May  6 08:29:21.530: INFO: observed event type MODIFIED
May  6 08:29:22.945: INFO: observed event type MODIFIED
May  6 08:29:23.819: INFO: observed event type MODIFIED
May  6 08:29:23.827: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May  6 08:29:23.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8119" for this suite. 05/06/23 08:29:23.836
------------------------------
â€¢ [3.939 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:29:19.903
    May  6 08:29:19.903: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename pods 05/06/23 08:29:19.903
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:29:19.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:29:19.918
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 05/06/23 08:29:19.924
    STEP: watching for Pod to be ready 05/06/23 08:29:19.931
    May  6 08:29:19.932: INFO: observed Pod pod-test in namespace pods-8119 in phase Pending with labels: map[test-pod-static:true] & conditions []
    May  6 08:29:19.934: INFO: observed Pod pod-test in namespace pods-8119 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC  }]
    May  6 08:29:19.945: INFO: observed Pod pod-test in namespace pods-8119 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC  }]
    May  6 08:29:20.405: INFO: observed Pod pod-test in namespace pods-8119 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC  }]
    May  6 08:29:20.808: INFO: Found Pod pod-test in namespace pods-8119 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:19 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 05/06/23 08:29:20.811
    STEP: getting the Pod and ensuring that it's patched 05/06/23 08:29:20.823
    STEP: replacing the Pod's status Ready condition to False 05/06/23 08:29:20.825
    STEP: check the Pod again to ensure its Ready conditions are False 05/06/23 08:29:20.834
    STEP: deleting the Pod via a Collection with a LabelSelector 05/06/23 08:29:20.834
    STEP: watching for the Pod to be deleted 05/06/23 08:29:20.842
    May  6 08:29:20.843: INFO: observed event type MODIFIED
    May  6 08:29:21.530: INFO: observed event type MODIFIED
    May  6 08:29:22.945: INFO: observed event type MODIFIED
    May  6 08:29:23.819: INFO: observed event type MODIFIED
    May  6 08:29:23.827: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May  6 08:29:23.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8119" for this suite. 05/06/23 08:29:23.836
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:29:23.843
May  6 08:29:23.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 08:29:23.844
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:29:23.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:29:23.864
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-b33f4692-c82c-42cc-b769-2c929145f942 05/06/23 08:29:23.866
STEP: Creating a pod to test consume configMaps 05/06/23 08:29:23.87
May  6 08:29:23.877: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-68f80c07-421d-4f7d-b98e-64ce7de18491" in namespace "projected-9770" to be "Succeeded or Failed"
May  6 08:29:23.879: INFO: Pod "pod-projected-configmaps-68f80c07-421d-4f7d-b98e-64ce7de18491": Phase="Pending", Reason="", readiness=false. Elapsed: 2.12562ms
May  6 08:29:25.883: INFO: Pod "pod-projected-configmaps-68f80c07-421d-4f7d-b98e-64ce7de18491": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005978946s
May  6 08:29:27.884: INFO: Pod "pod-projected-configmaps-68f80c07-421d-4f7d-b98e-64ce7de18491": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006252706s
STEP: Saw pod success 05/06/23 08:29:27.884
May  6 08:29:27.884: INFO: Pod "pod-projected-configmaps-68f80c07-421d-4f7d-b98e-64ce7de18491" satisfied condition "Succeeded or Failed"
May  6 08:29:27.887: INFO: Trying to get logs from node cncf-0 pod pod-projected-configmaps-68f80c07-421d-4f7d-b98e-64ce7de18491 container agnhost-container: <nil>
STEP: delete the pod 05/06/23 08:29:27.891
May  6 08:29:27.903: INFO: Waiting for pod pod-projected-configmaps-68f80c07-421d-4f7d-b98e-64ce7de18491 to disappear
May  6 08:29:27.906: INFO: Pod pod-projected-configmaps-68f80c07-421d-4f7d-b98e-64ce7de18491 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May  6 08:29:27.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9770" for this suite. 05/06/23 08:29:27.909
------------------------------
â€¢ [4.071 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:29:23.843
    May  6 08:29:23.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 08:29:23.844
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:29:23.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:29:23.864
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-b33f4692-c82c-42cc-b769-2c929145f942 05/06/23 08:29:23.866
    STEP: Creating a pod to test consume configMaps 05/06/23 08:29:23.87
    May  6 08:29:23.877: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-68f80c07-421d-4f7d-b98e-64ce7de18491" in namespace "projected-9770" to be "Succeeded or Failed"
    May  6 08:29:23.879: INFO: Pod "pod-projected-configmaps-68f80c07-421d-4f7d-b98e-64ce7de18491": Phase="Pending", Reason="", readiness=false. Elapsed: 2.12562ms
    May  6 08:29:25.883: INFO: Pod "pod-projected-configmaps-68f80c07-421d-4f7d-b98e-64ce7de18491": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005978946s
    May  6 08:29:27.884: INFO: Pod "pod-projected-configmaps-68f80c07-421d-4f7d-b98e-64ce7de18491": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006252706s
    STEP: Saw pod success 05/06/23 08:29:27.884
    May  6 08:29:27.884: INFO: Pod "pod-projected-configmaps-68f80c07-421d-4f7d-b98e-64ce7de18491" satisfied condition "Succeeded or Failed"
    May  6 08:29:27.887: INFO: Trying to get logs from node cncf-0 pod pod-projected-configmaps-68f80c07-421d-4f7d-b98e-64ce7de18491 container agnhost-container: <nil>
    STEP: delete the pod 05/06/23 08:29:27.891
    May  6 08:29:27.903: INFO: Waiting for pod pod-projected-configmaps-68f80c07-421d-4f7d-b98e-64ce7de18491 to disappear
    May  6 08:29:27.906: INFO: Pod pod-projected-configmaps-68f80c07-421d-4f7d-b98e-64ce7de18491 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May  6 08:29:27.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9770" for this suite. 05/06/23 08:29:27.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:29:27.914
May  6 08:29:27.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename statefulset 05/06/23 08:29:27.915
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:29:27.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:29:27.932
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1360 05/06/23 08:29:27.935
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-1360 05/06/23 08:29:27.939
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1360 05/06/23 08:29:27.946
May  6 08:29:27.948: INFO: Found 0 stateful pods, waiting for 1
May  6 08:29:37.951: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 05/06/23 08:29:37.951
May  6 08:29:37.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-1360 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 08:29:38.063: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 08:29:38.063: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 08:29:38.063: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 08:29:38.065: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  6 08:29:48.070: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  6 08:29:48.070: INFO: Waiting for statefulset status.replicas updated to 0
May  6 08:29:48.083: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
May  6 08:29:48.083: INFO: ss-0  cncf-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:27 +0000 UTC  }]
May  6 08:29:48.083: INFO: 
May  6 08:29:48.083: INFO: StatefulSet ss has not reached scale 3, at 1
May  6 08:29:49.087: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997760136s
May  6 08:29:50.091: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993251761s
May  6 08:29:51.094: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990335249s
May  6 08:29:52.097: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986702077s
May  6 08:29:53.101: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.983766541s
May  6 08:29:54.104: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980413843s
May  6 08:29:55.108: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.977138671s
May  6 08:29:56.110: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.973544783s
May  6 08:29:57.114: INFO: Verifying statefulset ss doesn't scale past 3 for another 970.831588ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1360 05/06/23 08:29:58.114
May  6 08:29:58.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-1360 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 08:29:58.229: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  6 08:29:58.229: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 08:29:58.229: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 08:29:58.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-1360 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 08:29:58.333: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May  6 08:29:58.333: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 08:29:58.333: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 08:29:58.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-1360 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  6 08:29:58.429: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May  6 08:29:58.429: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  6 08:29:58.429: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  6 08:29:58.433: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  6 08:29:58.433: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  6 08:29:58.433: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 05/06/23 08:29:58.433
May  6 08:29:58.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-1360 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 08:29:58.534: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 08:29:58.534: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 08:29:58.534: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 08:29:58.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-1360 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 08:29:58.640: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 08:29:58.640: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 08:29:58.640: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 08:29:58.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-1360 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  6 08:29:58.735: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  6 08:29:58.735: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  6 08:29:58.735: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  6 08:29:58.735: INFO: Waiting for statefulset status.replicas updated to 0
May  6 08:29:58.738: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May  6 08:30:08.745: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  6 08:30:08.745: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  6 08:30:08.745: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  6 08:30:08.759: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
May  6 08:30:08.759: INFO: ss-0  cncf-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:27 +0000 UTC  }]
May  6 08:30:08.759: INFO: ss-1  cncf-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:48 +0000 UTC  }]
May  6 08:30:08.759: INFO: ss-2  cncf-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:48 +0000 UTC  }]
May  6 08:30:08.759: INFO: 
May  6 08:30:08.759: INFO: StatefulSet ss has not reached scale 0, at 3
May  6 08:30:09.764: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
May  6 08:30:09.764: INFO: ss-0  cncf-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:27 +0000 UTC  }]
May  6 08:30:09.764: INFO: ss-1  cncf-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:48 +0000 UTC  }]
May  6 08:30:09.764: INFO: 
May  6 08:30:09.764: INFO: StatefulSet ss has not reached scale 0, at 2
May  6 08:30:10.766: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993025502s
May  6 08:30:11.770: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.989614792s
May  6 08:30:12.773: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.986376071s
May  6 08:30:13.776: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.983818792s
May  6 08:30:14.779: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.980669137s
May  6 08:30:15.781: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.978027369s
May  6 08:30:16.784: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.975228533s
May  6 08:30:17.788: INFO: Verifying statefulset ss doesn't scale past 0 for another 971.591731ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1360 05/06/23 08:30:18.788
May  6 08:30:18.792: INFO: Scaling statefulset ss to 0
May  6 08:30:18.800: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May  6 08:30:18.802: INFO: Deleting all statefulset in ns statefulset-1360
May  6 08:30:18.804: INFO: Scaling statefulset ss to 0
May  6 08:30:18.811: INFO: Waiting for statefulset status.replicas updated to 0
May  6 08:30:18.813: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May  6 08:30:18.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1360" for this suite. 05/06/23 08:30:18.825
------------------------------
â€¢ [SLOW TEST] [50.916 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:29:27.914
    May  6 08:29:27.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename statefulset 05/06/23 08:29:27.915
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:29:27.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:29:27.932
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1360 05/06/23 08:29:27.935
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-1360 05/06/23 08:29:27.939
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1360 05/06/23 08:29:27.946
    May  6 08:29:27.948: INFO: Found 0 stateful pods, waiting for 1
    May  6 08:29:37.951: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 05/06/23 08:29:37.951
    May  6 08:29:37.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-1360 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  6 08:29:38.063: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  6 08:29:38.063: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  6 08:29:38.063: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  6 08:29:38.065: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    May  6 08:29:48.070: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  6 08:29:48.070: INFO: Waiting for statefulset status.replicas updated to 0
    May  6 08:29:48.083: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
    May  6 08:29:48.083: INFO: ss-0  cncf-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:27 +0000 UTC  }]
    May  6 08:29:48.083: INFO: 
    May  6 08:29:48.083: INFO: StatefulSet ss has not reached scale 3, at 1
    May  6 08:29:49.087: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997760136s
    May  6 08:29:50.091: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993251761s
    May  6 08:29:51.094: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990335249s
    May  6 08:29:52.097: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986702077s
    May  6 08:29:53.101: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.983766541s
    May  6 08:29:54.104: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980413843s
    May  6 08:29:55.108: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.977138671s
    May  6 08:29:56.110: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.973544783s
    May  6 08:29:57.114: INFO: Verifying statefulset ss doesn't scale past 3 for another 970.831588ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1360 05/06/23 08:29:58.114
    May  6 08:29:58.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-1360 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  6 08:29:58.229: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  6 08:29:58.229: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  6 08:29:58.229: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  6 08:29:58.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-1360 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  6 08:29:58.333: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    May  6 08:29:58.333: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  6 08:29:58.333: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  6 08:29:58.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-1360 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  6 08:29:58.429: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    May  6 08:29:58.429: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  6 08:29:58.429: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  6 08:29:58.433: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May  6 08:29:58.433: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    May  6 08:29:58.433: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 05/06/23 08:29:58.433
    May  6 08:29:58.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-1360 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  6 08:29:58.534: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  6 08:29:58.534: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  6 08:29:58.534: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  6 08:29:58.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-1360 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  6 08:29:58.640: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  6 08:29:58.640: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  6 08:29:58.640: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  6 08:29:58.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=statefulset-1360 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  6 08:29:58.735: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  6 08:29:58.735: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  6 08:29:58.735: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  6 08:29:58.735: INFO: Waiting for statefulset status.replicas updated to 0
    May  6 08:29:58.738: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    May  6 08:30:08.745: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  6 08:30:08.745: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    May  6 08:30:08.745: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    May  6 08:30:08.759: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
    May  6 08:30:08.759: INFO: ss-0  cncf-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:27 +0000 UTC  }]
    May  6 08:30:08.759: INFO: ss-1  cncf-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:48 +0000 UTC  }]
    May  6 08:30:08.759: INFO: ss-2  cncf-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:48 +0000 UTC  }]
    May  6 08:30:08.759: INFO: 
    May  6 08:30:08.759: INFO: StatefulSet ss has not reached scale 0, at 3
    May  6 08:30:09.764: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
    May  6 08:30:09.764: INFO: ss-0  cncf-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:27 +0000 UTC  }]
    May  6 08:30:09.764: INFO: ss-1  cncf-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-06 08:29:48 +0000 UTC  }]
    May  6 08:30:09.764: INFO: 
    May  6 08:30:09.764: INFO: StatefulSet ss has not reached scale 0, at 2
    May  6 08:30:10.766: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993025502s
    May  6 08:30:11.770: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.989614792s
    May  6 08:30:12.773: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.986376071s
    May  6 08:30:13.776: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.983818792s
    May  6 08:30:14.779: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.980669137s
    May  6 08:30:15.781: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.978027369s
    May  6 08:30:16.784: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.975228533s
    May  6 08:30:17.788: INFO: Verifying statefulset ss doesn't scale past 0 for another 971.591731ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1360 05/06/23 08:30:18.788
    May  6 08:30:18.792: INFO: Scaling statefulset ss to 0
    May  6 08:30:18.800: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May  6 08:30:18.802: INFO: Deleting all statefulset in ns statefulset-1360
    May  6 08:30:18.804: INFO: Scaling statefulset ss to 0
    May  6 08:30:18.811: INFO: Waiting for statefulset status.replicas updated to 0
    May  6 08:30:18.813: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May  6 08:30:18.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1360" for this suite. 05/06/23 08:30:18.825
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:30:18.83
May  6 08:30:18.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename webhook 05/06/23 08:30:18.831
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:30:18.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:30:18.863
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/06/23 08:30:18.879
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 08:30:19.343
STEP: Deploying the webhook pod 05/06/23 08:30:19.349
STEP: Wait for the deployment to be ready 05/06/23 08:30:19.366
May  6 08:30:19.371: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/06/23 08:30:21.38
STEP: Verifying the service has paired with the endpoint 05/06/23 08:30:21.406
May  6 08:30:22.406: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 05/06/23 08:30:22.408
STEP: create a configmap that should be updated by the webhook 05/06/23 08:30:23.443
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 08:30:24.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5237" for this suite. 05/06/23 08:30:24.523
STEP: Destroying namespace "webhook-5237-markers" for this suite. 05/06/23 08:30:24.531
------------------------------
â€¢ [SLOW TEST] [5.709 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:30:18.83
    May  6 08:30:18.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename webhook 05/06/23 08:30:18.831
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:30:18.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:30:18.863
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/06/23 08:30:18.879
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 08:30:19.343
    STEP: Deploying the webhook pod 05/06/23 08:30:19.349
    STEP: Wait for the deployment to be ready 05/06/23 08:30:19.366
    May  6 08:30:19.371: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/06/23 08:30:21.38
    STEP: Verifying the service has paired with the endpoint 05/06/23 08:30:21.406
    May  6 08:30:22.406: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 05/06/23 08:30:22.408
    STEP: create a configmap that should be updated by the webhook 05/06/23 08:30:23.443
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 08:30:24.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5237" for this suite. 05/06/23 08:30:24.523
    STEP: Destroying namespace "webhook-5237-markers" for this suite. 05/06/23 08:30:24.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:30:24.54
May  6 08:30:24.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename replication-controller 05/06/23 08:30:24.541
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:30:24.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:30:24.563
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c 05/06/23 08:30:24.565
May  6 08:30:24.572: INFO: Pod name my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c: Found 0 pods out of 1
May  6 08:30:29.578: INFO: Pod name my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c: Found 1 pods out of 1
May  6 08:30:29.578: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c" are running
May  6 08:30:29.578: INFO: Waiting up to 5m0s for pod "my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c-bwmvf" in namespace "replication-controller-2360" to be "running"
May  6 08:30:29.581: INFO: Pod "my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c-bwmvf": Phase="Running", Reason="", readiness=true. Elapsed: 2.753062ms
May  6 08:30:29.581: INFO: Pod "my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c-bwmvf" satisfied condition "running"
May  6 08:30:29.581: INFO: Pod "my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c-bwmvf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-06 08:30:24 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-06 08:30:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-06 08:30:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-06 08:30:24 +0000 UTC Reason: Message:}])
May  6 08:30:29.581: INFO: Trying to dial the pod
May  6 08:30:34.591: INFO: Controller my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c: Got expected result from replica 1 [my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c-bwmvf]: "my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c-bwmvf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May  6 08:30:34.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2360" for this suite. 05/06/23 08:30:34.594
------------------------------
â€¢ [SLOW TEST] [10.060 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:30:24.54
    May  6 08:30:24.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename replication-controller 05/06/23 08:30:24.541
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:30:24.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:30:24.563
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c 05/06/23 08:30:24.565
    May  6 08:30:24.572: INFO: Pod name my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c: Found 0 pods out of 1
    May  6 08:30:29.578: INFO: Pod name my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c: Found 1 pods out of 1
    May  6 08:30:29.578: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c" are running
    May  6 08:30:29.578: INFO: Waiting up to 5m0s for pod "my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c-bwmvf" in namespace "replication-controller-2360" to be "running"
    May  6 08:30:29.581: INFO: Pod "my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c-bwmvf": Phase="Running", Reason="", readiness=true. Elapsed: 2.753062ms
    May  6 08:30:29.581: INFO: Pod "my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c-bwmvf" satisfied condition "running"
    May  6 08:30:29.581: INFO: Pod "my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c-bwmvf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-06 08:30:24 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-06 08:30:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-06 08:30:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-06 08:30:24 +0000 UTC Reason: Message:}])
    May  6 08:30:29.581: INFO: Trying to dial the pod
    May  6 08:30:34.591: INFO: Controller my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c: Got expected result from replica 1 [my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c-bwmvf]: "my-hostname-basic-500c0e69-da22-434c-8a7d-9df1fdd00c6c-bwmvf", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May  6 08:30:34.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2360" for this suite. 05/06/23 08:30:34.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:30:34.601
May  6 08:30:34.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename var-expansion 05/06/23 08:30:34.602
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:30:34.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:30:34.617
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 05/06/23 08:30:34.619
May  6 08:30:34.626: INFO: Waiting up to 5m0s for pod "var-expansion-70c10e78-acef-4add-9d72-11a89e967032" in namespace "var-expansion-1102" to be "Succeeded or Failed"
May  6 08:30:34.628: INFO: Pod "var-expansion-70c10e78-acef-4add-9d72-11a89e967032": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069974ms
May  6 08:30:36.631: INFO: Pod "var-expansion-70c10e78-acef-4add-9d72-11a89e967032": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004889468s
May  6 08:30:38.631: INFO: Pod "var-expansion-70c10e78-acef-4add-9d72-11a89e967032": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00515462s
STEP: Saw pod success 05/06/23 08:30:38.631
May  6 08:30:38.631: INFO: Pod "var-expansion-70c10e78-acef-4add-9d72-11a89e967032" satisfied condition "Succeeded or Failed"
May  6 08:30:38.633: INFO: Trying to get logs from node cncf-0 pod var-expansion-70c10e78-acef-4add-9d72-11a89e967032 container dapi-container: <nil>
STEP: delete the pod 05/06/23 08:30:38.638
May  6 08:30:38.649: INFO: Waiting for pod var-expansion-70c10e78-acef-4add-9d72-11a89e967032 to disappear
May  6 08:30:38.651: INFO: Pod var-expansion-70c10e78-acef-4add-9d72-11a89e967032 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May  6 08:30:38.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1102" for this suite. 05/06/23 08:30:38.654
------------------------------
â€¢ [4.058 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:30:34.601
    May  6 08:30:34.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename var-expansion 05/06/23 08:30:34.602
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:30:34.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:30:34.617
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 05/06/23 08:30:34.619
    May  6 08:30:34.626: INFO: Waiting up to 5m0s for pod "var-expansion-70c10e78-acef-4add-9d72-11a89e967032" in namespace "var-expansion-1102" to be "Succeeded or Failed"
    May  6 08:30:34.628: INFO: Pod "var-expansion-70c10e78-acef-4add-9d72-11a89e967032": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069974ms
    May  6 08:30:36.631: INFO: Pod "var-expansion-70c10e78-acef-4add-9d72-11a89e967032": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004889468s
    May  6 08:30:38.631: INFO: Pod "var-expansion-70c10e78-acef-4add-9d72-11a89e967032": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00515462s
    STEP: Saw pod success 05/06/23 08:30:38.631
    May  6 08:30:38.631: INFO: Pod "var-expansion-70c10e78-acef-4add-9d72-11a89e967032" satisfied condition "Succeeded or Failed"
    May  6 08:30:38.633: INFO: Trying to get logs from node cncf-0 pod var-expansion-70c10e78-acef-4add-9d72-11a89e967032 container dapi-container: <nil>
    STEP: delete the pod 05/06/23 08:30:38.638
    May  6 08:30:38.649: INFO: Waiting for pod var-expansion-70c10e78-acef-4add-9d72-11a89e967032 to disappear
    May  6 08:30:38.651: INFO: Pod var-expansion-70c10e78-acef-4add-9d72-11a89e967032 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May  6 08:30:38.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1102" for this suite. 05/06/23 08:30:38.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:30:38.66
May  6 08:30:38.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename downward-api 05/06/23 08:30:38.66
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:30:38.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:30:38.678
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 05/06/23 08:30:38.68
May  6 08:30:38.687: INFO: Waiting up to 5m0s for pod "downward-api-7562198e-4ed1-43e7-8fb3-bbd315966e60" in namespace "downward-api-5220" to be "Succeeded or Failed"
May  6 08:30:38.689: INFO: Pod "downward-api-7562198e-4ed1-43e7-8fb3-bbd315966e60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.450919ms
May  6 08:30:40.694: INFO: Pod "downward-api-7562198e-4ed1-43e7-8fb3-bbd315966e60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00711019s
May  6 08:30:42.693: INFO: Pod "downward-api-7562198e-4ed1-43e7-8fb3-bbd315966e60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006654349s
STEP: Saw pod success 05/06/23 08:30:42.693
May  6 08:30:42.694: INFO: Pod "downward-api-7562198e-4ed1-43e7-8fb3-bbd315966e60" satisfied condition "Succeeded or Failed"
May  6 08:30:42.696: INFO: Trying to get logs from node cncf-0 pod downward-api-7562198e-4ed1-43e7-8fb3-bbd315966e60 container dapi-container: <nil>
STEP: delete the pod 05/06/23 08:30:42.701
May  6 08:30:42.714: INFO: Waiting for pod downward-api-7562198e-4ed1-43e7-8fb3-bbd315966e60 to disappear
May  6 08:30:42.716: INFO: Pod downward-api-7562198e-4ed1-43e7-8fb3-bbd315966e60 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May  6 08:30:42.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5220" for this suite. 05/06/23 08:30:42.719
------------------------------
â€¢ [4.066 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:30:38.66
    May  6 08:30:38.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename downward-api 05/06/23 08:30:38.66
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:30:38.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:30:38.678
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 05/06/23 08:30:38.68
    May  6 08:30:38.687: INFO: Waiting up to 5m0s for pod "downward-api-7562198e-4ed1-43e7-8fb3-bbd315966e60" in namespace "downward-api-5220" to be "Succeeded or Failed"
    May  6 08:30:38.689: INFO: Pod "downward-api-7562198e-4ed1-43e7-8fb3-bbd315966e60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.450919ms
    May  6 08:30:40.694: INFO: Pod "downward-api-7562198e-4ed1-43e7-8fb3-bbd315966e60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00711019s
    May  6 08:30:42.693: INFO: Pod "downward-api-7562198e-4ed1-43e7-8fb3-bbd315966e60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006654349s
    STEP: Saw pod success 05/06/23 08:30:42.693
    May  6 08:30:42.694: INFO: Pod "downward-api-7562198e-4ed1-43e7-8fb3-bbd315966e60" satisfied condition "Succeeded or Failed"
    May  6 08:30:42.696: INFO: Trying to get logs from node cncf-0 pod downward-api-7562198e-4ed1-43e7-8fb3-bbd315966e60 container dapi-container: <nil>
    STEP: delete the pod 05/06/23 08:30:42.701
    May  6 08:30:42.714: INFO: Waiting for pod downward-api-7562198e-4ed1-43e7-8fb3-bbd315966e60 to disappear
    May  6 08:30:42.716: INFO: Pod downward-api-7562198e-4ed1-43e7-8fb3-bbd315966e60 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May  6 08:30:42.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5220" for this suite. 05/06/23 08:30:42.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:30:42.727
May  6 08:30:42.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename sched-pred 05/06/23 08:30:42.727
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:30:42.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:30:42.742
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
May  6 08:30:42.744: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  6 08:30:42.750: INFO: Waiting for terminating namespaces to be deleted...
May  6 08:30:42.752: INFO: 
Logging pods the apiserver thinks is on node cncf-0 before test
May  6 08:30:42.757: INFO: calico-node-vvl9m from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.757: INFO: 	Container calico-node ready: true, restart count 0
May  6 08:30:42.757: INFO: kube-apiserver-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.757: INFO: 	Container kube-apiserver ready: true, restart count 1
May  6 08:30:42.757: INFO: kube-controller-manager-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.757: INFO: 	Container kube-controller-manager ready: true, restart count 0
May  6 08:30:42.757: INFO: kube-proxy-cq72q from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.757: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 08:30:42.757: INFO: kube-scheduler-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.757: INFO: 	Container kube-scheduler ready: true, restart count 1
May  6 08:30:42.757: INFO: tigera-operator-585fc94df6-xnxld from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.757: INFO: 	Container tigera-operator ready: true, restart count 0
May  6 08:30:42.757: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-np4v2 from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 08:30:42.757: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 08:30:42.757: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 08:30:42.757: INFO: 
Logging pods the apiserver thinks is on node cncf-1 before test
May  6 08:30:42.762: INFO: calico-node-kz2jp from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.762: INFO: 	Container calico-node ready: true, restart count 0
May  6 08:30:42.762: INFO: calico-typha-7d85b4d7df-79mt7 from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.762: INFO: 	Container calico-typha ready: true, restart count 0
May  6 08:30:42.762: INFO: dns-test-6ffeff72-54ba-4fa1-9bbb-359bba68b89e from dns-3718 started at 2023-05-06 06:44:04 +0000 UTC (3 container statuses recorded)
May  6 08:30:42.762: INFO: 	Container jessie-querier ready: true, restart count 9
May  6 08:30:42.762: INFO: 	Container querier ready: true, restart count 8
May  6 08:30:42.762: INFO: 	Container webserver ready: true, restart count 0
May  6 08:30:42.762: INFO: coredns-5b98b988fd-8gbpp from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.762: INFO: 	Container coredns ready: true, restart count 0
May  6 08:30:42.762: INFO: kube-apiserver-cncf-1 from kube-system started at 2023-05-05 20:24:38 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.762: INFO: 	Container kube-apiserver ready: true, restart count 1
May  6 08:30:42.762: INFO: kube-controller-manager-cncf-1 from kube-system started at 2023-05-05 20:24:14 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.762: INFO: 	Container kube-controller-manager ready: true, restart count 0
May  6 08:30:42.762: INFO: kube-proxy-4wtf5 from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.762: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 08:30:42.762: INFO: kube-scheduler-cncf-1 from kube-system started at 2023-05-05 20:23:17 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.762: INFO: 	Container kube-scheduler ready: true, restart count 1
May  6 08:30:42.762: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-5lxlb from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 08:30:42.762: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 08:30:42.762: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 08:30:42.762: INFO: 
Logging pods the apiserver thinks is on node cncf-2 before test
May  6 08:30:42.767: INFO: calico-kube-controllers-574d8db6c-nnb5w from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.767: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  6 08:30:42.767: INFO: calico-node-q42q6 from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.767: INFO: 	Container calico-node ready: true, restart count 0
May  6 08:30:42.767: INFO: calico-typha-7d85b4d7df-6s76x from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.767: INFO: 	Container calico-typha ready: true, restart count 0
May  6 08:30:42.767: INFO: coredns-5b98b988fd-7r5vm from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.767: INFO: 	Container coredns ready: true, restart count 0
May  6 08:30:42.767: INFO: kube-apiserver-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.767: INFO: 	Container kube-apiserver ready: true, restart count 1
May  6 08:30:42.767: INFO: kube-controller-manager-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.767: INFO: 	Container kube-controller-manager ready: true, restart count 0
May  6 08:30:42.767: INFO: kube-proxy-jl76p from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.767: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 08:30:42.767: INFO: kube-scheduler-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.767: INFO: 	Container kube-scheduler ready: true, restart count 1
May  6 08:30:42.767: INFO: kubelet-rubber-stamp-c6b74568-7s6qs from kube-system started at 2023-05-06 08:14:12 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.767: INFO: 	Container kubelet-rubber-stamp ready: true, restart count 0
May  6 08:30:42.767: INFO: externalsvc-qglkg from services-2585 started at 2023-05-06 08:14:12 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.767: INFO: 	Container externalsvc ready: true, restart count 0
May  6 08:30:42.767: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-mjqth from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 08:30:42.767: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 08:30:42.767: INFO: 	Container systemd-logs ready: true, restart count 0
May  6 08:30:42.767: INFO: 
Logging pods the apiserver thinks is on node cncf-3 before test
May  6 08:30:42.772: INFO: calico-node-sv4fh from calico-system started at 2023-05-05 20:28:36 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.772: INFO: 	Container calico-node ready: true, restart count 0
May  6 08:30:42.772: INFO: kube-proxy-n8v5r from kube-system started at 2023-05-05 20:28:36 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.772: INFO: 	Container kube-proxy ready: true, restart count 0
May  6 08:30:42.772: INFO: controller-manager-v3-6b6b89c4f6-skqkm from rafay-system started at 2023-05-05 20:32:05 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.772: INFO: 	Container manager ready: true, restart count 0
May  6 08:30:42.772: INFO: edge-client-59448698df-gxb8q from rafay-system started at 2023-05-05 20:31:45 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.772: INFO: 	Container edge-client ready: true, restart count 0
May  6 08:30:42.772: INFO: rafay-connector-v3-79f986b9c6-rxjdn from rafay-system started at 2023-05-05 20:32:04 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.772: INFO: 	Container connector ready: true, restart count 0
May  6 08:30:42.772: INFO: v2-relay-agent-6b76bc6c6f-m6hgg from rafay-system started at 2023-05-05 20:32:06 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.772: INFO: 	Container relay-agent ready: true, restart count 0
May  6 08:30:42.772: INFO: externalsvc-45jwj from services-2585 started at 2023-05-06 06:58:05 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.772: INFO: 	Container externalsvc ready: true, restart count 0
May  6 08:30:42.772: INFO: sonobuoy from sonobuoy started at 2023-05-06 06:59:12 +0000 UTC (1 container statuses recorded)
May  6 08:30:42.772: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  6 08:30:42.772: INFO: sonobuoy-e2e-job-47910262329e4558 from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 08:30:42.772: INFO: 	Container e2e ready: true, restart count 0
May  6 08:30:42.772: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 08:30:42.772: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-ts2xw from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
May  6 08:30:42.772: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  6 08:30:42.772: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node cncf-0 05/06/23 08:30:42.789
STEP: verifying the node has the label node cncf-1 05/06/23 08:30:42.803
STEP: verifying the node has the label node cncf-2 05/06/23 08:30:42.816
STEP: verifying the node has the label node cncf-3 05/06/23 08:30:42.829
May  6 08:30:42.840: INFO: Pod calico-kube-controllers-574d8db6c-nnb5w requesting resource cpu=0m on Node cncf-2
May  6 08:30:42.840: INFO: Pod calico-node-kz2jp requesting resource cpu=0m on Node cncf-1
May  6 08:30:42.840: INFO: Pod calico-node-q42q6 requesting resource cpu=0m on Node cncf-2
May  6 08:30:42.840: INFO: Pod calico-node-sv4fh requesting resource cpu=0m on Node cncf-3
May  6 08:30:42.840: INFO: Pod calico-node-vvl9m requesting resource cpu=0m on Node cncf-0
May  6 08:30:42.840: INFO: Pod calico-typha-7d85b4d7df-6s76x requesting resource cpu=0m on Node cncf-2
May  6 08:30:42.840: INFO: Pod calico-typha-7d85b4d7df-79mt7 requesting resource cpu=0m on Node cncf-1
May  6 08:30:42.840: INFO: Pod dns-test-6ffeff72-54ba-4fa1-9bbb-359bba68b89e requesting resource cpu=0m on Node cncf-1
May  6 08:30:42.840: INFO: Pod coredns-5b98b988fd-7r5vm requesting resource cpu=50m on Node cncf-2
May  6 08:30:42.840: INFO: Pod coredns-5b98b988fd-8gbpp requesting resource cpu=50m on Node cncf-1
May  6 08:30:42.840: INFO: Pod kube-apiserver-cncf-0 requesting resource cpu=150m on Node cncf-0
May  6 08:30:42.840: INFO: Pod kube-apiserver-cncf-1 requesting resource cpu=150m on Node cncf-1
May  6 08:30:42.840: INFO: Pod kube-apiserver-cncf-2 requesting resource cpu=150m on Node cncf-2
May  6 08:30:42.840: INFO: Pod kube-controller-manager-cncf-0 requesting resource cpu=100m on Node cncf-0
May  6 08:30:42.840: INFO: Pod kube-controller-manager-cncf-1 requesting resource cpu=100m on Node cncf-1
May  6 08:30:42.840: INFO: Pod kube-controller-manager-cncf-2 requesting resource cpu=100m on Node cncf-2
May  6 08:30:42.840: INFO: Pod kube-proxy-4wtf5 requesting resource cpu=0m on Node cncf-1
May  6 08:30:42.840: INFO: Pod kube-proxy-cq72q requesting resource cpu=0m on Node cncf-0
May  6 08:30:42.840: INFO: Pod kube-proxy-jl76p requesting resource cpu=0m on Node cncf-2
May  6 08:30:42.840: INFO: Pod kube-proxy-n8v5r requesting resource cpu=0m on Node cncf-3
May  6 08:30:42.840: INFO: Pod kube-scheduler-cncf-0 requesting resource cpu=100m on Node cncf-0
May  6 08:30:42.840: INFO: Pod kube-scheduler-cncf-1 requesting resource cpu=100m on Node cncf-1
May  6 08:30:42.840: INFO: Pod kube-scheduler-cncf-2 requesting resource cpu=100m on Node cncf-2
May  6 08:30:42.840: INFO: Pod kubelet-rubber-stamp-c6b74568-7s6qs requesting resource cpu=2m on Node cncf-2
May  6 08:30:42.840: INFO: Pod tigera-operator-585fc94df6-xnxld requesting resource cpu=0m on Node cncf-0
May  6 08:30:42.840: INFO: Pod controller-manager-v3-6b6b89c4f6-skqkm requesting resource cpu=100m on Node cncf-3
May  6 08:30:42.840: INFO: Pod edge-client-59448698df-gxb8q requesting resource cpu=50m on Node cncf-3
May  6 08:30:42.840: INFO: Pod rafay-connector-v3-79f986b9c6-rxjdn requesting resource cpu=50m on Node cncf-3
May  6 08:30:42.840: INFO: Pod v2-relay-agent-6b76bc6c6f-m6hgg requesting resource cpu=100m on Node cncf-3
May  6 08:30:42.840: INFO: Pod externalsvc-45jwj requesting resource cpu=0m on Node cncf-3
May  6 08:30:42.840: INFO: Pod externalsvc-qglkg requesting resource cpu=0m on Node cncf-2
May  6 08:30:42.840: INFO: Pod sonobuoy requesting resource cpu=0m on Node cncf-3
May  6 08:30:42.840: INFO: Pod sonobuoy-e2e-job-47910262329e4558 requesting resource cpu=0m on Node cncf-3
May  6 08:30:42.840: INFO: Pod sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-5lxlb requesting resource cpu=0m on Node cncf-1
May  6 08:30:42.840: INFO: Pod sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-mjqth requesting resource cpu=0m on Node cncf-2
May  6 08:30:42.840: INFO: Pod sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-np4v2 requesting resource cpu=0m on Node cncf-0
May  6 08:30:42.840: INFO: Pod sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-ts2xw requesting resource cpu=0m on Node cncf-3
STEP: Starting Pods to consume most of the cluster CPU. 05/06/23 08:30:42.84
May  6 08:30:42.841: INFO: Creating a pod which consumes cpu=2499m on Node cncf-0
May  6 08:30:42.848: INFO: Creating a pod which consumes cpu=2464m on Node cncf-1
May  6 08:30:42.855: INFO: Creating a pod which consumes cpu=2462m on Node cncf-2
May  6 08:30:42.864: INFO: Creating a pod which consumes cpu=2534m on Node cncf-3
May  6 08:30:42.872: INFO: Waiting up to 5m0s for pod "filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c" in namespace "sched-pred-292" to be "running"
May  6 08:30:42.876: INFO: Pod "filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.707687ms
May  6 08:30:44.879: INFO: Pod "filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c": Phase="Running", Reason="", readiness=true. Elapsed: 2.006738553s
May  6 08:30:44.879: INFO: Pod "filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c" satisfied condition "running"
May  6 08:30:44.879: INFO: Waiting up to 5m0s for pod "filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4" in namespace "sched-pred-292" to be "running"
May  6 08:30:44.882: INFO: Pod "filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.969965ms
May  6 08:30:44.882: INFO: Pod "filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4" satisfied condition "running"
May  6 08:30:44.882: INFO: Waiting up to 5m0s for pod "filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b" in namespace "sched-pred-292" to be "running"
May  6 08:30:44.884: INFO: Pod "filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b": Phase="Running", Reason="", readiness=true. Elapsed: 1.825921ms
May  6 08:30:44.884: INFO: Pod "filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b" satisfied condition "running"
May  6 08:30:44.884: INFO: Waiting up to 5m0s for pod "filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f" in namespace "sched-pred-292" to be "running"
May  6 08:30:44.887: INFO: Pod "filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f": Phase="Running", Reason="", readiness=true. Elapsed: 2.684893ms
May  6 08:30:44.887: INFO: Pod "filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 05/06/23 08:30:44.887
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f.175c80ad616a08ae], Reason = [Scheduled], Message = [Successfully assigned sched-pred-292/filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f to cncf-3] 05/06/23 08:30:44.889
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f.175c80ad85116804], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 05/06/23 08:30:44.889
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f.175c80ad861e841f], Reason = [Created], Message = [Created container filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f] 05/06/23 08:30:44.889
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f.175c80ad8a09c1ce], Reason = [Started], Message = [Started container filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f] 05/06/23 08:30:44.889
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4.175c80ad605effb3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-292/filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4 to cncf-1] 05/06/23 08:30:44.889
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4.175c80ad8548fb9a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 05/06/23 08:30:44.889
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4.175c80ad8657bcaa], Reason = [Created], Message = [Created container filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4] 05/06/23 08:30:44.889
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4.175c80ad8b20ccf6], Reason = [Started], Message = [Started container filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4] 05/06/23 08:30:44.889
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c.175c80ad5fda8c9f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-292/filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c to cncf-0] 05/06/23 08:30:44.889
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c.175c80ad82d6f42f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 05/06/23 08:30:44.889
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c.175c80ad841a9393], Reason = [Created], Message = [Created container filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c] 05/06/23 08:30:44.89
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c.175c80ad88d6949f], Reason = [Started], Message = [Started container filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c] 05/06/23 08:30:44.89
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b.175c80ad60dde1e5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-292/filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b to cncf-2] 05/06/23 08:30:44.89
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b.175c80ad842c2616], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 05/06/23 08:30:44.89
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b.175c80ad850a3c32], Reason = [Created], Message = [Created container filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b] 05/06/23 08:30:44.89
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b.175c80ad89a9395b], Reason = [Started], Message = [Started container filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b] 05/06/23 08:30:44.89
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.175c80add97f9610], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu. preemption: 0/4 nodes are available: 4 No preemption victims found for incoming pod..] 05/06/23 08:30:44.903
STEP: removing the label node off the node cncf-3 05/06/23 08:30:45.9
STEP: verifying the node doesn't have the label node 05/06/23 08:30:45.911
STEP: removing the label node off the node cncf-0 05/06/23 08:30:45.914
STEP: verifying the node doesn't have the label node 05/06/23 08:30:45.927
STEP: removing the label node off the node cncf-1 05/06/23 08:30:45.929
STEP: verifying the node doesn't have the label node 05/06/23 08:30:45.94
STEP: removing the label node off the node cncf-2 05/06/23 08:30:45.943
STEP: verifying the node doesn't have the label node 05/06/23 08:30:45.953
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
May  6 08:30:45.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-292" for this suite. 05/06/23 08:30:45.959
------------------------------
â€¢ [3.240 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:30:42.727
    May  6 08:30:42.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename sched-pred 05/06/23 08:30:42.727
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:30:42.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:30:42.742
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    May  6 08:30:42.744: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  6 08:30:42.750: INFO: Waiting for terminating namespaces to be deleted...
    May  6 08:30:42.752: INFO: 
    Logging pods the apiserver thinks is on node cncf-0 before test
    May  6 08:30:42.757: INFO: calico-node-vvl9m from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.757: INFO: 	Container calico-node ready: true, restart count 0
    May  6 08:30:42.757: INFO: kube-apiserver-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.757: INFO: 	Container kube-apiserver ready: true, restart count 1
    May  6 08:30:42.757: INFO: kube-controller-manager-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.757: INFO: 	Container kube-controller-manager ready: true, restart count 0
    May  6 08:30:42.757: INFO: kube-proxy-cq72q from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.757: INFO: 	Container kube-proxy ready: true, restart count 0
    May  6 08:30:42.757: INFO: kube-scheduler-cncf-0 from kube-system started at 2023-05-05 20:26:22 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.757: INFO: 	Container kube-scheduler ready: true, restart count 1
    May  6 08:30:42.757: INFO: tigera-operator-585fc94df6-xnxld from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.757: INFO: 	Container tigera-operator ready: true, restart count 0
    May  6 08:30:42.757: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-np4v2 from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 08:30:42.757: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 08:30:42.757: INFO: 	Container systemd-logs ready: true, restart count 0
    May  6 08:30:42.757: INFO: 
    Logging pods the apiserver thinks is on node cncf-1 before test
    May  6 08:30:42.762: INFO: calico-node-kz2jp from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.762: INFO: 	Container calico-node ready: true, restart count 0
    May  6 08:30:42.762: INFO: calico-typha-7d85b4d7df-79mt7 from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.762: INFO: 	Container calico-typha ready: true, restart count 0
    May  6 08:30:42.762: INFO: dns-test-6ffeff72-54ba-4fa1-9bbb-359bba68b89e from dns-3718 started at 2023-05-06 06:44:04 +0000 UTC (3 container statuses recorded)
    May  6 08:30:42.762: INFO: 	Container jessie-querier ready: true, restart count 9
    May  6 08:30:42.762: INFO: 	Container querier ready: true, restart count 8
    May  6 08:30:42.762: INFO: 	Container webserver ready: true, restart count 0
    May  6 08:30:42.762: INFO: coredns-5b98b988fd-8gbpp from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.762: INFO: 	Container coredns ready: true, restart count 0
    May  6 08:30:42.762: INFO: kube-apiserver-cncf-1 from kube-system started at 2023-05-05 20:24:38 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.762: INFO: 	Container kube-apiserver ready: true, restart count 1
    May  6 08:30:42.762: INFO: kube-controller-manager-cncf-1 from kube-system started at 2023-05-05 20:24:14 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.762: INFO: 	Container kube-controller-manager ready: true, restart count 0
    May  6 08:30:42.762: INFO: kube-proxy-4wtf5 from kube-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.762: INFO: 	Container kube-proxy ready: true, restart count 0
    May  6 08:30:42.762: INFO: kube-scheduler-cncf-1 from kube-system started at 2023-05-05 20:23:17 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.762: INFO: 	Container kube-scheduler ready: true, restart count 1
    May  6 08:30:42.762: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-5lxlb from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 08:30:42.762: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 08:30:42.762: INFO: 	Container systemd-logs ready: true, restart count 0
    May  6 08:30:42.762: INFO: 
    Logging pods the apiserver thinks is on node cncf-2 before test
    May  6 08:30:42.767: INFO: calico-kube-controllers-574d8db6c-nnb5w from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.767: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  6 08:30:42.767: INFO: calico-node-q42q6 from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.767: INFO: 	Container calico-node ready: true, restart count 0
    May  6 08:30:42.767: INFO: calico-typha-7d85b4d7df-6s76x from calico-system started at 2023-05-05 20:25:46 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.767: INFO: 	Container calico-typha ready: true, restart count 0
    May  6 08:30:42.767: INFO: coredns-5b98b988fd-7r5vm from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.767: INFO: 	Container coredns ready: true, restart count 0
    May  6 08:30:42.767: INFO: kube-apiserver-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.767: INFO: 	Container kube-apiserver ready: true, restart count 1
    May  6 08:30:42.767: INFO: kube-controller-manager-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.767: INFO: 	Container kube-controller-manager ready: true, restart count 0
    May  6 08:30:42.767: INFO: kube-proxy-jl76p from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.767: INFO: 	Container kube-proxy ready: true, restart count 0
    May  6 08:30:42.767: INFO: kube-scheduler-cncf-2 from kube-system started at 2023-05-05 20:25:47 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.767: INFO: 	Container kube-scheduler ready: true, restart count 1
    May  6 08:30:42.767: INFO: kubelet-rubber-stamp-c6b74568-7s6qs from kube-system started at 2023-05-06 08:14:12 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.767: INFO: 	Container kubelet-rubber-stamp ready: true, restart count 0
    May  6 08:30:42.767: INFO: externalsvc-qglkg from services-2585 started at 2023-05-06 08:14:12 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.767: INFO: 	Container externalsvc ready: true, restart count 0
    May  6 08:30:42.767: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-mjqth from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 08:30:42.767: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 08:30:42.767: INFO: 	Container systemd-logs ready: true, restart count 0
    May  6 08:30:42.767: INFO: 
    Logging pods the apiserver thinks is on node cncf-3 before test
    May  6 08:30:42.772: INFO: calico-node-sv4fh from calico-system started at 2023-05-05 20:28:36 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.772: INFO: 	Container calico-node ready: true, restart count 0
    May  6 08:30:42.772: INFO: kube-proxy-n8v5r from kube-system started at 2023-05-05 20:28:36 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.772: INFO: 	Container kube-proxy ready: true, restart count 0
    May  6 08:30:42.772: INFO: controller-manager-v3-6b6b89c4f6-skqkm from rafay-system started at 2023-05-05 20:32:05 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.772: INFO: 	Container manager ready: true, restart count 0
    May  6 08:30:42.772: INFO: edge-client-59448698df-gxb8q from rafay-system started at 2023-05-05 20:31:45 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.772: INFO: 	Container edge-client ready: true, restart count 0
    May  6 08:30:42.772: INFO: rafay-connector-v3-79f986b9c6-rxjdn from rafay-system started at 2023-05-05 20:32:04 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.772: INFO: 	Container connector ready: true, restart count 0
    May  6 08:30:42.772: INFO: v2-relay-agent-6b76bc6c6f-m6hgg from rafay-system started at 2023-05-05 20:32:06 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.772: INFO: 	Container relay-agent ready: true, restart count 0
    May  6 08:30:42.772: INFO: externalsvc-45jwj from services-2585 started at 2023-05-06 06:58:05 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.772: INFO: 	Container externalsvc ready: true, restart count 0
    May  6 08:30:42.772: INFO: sonobuoy from sonobuoy started at 2023-05-06 06:59:12 +0000 UTC (1 container statuses recorded)
    May  6 08:30:42.772: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  6 08:30:42.772: INFO: sonobuoy-e2e-job-47910262329e4558 from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 08:30:42.772: INFO: 	Container e2e ready: true, restart count 0
    May  6 08:30:42.772: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 08:30:42.772: INFO: sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-ts2xw from sonobuoy started at 2023-05-06 06:59:13 +0000 UTC (2 container statuses recorded)
    May  6 08:30:42.772: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  6 08:30:42.772: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node cncf-0 05/06/23 08:30:42.789
    STEP: verifying the node has the label node cncf-1 05/06/23 08:30:42.803
    STEP: verifying the node has the label node cncf-2 05/06/23 08:30:42.816
    STEP: verifying the node has the label node cncf-3 05/06/23 08:30:42.829
    May  6 08:30:42.840: INFO: Pod calico-kube-controllers-574d8db6c-nnb5w requesting resource cpu=0m on Node cncf-2
    May  6 08:30:42.840: INFO: Pod calico-node-kz2jp requesting resource cpu=0m on Node cncf-1
    May  6 08:30:42.840: INFO: Pod calico-node-q42q6 requesting resource cpu=0m on Node cncf-2
    May  6 08:30:42.840: INFO: Pod calico-node-sv4fh requesting resource cpu=0m on Node cncf-3
    May  6 08:30:42.840: INFO: Pod calico-node-vvl9m requesting resource cpu=0m on Node cncf-0
    May  6 08:30:42.840: INFO: Pod calico-typha-7d85b4d7df-6s76x requesting resource cpu=0m on Node cncf-2
    May  6 08:30:42.840: INFO: Pod calico-typha-7d85b4d7df-79mt7 requesting resource cpu=0m on Node cncf-1
    May  6 08:30:42.840: INFO: Pod dns-test-6ffeff72-54ba-4fa1-9bbb-359bba68b89e requesting resource cpu=0m on Node cncf-1
    May  6 08:30:42.840: INFO: Pod coredns-5b98b988fd-7r5vm requesting resource cpu=50m on Node cncf-2
    May  6 08:30:42.840: INFO: Pod coredns-5b98b988fd-8gbpp requesting resource cpu=50m on Node cncf-1
    May  6 08:30:42.840: INFO: Pod kube-apiserver-cncf-0 requesting resource cpu=150m on Node cncf-0
    May  6 08:30:42.840: INFO: Pod kube-apiserver-cncf-1 requesting resource cpu=150m on Node cncf-1
    May  6 08:30:42.840: INFO: Pod kube-apiserver-cncf-2 requesting resource cpu=150m on Node cncf-2
    May  6 08:30:42.840: INFO: Pod kube-controller-manager-cncf-0 requesting resource cpu=100m on Node cncf-0
    May  6 08:30:42.840: INFO: Pod kube-controller-manager-cncf-1 requesting resource cpu=100m on Node cncf-1
    May  6 08:30:42.840: INFO: Pod kube-controller-manager-cncf-2 requesting resource cpu=100m on Node cncf-2
    May  6 08:30:42.840: INFO: Pod kube-proxy-4wtf5 requesting resource cpu=0m on Node cncf-1
    May  6 08:30:42.840: INFO: Pod kube-proxy-cq72q requesting resource cpu=0m on Node cncf-0
    May  6 08:30:42.840: INFO: Pod kube-proxy-jl76p requesting resource cpu=0m on Node cncf-2
    May  6 08:30:42.840: INFO: Pod kube-proxy-n8v5r requesting resource cpu=0m on Node cncf-3
    May  6 08:30:42.840: INFO: Pod kube-scheduler-cncf-0 requesting resource cpu=100m on Node cncf-0
    May  6 08:30:42.840: INFO: Pod kube-scheduler-cncf-1 requesting resource cpu=100m on Node cncf-1
    May  6 08:30:42.840: INFO: Pod kube-scheduler-cncf-2 requesting resource cpu=100m on Node cncf-2
    May  6 08:30:42.840: INFO: Pod kubelet-rubber-stamp-c6b74568-7s6qs requesting resource cpu=2m on Node cncf-2
    May  6 08:30:42.840: INFO: Pod tigera-operator-585fc94df6-xnxld requesting resource cpu=0m on Node cncf-0
    May  6 08:30:42.840: INFO: Pod controller-manager-v3-6b6b89c4f6-skqkm requesting resource cpu=100m on Node cncf-3
    May  6 08:30:42.840: INFO: Pod edge-client-59448698df-gxb8q requesting resource cpu=50m on Node cncf-3
    May  6 08:30:42.840: INFO: Pod rafay-connector-v3-79f986b9c6-rxjdn requesting resource cpu=50m on Node cncf-3
    May  6 08:30:42.840: INFO: Pod v2-relay-agent-6b76bc6c6f-m6hgg requesting resource cpu=100m on Node cncf-3
    May  6 08:30:42.840: INFO: Pod externalsvc-45jwj requesting resource cpu=0m on Node cncf-3
    May  6 08:30:42.840: INFO: Pod externalsvc-qglkg requesting resource cpu=0m on Node cncf-2
    May  6 08:30:42.840: INFO: Pod sonobuoy requesting resource cpu=0m on Node cncf-3
    May  6 08:30:42.840: INFO: Pod sonobuoy-e2e-job-47910262329e4558 requesting resource cpu=0m on Node cncf-3
    May  6 08:30:42.840: INFO: Pod sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-5lxlb requesting resource cpu=0m on Node cncf-1
    May  6 08:30:42.840: INFO: Pod sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-mjqth requesting resource cpu=0m on Node cncf-2
    May  6 08:30:42.840: INFO: Pod sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-np4v2 requesting resource cpu=0m on Node cncf-0
    May  6 08:30:42.840: INFO: Pod sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-ts2xw requesting resource cpu=0m on Node cncf-3
    STEP: Starting Pods to consume most of the cluster CPU. 05/06/23 08:30:42.84
    May  6 08:30:42.841: INFO: Creating a pod which consumes cpu=2499m on Node cncf-0
    May  6 08:30:42.848: INFO: Creating a pod which consumes cpu=2464m on Node cncf-1
    May  6 08:30:42.855: INFO: Creating a pod which consumes cpu=2462m on Node cncf-2
    May  6 08:30:42.864: INFO: Creating a pod which consumes cpu=2534m on Node cncf-3
    May  6 08:30:42.872: INFO: Waiting up to 5m0s for pod "filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c" in namespace "sched-pred-292" to be "running"
    May  6 08:30:42.876: INFO: Pod "filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.707687ms
    May  6 08:30:44.879: INFO: Pod "filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c": Phase="Running", Reason="", readiness=true. Elapsed: 2.006738553s
    May  6 08:30:44.879: INFO: Pod "filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c" satisfied condition "running"
    May  6 08:30:44.879: INFO: Waiting up to 5m0s for pod "filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4" in namespace "sched-pred-292" to be "running"
    May  6 08:30:44.882: INFO: Pod "filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.969965ms
    May  6 08:30:44.882: INFO: Pod "filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4" satisfied condition "running"
    May  6 08:30:44.882: INFO: Waiting up to 5m0s for pod "filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b" in namespace "sched-pred-292" to be "running"
    May  6 08:30:44.884: INFO: Pod "filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b": Phase="Running", Reason="", readiness=true. Elapsed: 1.825921ms
    May  6 08:30:44.884: INFO: Pod "filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b" satisfied condition "running"
    May  6 08:30:44.884: INFO: Waiting up to 5m0s for pod "filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f" in namespace "sched-pred-292" to be "running"
    May  6 08:30:44.887: INFO: Pod "filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f": Phase="Running", Reason="", readiness=true. Elapsed: 2.684893ms
    May  6 08:30:44.887: INFO: Pod "filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 05/06/23 08:30:44.887
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f.175c80ad616a08ae], Reason = [Scheduled], Message = [Successfully assigned sched-pred-292/filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f to cncf-3] 05/06/23 08:30:44.889
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f.175c80ad85116804], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 05/06/23 08:30:44.889
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f.175c80ad861e841f], Reason = [Created], Message = [Created container filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f] 05/06/23 08:30:44.889
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f.175c80ad8a09c1ce], Reason = [Started], Message = [Started container filler-pod-24deb97f-f02c-4440-9851-fec2c741be5f] 05/06/23 08:30:44.889
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4.175c80ad605effb3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-292/filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4 to cncf-1] 05/06/23 08:30:44.889
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4.175c80ad8548fb9a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 05/06/23 08:30:44.889
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4.175c80ad8657bcaa], Reason = [Created], Message = [Created container filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4] 05/06/23 08:30:44.889
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4.175c80ad8b20ccf6], Reason = [Started], Message = [Started container filler-pod-6c0b2e21-8dcf-4b7e-b324-c1753bb5b7a4] 05/06/23 08:30:44.889
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c.175c80ad5fda8c9f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-292/filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c to cncf-0] 05/06/23 08:30:44.889
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c.175c80ad82d6f42f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 05/06/23 08:30:44.889
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c.175c80ad841a9393], Reason = [Created], Message = [Created container filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c] 05/06/23 08:30:44.89
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c.175c80ad88d6949f], Reason = [Started], Message = [Started container filler-pod-dd16e48d-9b82-4158-b25f-2ef40a53257c] 05/06/23 08:30:44.89
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b.175c80ad60dde1e5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-292/filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b to cncf-2] 05/06/23 08:30:44.89
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b.175c80ad842c2616], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 05/06/23 08:30:44.89
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b.175c80ad850a3c32], Reason = [Created], Message = [Created container filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b] 05/06/23 08:30:44.89
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b.175c80ad89a9395b], Reason = [Started], Message = [Started container filler-pod-fd9812d3-a733-410f-9a6a-ed833788ec0b] 05/06/23 08:30:44.89
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.175c80add97f9610], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu. preemption: 0/4 nodes are available: 4 No preemption victims found for incoming pod..] 05/06/23 08:30:44.903
    STEP: removing the label node off the node cncf-3 05/06/23 08:30:45.9
    STEP: verifying the node doesn't have the label node 05/06/23 08:30:45.911
    STEP: removing the label node off the node cncf-0 05/06/23 08:30:45.914
    STEP: verifying the node doesn't have the label node 05/06/23 08:30:45.927
    STEP: removing the label node off the node cncf-1 05/06/23 08:30:45.929
    STEP: verifying the node doesn't have the label node 05/06/23 08:30:45.94
    STEP: removing the label node off the node cncf-2 05/06/23 08:30:45.943
    STEP: verifying the node doesn't have the label node 05/06/23 08:30:45.953
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    May  6 08:30:45.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-292" for this suite. 05/06/23 08:30:45.959
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:30:45.967
May  6 08:30:45.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename services 05/06/23 08:30:45.968
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:30:45.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:30:45.986
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-3231 05/06/23 08:30:45.988
STEP: creating service affinity-clusterip-transition in namespace services-3231 05/06/23 08:30:45.988
STEP: creating replication controller affinity-clusterip-transition in namespace services-3231 05/06/23 08:30:46
I0506 08:30:46.008921      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3231, replica count: 3
I0506 08:30:49.059486      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  6 08:30:49.064: INFO: Creating new exec pod
May  6 08:30:49.071: INFO: Waiting up to 5m0s for pod "execpod-affinity69vmd" in namespace "services-3231" to be "running"
May  6 08:30:49.073: INFO: Pod "execpod-affinity69vmd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.415251ms
May  6 08:30:51.077: INFO: Pod "execpod-affinity69vmd": Phase="Running", Reason="", readiness=true. Elapsed: 2.005939383s
May  6 08:30:51.077: INFO: Pod "execpod-affinity69vmd" satisfied condition "running"
May  6 08:30:52.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-3231 exec execpod-affinity69vmd -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
May  6 08:30:52.188: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May  6 08:30:52.188: INFO: stdout: ""
May  6 08:30:52.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-3231 exec execpod-affinity69vmd -- /bin/sh -x -c nc -v -z -w 2 10.110.214.110 80'
May  6 08:30:52.302: INFO: stderr: "+ nc -v -z -w 2 10.110.214.110 80\nConnection to 10.110.214.110 80 port [tcp/http] succeeded!\n"
May  6 08:30:52.302: INFO: stdout: ""
May  6 08:30:52.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-3231 exec execpod-affinity69vmd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.214.110:80/ ; done'
May  6 08:30:52.481: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n"
May  6 08:30:52.481: INFO: stdout: "\naffinity-clusterip-transition-whf25\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-fwcjd\naffinity-clusterip-transition-fwcjd\naffinity-clusterip-transition-whf25\naffinity-clusterip-transition-fwcjd\naffinity-clusterip-transition-whf25\naffinity-clusterip-transition-fwcjd\naffinity-clusterip-transition-fwcjd\naffinity-clusterip-transition-fwcjd\naffinity-clusterip-transition-whf25\naffinity-clusterip-transition-fwcjd\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-whf25\naffinity-clusterip-transition-whf25\naffinity-clusterip-transition-whf25"
May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-whf25
May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-64t8h
May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-fwcjd
May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-fwcjd
May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-whf25
May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-fwcjd
May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-whf25
May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-fwcjd
May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-fwcjd
May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-fwcjd
May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-whf25
May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-fwcjd
May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-64t8h
May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-whf25
May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-whf25
May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-whf25
May  6 08:30:52.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-3231 exec execpod-affinity69vmd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.214.110:80/ ; done'
May  6 08:30:52.653: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n"
May  6 08:30:52.653: INFO: stdout: "\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h"
May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
May  6 08:30:52.653: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3231, will wait for the garbage collector to delete the pods 05/06/23 08:30:52.666
May  6 08:30:52.734: INFO: Deleting ReplicationController affinity-clusterip-transition took: 14.975542ms
May  6 08:30:52.835: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.535803ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  6 08:30:55.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3231" for this suite. 05/06/23 08:30:55.058
------------------------------
â€¢ [SLOW TEST] [9.096 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:30:45.967
    May  6 08:30:45.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename services 05/06/23 08:30:45.968
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:30:45.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:30:45.986
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-3231 05/06/23 08:30:45.988
    STEP: creating service affinity-clusterip-transition in namespace services-3231 05/06/23 08:30:45.988
    STEP: creating replication controller affinity-clusterip-transition in namespace services-3231 05/06/23 08:30:46
    I0506 08:30:46.008921      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3231, replica count: 3
    I0506 08:30:49.059486      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  6 08:30:49.064: INFO: Creating new exec pod
    May  6 08:30:49.071: INFO: Waiting up to 5m0s for pod "execpod-affinity69vmd" in namespace "services-3231" to be "running"
    May  6 08:30:49.073: INFO: Pod "execpod-affinity69vmd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.415251ms
    May  6 08:30:51.077: INFO: Pod "execpod-affinity69vmd": Phase="Running", Reason="", readiness=true. Elapsed: 2.005939383s
    May  6 08:30:51.077: INFO: Pod "execpod-affinity69vmd" satisfied condition "running"
    May  6 08:30:52.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-3231 exec execpod-affinity69vmd -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    May  6 08:30:52.188: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    May  6 08:30:52.188: INFO: stdout: ""
    May  6 08:30:52.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-3231 exec execpod-affinity69vmd -- /bin/sh -x -c nc -v -z -w 2 10.110.214.110 80'
    May  6 08:30:52.302: INFO: stderr: "+ nc -v -z -w 2 10.110.214.110 80\nConnection to 10.110.214.110 80 port [tcp/http] succeeded!\n"
    May  6 08:30:52.302: INFO: stdout: ""
    May  6 08:30:52.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-3231 exec execpod-affinity69vmd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.214.110:80/ ; done'
    May  6 08:30:52.481: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n"
    May  6 08:30:52.481: INFO: stdout: "\naffinity-clusterip-transition-whf25\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-fwcjd\naffinity-clusterip-transition-fwcjd\naffinity-clusterip-transition-whf25\naffinity-clusterip-transition-fwcjd\naffinity-clusterip-transition-whf25\naffinity-clusterip-transition-fwcjd\naffinity-clusterip-transition-fwcjd\naffinity-clusterip-transition-fwcjd\naffinity-clusterip-transition-whf25\naffinity-clusterip-transition-fwcjd\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-whf25\naffinity-clusterip-transition-whf25\naffinity-clusterip-transition-whf25"
    May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-whf25
    May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-64t8h
    May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-fwcjd
    May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-fwcjd
    May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-whf25
    May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-fwcjd
    May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-whf25
    May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-fwcjd
    May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-fwcjd
    May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-fwcjd
    May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-whf25
    May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-fwcjd
    May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-64t8h
    May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-whf25
    May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-whf25
    May  6 08:30:52.481: INFO: Received response from host: affinity-clusterip-transition-whf25
    May  6 08:30:52.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-3231 exec execpod-affinity69vmd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.214.110:80/ ; done'
    May  6 08:30:52.653: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.110:80/\n"
    May  6 08:30:52.653: INFO: stdout: "\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h\naffinity-clusterip-transition-64t8h"
    May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
    May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
    May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
    May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
    May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
    May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
    May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
    May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
    May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
    May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
    May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
    May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
    May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
    May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
    May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
    May  6 08:30:52.653: INFO: Received response from host: affinity-clusterip-transition-64t8h
    May  6 08:30:52.653: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3231, will wait for the garbage collector to delete the pods 05/06/23 08:30:52.666
    May  6 08:30:52.734: INFO: Deleting ReplicationController affinity-clusterip-transition took: 14.975542ms
    May  6 08:30:52.835: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.535803ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  6 08:30:55.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3231" for this suite. 05/06/23 08:30:55.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:30:55.064
May  6 08:30:55.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename projected 05/06/23 08:30:55.065
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:30:55.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:30:55.08
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 05/06/23 08:30:55.082
May  6 08:30:55.089: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d533edec-bd89-42c3-9656-8eecc0747a71" in namespace "projected-9865" to be "Succeeded or Failed"
May  6 08:30:55.092: INFO: Pod "downwardapi-volume-d533edec-bd89-42c3-9656-8eecc0747a71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.994351ms
May  6 08:30:57.095: INFO: Pod "downwardapi-volume-d533edec-bd89-42c3-9656-8eecc0747a71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005444484s
May  6 08:30:59.096: INFO: Pod "downwardapi-volume-d533edec-bd89-42c3-9656-8eecc0747a71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006719973s
STEP: Saw pod success 05/06/23 08:30:59.096
May  6 08:30:59.096: INFO: Pod "downwardapi-volume-d533edec-bd89-42c3-9656-8eecc0747a71" satisfied condition "Succeeded or Failed"
May  6 08:30:59.098: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-d533edec-bd89-42c3-9656-8eecc0747a71 container client-container: <nil>
STEP: delete the pod 05/06/23 08:30:59.103
May  6 08:30:59.113: INFO: Waiting for pod downwardapi-volume-d533edec-bd89-42c3-9656-8eecc0747a71 to disappear
May  6 08:30:59.115: INFO: Pod downwardapi-volume-d533edec-bd89-42c3-9656-8eecc0747a71 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May  6 08:30:59.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9865" for this suite. 05/06/23 08:30:59.118
------------------------------
â€¢ [4.059 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:30:55.064
    May  6 08:30:55.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename projected 05/06/23 08:30:55.065
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:30:55.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:30:55.08
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 05/06/23 08:30:55.082
    May  6 08:30:55.089: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d533edec-bd89-42c3-9656-8eecc0747a71" in namespace "projected-9865" to be "Succeeded or Failed"
    May  6 08:30:55.092: INFO: Pod "downwardapi-volume-d533edec-bd89-42c3-9656-8eecc0747a71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.994351ms
    May  6 08:30:57.095: INFO: Pod "downwardapi-volume-d533edec-bd89-42c3-9656-8eecc0747a71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005444484s
    May  6 08:30:59.096: INFO: Pod "downwardapi-volume-d533edec-bd89-42c3-9656-8eecc0747a71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006719973s
    STEP: Saw pod success 05/06/23 08:30:59.096
    May  6 08:30:59.096: INFO: Pod "downwardapi-volume-d533edec-bd89-42c3-9656-8eecc0747a71" satisfied condition "Succeeded or Failed"
    May  6 08:30:59.098: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-d533edec-bd89-42c3-9656-8eecc0747a71 container client-container: <nil>
    STEP: delete the pod 05/06/23 08:30:59.103
    May  6 08:30:59.113: INFO: Waiting for pod downwardapi-volume-d533edec-bd89-42c3-9656-8eecc0747a71 to disappear
    May  6 08:30:59.115: INFO: Pod downwardapi-volume-d533edec-bd89-42c3-9656-8eecc0747a71 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May  6 08:30:59.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9865" for this suite. 05/06/23 08:30:59.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:30:59.124
May  6 08:30:59.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename init-container 05/06/23 08:30:59.124
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:30:59.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:30:59.142
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 05/06/23 08:30:59.144
May  6 08:30:59.144: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May  6 08:31:03.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-9216" for this suite. 05/06/23 08:31:03.035
------------------------------
â€¢ [3.917 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:30:59.124
    May  6 08:30:59.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename init-container 05/06/23 08:30:59.124
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:30:59.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:30:59.142
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 05/06/23 08:30:59.144
    May  6 08:30:59.144: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May  6 08:31:03.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-9216" for this suite. 05/06/23 08:31:03.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:31:03.043
May  6 08:31:03.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubelet-test 05/06/23 08:31:03.044
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:31:03.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:31:03.06
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May  6 08:31:03.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5882" for this suite. 05/06/23 08:31:03.086
------------------------------
â€¢ [0.048 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:31:03.043
    May  6 08:31:03.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubelet-test 05/06/23 08:31:03.044
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:31:03.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:31:03.06
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May  6 08:31:03.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5882" for this suite. 05/06/23 08:31:03.086
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:31:03.091
May  6 08:31:03.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubectl 05/06/23 08:31:03.092
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:31:03.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:31:03.109
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 05/06/23 08:31:03.112
May  6 08:31:03.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8850 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
May  6 08:31:03.167: INFO: stderr: ""
May  6 08:31:03.167: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 05/06/23 08:31:03.167
May  6 08:31:03.167: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May  6 08:31:03.167: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8850" to be "running and ready, or succeeded"
May  6 08:31:03.171: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.275476ms
May  6 08:31:03.171: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'cncf-0' to be 'Running' but was 'Pending'
May  6 08:31:05.175: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.007129708s
May  6 08:31:05.175: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May  6 08:31:05.175: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 05/06/23 08:31:05.175
May  6 08:31:05.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8850 logs logs-generator logs-generator'
May  6 08:31:05.232: INFO: stderr: ""
May  6 08:31:05.232: INFO: stdout: "I0506 08:31:03.841559       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/stt 347\nI0506 08:31:04.041684       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/4gv 404\nI0506 08:31:04.242368       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/84v 507\nI0506 08:31:04.441620       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/bw2w 362\nI0506 08:31:04.641918       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/98k 483\nI0506 08:31:04.842218       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/vbl 230\nI0506 08:31:05.042507       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/85jd 535\n"
STEP: limiting log lines 05/06/23 08:31:05.232
May  6 08:31:05.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8850 logs logs-generator logs-generator --tail=1'
May  6 08:31:05.286: INFO: stderr: ""
May  6 08:31:05.286: INFO: stdout: "I0506 08:31:05.241623       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/bnb 234\n"
May  6 08:31:05.286: INFO: got output "I0506 08:31:05.241623       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/bnb 234\n"
STEP: limiting log bytes 05/06/23 08:31:05.286
May  6 08:31:05.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8850 logs logs-generator logs-generator --limit-bytes=1'
May  6 08:31:05.339: INFO: stderr: ""
May  6 08:31:05.339: INFO: stdout: "I"
May  6 08:31:05.339: INFO: got output "I"
STEP: exposing timestamps 05/06/23 08:31:05.339
May  6 08:31:05.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8850 logs logs-generator logs-generator --tail=1 --timestamps'
May  6 08:31:05.392: INFO: stderr: ""
May  6 08:31:05.392: INFO: stdout: "2023-05-06T08:31:05.241676214Z I0506 08:31:05.241623       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/bnb 234\n"
May  6 08:31:05.392: INFO: got output "2023-05-06T08:31:05.241676214Z I0506 08:31:05.241623       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/bnb 234\n"
STEP: restricting to a time range 05/06/23 08:31:05.392
May  6 08:31:07.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8850 logs logs-generator logs-generator --since=1s'
May  6 08:31:07.952: INFO: stderr: ""
May  6 08:31:07.952: INFO: stdout: "I0506 08:31:07.042585       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/g85 530\nI0506 08:31:07.241919       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/bvqb 251\nI0506 08:31:07.442410       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/kws 328\nI0506 08:31:07.641712       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/fdp 259\nI0506 08:31:07.842128       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/q62 214\n"
May  6 08:31:07.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8850 logs logs-generator logs-generator --since=24h'
May  6 08:31:08.008: INFO: stderr: ""
May  6 08:31:08.008: INFO: stdout: "I0506 08:31:03.841559       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/stt 347\nI0506 08:31:04.041684       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/4gv 404\nI0506 08:31:04.242368       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/84v 507\nI0506 08:31:04.441620       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/bw2w 362\nI0506 08:31:04.641918       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/98k 483\nI0506 08:31:04.842218       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/vbl 230\nI0506 08:31:05.042507       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/85jd 535\nI0506 08:31:05.241623       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/bnb 234\nI0506 08:31:05.441947       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/jrx 220\nI0506 08:31:05.642265       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/24x 463\nI0506 08:31:05.842569       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/l627 262\nI0506 08:31:06.041927       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/jt54 438\nI0506 08:31:06.242395       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/qbt 430\nI0506 08:31:06.441640       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/647x 493\nI0506 08:31:06.641948       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/s8w 307\nI0506 08:31:06.842275       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/v7m 486\nI0506 08:31:07.042585       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/g85 530\nI0506 08:31:07.241919       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/bvqb 251\nI0506 08:31:07.442410       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/kws 328\nI0506 08:31:07.641712       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/fdp 259\nI0506 08:31:07.842128       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/q62 214\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
May  6 08:31:08.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8850 delete pod logs-generator'
May  6 08:31:09.053: INFO: stderr: ""
May  6 08:31:09.053: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  6 08:31:09.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8850" for this suite. 05/06/23 08:31:09.058
------------------------------
â€¢ [SLOW TEST] [5.972 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:31:03.091
    May  6 08:31:03.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubectl 05/06/23 08:31:03.092
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:31:03.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:31:03.109
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 05/06/23 08:31:03.112
    May  6 08:31:03.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8850 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    May  6 08:31:03.167: INFO: stderr: ""
    May  6 08:31:03.167: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 05/06/23 08:31:03.167
    May  6 08:31:03.167: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    May  6 08:31:03.167: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8850" to be "running and ready, or succeeded"
    May  6 08:31:03.171: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.275476ms
    May  6 08:31:03.171: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'cncf-0' to be 'Running' but was 'Pending'
    May  6 08:31:05.175: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.007129708s
    May  6 08:31:05.175: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    May  6 08:31:05.175: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 05/06/23 08:31:05.175
    May  6 08:31:05.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8850 logs logs-generator logs-generator'
    May  6 08:31:05.232: INFO: stderr: ""
    May  6 08:31:05.232: INFO: stdout: "I0506 08:31:03.841559       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/stt 347\nI0506 08:31:04.041684       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/4gv 404\nI0506 08:31:04.242368       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/84v 507\nI0506 08:31:04.441620       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/bw2w 362\nI0506 08:31:04.641918       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/98k 483\nI0506 08:31:04.842218       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/vbl 230\nI0506 08:31:05.042507       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/85jd 535\n"
    STEP: limiting log lines 05/06/23 08:31:05.232
    May  6 08:31:05.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8850 logs logs-generator logs-generator --tail=1'
    May  6 08:31:05.286: INFO: stderr: ""
    May  6 08:31:05.286: INFO: stdout: "I0506 08:31:05.241623       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/bnb 234\n"
    May  6 08:31:05.286: INFO: got output "I0506 08:31:05.241623       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/bnb 234\n"
    STEP: limiting log bytes 05/06/23 08:31:05.286
    May  6 08:31:05.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8850 logs logs-generator logs-generator --limit-bytes=1'
    May  6 08:31:05.339: INFO: stderr: ""
    May  6 08:31:05.339: INFO: stdout: "I"
    May  6 08:31:05.339: INFO: got output "I"
    STEP: exposing timestamps 05/06/23 08:31:05.339
    May  6 08:31:05.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8850 logs logs-generator logs-generator --tail=1 --timestamps'
    May  6 08:31:05.392: INFO: stderr: ""
    May  6 08:31:05.392: INFO: stdout: "2023-05-06T08:31:05.241676214Z I0506 08:31:05.241623       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/bnb 234\n"
    May  6 08:31:05.392: INFO: got output "2023-05-06T08:31:05.241676214Z I0506 08:31:05.241623       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/bnb 234\n"
    STEP: restricting to a time range 05/06/23 08:31:05.392
    May  6 08:31:07.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8850 logs logs-generator logs-generator --since=1s'
    May  6 08:31:07.952: INFO: stderr: ""
    May  6 08:31:07.952: INFO: stdout: "I0506 08:31:07.042585       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/g85 530\nI0506 08:31:07.241919       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/bvqb 251\nI0506 08:31:07.442410       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/kws 328\nI0506 08:31:07.641712       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/fdp 259\nI0506 08:31:07.842128       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/q62 214\n"
    May  6 08:31:07.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8850 logs logs-generator logs-generator --since=24h'
    May  6 08:31:08.008: INFO: stderr: ""
    May  6 08:31:08.008: INFO: stdout: "I0506 08:31:03.841559       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/stt 347\nI0506 08:31:04.041684       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/4gv 404\nI0506 08:31:04.242368       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/84v 507\nI0506 08:31:04.441620       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/bw2w 362\nI0506 08:31:04.641918       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/98k 483\nI0506 08:31:04.842218       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/vbl 230\nI0506 08:31:05.042507       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/85jd 535\nI0506 08:31:05.241623       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/bnb 234\nI0506 08:31:05.441947       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/jrx 220\nI0506 08:31:05.642265       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/24x 463\nI0506 08:31:05.842569       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/l627 262\nI0506 08:31:06.041927       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/jt54 438\nI0506 08:31:06.242395       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/qbt 430\nI0506 08:31:06.441640       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/647x 493\nI0506 08:31:06.641948       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/s8w 307\nI0506 08:31:06.842275       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/v7m 486\nI0506 08:31:07.042585       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/g85 530\nI0506 08:31:07.241919       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/bvqb 251\nI0506 08:31:07.442410       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/kws 328\nI0506 08:31:07.641712       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/fdp 259\nI0506 08:31:07.842128       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/q62 214\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    May  6 08:31:08.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-8850 delete pod logs-generator'
    May  6 08:31:09.053: INFO: stderr: ""
    May  6 08:31:09.053: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  6 08:31:09.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8850" for this suite. 05/06/23 08:31:09.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:31:09.064
May  6 08:31:09.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename statefulset 05/06/23 08:31:09.065
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:31:09.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:31:09.08
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4463 05/06/23 08:31:09.082
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-4463 05/06/23 08:31:09.089
May  6 08:31:09.096: INFO: Found 0 stateful pods, waiting for 1
May  6 08:31:19.100: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 05/06/23 08:31:19.105
STEP: Getting /status 05/06/23 08:31:19.11
May  6 08:31:19.113: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 05/06/23 08:31:19.113
May  6 08:31:19.121: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 05/06/23 08:31:19.121
May  6 08:31:19.123: INFO: Observed &StatefulSet event: ADDED
May  6 08:31:19.123: INFO: Found Statefulset ss in namespace statefulset-4463 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  6 08:31:19.123: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 05/06/23 08:31:19.123
May  6 08:31:19.123: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May  6 08:31:19.130: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 05/06/23 08:31:19.13
May  6 08:31:19.131: INFO: Observed &StatefulSet event: ADDED
May  6 08:31:19.131: INFO: Observed Statefulset ss in namespace statefulset-4463 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  6 08:31:19.131: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May  6 08:31:19.131: INFO: Deleting all statefulset in ns statefulset-4463
May  6 08:31:19.133: INFO: Scaling statefulset ss to 0
May  6 08:31:29.148: INFO: Waiting for statefulset status.replicas updated to 0
May  6 08:31:29.150: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May  6 08:31:29.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4463" for this suite. 05/06/23 08:31:29.164
------------------------------
â€¢ [SLOW TEST] [20.106 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:31:09.064
    May  6 08:31:09.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename statefulset 05/06/23 08:31:09.065
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:31:09.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:31:09.08
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4463 05/06/23 08:31:09.082
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-4463 05/06/23 08:31:09.089
    May  6 08:31:09.096: INFO: Found 0 stateful pods, waiting for 1
    May  6 08:31:19.100: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 05/06/23 08:31:19.105
    STEP: Getting /status 05/06/23 08:31:19.11
    May  6 08:31:19.113: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 05/06/23 08:31:19.113
    May  6 08:31:19.121: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 05/06/23 08:31:19.121
    May  6 08:31:19.123: INFO: Observed &StatefulSet event: ADDED
    May  6 08:31:19.123: INFO: Found Statefulset ss in namespace statefulset-4463 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  6 08:31:19.123: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 05/06/23 08:31:19.123
    May  6 08:31:19.123: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May  6 08:31:19.130: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 05/06/23 08:31:19.13
    May  6 08:31:19.131: INFO: Observed &StatefulSet event: ADDED
    May  6 08:31:19.131: INFO: Observed Statefulset ss in namespace statefulset-4463 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  6 08:31:19.131: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May  6 08:31:19.131: INFO: Deleting all statefulset in ns statefulset-4463
    May  6 08:31:19.133: INFO: Scaling statefulset ss to 0
    May  6 08:31:29.148: INFO: Waiting for statefulset status.replicas updated to 0
    May  6 08:31:29.150: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May  6 08:31:29.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4463" for this suite. 05/06/23 08:31:29.164
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:31:29.17
May  6 08:31:29.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename job 05/06/23 08:31:29.171
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:31:29.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:31:29.189
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 05/06/23 08:31:29.191
STEP: Ensuring active pods == parallelism 05/06/23 08:31:29.196
STEP: Orphaning one of the Job's Pods 05/06/23 08:31:31.2
May  6 08:31:31.714: INFO: Successfully updated pod "adopt-release-hk4k8"
STEP: Checking that the Job readopts the Pod 05/06/23 08:31:31.714
May  6 08:31:31.715: INFO: Waiting up to 15m0s for pod "adopt-release-hk4k8" in namespace "job-1395" to be "adopted"
May  6 08:31:31.725: INFO: Pod "adopt-release-hk4k8": Phase="Running", Reason="", readiness=true. Elapsed: 10.032537ms
May  6 08:31:33.728: INFO: Pod "adopt-release-hk4k8": Phase="Running", Reason="", readiness=true. Elapsed: 2.013715095s
May  6 08:31:33.728: INFO: Pod "adopt-release-hk4k8" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 05/06/23 08:31:33.728
May  6 08:31:34.240: INFO: Successfully updated pod "adopt-release-hk4k8"
STEP: Checking that the Job releases the Pod 05/06/23 08:31:34.24
May  6 08:31:34.240: INFO: Waiting up to 15m0s for pod "adopt-release-hk4k8" in namespace "job-1395" to be "released"
May  6 08:31:34.243: INFO: Pod "adopt-release-hk4k8": Phase="Running", Reason="", readiness=true. Elapsed: 2.938935ms
May  6 08:31:36.246: INFO: Pod "adopt-release-hk4k8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005790394s
May  6 08:31:36.246: INFO: Pod "adopt-release-hk4k8" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May  6 08:31:36.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1395" for this suite. 05/06/23 08:31:36.25
------------------------------
â€¢ [SLOW TEST] [7.087 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:31:29.17
    May  6 08:31:29.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename job 05/06/23 08:31:29.171
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:31:29.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:31:29.189
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 05/06/23 08:31:29.191
    STEP: Ensuring active pods == parallelism 05/06/23 08:31:29.196
    STEP: Orphaning one of the Job's Pods 05/06/23 08:31:31.2
    May  6 08:31:31.714: INFO: Successfully updated pod "adopt-release-hk4k8"
    STEP: Checking that the Job readopts the Pod 05/06/23 08:31:31.714
    May  6 08:31:31.715: INFO: Waiting up to 15m0s for pod "adopt-release-hk4k8" in namespace "job-1395" to be "adopted"
    May  6 08:31:31.725: INFO: Pod "adopt-release-hk4k8": Phase="Running", Reason="", readiness=true. Elapsed: 10.032537ms
    May  6 08:31:33.728: INFO: Pod "adopt-release-hk4k8": Phase="Running", Reason="", readiness=true. Elapsed: 2.013715095s
    May  6 08:31:33.728: INFO: Pod "adopt-release-hk4k8" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 05/06/23 08:31:33.728
    May  6 08:31:34.240: INFO: Successfully updated pod "adopt-release-hk4k8"
    STEP: Checking that the Job releases the Pod 05/06/23 08:31:34.24
    May  6 08:31:34.240: INFO: Waiting up to 15m0s for pod "adopt-release-hk4k8" in namespace "job-1395" to be "released"
    May  6 08:31:34.243: INFO: Pod "adopt-release-hk4k8": Phase="Running", Reason="", readiness=true. Elapsed: 2.938935ms
    May  6 08:31:36.246: INFO: Pod "adopt-release-hk4k8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005790394s
    May  6 08:31:36.246: INFO: Pod "adopt-release-hk4k8" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May  6 08:31:36.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1395" for this suite. 05/06/23 08:31:36.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:31:36.257
May  6 08:31:36.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename gc 05/06/23 08:31:36.258
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:31:36.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:31:36.278
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 05/06/23 08:31:36.283
STEP: create the rc2 05/06/23 08:31:36.289
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 05/06/23 08:31:41.32
STEP: delete the rc simpletest-rc-to-be-deleted 05/06/23 08:31:42.252
STEP: wait for the rc to be deleted 05/06/23 08:31:42.272
May  6 08:31:47.284: INFO: 68 pods remaining
May  6 08:31:47.284: INFO: 68 pods has nil DeletionTimestamp
May  6 08:31:47.284: INFO: 
STEP: Gathering metrics 05/06/23 08:31:52.281
May  6 08:31:52.518: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-2" in namespace "kube-system" to be "running and ready"
May  6 08:31:52.520: INFO: Pod "kube-controller-manager-cncf-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.391445ms
May  6 08:31:52.520: INFO: The phase of Pod kube-controller-manager-cncf-2 is Running (Ready = true)
May  6 08:31:52.520: INFO: Pod "kube-controller-manager-cncf-2" satisfied condition "running and ready"
May  6 08:31:52.558: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May  6 08:31:52.558: INFO: Deleting pod "simpletest-rc-to-be-deleted-2b6kf" in namespace "gc-4099"
May  6 08:31:52.573: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bl4p" in namespace "gc-4099"
May  6 08:31:52.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hgp9" in namespace "gc-4099"
May  6 08:31:52.600: INFO: Deleting pod "simpletest-rc-to-be-deleted-4dznp" in namespace "gc-4099"
May  6 08:31:52.619: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jblz" in namespace "gc-4099"
May  6 08:31:52.638: INFO: Deleting pod "simpletest-rc-to-be-deleted-5cdtb" in namespace "gc-4099"
May  6 08:31:52.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-5l2tt" in namespace "gc-4099"
May  6 08:31:52.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mk6j" in namespace "gc-4099"
May  6 08:31:52.692: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vt55" in namespace "gc-4099"
May  6 08:31:52.713: INFO: Deleting pod "simpletest-rc-to-be-deleted-62cnz" in namespace "gc-4099"
May  6 08:31:52.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gwvz" in namespace "gc-4099"
May  6 08:31:52.782: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mqqn" in namespace "gc-4099"
May  6 08:31:52.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-7js5j" in namespace "gc-4099"
May  6 08:31:52.830: INFO: Deleting pod "simpletest-rc-to-be-deleted-7znh4" in namespace "gc-4099"
May  6 08:31:52.844: INFO: Deleting pod "simpletest-rc-to-be-deleted-844sl" in namespace "gc-4099"
May  6 08:31:52.867: INFO: Deleting pod "simpletest-rc-to-be-deleted-86fdv" in namespace "gc-4099"
May  6 08:31:52.889: INFO: Deleting pod "simpletest-rc-to-be-deleted-97whk" in namespace "gc-4099"
May  6 08:31:52.935: INFO: Deleting pod "simpletest-rc-to-be-deleted-9c5dz" in namespace "gc-4099"
May  6 08:31:52.995: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fkd4" in namespace "gc-4099"
May  6 08:31:53.027: INFO: Deleting pod "simpletest-rc-to-be-deleted-bk6xz" in namespace "gc-4099"
May  6 08:31:53.074: INFO: Deleting pod "simpletest-rc-to-be-deleted-brxd8" in namespace "gc-4099"
May  6 08:31:53.114: INFO: Deleting pod "simpletest-rc-to-be-deleted-bsjwf" in namespace "gc-4099"
May  6 08:31:53.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-bxskp" in namespace "gc-4099"
May  6 08:31:53.183: INFO: Deleting pod "simpletest-rc-to-be-deleted-c62wm" in namespace "gc-4099"
May  6 08:31:53.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-cc4lf" in namespace "gc-4099"
May  6 08:31:53.277: INFO: Deleting pod "simpletest-rc-to-be-deleted-crxpf" in namespace "gc-4099"
May  6 08:31:53.305: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7lks" in namespace "gc-4099"
May  6 08:31:53.327: INFO: Deleting pod "simpletest-rc-to-be-deleted-dszmd" in namespace "gc-4099"
May  6 08:31:53.371: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtwwj" in namespace "gc-4099"
May  6 08:31:53.393: INFO: Deleting pod "simpletest-rc-to-be-deleted-dv6bd" in namespace "gc-4099"
May  6 08:31:53.425: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmgcl" in namespace "gc-4099"
May  6 08:31:53.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmh9q" in namespace "gc-4099"
May  6 08:31:53.618: INFO: Deleting pod "simpletest-rc-to-be-deleted-fv7hb" in namespace "gc-4099"
May  6 08:31:53.658: INFO: Deleting pod "simpletest-rc-to-be-deleted-fw5nq" in namespace "gc-4099"
May  6 08:31:53.690: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6r8b" in namespace "gc-4099"
May  6 08:31:53.733: INFO: Deleting pod "simpletest-rc-to-be-deleted-gc4h9" in namespace "gc-4099"
May  6 08:31:53.767: INFO: Deleting pod "simpletest-rc-to-be-deleted-gddxg" in namespace "gc-4099"
May  6 08:31:53.802: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjzfq" in namespace "gc-4099"
May  6 08:31:53.838: INFO: Deleting pod "simpletest-rc-to-be-deleted-glfvc" in namespace "gc-4099"
May  6 08:31:53.862: INFO: Deleting pod "simpletest-rc-to-be-deleted-gq7sc" in namespace "gc-4099"
May  6 08:31:53.886: INFO: Deleting pod "simpletest-rc-to-be-deleted-gq9wd" in namespace "gc-4099"
May  6 08:31:53.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-gqthm" in namespace "gc-4099"
May  6 08:31:53.955: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsrp7" in namespace "gc-4099"
May  6 08:31:53.972: INFO: Deleting pod "simpletest-rc-to-be-deleted-gztg5" in namespace "gc-4099"
May  6 08:31:53.998: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8p65" in namespace "gc-4099"
May  6 08:31:54.026: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbf79" in namespace "gc-4099"
May  6 08:31:54.065: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdkz9" in namespace "gc-4099"
May  6 08:31:54.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhbmm" in namespace "gc-4099"
May  6 08:31:54.150: INFO: Deleting pod "simpletest-rc-to-be-deleted-hkjbm" in namespace "gc-4099"
May  6 08:31:54.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-hn79h" in namespace "gc-4099"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May  6 08:31:54.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4099" for this suite. 05/06/23 08:31:54.267
------------------------------
â€¢ [SLOW TEST] [18.032 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:31:36.257
    May  6 08:31:36.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename gc 05/06/23 08:31:36.258
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:31:36.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:31:36.278
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 05/06/23 08:31:36.283
    STEP: create the rc2 05/06/23 08:31:36.289
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 05/06/23 08:31:41.32
    STEP: delete the rc simpletest-rc-to-be-deleted 05/06/23 08:31:42.252
    STEP: wait for the rc to be deleted 05/06/23 08:31:42.272
    May  6 08:31:47.284: INFO: 68 pods remaining
    May  6 08:31:47.284: INFO: 68 pods has nil DeletionTimestamp
    May  6 08:31:47.284: INFO: 
    STEP: Gathering metrics 05/06/23 08:31:52.281
    May  6 08:31:52.518: INFO: Waiting up to 5m0s for pod "kube-controller-manager-cncf-2" in namespace "kube-system" to be "running and ready"
    May  6 08:31:52.520: INFO: Pod "kube-controller-manager-cncf-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.391445ms
    May  6 08:31:52.520: INFO: The phase of Pod kube-controller-manager-cncf-2 is Running (Ready = true)
    May  6 08:31:52.520: INFO: Pod "kube-controller-manager-cncf-2" satisfied condition "running and ready"
    May  6 08:31:52.558: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    May  6 08:31:52.558: INFO: Deleting pod "simpletest-rc-to-be-deleted-2b6kf" in namespace "gc-4099"
    May  6 08:31:52.573: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bl4p" in namespace "gc-4099"
    May  6 08:31:52.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hgp9" in namespace "gc-4099"
    May  6 08:31:52.600: INFO: Deleting pod "simpletest-rc-to-be-deleted-4dznp" in namespace "gc-4099"
    May  6 08:31:52.619: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jblz" in namespace "gc-4099"
    May  6 08:31:52.638: INFO: Deleting pod "simpletest-rc-to-be-deleted-5cdtb" in namespace "gc-4099"
    May  6 08:31:52.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-5l2tt" in namespace "gc-4099"
    May  6 08:31:52.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mk6j" in namespace "gc-4099"
    May  6 08:31:52.692: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vt55" in namespace "gc-4099"
    May  6 08:31:52.713: INFO: Deleting pod "simpletest-rc-to-be-deleted-62cnz" in namespace "gc-4099"
    May  6 08:31:52.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gwvz" in namespace "gc-4099"
    May  6 08:31:52.782: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mqqn" in namespace "gc-4099"
    May  6 08:31:52.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-7js5j" in namespace "gc-4099"
    May  6 08:31:52.830: INFO: Deleting pod "simpletest-rc-to-be-deleted-7znh4" in namespace "gc-4099"
    May  6 08:31:52.844: INFO: Deleting pod "simpletest-rc-to-be-deleted-844sl" in namespace "gc-4099"
    May  6 08:31:52.867: INFO: Deleting pod "simpletest-rc-to-be-deleted-86fdv" in namespace "gc-4099"
    May  6 08:31:52.889: INFO: Deleting pod "simpletest-rc-to-be-deleted-97whk" in namespace "gc-4099"
    May  6 08:31:52.935: INFO: Deleting pod "simpletest-rc-to-be-deleted-9c5dz" in namespace "gc-4099"
    May  6 08:31:52.995: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fkd4" in namespace "gc-4099"
    May  6 08:31:53.027: INFO: Deleting pod "simpletest-rc-to-be-deleted-bk6xz" in namespace "gc-4099"
    May  6 08:31:53.074: INFO: Deleting pod "simpletest-rc-to-be-deleted-brxd8" in namespace "gc-4099"
    May  6 08:31:53.114: INFO: Deleting pod "simpletest-rc-to-be-deleted-bsjwf" in namespace "gc-4099"
    May  6 08:31:53.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-bxskp" in namespace "gc-4099"
    May  6 08:31:53.183: INFO: Deleting pod "simpletest-rc-to-be-deleted-c62wm" in namespace "gc-4099"
    May  6 08:31:53.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-cc4lf" in namespace "gc-4099"
    May  6 08:31:53.277: INFO: Deleting pod "simpletest-rc-to-be-deleted-crxpf" in namespace "gc-4099"
    May  6 08:31:53.305: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7lks" in namespace "gc-4099"
    May  6 08:31:53.327: INFO: Deleting pod "simpletest-rc-to-be-deleted-dszmd" in namespace "gc-4099"
    May  6 08:31:53.371: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtwwj" in namespace "gc-4099"
    May  6 08:31:53.393: INFO: Deleting pod "simpletest-rc-to-be-deleted-dv6bd" in namespace "gc-4099"
    May  6 08:31:53.425: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmgcl" in namespace "gc-4099"
    May  6 08:31:53.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmh9q" in namespace "gc-4099"
    May  6 08:31:53.618: INFO: Deleting pod "simpletest-rc-to-be-deleted-fv7hb" in namespace "gc-4099"
    May  6 08:31:53.658: INFO: Deleting pod "simpletest-rc-to-be-deleted-fw5nq" in namespace "gc-4099"
    May  6 08:31:53.690: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6r8b" in namespace "gc-4099"
    May  6 08:31:53.733: INFO: Deleting pod "simpletest-rc-to-be-deleted-gc4h9" in namespace "gc-4099"
    May  6 08:31:53.767: INFO: Deleting pod "simpletest-rc-to-be-deleted-gddxg" in namespace "gc-4099"
    May  6 08:31:53.802: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjzfq" in namespace "gc-4099"
    May  6 08:31:53.838: INFO: Deleting pod "simpletest-rc-to-be-deleted-glfvc" in namespace "gc-4099"
    May  6 08:31:53.862: INFO: Deleting pod "simpletest-rc-to-be-deleted-gq7sc" in namespace "gc-4099"
    May  6 08:31:53.886: INFO: Deleting pod "simpletest-rc-to-be-deleted-gq9wd" in namespace "gc-4099"
    May  6 08:31:53.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-gqthm" in namespace "gc-4099"
    May  6 08:31:53.955: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsrp7" in namespace "gc-4099"
    May  6 08:31:53.972: INFO: Deleting pod "simpletest-rc-to-be-deleted-gztg5" in namespace "gc-4099"
    May  6 08:31:53.998: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8p65" in namespace "gc-4099"
    May  6 08:31:54.026: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbf79" in namespace "gc-4099"
    May  6 08:31:54.065: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdkz9" in namespace "gc-4099"
    May  6 08:31:54.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhbmm" in namespace "gc-4099"
    May  6 08:31:54.150: INFO: Deleting pod "simpletest-rc-to-be-deleted-hkjbm" in namespace "gc-4099"
    May  6 08:31:54.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-hn79h" in namespace "gc-4099"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May  6 08:31:54.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4099" for this suite. 05/06/23 08:31:54.267
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:31:54.29
May  6 08:31:54.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename configmap 05/06/23 08:31:54.291
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:31:54.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:31:54.337
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-9eb39ec0-aeb6-43b3-99fa-1d9f6caa5af1 05/06/23 08:31:54.349
STEP: Creating configMap with name cm-test-opt-upd-999309dd-8bd2-45cc-a2d4-b766f7986bf7 05/06/23 08:31:54.366
STEP: Creating the pod 05/06/23 08:31:54.385
May  6 08:31:54.401: INFO: Waiting up to 5m0s for pod "pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3" in namespace "configmap-3646" to be "running and ready"
May  6 08:31:54.413: INFO: Pod "pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.304255ms
May  6 08:31:54.413: INFO: The phase of Pod pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3 is Pending, waiting for it to be Running (with Ready = true)
May  6 08:31:56.417: INFO: Pod "pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015604555s
May  6 08:31:56.417: INFO: The phase of Pod pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3 is Pending, waiting for it to be Running (with Ready = true)
May  6 08:31:58.417: INFO: Pod "pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015662179s
May  6 08:31:58.417: INFO: The phase of Pod pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3 is Pending, waiting for it to be Running (with Ready = true)
May  6 08:32:00.417: INFO: Pod "pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3": Phase="Running", Reason="", readiness=true. Elapsed: 6.015613093s
May  6 08:32:00.417: INFO: The phase of Pod pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3 is Running (Ready = true)
May  6 08:32:00.417: INFO: Pod "pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-9eb39ec0-aeb6-43b3-99fa-1d9f6caa5af1 05/06/23 08:32:00.435
STEP: Updating configmap cm-test-opt-upd-999309dd-8bd2-45cc-a2d4-b766f7986bf7 05/06/23 08:32:00.441
STEP: Creating configMap with name cm-test-opt-create-2c036d0c-8702-464f-8a22-9028c907d9e8 05/06/23 08:32:00.446
STEP: waiting to observe update in volume 05/06/23 08:32:00.45
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  6 08:33:06.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3646" for this suite. 05/06/23 08:33:06.691
------------------------------
â€¢ [SLOW TEST] [72.407 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:31:54.29
    May  6 08:31:54.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename configmap 05/06/23 08:31:54.291
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:31:54.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:31:54.337
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-9eb39ec0-aeb6-43b3-99fa-1d9f6caa5af1 05/06/23 08:31:54.349
    STEP: Creating configMap with name cm-test-opt-upd-999309dd-8bd2-45cc-a2d4-b766f7986bf7 05/06/23 08:31:54.366
    STEP: Creating the pod 05/06/23 08:31:54.385
    May  6 08:31:54.401: INFO: Waiting up to 5m0s for pod "pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3" in namespace "configmap-3646" to be "running and ready"
    May  6 08:31:54.413: INFO: Pod "pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.304255ms
    May  6 08:31:54.413: INFO: The phase of Pod pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3 is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:31:56.417: INFO: Pod "pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015604555s
    May  6 08:31:56.417: INFO: The phase of Pod pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3 is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:31:58.417: INFO: Pod "pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015662179s
    May  6 08:31:58.417: INFO: The phase of Pod pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3 is Pending, waiting for it to be Running (with Ready = true)
    May  6 08:32:00.417: INFO: Pod "pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3": Phase="Running", Reason="", readiness=true. Elapsed: 6.015613093s
    May  6 08:32:00.417: INFO: The phase of Pod pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3 is Running (Ready = true)
    May  6 08:32:00.417: INFO: Pod "pod-configmaps-99179f99-8221-4420-ac26-e920c33193f3" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-9eb39ec0-aeb6-43b3-99fa-1d9f6caa5af1 05/06/23 08:32:00.435
    STEP: Updating configmap cm-test-opt-upd-999309dd-8bd2-45cc-a2d4-b766f7986bf7 05/06/23 08:32:00.441
    STEP: Creating configMap with name cm-test-opt-create-2c036d0c-8702-464f-8a22-9028c907d9e8 05/06/23 08:32:00.446
    STEP: waiting to observe update in volume 05/06/23 08:32:00.45
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  6 08:33:06.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3646" for this suite. 05/06/23 08:33:06.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:33:06.698
May  6 08:33:06.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename resourcequota 05/06/23 08:33:06.699
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:33:06.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:33:06.715
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 05/06/23 08:33:23.719
STEP: Creating a ResourceQuota 05/06/23 08:33:28.722
STEP: Ensuring resource quota status is calculated 05/06/23 08:33:28.727
STEP: Creating a ConfigMap 05/06/23 08:33:30.729
STEP: Ensuring resource quota status captures configMap creation 05/06/23 08:33:30.74
STEP: Deleting a ConfigMap 05/06/23 08:33:32.744
STEP: Ensuring resource quota status released usage 05/06/23 08:33:32.749
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May  6 08:33:34.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9470" for this suite. 05/06/23 08:33:34.756
------------------------------
â€¢ [SLOW TEST] [28.063 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:33:06.698
    May  6 08:33:06.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename resourcequota 05/06/23 08:33:06.699
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:33:06.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:33:06.715
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 05/06/23 08:33:23.719
    STEP: Creating a ResourceQuota 05/06/23 08:33:28.722
    STEP: Ensuring resource quota status is calculated 05/06/23 08:33:28.727
    STEP: Creating a ConfigMap 05/06/23 08:33:30.729
    STEP: Ensuring resource quota status captures configMap creation 05/06/23 08:33:30.74
    STEP: Deleting a ConfigMap 05/06/23 08:33:32.744
    STEP: Ensuring resource quota status released usage 05/06/23 08:33:32.749
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May  6 08:33:34.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9470" for this suite. 05/06/23 08:33:34.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:33:34.762
May  6 08:33:34.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename configmap 05/06/23 08:33:34.763
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:33:34.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:33:34.78
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-ba600253-30e7-467a-9660-2012168d53c5 05/06/23 08:33:34.782
STEP: Creating a pod to test consume configMaps 05/06/23 08:33:34.785
May  6 08:33:34.792: INFO: Waiting up to 5m0s for pod "pod-configmaps-a09feaec-99f4-438d-9ce6-310b2825645a" in namespace "configmap-1007" to be "Succeeded or Failed"
May  6 08:33:34.795: INFO: Pod "pod-configmaps-a09feaec-99f4-438d-9ce6-310b2825645a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.179212ms
May  6 08:33:36.799: INFO: Pod "pod-configmaps-a09feaec-99f4-438d-9ce6-310b2825645a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006975482s
May  6 08:33:38.798: INFO: Pod "pod-configmaps-a09feaec-99f4-438d-9ce6-310b2825645a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005704232s
STEP: Saw pod success 05/06/23 08:33:38.798
May  6 08:33:38.798: INFO: Pod "pod-configmaps-a09feaec-99f4-438d-9ce6-310b2825645a" satisfied condition "Succeeded or Failed"
May  6 08:33:38.800: INFO: Trying to get logs from node cncf-0 pod pod-configmaps-a09feaec-99f4-438d-9ce6-310b2825645a container agnhost-container: <nil>
STEP: delete the pod 05/06/23 08:33:38.805
May  6 08:33:38.815: INFO: Waiting for pod pod-configmaps-a09feaec-99f4-438d-9ce6-310b2825645a to disappear
May  6 08:33:38.817: INFO: Pod pod-configmaps-a09feaec-99f4-438d-9ce6-310b2825645a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  6 08:33:38.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1007" for this suite. 05/06/23 08:33:38.819
------------------------------
â€¢ [4.062 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:33:34.762
    May  6 08:33:34.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename configmap 05/06/23 08:33:34.763
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:33:34.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:33:34.78
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-ba600253-30e7-467a-9660-2012168d53c5 05/06/23 08:33:34.782
    STEP: Creating a pod to test consume configMaps 05/06/23 08:33:34.785
    May  6 08:33:34.792: INFO: Waiting up to 5m0s for pod "pod-configmaps-a09feaec-99f4-438d-9ce6-310b2825645a" in namespace "configmap-1007" to be "Succeeded or Failed"
    May  6 08:33:34.795: INFO: Pod "pod-configmaps-a09feaec-99f4-438d-9ce6-310b2825645a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.179212ms
    May  6 08:33:36.799: INFO: Pod "pod-configmaps-a09feaec-99f4-438d-9ce6-310b2825645a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006975482s
    May  6 08:33:38.798: INFO: Pod "pod-configmaps-a09feaec-99f4-438d-9ce6-310b2825645a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005704232s
    STEP: Saw pod success 05/06/23 08:33:38.798
    May  6 08:33:38.798: INFO: Pod "pod-configmaps-a09feaec-99f4-438d-9ce6-310b2825645a" satisfied condition "Succeeded or Failed"
    May  6 08:33:38.800: INFO: Trying to get logs from node cncf-0 pod pod-configmaps-a09feaec-99f4-438d-9ce6-310b2825645a container agnhost-container: <nil>
    STEP: delete the pod 05/06/23 08:33:38.805
    May  6 08:33:38.815: INFO: Waiting for pod pod-configmaps-a09feaec-99f4-438d-9ce6-310b2825645a to disappear
    May  6 08:33:38.817: INFO: Pod pod-configmaps-a09feaec-99f4-438d-9ce6-310b2825645a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  6 08:33:38.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1007" for this suite. 05/06/23 08:33:38.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:33:38.826
May  6 08:33:38.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename job 05/06/23 08:33:38.827
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:33:38.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:33:38.844
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 05/06/23 08:33:38.846
STEP: Ensuring job reaches completions 05/06/23 08:33:38.85
STEP: Ensuring pods with index for job exist 05/06/23 08:33:46.853
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May  6 08:33:46.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2034" for this suite. 05/06/23 08:33:46.859
------------------------------
â€¢ [SLOW TEST] [8.039 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:33:38.826
    May  6 08:33:38.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename job 05/06/23 08:33:38.827
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:33:38.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:33:38.844
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 05/06/23 08:33:38.846
    STEP: Ensuring job reaches completions 05/06/23 08:33:38.85
    STEP: Ensuring pods with index for job exist 05/06/23 08:33:46.853
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May  6 08:33:46.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2034" for this suite. 05/06/23 08:33:46.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:33:46.865
May  6 08:33:46.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename cronjob 05/06/23 08:33:46.865
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:33:46.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:33:46.883
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 05/06/23 08:33:46.885
STEP: Ensuring a job is scheduled 05/06/23 08:33:46.89
STEP: Ensuring exactly one is scheduled 05/06/23 08:34:00.892
STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/06/23 08:34:00.895
STEP: Ensuring the job is replaced with a new one 05/06/23 08:34:00.898
STEP: Removing cronjob 05/06/23 08:35:00.901
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May  6 08:35:00.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1081" for this suite. 05/06/23 08:35:00.909
------------------------------
â€¢ [SLOW TEST] [74.050 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:33:46.865
    May  6 08:33:46.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename cronjob 05/06/23 08:33:46.865
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:33:46.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:33:46.883
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 05/06/23 08:33:46.885
    STEP: Ensuring a job is scheduled 05/06/23 08:33:46.89
    STEP: Ensuring exactly one is scheduled 05/06/23 08:34:00.892
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/06/23 08:34:00.895
    STEP: Ensuring the job is replaced with a new one 05/06/23 08:34:00.898
    STEP: Removing cronjob 05/06/23 08:35:00.901
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May  6 08:35:00.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1081" for this suite. 05/06/23 08:35:00.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:35:00.915
May  6 08:35:00.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename dns 05/06/23 08:35:00.916
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:35:00.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:35:00.935
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 05/06/23 08:35:00.937
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3271.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local; sleep 1; done
 05/06/23 08:35:00.94
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3271.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local; sleep 1; done
 05/06/23 08:35:00.94
STEP: creating a pod to probe DNS 05/06/23 08:35:00.941
STEP: submitting the pod to kubernetes 05/06/23 08:35:00.941
May  6 08:35:00.948: INFO: Waiting up to 15m0s for pod "dns-test-f2d5229e-9107-4f65-995a-de46d094499f" in namespace "dns-3271" to be "running"
May  6 08:35:00.951: INFO: Pod "dns-test-f2d5229e-9107-4f65-995a-de46d094499f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.531199ms
May  6 08:35:02.954: INFO: Pod "dns-test-f2d5229e-9107-4f65-995a-de46d094499f": Phase="Running", Reason="", readiness=true. Elapsed: 2.005879884s
May  6 08:35:02.954: INFO: Pod "dns-test-f2d5229e-9107-4f65-995a-de46d094499f" satisfied condition "running"
STEP: retrieving the pod 05/06/23 08:35:02.954
STEP: looking for the results for each expected name from probers 05/06/23 08:35:02.956
May  6 08:35:02.963: INFO: DNS probes using dns-test-f2d5229e-9107-4f65-995a-de46d094499f succeeded

STEP: deleting the pod 05/06/23 08:35:02.963
STEP: changing the externalName to bar.example.com 05/06/23 08:35:02.974
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3271.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local; sleep 1; done
 05/06/23 08:35:02.981
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3271.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local; sleep 1; done
 05/06/23 08:35:02.981
STEP: creating a second pod to probe DNS 05/06/23 08:35:02.981
STEP: submitting the pod to kubernetes 05/06/23 08:35:02.981
May  6 08:35:02.987: INFO: Waiting up to 15m0s for pod "dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd" in namespace "dns-3271" to be "running"
May  6 08:35:02.990: INFO: Pod "dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.717024ms
May  6 08:35:04.994: INFO: Pod "dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd": Phase="Running", Reason="", readiness=true. Elapsed: 2.006650712s
May  6 08:35:04.994: INFO: Pod "dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd" satisfied condition "running"
STEP: retrieving the pod 05/06/23 08:35:04.994
STEP: looking for the results for each expected name from probers 05/06/23 08:35:04.996
May  6 08:35:05.000: INFO: File wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
' instead of 'bar.example.com.'
May  6 08:35:05.004: INFO: File jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
' instead of 'bar.example.com.'
May  6 08:35:05.004: INFO: Lookups using dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd failed for: [wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local]

May  6 08:35:10.008: INFO: File wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
' instead of 'bar.example.com.'
May  6 08:35:10.012: INFO: File jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
' instead of 'bar.example.com.'
May  6 08:35:10.012: INFO: Lookups using dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd failed for: [wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local]

May  6 08:35:15.007: INFO: File wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
' instead of 'bar.example.com.'
May  6 08:35:15.011: INFO: File jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
' instead of 'bar.example.com.'
May  6 08:35:15.011: INFO: Lookups using dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd failed for: [wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local]

May  6 08:35:20.008: INFO: File wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
' instead of 'bar.example.com.'
May  6 08:35:20.012: INFO: File jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
' instead of 'bar.example.com.'
May  6 08:35:20.012: INFO: Lookups using dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd failed for: [wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local]

May  6 08:35:25.008: INFO: File wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
' instead of 'bar.example.com.'
May  6 08:35:25.011: INFO: File jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
' instead of 'bar.example.com.'
May  6 08:35:25.011: INFO: Lookups using dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd failed for: [wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local]

May  6 08:35:30.008: INFO: File wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
' instead of 'bar.example.com.'
May  6 08:35:30.011: INFO: File jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
' instead of 'bar.example.com.'
May  6 08:35:30.011: INFO: Lookups using dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd failed for: [wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local]

May  6 08:35:35.011: INFO: DNS probes using dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd succeeded

STEP: deleting the pod 05/06/23 08:35:35.011
STEP: changing the service to type=ClusterIP 05/06/23 08:35:35.028
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3271.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local; sleep 1; done
 05/06/23 08:35:35.043
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3271.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local; sleep 1; done
 05/06/23 08:35:35.043
STEP: creating a third pod to probe DNS 05/06/23 08:35:35.043
STEP: submitting the pod to kubernetes 05/06/23 08:35:35.046
May  6 08:35:35.057: INFO: Waiting up to 15m0s for pod "dns-test-3641d913-a81a-4afa-83a5-327c219523d2" in namespace "dns-3271" to be "running"
May  6 08:35:35.059: INFO: Pod "dns-test-3641d913-a81a-4afa-83a5-327c219523d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.438133ms
May  6 08:35:37.063: INFO: Pod "dns-test-3641d913-a81a-4afa-83a5-327c219523d2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006392631s
May  6 08:35:37.063: INFO: Pod "dns-test-3641d913-a81a-4afa-83a5-327c219523d2" satisfied condition "running"
STEP: retrieving the pod 05/06/23 08:35:37.063
STEP: looking for the results for each expected name from probers 05/06/23 08:35:37.066
May  6 08:35:37.073: INFO: DNS probes using dns-test-3641d913-a81a-4afa-83a5-327c219523d2 succeeded

STEP: deleting the pod 05/06/23 08:35:37.073
STEP: deleting the test externalName service 05/06/23 08:35:37.083
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May  6 08:35:37.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3271" for this suite. 05/06/23 08:35:37.101
------------------------------
â€¢ [SLOW TEST] [36.193 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:35:00.915
    May  6 08:35:00.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename dns 05/06/23 08:35:00.916
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:35:00.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:35:00.935
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 05/06/23 08:35:00.937
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3271.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local; sleep 1; done
     05/06/23 08:35:00.94
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3271.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local; sleep 1; done
     05/06/23 08:35:00.94
    STEP: creating a pod to probe DNS 05/06/23 08:35:00.941
    STEP: submitting the pod to kubernetes 05/06/23 08:35:00.941
    May  6 08:35:00.948: INFO: Waiting up to 15m0s for pod "dns-test-f2d5229e-9107-4f65-995a-de46d094499f" in namespace "dns-3271" to be "running"
    May  6 08:35:00.951: INFO: Pod "dns-test-f2d5229e-9107-4f65-995a-de46d094499f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.531199ms
    May  6 08:35:02.954: INFO: Pod "dns-test-f2d5229e-9107-4f65-995a-de46d094499f": Phase="Running", Reason="", readiness=true. Elapsed: 2.005879884s
    May  6 08:35:02.954: INFO: Pod "dns-test-f2d5229e-9107-4f65-995a-de46d094499f" satisfied condition "running"
    STEP: retrieving the pod 05/06/23 08:35:02.954
    STEP: looking for the results for each expected name from probers 05/06/23 08:35:02.956
    May  6 08:35:02.963: INFO: DNS probes using dns-test-f2d5229e-9107-4f65-995a-de46d094499f succeeded

    STEP: deleting the pod 05/06/23 08:35:02.963
    STEP: changing the externalName to bar.example.com 05/06/23 08:35:02.974
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3271.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local; sleep 1; done
     05/06/23 08:35:02.981
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3271.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local; sleep 1; done
     05/06/23 08:35:02.981
    STEP: creating a second pod to probe DNS 05/06/23 08:35:02.981
    STEP: submitting the pod to kubernetes 05/06/23 08:35:02.981
    May  6 08:35:02.987: INFO: Waiting up to 15m0s for pod "dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd" in namespace "dns-3271" to be "running"
    May  6 08:35:02.990: INFO: Pod "dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.717024ms
    May  6 08:35:04.994: INFO: Pod "dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd": Phase="Running", Reason="", readiness=true. Elapsed: 2.006650712s
    May  6 08:35:04.994: INFO: Pod "dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd" satisfied condition "running"
    STEP: retrieving the pod 05/06/23 08:35:04.994
    STEP: looking for the results for each expected name from probers 05/06/23 08:35:04.996
    May  6 08:35:05.000: INFO: File wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  6 08:35:05.004: INFO: File jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  6 08:35:05.004: INFO: Lookups using dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd failed for: [wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local]

    May  6 08:35:10.008: INFO: File wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  6 08:35:10.012: INFO: File jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  6 08:35:10.012: INFO: Lookups using dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd failed for: [wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local]

    May  6 08:35:15.007: INFO: File wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  6 08:35:15.011: INFO: File jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  6 08:35:15.011: INFO: Lookups using dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd failed for: [wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local]

    May  6 08:35:20.008: INFO: File wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  6 08:35:20.012: INFO: File jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  6 08:35:20.012: INFO: Lookups using dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd failed for: [wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local]

    May  6 08:35:25.008: INFO: File wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  6 08:35:25.011: INFO: File jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  6 08:35:25.011: INFO: Lookups using dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd failed for: [wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local]

    May  6 08:35:30.008: INFO: File wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  6 08:35:30.011: INFO: File jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local from pod  dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May  6 08:35:30.011: INFO: Lookups using dns-3271/dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd failed for: [wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local]

    May  6 08:35:35.011: INFO: DNS probes using dns-test-966e4ec6-2b6f-454c-b806-00dfe64c5bcd succeeded

    STEP: deleting the pod 05/06/23 08:35:35.011
    STEP: changing the service to type=ClusterIP 05/06/23 08:35:35.028
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3271.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3271.svc.cluster.local; sleep 1; done
     05/06/23 08:35:35.043
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3271.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3271.svc.cluster.local; sleep 1; done
     05/06/23 08:35:35.043
    STEP: creating a third pod to probe DNS 05/06/23 08:35:35.043
    STEP: submitting the pod to kubernetes 05/06/23 08:35:35.046
    May  6 08:35:35.057: INFO: Waiting up to 15m0s for pod "dns-test-3641d913-a81a-4afa-83a5-327c219523d2" in namespace "dns-3271" to be "running"
    May  6 08:35:35.059: INFO: Pod "dns-test-3641d913-a81a-4afa-83a5-327c219523d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.438133ms
    May  6 08:35:37.063: INFO: Pod "dns-test-3641d913-a81a-4afa-83a5-327c219523d2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006392631s
    May  6 08:35:37.063: INFO: Pod "dns-test-3641d913-a81a-4afa-83a5-327c219523d2" satisfied condition "running"
    STEP: retrieving the pod 05/06/23 08:35:37.063
    STEP: looking for the results for each expected name from probers 05/06/23 08:35:37.066
    May  6 08:35:37.073: INFO: DNS probes using dns-test-3641d913-a81a-4afa-83a5-327c219523d2 succeeded

    STEP: deleting the pod 05/06/23 08:35:37.073
    STEP: deleting the test externalName service 05/06/23 08:35:37.083
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May  6 08:35:37.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3271" for this suite. 05/06/23 08:35:37.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:35:37.109
May  6 08:35:37.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename deployment 05/06/23 08:35:37.11
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:35:37.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:35:37.127
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
May  6 08:35:37.129: INFO: Creating deployment "test-recreate-deployment"
May  6 08:35:37.134: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May  6 08:35:37.140: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
May  6 08:35:39.146: INFO: Waiting deployment "test-recreate-deployment" to complete
May  6 08:35:39.149: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May  6 08:35:39.157: INFO: Updating deployment test-recreate-deployment
May  6 08:35:39.157: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  6 08:35:39.223: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2473  2093f846-0afd-44e9-8a36-d24a8ae022f7 189037 2 2023-05-06 08:35:37 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-06 08:35:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 08:35:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00260f8e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-06 08:35:39 +0000 UTC,LastTransitionTime:2023-05-06 08:35:39 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-05-06 08:35:39 +0000 UTC,LastTransitionTime:2023-05-06 08:35:37 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May  6 08:35:39.225: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-2473  46d6ec6e-357f-421f-8345-01350044b6c6 189035 1 2023-05-06 08:35:39 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 2093f846-0afd-44e9-8a36-d24a8ae022f7 0xc002eda860 0xc002eda861}] [] [{kube-controller-manager Update apps/v1 2023-05-06 08:35:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2093f846-0afd-44e9-8a36-d24a8ae022f7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 08:35:39 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002eda8f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 08:35:39.225: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May  6 08:35:39.225: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-2473  07925189-efdf-4462-bf46-a5eb793d4854 189025 2 2023-05-06 08:35:37 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 2093f846-0afd-44e9-8a36-d24a8ae022f7 0xc002eda747 0xc002eda748}] [] [{kube-controller-manager Update apps/v1 2023-05-06 08:35:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2093f846-0afd-44e9-8a36-d24a8ae022f7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 08:35:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002eda7f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  6 08:35:39.227: INFO: Pod "test-recreate-deployment-cff6dc657-kf5fn" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-kf5fn test-recreate-deployment-cff6dc657- deployment-2473  265106c7-ef84-4537-bbe6-3be456d2c7d3 189036 0 2023-05-06 08:35:39 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 46d6ec6e-357f-421f-8345-01350044b6c6 0xc002edad50 0xc002edad51}] [] [{kube-controller-manager Update v1 2023-05-06 08:35:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46d6ec6e-357f-421f-8345-01350044b6c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 08:35:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qsjzf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qsjzf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:35:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:35:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:35:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:35:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:,StartTime:2023-05-06 08:35:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May  6 08:35:39.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2473" for this suite. 05/06/23 08:35:39.23
------------------------------
â€¢ [2.128 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:35:37.109
    May  6 08:35:37.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename deployment 05/06/23 08:35:37.11
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:35:37.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:35:37.127
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    May  6 08:35:37.129: INFO: Creating deployment "test-recreate-deployment"
    May  6 08:35:37.134: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    May  6 08:35:37.140: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
    May  6 08:35:39.146: INFO: Waiting deployment "test-recreate-deployment" to complete
    May  6 08:35:39.149: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    May  6 08:35:39.157: INFO: Updating deployment test-recreate-deployment
    May  6 08:35:39.157: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  6 08:35:39.223: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-2473  2093f846-0afd-44e9-8a36-d24a8ae022f7 189037 2 2023-05-06 08:35:37 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-06 08:35:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 08:35:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00260f8e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-06 08:35:39 +0000 UTC,LastTransitionTime:2023-05-06 08:35:39 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-05-06 08:35:39 +0000 UTC,LastTransitionTime:2023-05-06 08:35:37 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    May  6 08:35:39.225: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-2473  46d6ec6e-357f-421f-8345-01350044b6c6 189035 1 2023-05-06 08:35:39 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 2093f846-0afd-44e9-8a36-d24a8ae022f7 0xc002eda860 0xc002eda861}] [] [{kube-controller-manager Update apps/v1 2023-05-06 08:35:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2093f846-0afd-44e9-8a36-d24a8ae022f7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 08:35:39 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002eda8f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  6 08:35:39.225: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    May  6 08:35:39.225: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-2473  07925189-efdf-4462-bf46-a5eb793d4854 189025 2 2023-05-06 08:35:37 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 2093f846-0afd-44e9-8a36-d24a8ae022f7 0xc002eda747 0xc002eda748}] [] [{kube-controller-manager Update apps/v1 2023-05-06 08:35:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2093f846-0afd-44e9-8a36-d24a8ae022f7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-06 08:35:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002eda7f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  6 08:35:39.227: INFO: Pod "test-recreate-deployment-cff6dc657-kf5fn" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-kf5fn test-recreate-deployment-cff6dc657- deployment-2473  265106c7-ef84-4537-bbe6-3be456d2c7d3 189036 0 2023-05-06 08:35:39 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 46d6ec6e-357f-421f-8345-01350044b6c6 0xc002edad50 0xc002edad51}] [] [{kube-controller-manager Update v1 2023-05-06 08:35:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46d6ec6e-357f-421f-8345-01350044b6c6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-06 08:35:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qsjzf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qsjzf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:35:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:35:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:35:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-06 08:35:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.134,PodIP:,StartTime:2023-05-06 08:35:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May  6 08:35:39.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2473" for this suite. 05/06/23 08:35:39.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:35:39.237
May  6 08:35:39.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename services 05/06/23 08:35:39.238
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:35:39.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:35:39.254
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  6 08:35:39.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9990" for this suite. 05/06/23 08:35:39.26
------------------------------
â€¢ [0.028 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:35:39.237
    May  6 08:35:39.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename services 05/06/23 08:35:39.238
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:35:39.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:35:39.254
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  6 08:35:39.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9990" for this suite. 05/06/23 08:35:39.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:35:39.265
May  6 08:35:39.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename services 05/06/23 08:35:39.266
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:35:39.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:35:39.283
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5963 05/06/23 08:35:39.284
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/06/23 08:35:39.299
STEP: creating service externalsvc in namespace services-5963 05/06/23 08:35:39.299
STEP: creating replication controller externalsvc in namespace services-5963 05/06/23 08:35:39.32
I0506 08:35:39.327980      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5963, replica count: 2
I0506 08:35:42.379474      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 05/06/23 08:35:42.382
May  6 08:35:42.403: INFO: Creating new exec pod
May  6 08:35:42.417: INFO: Waiting up to 5m0s for pod "execpod5g5fp" in namespace "services-5963" to be "running"
May  6 08:35:42.420: INFO: Pod "execpod5g5fp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.718286ms
May  6 08:35:44.423: INFO: Pod "execpod5g5fp": Phase="Running", Reason="", readiness=true. Elapsed: 2.005573534s
May  6 08:35:44.423: INFO: Pod "execpod5g5fp" satisfied condition "running"
May  6 08:35:44.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-5963 exec execpod5g5fp -- /bin/sh -x -c nslookup nodeport-service.services-5963.svc.cluster.local'
May  6 08:35:44.550: INFO: stderr: "+ nslookup nodeport-service.services-5963.svc.cluster.local\n"
May  6 08:35:44.550: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-5963.svc.cluster.local\tcanonical name = externalsvc.services-5963.svc.cluster.local.\nName:\texternalsvc.services-5963.svc.cluster.local\nAddress: 10.96.98.38\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5963, will wait for the garbage collector to delete the pods 05/06/23 08:35:44.55
May  6 08:35:44.611: INFO: Deleting ReplicationController externalsvc took: 6.940039ms
May  6 08:35:44.712: INFO: Terminating ReplicationController externalsvc pods took: 100.74507ms
May  6 08:35:46.937: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May  6 08:35:46.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5963" for this suite. 05/06/23 08:35:46.953
------------------------------
â€¢ [SLOW TEST] [7.694 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:35:39.265
    May  6 08:35:39.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename services 05/06/23 08:35:39.266
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:35:39.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:35:39.283
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-5963 05/06/23 08:35:39.284
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/06/23 08:35:39.299
    STEP: creating service externalsvc in namespace services-5963 05/06/23 08:35:39.299
    STEP: creating replication controller externalsvc in namespace services-5963 05/06/23 08:35:39.32
    I0506 08:35:39.327980      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5963, replica count: 2
    I0506 08:35:42.379474      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 05/06/23 08:35:42.382
    May  6 08:35:42.403: INFO: Creating new exec pod
    May  6 08:35:42.417: INFO: Waiting up to 5m0s for pod "execpod5g5fp" in namespace "services-5963" to be "running"
    May  6 08:35:42.420: INFO: Pod "execpod5g5fp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.718286ms
    May  6 08:35:44.423: INFO: Pod "execpod5g5fp": Phase="Running", Reason="", readiness=true. Elapsed: 2.005573534s
    May  6 08:35:44.423: INFO: Pod "execpod5g5fp" satisfied condition "running"
    May  6 08:35:44.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=services-5963 exec execpod5g5fp -- /bin/sh -x -c nslookup nodeport-service.services-5963.svc.cluster.local'
    May  6 08:35:44.550: INFO: stderr: "+ nslookup nodeport-service.services-5963.svc.cluster.local\n"
    May  6 08:35:44.550: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-5963.svc.cluster.local\tcanonical name = externalsvc.services-5963.svc.cluster.local.\nName:\texternalsvc.services-5963.svc.cluster.local\nAddress: 10.96.98.38\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-5963, will wait for the garbage collector to delete the pods 05/06/23 08:35:44.55
    May  6 08:35:44.611: INFO: Deleting ReplicationController externalsvc took: 6.940039ms
    May  6 08:35:44.712: INFO: Terminating ReplicationController externalsvc pods took: 100.74507ms
    May  6 08:35:46.937: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May  6 08:35:46.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5963" for this suite. 05/06/23 08:35:46.953
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:35:46.959
May  6 08:35:46.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename emptydir 05/06/23 08:35:46.96
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:35:46.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:35:46.977
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 05/06/23 08:35:46.979
May  6 08:35:46.987: INFO: Waiting up to 5m0s for pod "pod-3aaea837-0239-4f2e-b71c-9ce29f207748" in namespace "emptydir-3727" to be "Succeeded or Failed"
May  6 08:35:46.989: INFO: Pod "pod-3aaea837-0239-4f2e-b71c-9ce29f207748": Phase="Pending", Reason="", readiness=false. Elapsed: 2.307736ms
May  6 08:35:48.992: INFO: Pod "pod-3aaea837-0239-4f2e-b71c-9ce29f207748": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005535592s
May  6 08:35:50.993: INFO: Pod "pod-3aaea837-0239-4f2e-b71c-9ce29f207748": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00588539s
STEP: Saw pod success 05/06/23 08:35:50.993
May  6 08:35:50.993: INFO: Pod "pod-3aaea837-0239-4f2e-b71c-9ce29f207748" satisfied condition "Succeeded or Failed"
May  6 08:35:50.995: INFO: Trying to get logs from node cncf-0 pod pod-3aaea837-0239-4f2e-b71c-9ce29f207748 container test-container: <nil>
STEP: delete the pod 05/06/23 08:35:51.003
May  6 08:35:51.014: INFO: Waiting for pod pod-3aaea837-0239-4f2e-b71c-9ce29f207748 to disappear
May  6 08:35:51.016: INFO: Pod pod-3aaea837-0239-4f2e-b71c-9ce29f207748 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May  6 08:35:51.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3727" for this suite. 05/06/23 08:35:51.019
------------------------------
â€¢ [4.065 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:35:46.959
    May  6 08:35:46.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename emptydir 05/06/23 08:35:46.96
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:35:46.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:35:46.977
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 05/06/23 08:35:46.979
    May  6 08:35:46.987: INFO: Waiting up to 5m0s for pod "pod-3aaea837-0239-4f2e-b71c-9ce29f207748" in namespace "emptydir-3727" to be "Succeeded or Failed"
    May  6 08:35:46.989: INFO: Pod "pod-3aaea837-0239-4f2e-b71c-9ce29f207748": Phase="Pending", Reason="", readiness=false. Elapsed: 2.307736ms
    May  6 08:35:48.992: INFO: Pod "pod-3aaea837-0239-4f2e-b71c-9ce29f207748": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005535592s
    May  6 08:35:50.993: INFO: Pod "pod-3aaea837-0239-4f2e-b71c-9ce29f207748": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00588539s
    STEP: Saw pod success 05/06/23 08:35:50.993
    May  6 08:35:50.993: INFO: Pod "pod-3aaea837-0239-4f2e-b71c-9ce29f207748" satisfied condition "Succeeded or Failed"
    May  6 08:35:50.995: INFO: Trying to get logs from node cncf-0 pod pod-3aaea837-0239-4f2e-b71c-9ce29f207748 container test-container: <nil>
    STEP: delete the pod 05/06/23 08:35:51.003
    May  6 08:35:51.014: INFO: Waiting for pod pod-3aaea837-0239-4f2e-b71c-9ce29f207748 to disappear
    May  6 08:35:51.016: INFO: Pod pod-3aaea837-0239-4f2e-b71c-9ce29f207748 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May  6 08:35:51.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3727" for this suite. 05/06/23 08:35:51.019
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:35:51.024
May  6 08:35:51.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename limitrange 05/06/23 08:35:51.025
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:35:51.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:35:51.043
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-dcff4" in namespace "limitrange-9225" 05/06/23 08:35:51.045
STEP: Creating another limitRange in another namespace 05/06/23 08:35:51.05
May  6 08:35:51.062: INFO: Namespace "e2e-limitrange-dcff4-8635" created
May  6 08:35:51.062: INFO: Creating LimitRange "e2e-limitrange-dcff4" in namespace "e2e-limitrange-dcff4-8635"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-dcff4" 05/06/23 08:35:51.068
May  6 08:35:51.070: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-dcff4" in "limitrange-9225" namespace 05/06/23 08:35:51.071
May  6 08:35:51.075: INFO: LimitRange "e2e-limitrange-dcff4" has been patched
STEP: Delete LimitRange "e2e-limitrange-dcff4" by Collection with labelSelector: "e2e-limitrange-dcff4=patched" 05/06/23 08:35:51.076
STEP: Confirm that the limitRange "e2e-limitrange-dcff4" has been deleted 05/06/23 08:35:51.082
May  6 08:35:51.082: INFO: Requesting list of LimitRange to confirm quantity
May  6 08:35:51.084: INFO: Found 0 LimitRange with label "e2e-limitrange-dcff4=patched"
May  6 08:35:51.084: INFO: LimitRange "e2e-limitrange-dcff4" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-dcff4" 05/06/23 08:35:51.084
May  6 08:35:51.086: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
May  6 08:35:51.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-9225" for this suite. 05/06/23 08:35:51.089
STEP: Destroying namespace "e2e-limitrange-dcff4-8635" for this suite. 05/06/23 08:35:51.093
------------------------------
â€¢ [0.075 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:35:51.024
    May  6 08:35:51.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename limitrange 05/06/23 08:35:51.025
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:35:51.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:35:51.043
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-dcff4" in namespace "limitrange-9225" 05/06/23 08:35:51.045
    STEP: Creating another limitRange in another namespace 05/06/23 08:35:51.05
    May  6 08:35:51.062: INFO: Namespace "e2e-limitrange-dcff4-8635" created
    May  6 08:35:51.062: INFO: Creating LimitRange "e2e-limitrange-dcff4" in namespace "e2e-limitrange-dcff4-8635"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-dcff4" 05/06/23 08:35:51.068
    May  6 08:35:51.070: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-dcff4" in "limitrange-9225" namespace 05/06/23 08:35:51.071
    May  6 08:35:51.075: INFO: LimitRange "e2e-limitrange-dcff4" has been patched
    STEP: Delete LimitRange "e2e-limitrange-dcff4" by Collection with labelSelector: "e2e-limitrange-dcff4=patched" 05/06/23 08:35:51.076
    STEP: Confirm that the limitRange "e2e-limitrange-dcff4" has been deleted 05/06/23 08:35:51.082
    May  6 08:35:51.082: INFO: Requesting list of LimitRange to confirm quantity
    May  6 08:35:51.084: INFO: Found 0 LimitRange with label "e2e-limitrange-dcff4=patched"
    May  6 08:35:51.084: INFO: LimitRange "e2e-limitrange-dcff4" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-dcff4" 05/06/23 08:35:51.084
    May  6 08:35:51.086: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    May  6 08:35:51.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-9225" for this suite. 05/06/23 08:35:51.089
    STEP: Destroying namespace "e2e-limitrange-dcff4-8635" for this suite. 05/06/23 08:35:51.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:35:51.1
May  6 08:35:51.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename configmap 05/06/23 08:35:51.101
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:35:51.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:35:51.116
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-9dbad149-1f1f-4bc2-8063-0b2fd1780e93 05/06/23 08:35:51.118
STEP: Creating a pod to test consume configMaps 05/06/23 08:35:51.121
May  6 08:35:51.128: INFO: Waiting up to 5m0s for pod "pod-configmaps-f7b5be29-c6e7-4837-afcf-964bcb417ce0" in namespace "configmap-5629" to be "Succeeded or Failed"
May  6 08:35:51.130: INFO: Pod "pod-configmaps-f7b5be29-c6e7-4837-afcf-964bcb417ce0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.85764ms
May  6 08:35:53.134: INFO: Pod "pod-configmaps-f7b5be29-c6e7-4837-afcf-964bcb417ce0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005428189s
May  6 08:35:55.132: INFO: Pod "pod-configmaps-f7b5be29-c6e7-4837-afcf-964bcb417ce0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004312611s
STEP: Saw pod success 05/06/23 08:35:55.132
May  6 08:35:55.132: INFO: Pod "pod-configmaps-f7b5be29-c6e7-4837-afcf-964bcb417ce0" satisfied condition "Succeeded or Failed"
May  6 08:35:55.135: INFO: Trying to get logs from node cncf-0 pod pod-configmaps-f7b5be29-c6e7-4837-afcf-964bcb417ce0 container agnhost-container: <nil>
STEP: delete the pod 05/06/23 08:35:55.139
May  6 08:35:55.152: INFO: Waiting for pod pod-configmaps-f7b5be29-c6e7-4837-afcf-964bcb417ce0 to disappear
May  6 08:35:55.154: INFO: Pod pod-configmaps-f7b5be29-c6e7-4837-afcf-964bcb417ce0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May  6 08:35:55.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5629" for this suite. 05/06/23 08:35:55.158
------------------------------
â€¢ [4.063 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:35:51.1
    May  6 08:35:51.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename configmap 05/06/23 08:35:51.101
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:35:51.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:35:51.116
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-9dbad149-1f1f-4bc2-8063-0b2fd1780e93 05/06/23 08:35:51.118
    STEP: Creating a pod to test consume configMaps 05/06/23 08:35:51.121
    May  6 08:35:51.128: INFO: Waiting up to 5m0s for pod "pod-configmaps-f7b5be29-c6e7-4837-afcf-964bcb417ce0" in namespace "configmap-5629" to be "Succeeded or Failed"
    May  6 08:35:51.130: INFO: Pod "pod-configmaps-f7b5be29-c6e7-4837-afcf-964bcb417ce0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.85764ms
    May  6 08:35:53.134: INFO: Pod "pod-configmaps-f7b5be29-c6e7-4837-afcf-964bcb417ce0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005428189s
    May  6 08:35:55.132: INFO: Pod "pod-configmaps-f7b5be29-c6e7-4837-afcf-964bcb417ce0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004312611s
    STEP: Saw pod success 05/06/23 08:35:55.132
    May  6 08:35:55.132: INFO: Pod "pod-configmaps-f7b5be29-c6e7-4837-afcf-964bcb417ce0" satisfied condition "Succeeded or Failed"
    May  6 08:35:55.135: INFO: Trying to get logs from node cncf-0 pod pod-configmaps-f7b5be29-c6e7-4837-afcf-964bcb417ce0 container agnhost-container: <nil>
    STEP: delete the pod 05/06/23 08:35:55.139
    May  6 08:35:55.152: INFO: Waiting for pod pod-configmaps-f7b5be29-c6e7-4837-afcf-964bcb417ce0 to disappear
    May  6 08:35:55.154: INFO: Pod pod-configmaps-f7b5be29-c6e7-4837-afcf-964bcb417ce0 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May  6 08:35:55.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5629" for this suite. 05/06/23 08:35:55.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:35:55.164
May  6 08:35:55.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename kubectl 05/06/23 08:35:55.164
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:35:55.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:35:55.181
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
May  6 08:35:55.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-5419 create -f -'
May  6 08:35:55.897: INFO: stderr: ""
May  6 08:35:55.897: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May  6 08:35:55.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-5419 create -f -'
May  6 08:35:56.717: INFO: stderr: ""
May  6 08:35:56.717: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/06/23 08:35:56.717
May  6 08:35:57.721: INFO: Selector matched 1 pods for map[app:agnhost]
May  6 08:35:57.721: INFO: Found 1 / 1
May  6 08:35:57.721: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  6 08:35:57.724: INFO: Selector matched 1 pods for map[app:agnhost]
May  6 08:35:57.724: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  6 08:35:57.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-5419 describe pod agnhost-primary-69qf2'
May  6 08:35:57.785: INFO: stderr: ""
May  6 08:35:57.785: INFO: stdout: "Name:             agnhost-primary-69qf2\nNamespace:        kubectl-5419\nPriority:         0\nService Account:  default\nNode:             cncf-0/10.0.0.134\nStart Time:       Sat, 06 May 2023 08:35:55 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 34fae8c99b913e85fb2636228d60e685feae58f1ef905acb4a1a96a7ae7d04cf\n                  cni.projectcalico.org/podIP: 10.244.174.161/32\n                  cni.projectcalico.org/podIPs: 10.244.174.161/32\nStatus:           Running\nIP:               10.244.174.161\nIPs:\n  IP:           10.244.174.161\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://7401a4404ab210d00c0462dd0affa28323cbfe5c1ecb599c95d9de17a2e74750\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 06 May 2023 08:35:56 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pvbq5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-pvbq5:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-5419/agnhost-primary-69qf2 to cncf-0\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
May  6 08:35:57.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-5419 describe rc agnhost-primary'
May  6 08:35:57.846: INFO: stderr: ""
May  6 08:35:57.846: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5419\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-69qf2\n"
May  6 08:35:57.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-5419 describe service agnhost-primary'
May  6 08:35:57.902: INFO: stderr: ""
May  6 08:35:57.902: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5419\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.110.158.145\nIPs:               10.110.158.145\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.174.161:6379\nSession Affinity:  None\nEvents:            <none>\n"
May  6 08:35:57.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-5419 describe node cncf-0'
May  6 08:35:57.974: INFO: stderr: ""
May  6 08:35:57.974: INFO: stdout: "Name:               cncf-0\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=cncf-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.0.134/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.244.174.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 05 May 2023 20:21:35 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  cncf-0\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 06 May 2023 08:35:54 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 05 May 2023 20:25:55 +0000   Fri, 05 May 2023 20:25:55 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sat, 06 May 2023 08:35:48 +0000   Fri, 05 May 2023 20:21:35 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 06 May 2023 08:35:48 +0000   Fri, 05 May 2023 20:21:35 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 06 May 2023 08:35:48 +0000   Fri, 05 May 2023 20:21:35 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 06 May 2023 08:35:48 +0000   Fri, 05 May 2023 20:29:31 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.0.0.134\n  Hostname:    cncf-0\nCapacity:\n  cpu:                  4\n  ephemeral-storage:    50620216Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               4012420Ki\n  pods:                 110\nAllocatable:\n  cpu:                  3920m\n  ephemeral-storage:    46651590989\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               2906500Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 a4c37fb4bb8049a3a1e0706be7038830\n  System UUID:                a4c37fb4-bb80-49a3-a1e0-706be7038830\n  Boot ID:                    ef97aa77-d374-465a-8a8d-eff68853e1e0\n  Kernel Version:             5.15.0-1033-oracle\n  OS Image:                   Ubuntu 20.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.10\n  Kubelet Version:            v1.26.3\n  Kube-Proxy Version:         v1.26.3\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system               calico-node-vvl9m                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         12h\n  kube-system                 kube-apiserver-cncf-0                                      150m (3%)     0 (0%)      0 (0%)           0 (0%)         12h\n  kube-system                 kube-controller-manager-cncf-0                             100m (2%)     0 (0%)      0 (0%)           0 (0%)         12h\n  kube-system                 kube-proxy-cq72q                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         12h\n  kube-system                 kube-scheduler-cncf-0                                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         12h\n  kube-system                 tigera-operator-585fc94df6-xnxld                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         12h\n  kubectl-5419                agnhost-primary-69qf2                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-np4v2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         96m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests   Limits\n  --------             --------   ------\n  cpu                  350m (8%)  0 (0%)\n  memory               0 (0%)     0 (0%)\n  ephemeral-storage    0 (0%)     0 (0%)\n  hugepages-1Gi        0 (0%)     0 (0%)\n  hugepages-2Mi        0 (0%)     0 (0%)\n  example.com/fakecpu  0          0\nEvents:                <none>\n"
May  6 08:35:57.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-5419 describe namespace kubectl-5419'
May  6 08:35:58.032: INFO: stderr: ""
May  6 08:35:58.032: INFO: stdout: "Name:         kubectl-5419\nLabels:       e2e-framework=kubectl\n              e2e-run=a676f9dd-4d35-458a-89a0-5f79d2a64d54\n              kubernetes.io/metadata.name=kubectl-5419\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May  6 08:35:58.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5419" for this suite. 05/06/23 08:35:58.035
------------------------------
â€¢ [2.877 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:35:55.164
    May  6 08:35:55.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename kubectl 05/06/23 08:35:55.164
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:35:55.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:35:55.181
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    May  6 08:35:55.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-5419 create -f -'
    May  6 08:35:55.897: INFO: stderr: ""
    May  6 08:35:55.897: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    May  6 08:35:55.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-5419 create -f -'
    May  6 08:35:56.717: INFO: stderr: ""
    May  6 08:35:56.717: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/06/23 08:35:56.717
    May  6 08:35:57.721: INFO: Selector matched 1 pods for map[app:agnhost]
    May  6 08:35:57.721: INFO: Found 1 / 1
    May  6 08:35:57.721: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    May  6 08:35:57.724: INFO: Selector matched 1 pods for map[app:agnhost]
    May  6 08:35:57.724: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May  6 08:35:57.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-5419 describe pod agnhost-primary-69qf2'
    May  6 08:35:57.785: INFO: stderr: ""
    May  6 08:35:57.785: INFO: stdout: "Name:             agnhost-primary-69qf2\nNamespace:        kubectl-5419\nPriority:         0\nService Account:  default\nNode:             cncf-0/10.0.0.134\nStart Time:       Sat, 06 May 2023 08:35:55 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 34fae8c99b913e85fb2636228d60e685feae58f1ef905acb4a1a96a7ae7d04cf\n                  cni.projectcalico.org/podIP: 10.244.174.161/32\n                  cni.projectcalico.org/podIPs: 10.244.174.161/32\nStatus:           Running\nIP:               10.244.174.161\nIPs:\n  IP:           10.244.174.161\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://7401a4404ab210d00c0462dd0affa28323cbfe5c1ecb599c95d9de17a2e74750\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 06 May 2023 08:35:56 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pvbq5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-pvbq5:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-5419/agnhost-primary-69qf2 to cncf-0\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    May  6 08:35:57.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-5419 describe rc agnhost-primary'
    May  6 08:35:57.846: INFO: stderr: ""
    May  6 08:35:57.846: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5419\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-69qf2\n"
    May  6 08:35:57.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-5419 describe service agnhost-primary'
    May  6 08:35:57.902: INFO: stderr: ""
    May  6 08:35:57.902: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5419\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.110.158.145\nIPs:               10.110.158.145\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.174.161:6379\nSession Affinity:  None\nEvents:            <none>\n"
    May  6 08:35:57.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-5419 describe node cncf-0'
    May  6 08:35:57.974: INFO: stderr: ""
    May  6 08:35:57.974: INFO: stdout: "Name:               cncf-0\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=cncf-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.0.134/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.244.174.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 05 May 2023 20:21:35 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  cncf-0\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 06 May 2023 08:35:54 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 05 May 2023 20:25:55 +0000   Fri, 05 May 2023 20:25:55 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sat, 06 May 2023 08:35:48 +0000   Fri, 05 May 2023 20:21:35 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 06 May 2023 08:35:48 +0000   Fri, 05 May 2023 20:21:35 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 06 May 2023 08:35:48 +0000   Fri, 05 May 2023 20:21:35 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 06 May 2023 08:35:48 +0000   Fri, 05 May 2023 20:29:31 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.0.0.134\n  Hostname:    cncf-0\nCapacity:\n  cpu:                  4\n  ephemeral-storage:    50620216Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               4012420Ki\n  pods:                 110\nAllocatable:\n  cpu:                  3920m\n  ephemeral-storage:    46651590989\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               2906500Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 a4c37fb4bb8049a3a1e0706be7038830\n  System UUID:                a4c37fb4-bb80-49a3-a1e0-706be7038830\n  Boot ID:                    ef97aa77-d374-465a-8a8d-eff68853e1e0\n  Kernel Version:             5.15.0-1033-oracle\n  OS Image:                   Ubuntu 20.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.10\n  Kubelet Version:            v1.26.3\n  Kube-Proxy Version:         v1.26.3\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system               calico-node-vvl9m                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         12h\n  kube-system                 kube-apiserver-cncf-0                                      150m (3%)     0 (0%)      0 (0%)           0 (0%)         12h\n  kube-system                 kube-controller-manager-cncf-0                             100m (2%)     0 (0%)      0 (0%)           0 (0%)         12h\n  kube-system                 kube-proxy-cq72q                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         12h\n  kube-system                 kube-scheduler-cncf-0                                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         12h\n  kube-system                 tigera-operator-585fc94df6-xnxld                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         12h\n  kubectl-5419                agnhost-primary-69qf2                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-2e46f419a9f249f6-np4v2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         96m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests   Limits\n  --------             --------   ------\n  cpu                  350m (8%)  0 (0%)\n  memory               0 (0%)     0 (0%)\n  ephemeral-storage    0 (0%)     0 (0%)\n  hugepages-1Gi        0 (0%)     0 (0%)\n  hugepages-2Mi        0 (0%)     0 (0%)\n  example.com/fakecpu  0          0\nEvents:                <none>\n"
    May  6 08:35:57.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=kubectl-5419 describe namespace kubectl-5419'
    May  6 08:35:58.032: INFO: stderr: ""
    May  6 08:35:58.032: INFO: stdout: "Name:         kubectl-5419\nLabels:       e2e-framework=kubectl\n              e2e-run=a676f9dd-4d35-458a-89a0-5f79d2a64d54\n              kubernetes.io/metadata.name=kubectl-5419\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May  6 08:35:58.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5419" for this suite. 05/06/23 08:35:58.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:35:58.042
May  6 08:35:58.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename webhook 05/06/23 08:35:58.043
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:35:58.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:35:58.061
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/06/23 08:35:58.075
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 08:35:58.392
STEP: Deploying the webhook pod 05/06/23 08:35:58.399
STEP: Wait for the deployment to be ready 05/06/23 08:35:58.41
May  6 08:35:58.413: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/06/23 08:36:00.422
STEP: Verifying the service has paired with the endpoint 05/06/23 08:36:00.434
May  6 08:36:01.435: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 05/06/23 08:36:01.438
STEP: Creating a configMap that does not comply to the validation webhook rules 05/06/23 08:36:02.452
STEP: Updating a validating webhook configuration's rules to not include the create operation 05/06/23 08:36:03.475
STEP: Creating a configMap that does not comply to the validation webhook rules 05/06/23 08:36:03.483
STEP: Patching a validating webhook configuration's rules to include the create operation 05/06/23 08:36:03.492
STEP: Creating a configMap that does not comply to the validation webhook rules 05/06/23 08:36:03.497
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 08:36:03.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1534" for this suite. 05/06/23 08:36:03.652
STEP: Destroying namespace "webhook-1534-markers" for this suite. 05/06/23 08:36:03.659
------------------------------
â€¢ [SLOW TEST] [5.623 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:35:58.042
    May  6 08:35:58.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename webhook 05/06/23 08:35:58.043
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:35:58.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:35:58.061
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/06/23 08:35:58.075
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 08:35:58.392
    STEP: Deploying the webhook pod 05/06/23 08:35:58.399
    STEP: Wait for the deployment to be ready 05/06/23 08:35:58.41
    May  6 08:35:58.413: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/06/23 08:36:00.422
    STEP: Verifying the service has paired with the endpoint 05/06/23 08:36:00.434
    May  6 08:36:01.435: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 05/06/23 08:36:01.438
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/06/23 08:36:02.452
    STEP: Updating a validating webhook configuration's rules to not include the create operation 05/06/23 08:36:03.475
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/06/23 08:36:03.483
    STEP: Patching a validating webhook configuration's rules to include the create operation 05/06/23 08:36:03.492
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/06/23 08:36:03.497
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 08:36:03.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1534" for this suite. 05/06/23 08:36:03.652
    STEP: Destroying namespace "webhook-1534-markers" for this suite. 05/06/23 08:36:03.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:36:03.665
May  6 08:36:03.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename svcaccounts 05/06/23 08:36:03.666
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:36:03.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:36:03.685
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
May  6 08:36:03.701: INFO: created pod pod-service-account-defaultsa
May  6 08:36:03.701: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May  6 08:36:03.706: INFO: created pod pod-service-account-mountsa
May  6 08:36:03.706: INFO: pod pod-service-account-mountsa service account token volume mount: true
May  6 08:36:03.715: INFO: created pod pod-service-account-nomountsa
May  6 08:36:03.715: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May  6 08:36:03.723: INFO: created pod pod-service-account-defaultsa-mountspec
May  6 08:36:03.723: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May  6 08:36:03.729: INFO: created pod pod-service-account-mountsa-mountspec
May  6 08:36:03.729: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May  6 08:36:03.740: INFO: created pod pod-service-account-nomountsa-mountspec
May  6 08:36:03.740: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May  6 08:36:03.751: INFO: created pod pod-service-account-defaultsa-nomountspec
May  6 08:36:03.751: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May  6 08:36:03.755: INFO: created pod pod-service-account-mountsa-nomountspec
May  6 08:36:03.756: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May  6 08:36:03.764: INFO: created pod pod-service-account-nomountsa-nomountspec
May  6 08:36:03.764: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May  6 08:36:03.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1501" for this suite. 05/06/23 08:36:03.771
------------------------------
â€¢ [0.126 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:36:03.665
    May  6 08:36:03.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename svcaccounts 05/06/23 08:36:03.666
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:36:03.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:36:03.685
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    May  6 08:36:03.701: INFO: created pod pod-service-account-defaultsa
    May  6 08:36:03.701: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    May  6 08:36:03.706: INFO: created pod pod-service-account-mountsa
    May  6 08:36:03.706: INFO: pod pod-service-account-mountsa service account token volume mount: true
    May  6 08:36:03.715: INFO: created pod pod-service-account-nomountsa
    May  6 08:36:03.715: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    May  6 08:36:03.723: INFO: created pod pod-service-account-defaultsa-mountspec
    May  6 08:36:03.723: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    May  6 08:36:03.729: INFO: created pod pod-service-account-mountsa-mountspec
    May  6 08:36:03.729: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    May  6 08:36:03.740: INFO: created pod pod-service-account-nomountsa-mountspec
    May  6 08:36:03.740: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    May  6 08:36:03.751: INFO: created pod pod-service-account-defaultsa-nomountspec
    May  6 08:36:03.751: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    May  6 08:36:03.755: INFO: created pod pod-service-account-mountsa-nomountspec
    May  6 08:36:03.756: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    May  6 08:36:03.764: INFO: created pod pod-service-account-nomountsa-nomountspec
    May  6 08:36:03.764: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May  6 08:36:03.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1501" for this suite. 05/06/23 08:36:03.771
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:36:03.793
May  6 08:36:03.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename downward-api 05/06/23 08:36:03.794
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:36:03.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:36:03.818
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 05/06/23 08:36:03.82
May  6 08:36:03.833: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15251dd9-6095-4472-bd71-c71e22a46443" in namespace "downward-api-7153" to be "Succeeded or Failed"
May  6 08:36:03.836: INFO: Pod "downwardapi-volume-15251dd9-6095-4472-bd71-c71e22a46443": Phase="Pending", Reason="", readiness=false. Elapsed: 2.965215ms
May  6 08:36:05.840: INFO: Pod "downwardapi-volume-15251dd9-6095-4472-bd71-c71e22a46443": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006595137s
May  6 08:36:07.840: INFO: Pod "downwardapi-volume-15251dd9-6095-4472-bd71-c71e22a46443": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006839664s
May  6 08:36:09.839: INFO: Pod "downwardapi-volume-15251dd9-6095-4472-bd71-c71e22a46443": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006505453s
STEP: Saw pod success 05/06/23 08:36:09.839
May  6 08:36:09.840: INFO: Pod "downwardapi-volume-15251dd9-6095-4472-bd71-c71e22a46443" satisfied condition "Succeeded or Failed"
May  6 08:36:09.842: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-15251dd9-6095-4472-bd71-c71e22a46443 container client-container: <nil>
STEP: delete the pod 05/06/23 08:36:09.847
May  6 08:36:09.857: INFO: Waiting for pod downwardapi-volume-15251dd9-6095-4472-bd71-c71e22a46443 to disappear
May  6 08:36:09.859: INFO: Pod downwardapi-volume-15251dd9-6095-4472-bd71-c71e22a46443 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May  6 08:36:09.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7153" for this suite. 05/06/23 08:36:09.862
------------------------------
â€¢ [SLOW TEST] [6.074 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:36:03.793
    May  6 08:36:03.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename downward-api 05/06/23 08:36:03.794
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:36:03.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:36:03.818
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 05/06/23 08:36:03.82
    May  6 08:36:03.833: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15251dd9-6095-4472-bd71-c71e22a46443" in namespace "downward-api-7153" to be "Succeeded or Failed"
    May  6 08:36:03.836: INFO: Pod "downwardapi-volume-15251dd9-6095-4472-bd71-c71e22a46443": Phase="Pending", Reason="", readiness=false. Elapsed: 2.965215ms
    May  6 08:36:05.840: INFO: Pod "downwardapi-volume-15251dd9-6095-4472-bd71-c71e22a46443": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006595137s
    May  6 08:36:07.840: INFO: Pod "downwardapi-volume-15251dd9-6095-4472-bd71-c71e22a46443": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006839664s
    May  6 08:36:09.839: INFO: Pod "downwardapi-volume-15251dd9-6095-4472-bd71-c71e22a46443": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006505453s
    STEP: Saw pod success 05/06/23 08:36:09.839
    May  6 08:36:09.840: INFO: Pod "downwardapi-volume-15251dd9-6095-4472-bd71-c71e22a46443" satisfied condition "Succeeded or Failed"
    May  6 08:36:09.842: INFO: Trying to get logs from node cncf-0 pod downwardapi-volume-15251dd9-6095-4472-bd71-c71e22a46443 container client-container: <nil>
    STEP: delete the pod 05/06/23 08:36:09.847
    May  6 08:36:09.857: INFO: Waiting for pod downwardapi-volume-15251dd9-6095-4472-bd71-c71e22a46443 to disappear
    May  6 08:36:09.859: INFO: Pod downwardapi-volume-15251dd9-6095-4472-bd71-c71e22a46443 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May  6 08:36:09.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7153" for this suite. 05/06/23 08:36:09.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:36:09.868
May  6 08:36:09.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename webhook 05/06/23 08:36:09.869
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:36:09.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:36:09.885
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/06/23 08:36:09.9
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 08:36:10.422
STEP: Deploying the webhook pod 05/06/23 08:36:10.426
STEP: Wait for the deployment to be ready 05/06/23 08:36:10.438
May  6 08:36:10.442: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/06/23 08:36:12.452
STEP: Verifying the service has paired with the endpoint 05/06/23 08:36:12.463
May  6 08:36:13.464: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 05/06/23 08:36:13.466
STEP: create a pod 05/06/23 08:36:14.488
May  6 08:36:14.494: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4996" to be "running"
May  6 08:36:14.497: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.064364ms
May  6 08:36:16.501: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006679999s
May  6 08:36:16.501: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 05/06/23 08:36:16.501
May  6 08:36:16.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=webhook-4996 attach --namespace=webhook-4996 to-be-attached-pod -i -c=container1'
May  6 08:36:17.579: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May  6 08:36:17.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4996" for this suite. 05/06/23 08:36:17.628
STEP: Destroying namespace "webhook-4996-markers" for this suite. 05/06/23 08:36:17.639
------------------------------
â€¢ [SLOW TEST] [7.779 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:36:09.868
    May  6 08:36:09.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename webhook 05/06/23 08:36:09.869
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:36:09.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:36:09.885
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/06/23 08:36:09.9
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/06/23 08:36:10.422
    STEP: Deploying the webhook pod 05/06/23 08:36:10.426
    STEP: Wait for the deployment to be ready 05/06/23 08:36:10.438
    May  6 08:36:10.442: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/06/23 08:36:12.452
    STEP: Verifying the service has paired with the endpoint 05/06/23 08:36:12.463
    May  6 08:36:13.464: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 05/06/23 08:36:13.466
    STEP: create a pod 05/06/23 08:36:14.488
    May  6 08:36:14.494: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4996" to be "running"
    May  6 08:36:14.497: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.064364ms
    May  6 08:36:16.501: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006679999s
    May  6 08:36:16.501: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 05/06/23 08:36:16.501
    May  6 08:36:16.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2567509600 --namespace=webhook-4996 attach --namespace=webhook-4996 to-be-attached-pod -i -c=container1'
    May  6 08:36:17.579: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May  6 08:36:17.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4996" for this suite. 05/06/23 08:36:17.628
    STEP: Destroying namespace "webhook-4996-markers" for this suite. 05/06/23 08:36:17.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/06/23 08:36:17.65
May  6 08:36:17.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
STEP: Building a namespace api object, basename csiinlinevolumes 05/06/23 08:36:17.651
STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:36:17.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:36:17.671
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 05/06/23 08:36:17.673
STEP: getting 05/06/23 08:36:17.689
STEP: listing in namespace 05/06/23 08:36:17.692
STEP: patching 05/06/23 08:36:17.695
STEP: deleting 05/06/23 08:36:17.702
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
May  6 08:36:17.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-2662" for this suite. 05/06/23 08:36:17.717
------------------------------
â€¢ [0.074 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/06/23 08:36:17.65
    May  6 08:36:17.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2567509600
    STEP: Building a namespace api object, basename csiinlinevolumes 05/06/23 08:36:17.651
    STEP: Waiting for a default service account to be provisioned in namespace 05/06/23 08:36:17.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/06/23 08:36:17.671
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 05/06/23 08:36:17.673
    STEP: getting 05/06/23 08:36:17.689
    STEP: listing in namespace 05/06/23 08:36:17.692
    STEP: patching 05/06/23 08:36:17.695
    STEP: deleting 05/06/23 08:36:17.702
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    May  6 08:36:17.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-2662" for this suite. 05/06/23 08:36:17.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
May  6 08:36:17.726: INFO: Running AfterSuite actions on node 1
May  6 08:36:17.726: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    May  6 08:36:17.726: INFO: Running AfterSuite actions on node 1
    May  6 08:36:17.726: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.064 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5823.321 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h37m3.634030645s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

